Train U-net2


import os, glob, json, math, random
from dataclasses import dataclass, asdict
from typing import List, Dict, Tuple

import numpy as np
import pandas as pd
import cv2
import torch
import torch.nn as nn
import torch.nn.functional as F
import matplotlib.pyplot as plt
from scipy.ndimage import distance_transform_edt


# =========================
# Config (keep aligned with U1 run_dir)
# =========================
@dataclass
class CFG:
    run_dir: str = "./runs/exp_unet1"   # <-- U1 run dir (contains split.json, config.json, cache_u1_logits)
    device: str = "cuda" if torch.cuda.is_available() else "cpu"
    seed: int = 42

    # U2 training
    batch: int = 4
    epochs: int = 40
    lr: float = 1e-3
    weight_decay: float = 1e-5

    # U2 model capacity (보수적으로 32 추천; 필요시 64로)
    base: int = 32
    gn_groups: int = 16

    # t embedding / refinement
    K: int = 5
    lambda_schedule: Tuple[float, ...] = (0.0, 0.25, 0.5, 0.75, 0.9)

    # logit-EMA in evaluation refinement
    beta_ema: float = 0.7

    # eval/visualize
    thr_vis: float = 0.5
    exp_name: str = "exp_unet2"


def set_seed(seed: int):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True

def worker_init_fn(worker_id: int):
    seed = torch.initial_seed() % (2**32)
    np.random.seed(seed + worker_id)
    random.seed(seed + worker_id)

def read_gray(path: str) -> np.ndarray:
    im = cv2.imread(path, cv2.IMREAD_GRAYSCALE)
    if im is None:
        raise FileNotFoundError(path)
    return im

def read_label_bgr(path: str) -> np.ndarray:
    im = cv2.imread(path, cv2.IMREAD_COLOR)
    if im is None:
        raise FileNotFoundError(path)
    return im

def overlay_boundary(gray01: np.ndarray, gt01: np.ndarray, pred01: np.ndarray) -> np.ndarray:
    g = np.clip(gray01 * 255.0, 0, 255).astype(np.uint8)
    rgb = cv2.cvtColor(g, cv2.COLOR_GRAY2BGR)
    rgb[gt01 > 0] = (0, 0, 255)   # GT red
    rgb[pred01 > 0] = (0, 255, 0) # Pred green
    return rgb

def crop_valid(img: np.ndarray, crop_h: int, crop_w: int) -> np.ndarray:
    return img[:crop_h, :crop_w]

def apply_clahe(img01: np.ndarray, clip: float, tile: int) -> np.ndarray:
    img8 = np.clip(img01 * 255.0, 0, 255).astype(np.uint8)
    clahe = cv2.createCLAHE(clipLimit=clip, tileGridSize=(tile, tile))
    out = clahe.apply(img8).astype(np.float32) / 255.0
    return out

def estimate_background(img01: np.ndarray, k: int) -> np.ndarray:
    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (k, k))
    img8 = np.clip(img01 * 255.0, 0, 255).astype(np.uint8)
    bg8 = cv2.morphologyEx(img8, cv2.MORPH_OPEN, kernel)
    return bg8.astype(np.float32) / 255.0

def robust_rescale(x: np.ndarray, p_lo: float, p_hi: float) -> np.ndarray:
    lo = np.percentile(x, p_lo)
    hi = np.percentile(x, p_hi)
    if hi <= lo + 1e-8:
        return np.clip(x, 0, 1)
    y = (x - lo) / (hi - lo)
    return np.clip(y, 0, 1)

def preprocess_sem(img_gray: np.ndarray, cfg_u1: dict) -> np.ndarray:
    # use EXACT U1 preprocessing settings for reproducibility
    H = int(cfg_u1["crop_h"]); W = int(cfg_u1["crop_w"])
    x = crop_valid(img_gray, H, W).astype(np.float32) / 255.0
    if bool(cfg_u1["use_clahe"]):
        x = apply_clahe(x, float(cfg_u1["clahe_clip"]), int(cfg_u1["clahe_tile"]))
    bg = estimate_background(x, int(cfg_u1["bg_kernel"]))
    x = x - bg
    x = robust_rescale(x, float(cfg_u1["robust_p_lo"]), float(cfg_u1["robust_p_hi"]))
    return x.astype(np.float32)

def extract_red_boundary_mask(label_bgr: np.ndarray, cfg_u1: dict) -> np.ndarray:
    H = int(cfg_u1["crop_h"]); W = int(cfg_u1["crop_w"])
    r_thr = int(cfg_u1["r_thr"]); g_thr = int(cfg_u1["g_thr"]); b_thr = int(cfg_u1["b_thr"])
    lab = crop_valid(label_bgr, H, W)
    B, G, R = lab[...,0], lab[...,1], lab[...,2]
    m = (R > r_thr) & (G < g_thr) & (B < b_thr)
    return m.astype(np.uint8)

def boundary_field_from_mask(mask01: np.ndarray, sigma: float) -> np.ndarray:
    boundary = (mask01 > 0)
    inv = ~boundary
    d = distance_transform_edt(inv).astype(np.float32)
    field = np.exp(-(d*d) / (2.0*sigma*sigma)).astype(np.float32)
    return field  # [0,1]

def compute_offsets(L: int, patch: int, stride: int) -> List[int]:
    max0 = L - patch
    if max0 < 0:
        return [0]
    offs = list(range(0, max0 + 1, stride))
    if offs[-1] != max0:
        offs.append(max0)
    return offs

def rot_k(a: np.ndarray, k: int) -> np.ndarray:
    return np.rot90(a, k).copy()

def soft_dice_loss(logits: torch.Tensor, target: torch.Tensor, eps: float = 1e-6) -> torch.Tensor:
    prob = torch.sigmoid(logits)
    num = 2.0 * (prob * target).sum(dim=(2,3))
    den = (prob * prob).sum(dim=(2,3)) + (target * target).sum(dim=(2,3)) + eps
    return (1.0 - (num / den)).mean()

def bce_dice_loss(logits: torch.Tensor, target: torch.Tensor) -> Tuple[torch.Tensor, float, float]:
    bce = F.binary_cross_entropy_with_logits(logits, target)
    d = soft_dice_loss(logits, target)
    return bce + d, float(bce.item()), float(d.item())

@torch.no_grad()
def dice_binary(pred: torch.Tensor, gt: torch.Tensor, eps: float = 1e-6) -> float:
    pred = pred.float()
    gt = gt.float()
    inter = (pred * gt).sum(dim=(2,3))
    den = pred.sum(dim=(2,3)) + gt.sum(dim=(2,3)) + eps
    return float(((2.0 * inter) / den).mean().item())


# =========================
# t-UNet (GroupNorm + FiLM)
# =========================
class SinusoidalTimeEmbedding(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        self.dim = dim
    def forward(self, t: torch.Tensor) -> torch.Tensor:
        if t.dim() != 1: t = t.view(-1)
        half = self.dim // 2
        freqs = torch.exp(-math.log(10000) * torch.arange(0, half, device=t.device).float() / max(half-1,1))
        ang = t.float()[:, None] * freqs[None, :]
        emb = torch.cat([torch.sin(ang), torch.cos(ang)], dim=1)
        if self.dim % 2 == 1:
            emb = F.pad(emb, (0,1))
        return emb

class FiLM(nn.Module):
    def __init__(self, time_dim: int, channels: int):
        super().__init__()
        self.mlp = nn.Sequential(nn.SiLU(), nn.Linear(time_dim, channels*2))
    def forward(self, x: torch.Tensor, t_emb: torch.Tensor) -> torch.Tensor:
        gb = self.mlp(t_emb)
        g, b = gb.chunk(2, dim=1)
        g = g[:, :, None, None]
        b = b[:, :, None, None]
        return x * (1.0 + g) + b

class ResBlockT(nn.Module):
    def __init__(self, in_ch, out_ch, groups: int, time_dim: int):
        super().__init__()
        self.c1 = nn.Conv2d(in_ch, out_ch, 3, padding=1, bias=False)
        self.g1 = nn.GroupNorm(num_groups=min(groups, out_ch), num_channels=out_ch)
        self.f1 = FiLM(time_dim, out_ch)

        self.c2 = nn.Conv2d(out_ch, out_ch, 3, padding=1, bias=False)
        self.g2 = nn.GroupNorm(num_groups=min(groups, out_ch), num_channels=out_ch)
        self.f2 = FiLM(time_dim, out_ch)

        self.act = nn.SiLU(inplace=True)
        self.skip = None
        if in_ch != out_ch:
            self.skip = nn.Conv2d(in_ch, out_ch, 1, bias=False)

    def forward(self, x, t_emb):
        h = self.c1(x)
        h = self.act(self.f1(self.g1(h), t_emb))
        h = self.c2(h)
        h = self.f2(self.g2(h), t_emb)
        s = x if self.skip is None else self.skip(x)
        return self.act(h + s)

class DownT(nn.Module):
    def __init__(self, in_ch, out_ch, groups: int, time_dim: int):
        super().__init__()
        self.pool = nn.MaxPool2d(2)
        self.block = ResBlockT(in_ch, out_ch, groups, time_dim)
    def forward(self, x, t_emb):
        return self.block(self.pool(x), t_emb)

class UpT(nn.Module):
    def __init__(self, in_ch, out_ch, groups: int, time_dim: int):
        super().__init__()
        self.up = nn.Upsample(scale_factor=2, mode="bilinear", align_corners=False)
        self.block = ResBlockT(in_ch, out_ch, groups, time_dim)
    def forward(self, x1, x2, t_emb):
        x1 = self.up(x1)
        dy = x2.size(2) - x1.size(2)
        dx = x2.size(3) - x1.size(3)
        x1 = F.pad(x1, [dx//2, dx-dx//2, dy//2, dy-dy//2])
        return self.block(torch.cat([x2, x1], dim=1), t_emb)

class TResUNet(nn.Module):
    def __init__(self, in_channels=2, out_channels=1, base=32, groups=16, time_dim=128):
        super().__init__()
        self.time_mlp = nn.Sequential(
            SinusoidalTimeEmbedding(time_dim),
            nn.Linear(time_dim, time_dim),
            nn.SiLU(),
            nn.Linear(time_dim, time_dim),
        )
        self.inc = ResBlockT(in_channels, base, groups, time_dim)
        self.d1 = DownT(base, base*2, groups, time_dim)
        self.d2 = DownT(base*2, base*4, groups, time_dim)
        self.d3 = DownT(base*4, base*8, groups, time_dim)
        self.d4 = DownT(base*8, base*16, groups, time_dim)

        self.u1 = UpT(base*16 + base*8, base*8, groups, time_dim)
        self.u2 = UpT(base*8 + base*4, base*4, groups, time_dim)
        self.u3 = UpT(base*4 + base*2, base*2, groups, time_dim)
        self.u4 = UpT(base*2 + base, base, groups, time_dim)

        self.outc = nn.Conv2d(base, out_channels, 1)

    def forward(self, x, t):
        if t.dim() != 1: t = t.view(-1)
        t_emb = self.time_mlp(t)
        x1 = self.inc(x, t_emb)
        x2 = self.d1(x1, t_emb)
        x3 = self.d2(x2, t_emb)
        x4 = self.d3(x3, t_emb)
        x5 = self.d4(x4, t_emb)
        x = self.u1(x5, x4, t_emb)
        x = self.u2(x, x3, t_emb)
        x = self.u3(x, x2, t_emb)
        x = self.u4(x, x1, t_emb)
        return self.outc(x)  # logits


# =========================
# Dataset (uses U1 logits cache)
# =========================
class U2PatchDS(torch.utils.data.Dataset):
    def __init__(self, jpg_paths: List[str], cache_dir: str, cfg_u1: dict, patch: int, stride: int, K: int, lam_sched: Tuple[float,...]):
        self.jpg_paths = jpg_paths
        self.cache_dir = cache_dir
        self.cfg_u1 = cfg_u1
        self.patch = patch
        self.stride = stride
        self.K = K
        self.lam = list(lam_sched)

        self.cache_img: Dict[str, Dict[str, np.ndarray]] = {}
        self.index: List[Tuple[int,int,int,int]] = []

        for i, p in enumerate(jpg_paths):
            img = preprocess_sem(read_gray(p), cfg_u1)
            ys = compute_offsets(img.shape[0], patch, stride)
            xs = compute_offsets(img.shape[1], patch, stride)
            for y in ys:
                for x in xs:
                    for rk in (0,1,2,3):
                        self.index.append((i,y,x,rk))

    def __len__(self): return len(self.index)

    def _load(self, i: int):
        p = self.jpg_paths[i]
        if p in self.cache_img:
            return
        base = os.path.splitext(os.path.basename(p))[0]
        npz = np.load(os.path.join(self.cache_dir, f"{base}.npz"))
        L0 = npz["logits"].astype(np.float32)  # (H,W) float16 stored -> float32 here
        img01 = preprocess_sem(read_gray(p), self.cfg_u1)

        lab = read_label_bgr(os.path.splitext(p)[0] + ".png")
        gt = extract_red_boundary_mask(lab, self.cfg_u1).astype(np.uint8)
        # soft target field (stable for thin labels)
        sigma = 2.0  # conservative default
        field = boundary_field_from_mask(gt, sigma).astype(np.float32)

        self.cache_img[p] = {"img": img01, "L0": L0, "gt": gt, "field": field}

    def __getitem__(self, idx: int):
        i,y,x,rk = self.index[idx]
        self._load(i)
        p = self.jpg_paths[i]
        c = self.cache_img[p]

        img = c["img"][y:y+self.patch, x:x+self.patch]
        L0  = c["L0"][y:y+self.patch, x:x+self.patch]
        gt  = c["gt"][y:y+self.patch, x:x+self.patch]
        field = c["field"][y:y+self.patch, x:x+self.patch]

        img = rot_k(img, rk)
        L0  = rot_k(L0, rk)
        gt  = rot_k(gt, rk)
        field = rot_k(field, rk)

        I = torch.from_numpy(img[None].astype(np.float32))      # (1,H,W)
        L0t = torch.from_numpy(L0[None].astype(np.float32))     # (1,H,W)
        Ybin = torch.from_numpy(gt[None].astype(np.float32))    # (1,H,W)
        Yfield = torch.from_numpy(field[None].astype(np.float32)) # (1,H,W)
        return I, L0t, Ybin, Yfield


# =========================
# Eval: refinement with logit-EMA
# =========================
@torch.no_grad()
def refine_logit_ema(unet2: nn.Module, I: torch.Tensor, L0: torch.Tensor, K: int, beta: float) -> torch.Tensor:
    """
    I: (B,1,H,W)
    L0: (B,1,H,W)
    returns P_final (B,1,H,W)
    """
    L = L0.clone()
    for t in range(K):
        P = torch.sigmoid(L)
        x = torch.cat([I, P], dim=1)
        tt = torch.full((I.size(0),), float(t), device=I.device)
        dL = unet2(x, tt)  # logits update (treated as "absolute-like" update)
        L = (1.0 - beta) * L + beta * dL
    return torch.sigmoid(L)

@torch.no_grad()
def eval_unet2(unet2: nn.Module, loader, cfg: CFG) -> Dict[str, float]:
    unet2.eval()
    dices = []
    for I, L0, Ybin, Yfield in loader:
        I = I.to(cfg.device)
        L0 = L0.to(cfg.device)
        Y = Ybin.to(cfg.device)
        Pf = refine_logit_ema(unet2, I, L0, cfg.K, cfg.beta_ema)
        pred = (Pf > cfg.thr_vis).float()
        dices.append(dice_binary(pred, (Y > 0.5).float()))
    return {"dice": float(np.mean(dices))}

@torch.no_grad()
def save_val_overlays(unet2: nn.Module, jpg_paths: List[str], cache_dir: str, out_dir: str, cfg_u1: dict, cfg: CFG):
    os.makedirs(out_dir, exist_ok=True)
    unet2.eval()
    for p in jpg_paths:
        base = os.path.splitext(os.path.basename(p))[0]
        npz = np.load(os.path.join(cache_dir, f"{base}.npz"))
        L0 = npz["logits"].astype(np.float32)
        img01 = preprocess_sem(read_gray(p), cfg_u1)

        lab = read_label_bgr(os.path.splitext(p)[0] + ".png")
        gt = extract_red_boundary_mask(lab, cfg_u1).astype(np.uint8)

        I = torch.from_numpy(img01[None,None].astype(np.float32)).to(cfg.device)
        L0t = torch.from_numpy(L0[None,None].astype(np.float32)).to(cfg.device)
        Pf = refine_logit_ema(unet2, I, L0t, cfg.K, cfg.beta_ema)[0,0].cpu().numpy()
        pred = (Pf > cfg.thr_vis).astype(np.uint8)

        ov = overlay_boundary(img01, gt, pred)
        cv2.imwrite(os.path.join(out_dir, f"{base}_U2_overlay.png"), ov)


# =========================
# Plotting
# =========================
def plot_curves(df_tr: pd.DataFrame, df_va: pd.DataFrame, out_png: str, title: str):
    plt.figure()
    plt.plot(df_tr["epoch"], df_tr["loss"], label="train loss")
    plt.plot(df_va["epoch"], df_va["dice"], label="val dice")
    plt.xlabel("epoch"); plt.legend(); plt.title(title); plt.tight_layout()
    plt.savefig(out_png, dpi=200)
    plt.close()

def plot_scatter(df_tr: pd.DataFrame, df_va: pd.DataFrame, out_png: str):
    plt.figure()
    plt.scatter(df_tr["loss"], df_va["dice"])
    plt.xlabel("train loss"); plt.ylabel("val dice")
    plt.tight_layout()
    plt.savefig(out_png, dpi=200)
    plt.close()


# =========================
# Main
# =========================
def main():
    cfg = CFG()
    set_seed(cfg.seed)

    # --- load U1 run artifacts
    run1 = cfg.run_dir
    with open(os.path.join(run1, "split.json"), "r", encoding="utf-8") as f:
        split = json.load(f)
    with open(os.path.join(run1, "config.json"), "r", encoding="utf-8") as f:
        cfg_u1 = json.load(f)

    root = cfg_u1["root"]
    train_dir = cfg_u1["train_dir"]
    test_dir = cfg_u1["test_dir"]

    train_folder = os.path.join(root, train_dir)
    test_folder = os.path.join(root, test_dir)

    train_paths = [os.path.join(train_folder, fn) for fn in split["train_files"]]
    val_paths   = [os.path.join(train_folder, fn) for fn in split["val_files"]]
    test_paths  = [os.path.join(test_folder, fn) for fn in split["test_files"]]

    cache_root = os.path.join(run1, "cache_u1_logits")
    cache_train = os.path.join(cache_root, "train")
    cache_val   = os.path.join(cache_root, "val")
    cache_test  = os.path.join(cache_root, "test")

    # --- create U2 run dir
    run2 = os.path.join("runs", cfg.exp_name)
    ckpt_dir = os.path.join(run2, "checkpoints")
    os.makedirs(ckpt_dir, exist_ok=True)
    os.makedirs(run2, exist_ok=True)

    # Save merged config for reproducibility
    merged = {"cfg_u2": asdict(cfg), "cfg_u1": cfg_u1, "split": split}
    with open(os.path.join(run2, "config_merged.json"), "w", encoding="utf-8") as f:
        json.dump(merged, f, indent=2)

    # data
    g = torch.Generator().manual_seed(cfg.seed)

    patch = int(cfg_u1["patch"])
    stride = int(cfg_u1["stride"])

    ds_tr = U2PatchDS(train_paths, cache_train, cfg_u1, patch, stride, cfg.K, cfg.lambda_schedule)
    ds_va = U2PatchDS(val_paths,   cache_val,   cfg_u1, patch, stride, cfg.K, cfg.lambda_schedule)

    dl_tr = torch.utils.data.DataLoader(ds_tr, batch_size=cfg.batch, shuffle=True, num_workers=4,
                                        pin_memory=True, worker_init_fn=worker_init_fn, generator=g)
    dl_va = torch.utils.data.DataLoader(ds_va, batch_size=cfg.batch, shuffle=False, num_workers=2,
                                        pin_memory=True, worker_init_fn=worker_init_fn, generator=g)

    print("Train images:", len(train_paths), "Val images:", len(val_paths), "Test images:", len(test_paths))
    print("Train patches:", len(ds_tr), "Val patches:", len(ds_va))

    # model
    unet2 = TResUNet(in_channels=2, out_channels=1, base=cfg.base, groups=cfg.gn_groups, time_dim=128).to(cfg.device)
    opt = torch.optim.AdamW(unet2.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)

    lam = list(cfg.lambda_schedule)
    assert len(lam) == cfg.K, "lambda_schedule length must equal K"

    best_dice = -1.0
    rows_tr, rows_va = [], []

    for epoch in range(1, cfg.epochs + 1):
        unet2.train()
        losses, bces, dices = [], [], []

        for I, L0, Ybin, Yfield in dl_tr:
            I = I.to(cfg.device)
            L0 = L0.to(cfg.device)
            Yf = Yfield.to(cfg.device)

            # sample timestep
            t_int = torch.randint(low=0, high=cfg.K, size=(I.size(0),), device=cfg.device)
            lam_t = torch.tensor([lam[int(x)] for x in t_int.tolist()], device=cfg.device).view(-1,1,1,1)

            # build Pt = mix(sigmoid(L0), Y_field) for stage-aware training
            P0 = torch.sigmoid(L0)
            Pt = (1.0 - lam_t) * P0 + lam_t * Yf

            x = torch.cat([I, Pt], dim=1)
            opt.zero_grad(set_to_none=True)
            out_logits = unet2(x, t_int.float())

            # conservative: supervise output logits toward field target
            loss, bce_v, dice_v = bce_dice_loss(out_logits, Yf)
            loss.backward()
            opt.step()

            losses.append(float(loss.item()))
            bces.append(bce_v)
            dices.append(dice_v)

        tr = {"epoch": epoch, "loss": float(np.mean(losses)), "bce": float(np.mean(bces)), "dice_loss": float(np.mean(dices))}
        va = eval_unet2(unet2, dl_va, cfg)
        va["epoch"] = epoch

        rows_tr.append(tr)
        rows_va.append(va)

        # save last
        last_path = os.path.join(ckpt_dir, "unet2_last.pth")
        torch.save({
            "model": unet2.state_dict(),
            "cfg_u2": asdict(cfg),
            "cfg_u1": cfg_u1,
            "epoch": epoch,
            "val_metric": va,
            "seed": cfg.seed,
        }, last_path)

        # save best
        if va["dice"] > best_dice:
            best_dice = va["dice"]
            best_path = os.path.join(ckpt_dir, "unet2_best.pth")
            torch.save({
                "model": unet2.state_dict(),
                "cfg_u2": asdict(cfg),
                "cfg_u1": cfg_u1,
                "epoch": epoch,
                "val_metric": va,
                "seed": cfg.seed,
            }, best_path)

        print(f"[U2][{epoch:03d}] train_loss={tr['loss']:.4f} val_dice={va['dice']:.4f}")

    # logs
    df_tr = pd.DataFrame(rows_tr)
    df_va = pd.DataFrame(rows_va)
    df_tr.to_csv(os.path.join(run2, "unet2_train.csv"), index=False)
    df_va.to_csv(os.path.join(run2, "unet2_val.csv"), index=False)

    plot_curves(df_tr, df_va, os.path.join(run2, "unet2_curves.png"), "U-Net2 (t-embedding)")
    plot_scatter(df_tr, df_va, os.path.join(run2, "unet2_scatter_loss_vs_valdice.png"))

    # overlays on val for qualitative check (best checkpoint)
    best_ckpt_path = os.path.join(ckpt_dir, "unet2_best.pth")
    ckpt = torch.load(best_ckpt_path, map_location=cfg.device)
    unet2.load_state_dict(ckpt["model"])
    unet2.eval()
    save_val_overlays(unet2, val_paths, cache_val, os.path.join(run2, "overlays_u2_val"), cfg_u1, cfg)

    print("U-Net2 done.")
    print("Artifacts saved to:", run2)


if __name__ == "__main__":
    main()