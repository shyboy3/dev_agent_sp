뉴뉴뉴 트레인 유넷2

import os, json, math, random
from dataclasses import dataclass, asdict
from datetime import datetime
from typing import List, Tuple, Dict

import cv2
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.nn.functional as F
import matplotlib.pyplot as plt
from scipy.ndimage import distance_transform_edt


# =========================
# Config (edit here)
# =========================
@dataclass
class CFG:
    root: str = "./DATA"
    train2_dir: str = "TRAIN_2_SEM_IMAGE"   # created by U1 best on VALID
    test_dir: str = "TEST_SEM_IMAGE"       # monitoring + final report only

    # U1 best checkpoint (needed to create Pt for TEST monitoring)
    u1_best_ckpt: str = ""  # <-- set to runs/exp_unet1/.../checkpoints/unet1_best.pth

    device: str = "cuda" if torch.cuda.is_available() else "cpu"
    seed: int = 42

    # preprocessing params (will be overwritten from U1 ckpt cfg if available)
    crop_h: int = 896
    crop_w: int = 1280
    use_clahe: bool = True
    clahe_clip: float = 2.0
    clahe_tile: int = 8
    bg_kernel: int = 49
    robust_p_lo: float = 1.0
    robust_p_hi: float = 99.0

    # patch / aug
    patch: int = 384
    stride: int = 48
    rot_aug: bool = True

    # label extraction (red)
    r_thr: int = 150
    g_thr: int = 120
    b_thr: int = 120

    # U2 training
    batch: int = 4
    epochs: int = 50
    lr: float = 1e-3
    weight_decay: float = 1e-5

    # U2 model
    base_u2: int = 32
    gn_groups: int = 16
    time_dim: int = 128

    # time schedule / refinement  (K increased by +1)
    K: int = 6
    lambda_schedule: Tuple[float, ...] = (0.0, 0.20, 0.40, 0.60, 0.80, 0.90)
    beta_ema: float = 0.7

    # targets
    field_sigma: float = 2.0

    # mixed loss weights
    w_field: float = 0.7
    w_thin: float = 0.3

    # eval / visualization
    thr_vis: float = 0.5
    sweep_thr_min: float = 0.05
    sweep_thr_max: float = 0.95
    sweep_thr_steps: int = 19

    # region metric dilation radii
    region_r_list: Tuple[int, ...] = (2, 3, 4)
    region_select_r: int = 3  # best score uses this r

    # dataloader
    num_workers: int = 0

    # U2-val split from TRAIN_2
    u2_val_count: int = 4  # 8장 중 마지막 4장 holdout (더 안정적)

    # output
    exp_name: str = "exp_unet2_v3"


# =========================
# Repro
# =========================
def set_seed(seed: int):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


# =========================
# Utils / preprocessing
# =========================
def ensure_dir(p: str):
    os.makedirs(p, exist_ok=True)

def list_jpg(folder: str) -> List[str]:
    files = [f for f in os.listdir(folder) if f.lower().endswith(".jpg")]
    files.sort()
    return [os.path.join(folder, f) for f in files]

def read_gray(path: str) -> np.ndarray:
    im = cv2.imread(path, cv2.IMREAD_GRAYSCALE)
    if im is None: raise FileNotFoundError(path)
    return im

def read_bgr(path: str) -> np.ndarray:
    im = cv2.imread(path, cv2.IMREAD_COLOR)
    if im is None: raise FileNotFoundError(path)
    return im

def crop_valid(img: np.ndarray, H: int, W: int) -> np.ndarray:
    return img[:H, :W]

def robust_rescale(x: np.ndarray, p_lo: float, p_hi: float) -> np.ndarray:
    lo = np.percentile(x, p_lo)
    hi = np.percentile(x, p_hi)
    if hi <= lo + 1e-8:
        return np.clip(x, 0, 1)
    y = (x - lo) / (hi - lo)
    return np.clip(y, 0, 1)

def apply_clahe(img01: np.ndarray, clip: float, tile: int) -> np.ndarray:
    img8 = np.clip(img01 * 255.0, 0, 255).astype(np.uint8)
    clahe = cv2.createCLAHE(clipLimit=clip, tileGridSize=(tile, tile))
    out = clahe.apply(img8).astype(np.float32) / 255.0
    return out

def estimate_background(img01: np.ndarray, k: int) -> np.ndarray:
    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (k, k))
    img8 = np.clip(img01 * 255.0, 0, 255).astype(np.uint8)
    bg8 = cv2.morphologyEx(img8, cv2.MORPH_OPEN, kernel)
    return bg8.astype(np.float32) / 255.0

def preprocess_sem(gray: np.ndarray, cfg: CFG) -> np.ndarray:
    x = crop_valid(gray, cfg.crop_h, cfg.crop_w).astype(np.float32) / 255.0
    if cfg.use_clahe:
        x = apply_clahe(x, cfg.clahe_clip, cfg.clahe_tile)
    bg = estimate_background(x, cfg.bg_kernel)
    x = x - bg
    x = robust_rescale(x, cfg.robust_p_lo, cfg.robust_p_hi)
    return x.astype(np.float32)

def extract_red_mask(label_bgr: np.ndarray, cfg: CFG) -> np.ndarray:
    lab = crop_valid(label_bgr, cfg.crop_h, cfg.crop_w)
    B, G, R = lab[...,0], lab[...,1], lab[...,2]
    m = (R > cfg.r_thr) & (G < cfg.g_thr) & (B < cfg.b_thr)
    return m.astype(np.uint8)

def boundary_field_from_mask(mask01: np.ndarray, sigma: float) -> np.ndarray:
    boundary = (mask01 > 0)
    inv = ~boundary
    d = distance_transform_edt(inv).astype(np.float32)
    field = np.exp(-(d*d) / (2.0*sigma*sigma)).astype(np.float32)
    return field

def compute_offsets(L: int, patch: int, stride: int) -> List[int]:
    max0 = L - patch
    if max0 < 0:
        return [0]
    offs = list(range(0, max0 + 1, stride))
    if offs[-1] != max0:
        offs.append(max0)
    return offs

def rot_k(a: np.ndarray, k: int) -> np.ndarray:
    return np.rot90(a, k).copy()

def to_u8(x01: np.ndarray) -> np.ndarray:
    return np.clip(x01 * 255.0, 0, 255).astype(np.uint8)

def overlay(gray01: np.ndarray, gt01: np.ndarray, pred01: np.ndarray) -> np.ndarray:
    g = to_u8(gray01)
    rgb = cv2.cvtColor(g, cv2.COLOR_GRAY2BGR)
    rgb[gt01 > 0] = (0,0,255)     # GT red
    rgb[pred01 > 0] = (0,255,0)   # Pred green
    return rgb


# =========================
# Metrics: boundary + region
# =========================
def soft_dice_score_np(prob: np.ndarray, gt: np.ndarray, eps: float = 1e-6) -> float:
    prob = prob.astype(np.float64)
    gt = gt.astype(np.float64)
    num = 2.0 * np.sum(prob * gt)
    den = np.sum(prob) + np.sum(gt) + eps
    return float(num / den)

def average_precision_np(y_true: np.ndarray, y_score: np.ndarray, eps: float = 1e-12) -> float:
    y_true = y_true.astype(np.uint8)
    y_score = y_score.astype(np.float64)
    pos = int(y_true.sum())
    if pos == 0: return 0.0
    order = np.argsort(-y_score)
    y_true_sorted = y_true[order]
    tp = np.cumsum(y_true_sorted == 1)
    fp = np.cumsum(y_true_sorted == 0)
    precision = tp / np.maximum(tp + fp, 1)
    ap = float(np.sum(precision[y_true_sorted == 1]) / (pos + eps))
    return ap

def sweep_best_f1(y_true: np.ndarray, y_score: np.ndarray, thresholds: np.ndarray, eps: float = 1e-12):
    y_true = y_true.astype(np.uint8)
    y_score = y_score.astype(np.float64)
    pos = y_true.sum()
    if pos == 0:
        return 0.0, float(thresholds[0])
    best_f1, best_thr = 0.0, float(thresholds[0])
    for thr in thresholds:
        y_pred = (y_score >= thr).astype(np.uint8)
        tp = int((y_pred & y_true).sum())
        fp = int((y_pred & (1-y_true)).sum())
        fn = int(((1-y_pred) & y_true).sum())
        precision = tp / (tp + fp + eps)
        recall = tp / (tp + fn + eps)
        f1 = 2.0 * precision * recall / (precision + recall + eps)
        if f1 > best_f1:
            best_f1, best_thr = float(f1), float(thr)
    return best_f1, best_thr

def dilate_binary(mask01: np.ndarray, r: int) -> np.ndarray:
    if r <= 0:
        return (mask01 > 0).astype(np.uint8)
    k = 2*r + 1
    ker = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (k, k))
    return cv2.dilate((mask01 > 0).astype(np.uint8), ker, iterations=1)

def region_from_boundary(boundary01: np.ndarray, r: int) -> np.ndarray:
    bd = dilate_binary(boundary01, r)
    region = (1 - bd).astype(np.uint8)
    return region

def iou_np(a: np.ndarray, b: np.ndarray, eps: float = 1e-6) -> float:
    a = (a > 0).astype(np.uint8)
    b = (b > 0).astype(np.uint8)
    inter = float((a & b).sum())
    union = float((a | b).sum()) + eps
    return inter / union

def dice_bin_np(a: np.ndarray, b: np.ndarray, eps: float = 1e-6) -> float:
    a = (a > 0).astype(np.uint8)
    b = (b > 0).astype(np.uint8)
    inter = float((a & b).sum())
    den = float(a.sum() + b.sum()) + eps
    return (2.0 * inter) / den


# =========================
# U1 model (for TEST Pt generation)
# =========================
class ResBlock(nn.Module):
    def __init__(self, in_ch, out_ch, groups=16):
        super().__init__()
        self.c1 = nn.Conv2d(in_ch, out_ch, 3, padding=1, bias=False)
        self.g1 = nn.GroupNorm(num_groups=min(groups, out_ch), num_channels=out_ch)
        self.c2 = nn.Conv2d(out_ch, out_ch, 3, padding=1, bias=False)
        self.g2 = nn.GroupNorm(num_groups=min(groups, out_ch), num_channels=out_ch)
        self.act = nn.SiLU(inplace=True)
        self.skip = None
        if in_ch != out_ch:
            self.skip = nn.Conv2d(in_ch, out_ch, 1, bias=False)
    def forward(self, x):
        h = self.act(self.g1(self.c1(x)))
        h = self.g2(self.c2(h))
        s = x if self.skip is None else self.skip(x)
        return self.act(h + s)

class Down(nn.Module):
    def __init__(self, in_ch, out_ch, groups=16):
        super().__init__()
        self.pool = nn.MaxPool2d(2)
        self.block = ResBlock(in_ch, out_ch, groups)
    def forward(self, x):
        return self.block(self.pool(x))

class Up(nn.Module):
    def __init__(self, in_ch, out_ch, groups=16):
        super().__init__()
        self.up = nn.Upsample(scale_factor=2, mode="bilinear", align_corners=False)
        self.block = ResBlock(in_ch, out_ch, groups)
    def forward(self, x1, x2):
        x1 = self.up(x1)
        dy = x2.size(2) - x1.size(2)
        dx = x2.size(3) - x1.size(3)
        x1 = F.pad(x1, [dx//2, dx-dx//2, dy//2, dy-dy//2])
        return self.block(torch.cat([x2, x1], dim=1))

class ResUNet(nn.Module):
    def __init__(self, in_ch=1, out_ch=1, base=64, groups=16):
        super().__init__()
        self.inc = ResBlock(in_ch, base, groups)
        self.d1 = Down(base, base*2, groups)
        self.d2 = Down(base*2, base*4, groups)
        self.d3 = Down(base*4, base*8, groups)
        self.d4 = Down(base*8, base*16, groups)
        self.u1 = Up(base*16 + base*8, base*8, groups)
        self.u2 = Up(base*8 + base*4, base*4, groups)
        self.u3 = Up(base*4 + base*2, base*2, groups)
        self.u4 = Up(base*2 + base, base, groups)
        self.outc = nn.Conv2d(base, out_ch, 1)
    def forward(self, x):
        x1 = self.inc(x)
        x2 = self.d1(x1)
        x3 = self.d2(x2)
        x4 = self.d3(x3)
        x5 = self.d4(x4)
        x = self.u1(x5, x4)
        x = self.u2(x, x3)
        x = self.u3(x, x2)
        x = self.u4(x, x1)
        return self.outc(x)


# =========================
# U2 model: t-embedding ResUNet (FiLM)
# =========================
class SinusoidalTimeEmbedding(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        self.dim = dim
    def forward(self, t: torch.Tensor) -> torch.Tensor:
        if t.dim() != 1:
            t = t.view(-1)
        half = self.dim // 2
        freqs = torch.exp(-math.log(10000) * torch.arange(0, half, device=t.device).float() / max(half-1, 1))
        ang = t.float()[:, None] * freqs[None, :]
        emb = torch.cat([torch.sin(ang), torch.cos(ang)], dim=1)
        if self.dim % 2 == 1:
            emb = F.pad(emb, (0,1))
        return emb

class FiLM(nn.Module):
    def __init__(self, time_dim: int, channels: int):
        super().__init__()
        self.mlp = nn.Sequential(nn.SiLU(), nn.Linear(time_dim, channels*2))
    def forward(self, x: torch.Tensor, t_emb: torch.Tensor) -> torch.Tensor:
        gb = self.mlp(t_emb)
        g, b = gb.chunk(2, dim=1)
        g = g[:, :, None, None]
        b = b[:, :, None, None]
        return x * (1.0 + g) + b

class ResBlockT(nn.Module):
    def __init__(self, in_ch, out_ch, groups: int, time_dim: int):
        super().__init__()
        self.c1 = nn.Conv2d(in_ch, out_ch, 3, padding=1, bias=False)
        self.g1 = nn.GroupNorm(num_groups=min(groups, out_ch), num_channels=out_ch)
        self.f1 = FiLM(time_dim, out_ch)
        self.c2 = nn.Conv2d(out_ch, out_ch, 3, padding=1, bias=False)
        self.g2 = nn.GroupNorm(num_groups=min(groups, out_ch), num_channels=out_ch)
        self.f2 = FiLM(time_dim, out_ch)
        self.act = nn.SiLU(inplace=True)
        self.skip = None
        if in_ch != out_ch:
            self.skip = nn.Conv2d(in_ch, out_ch, 1, bias=False)
    def forward(self, x, t_emb):
        h = self.c1(x)
        h = self.act(self.f1(self.g1(h), t_emb))
        h = self.c2(h)
        h = self.f2(self.g2(h), t_emb)
        s = x if self.skip is None else self.skip(x)
        return self.act(h + s)

class DownT(nn.Module):
    def __init__(self, in_ch, out_ch, groups: int, time_dim: int):
        super().__init__()
        self.pool = nn.MaxPool2d(2)
        self.block = ResBlockT(in_ch, out_ch, groups, time_dim)
    def forward(self, x, t_emb):
        return self.block(self.pool(x), t_emb)

class UpT(nn.Module):
    def __init__(self, in_ch, out_ch, groups: int, time_dim: int):
        super().__init__()
        self.up = nn.Upsample(scale_factor=2, mode="bilinear", align_corners=False)
        self.block = ResBlockT(in_ch, out_ch, groups, time_dim)
    def forward(self, x1, x2, t_emb):
        x1 = self.up(x1)
        dy = x2.size(2) - x1.size(2)
        dx = x2.size(3) - x1.size(3)
        x1 = F.pad(x1, [dx//2, dx-dx//2, dy//2, dy-dy//2])
        return self.block(torch.cat([x2, x1], dim=1), t_emb)

class TResUNet(nn.Module):
    def __init__(self, in_ch=2, out_ch=1, base=32, groups=16, time_dim=128):
        super().__init__()
        self.time_mlp = nn.Sequential(
            SinusoidalTimeEmbedding(time_dim),
            nn.Linear(time_dim, time_dim),
            nn.SiLU(),
            nn.Linear(time_dim, time_dim),
        )
        self.inc = ResBlockT(in_ch, base, groups, time_dim)
        self.d1 = DownT(base, base*2, groups, time_dim)
        self.d2 = DownT(base*2, base*4, groups, time_dim)
        self.d3 = DownT(base*4, base*8, groups, time_dim)
        self.d4 = DownT(base*8, base*16, groups, time_dim)
        self.u1 = UpT(base*16 + base*8, base*8, groups, time_dim)
        self.u2 = UpT(base*8 + base*4, base*4, groups, time_dim)
        self.u3 = UpT(base*4 + base*2, base*2, groups, time_dim)
        self.u4 = UpT(base*2 + base, base, groups, time_dim)
        self.outc = nn.Conv2d(base, out_ch, 1)

    def forward(self, x, t):
        if t.dim() != 1:
            t = t.view(-1)
        t_emb = self.time_mlp(t)
        x1 = self.inc(x, t_emb)
        x2 = self.d1(x1, t_emb)
        x3 = self.d2(x2, t_emb)
        x4 = self.d3(x3, t_emb)
        x5 = self.d4(x4, t_emb)
        x = self.u1(x5, x4, t_emb)
        x = self.u2(x, x3, t_emb)
        x = self.u3(x, x2, t_emb)
        x = self.u4(x, x1, t_emb)
        return self.outc(x)


# =========================
# TRAIN_2: Pt loader
# =========================
def load_pt_for_base(folder: str, base: str) -> np.ndarray:
    npz_path = os.path.join(folder, f"{base}_pt.npz")
    png_path = os.path.join(folder, f"{base}_pt.png")
    if os.path.isfile(npz_path):
        z = np.load(npz_path)
        pt = z["pt"].astype(np.float32)
        return np.clip(pt, 0, 1)
    if os.path.isfile(png_path):
        im = cv2.imread(png_path, cv2.IMREAD_GRAYSCALE)
        if im is None: raise FileNotFoundError(png_path)
        return im.astype(np.float32) / 255.0
    raise FileNotFoundError(f"No Pt found for {base} in {folder}")


# =========================
# Dataset: TRAIN_2 patches
# =========================
class Train2PatchDS(torch.utils.data.Dataset):
    def __init__(self, train2_folder: str, jpg_paths: List[str], cfg: CFG):
        self.folder = train2_folder
        self.cfg = cfg
        self.jpgs = jpg_paths
        assert len(self.jpgs) > 0, "Empty train subset for TRAIN_2."

        self.cache: Dict[str, Dict[str, np.ndarray]] = {}
        self.index: List[Tuple[int,int,int,int]] = []

        for i, p in enumerate(self.jpgs):
            img = preprocess_sem(read_gray(p), cfg)
            ys = compute_offsets(img.shape[0], cfg.patch, cfg.stride)
            xs = compute_offsets(img.shape[1], cfg.patch, cfg.stride)
            for y in ys:
                for x in xs:
                    rots = (0,1,2,3) if cfg.rot_aug else (0,)
                    for rk in rots:
                        self.index.append((i,y,x,rk))

    def __len__(self): return len(self.index)

    def _load(self, i: int):
        p = self.jpgs[i]
        if p in self.cache:
            return
        base = os.path.splitext(os.path.basename(p))[0]

        img01 = preprocess_sem(read_gray(p), self.cfg)

        lab = read_bgr(os.path.join(self.folder, f"{base}.png"))
        gt = extract_red_mask(lab, self.cfg).astype(np.uint8)
        field = boundary_field_from_mask(gt, sigma=float(self.cfg.field_sigma)).astype(np.float32)

        pt = load_pt_for_base(self.folder, base).astype(np.float32)
        pt = pt[:self.cfg.crop_h, :self.cfg.crop_w]

        self.cache[p] = {"img": img01, "gt": gt, "field": field, "pt": pt}

    def __getitem__(self, idx: int):
        i,y,x,rk = self.index[idx]
        self._load(i)
        p = self.jpgs[i]
        c = self.cache[p]

        img = c["img"][y:y+self.cfg.patch, x:x+self.cfg.patch]
        pt  = c["pt"][y:y+self.cfg.patch, x:x+self.cfg.patch]
        gt  = c["gt"][y:y+self.cfg.patch, x:x+self.cfg.patch]
        field = c["field"][y:y+self.cfg.patch, x:x+self.cfg.patch]

        img = rot_k(img, rk)
        pt  = rot_k(pt, rk)
        gt  = rot_k(gt, rk)
        field = rot_k(field, rk)

        I = torch.from_numpy(img[None].astype(np.float32))
        Pt = torch.from_numpy(pt[None].astype(np.float32))
        Ybin = torch.from_numpy(gt[None].astype(np.float32))
        Yfield = torch.from_numpy(field[None].astype(np.float32))
        return I, Pt, Ybin, Yfield


# =========================
# Loss helpers
# =========================
def bce_dice_loss_from_logits(logits: torch.Tensor, target: torch.Tensor) -> torch.Tensor:
    bce = F.binary_cross_entropy_with_logits(logits, target)
    prob = torch.sigmoid(logits)
    num = 2.0 * (prob * target).sum(dim=(2,3))
    den = (prob * prob).sum(dim=(2,3)) + (target * target).sum(dim=(2,3)) + 1e-6
    dice_loss = (1.0 - (num / den)).mean()
    return bce + dice_loss

def bce_softdice_from_prob(prob: torch.Tensor, target: torch.Tensor) -> torch.Tensor:
    # prob in [0,1]
    prob = prob.clamp(1e-6, 1-1e-6)
    bce = F.binary_cross_entropy(prob, target)
    num = 2.0 * (prob * target).sum(dim=(2,3))
    den = (prob * prob).sum(dim=(2,3)) + (target * target).sum(dim=(2,3)) + 1e-6
    dice_loss = (1.0 - (num / den)).mean()
    return bce + dice_loss


# =========================
# Refinement (logit-EMA)
# =========================
def safe_logit(p: torch.Tensor, eps: float = 1e-6) -> torch.Tensor:
    p = p.clamp(eps, 1-eps)
    return torch.log(p / (1-p))

@torch.no_grad()
def refine_logit_ema(u2: nn.Module, I: torch.Tensor, L0: torch.Tensor, cfg: CFG) -> torch.Tensor:
    L = L0.clone()
    for t in range(cfg.K):
        P = torch.sigmoid(L)
        x = torch.cat([I, P], dim=1)
        tt = torch.full((I.size(0),), float(t), device=I.device)
        dL = u2(x, tt)
        L = (1.0 - cfg.beta_ema) * L + cfg.beta_ema * dL
    return torch.sigmoid(L)  # Pf


# =========================
# Eval: U2-val on TRAIN_2 images (uses stored Pt)
# =========================
@torch.no_grad()
def eval_u2_on_train2_images(u2: nn.Module, train2_folder: str, jpg_paths: List[str], cfg: CFG) -> Dict[str, float]:
    u2.eval()
    thresholds = np.linspace(cfg.sweep_thr_min, cfg.sweep_thr_max, cfg.sweep_thr_steps)

    all_pf = []
    all_gt = []

    # region metrics will be computed using a single threshold (best_thr)
    # so we first collect pf,gt then compute best_thr by F1 on boundary.
    for p in jpg_paths:
        base = os.path.splitext(os.path.basename(p))[0]
        img01 = preprocess_sem(read_gray(p), cfg)
        pt = load_pt_for_base(train2_folder, base).astype(np.float32)[:cfg.crop_h, :cfg.crop_w]

        lab = read_bgr(os.path.join(train2_folder, f"{base}.png"))
        gt = extract_red_mask(lab, cfg).astype(np.uint8)

        I = torch.from_numpy(img01[None,None].astype(np.float32)).to(cfg.device)
        Pt = torch.from_numpy(pt[None,None].astype(np.float32)).to(cfg.device)
        L0 = safe_logit(Pt)

        Pf = refine_logit_ema(u2, I, L0, cfg)[0,0].detach().cpu().numpy().astype(np.float32)

        all_pf.append(Pf)
        all_gt.append(gt)

    # boundary metrics (global)
    pf_flat = np.concatenate([x.reshape(-1) for x in all_pf], 0)
    gt_flat = np.concatenate([y.reshape(-1).astype(np.uint8) for y in all_gt], 0)

    ap = average_precision_np(gt_flat, pf_flat)
    softdice = soft_dice_score_np(pf_flat, gt_flat)
    best_f1, best_thr = sweep_best_f1(gt_flat, pf_flat, thresholds)

    pred05 = (pf_flat >= cfg.thr_vis).astype(np.uint8)
    inter = 2.0 * float(np.sum(pred05 * gt_flat))
    den = float(np.sum(pred05) + np.sum(gt_flat) + 1e-6)
    dice05 = inter / den

    # region metrics @ best_thr (per-image, then average)
    region_stats = {}
    for r in cfg.region_r_list:
        ious, dices = [], []
        for Pf, GT in zip(all_pf, all_gt):
            Pbin = (Pf >= best_thr).astype(np.uint8)
            rg_gt = region_from_boundary(GT, r)
            rg_pr = region_from_boundary(Pbin, r)
            ious.append(iou_np(rg_pr, rg_gt))
            dices.append(dice_bin_np(rg_pr, rg_gt))
        region_stats[f"region_iou_r{r}"] = float(np.mean(ious))
        region_stats[f"region_dice_r{r}"] = float(np.mean(dices))

    out = {
        "ap": float(ap),
        "softdice": float(softdice),
        "dice05": float(dice05),
        "best_f1": float(best_f1),
        "best_thr": float(best_thr),
    }
    out.update(region_stats)
    return out


# =========================
# TEST monitoring (Pt from U1 on the fly)
# =========================
@torch.no_grad()
def u1_infer_pt(u1: nn.Module, sem_jpg: str, cfg: CFG) -> Tuple[np.ndarray, np.ndarray]:
    img01 = preprocess_sem(read_gray(sem_jpg), cfg)
    t = torch.from_numpy(img01[None,None].astype(np.float32)).to(cfg.device)
    logits = u1(t)[0,0].detach().cpu().numpy().astype(np.float32)
    pt = 1.0 / (1.0 + np.exp(-logits))
    return img01, pt

@torch.no_grad()
def eval_pipeline_on_test(u1: nn.Module, u2: nn.Module, test_jpgs: List[str], cfg: CFG) -> Dict[str, float]:
    u1.eval(); u2.eval()
    thresholds = np.linspace(cfg.sweep_thr_min, cfg.sweep_thr_max, cfg.sweep_thr_steps)

    all_pf = []
    all_gt = []

    for p in test_jpgs:
        img01, pt = u1_infer_pt(u1, p, cfg)
        lab = read_bgr(os.path.splitext(p)[0] + ".png")
        gt = extract_red_mask(lab, cfg).astype(np.uint8)

        I = torch.from_numpy(img01[None,None].astype(np.float32)).to(cfg.device)
        Pt = torch.from_numpy(pt[None,None].astype(np.float32)).to(cfg.device)
        L0 = safe_logit(Pt)

        Pf = refine_logit_ema(u2, I, L0, cfg)[0,0].detach().cpu().numpy().astype(np.float32)

        all_pf.append(Pf)
        all_gt.append(gt)

    pf_flat = np.concatenate([x.reshape(-1) for x in all_pf], 0)
    gt_flat = np.concatenate([y.reshape(-1).astype(np.uint8) for y in all_gt], 0)

    ap = average_precision_np(gt_flat, pf_flat)
    softdice = soft_dice_score_np(pf_flat, gt_flat)
    best_f1, best_thr = sweep_best_f1(gt_flat, pf_flat, thresholds)

    pred05 = (pf_flat >= cfg.thr_vis).astype(np.uint8)
    inter = 2.0 * float(np.sum(pred05 * gt_flat))
    den = float(np.sum(pred05) + np.sum(gt_flat) + 1e-6)
    dice05 = inter / den

    # region metrics @ best_thr
    region_stats = {}
    for r in cfg.region_r_list:
        ious, dices = [], []
        for Pf, GT in zip(all_pf, all_gt):
            Pbin = (Pf >= best_thr).astype(np.uint8)
            rg_gt = region_from_boundary(GT, r)
            rg_pr = region_from_boundary(Pbin, r)
            ious.append(iou_np(rg_pr, rg_gt))
            dices.append(dice_bin_np(rg_pr, rg_gt))
        region_stats[f"region_iou_r{r}"] = float(np.mean(ious))
        region_stats[f"region_dice_r{r}"] = float(np.mean(dices))

    out = {
        "ap": float(ap),
        "softdice": float(softdice),
        "dice05": float(dice05),
        "best_f1": float(best_f1),
        "best_thr": float(best_thr),
    }
    out.update(region_stats)
    return out


@torch.no_grad()
def save_test_outputs(u1: nn.Module, u2: nn.Module, test_jpgs: List[str], out_dir: str, cfg: CFG, tag: str, thr_best: float):
    ensure_dir(out_dir)
    u1.eval(); u2.eval()

    for p in test_jpgs:
        base = os.path.splitext(os.path.basename(p))[0]
        img01, pt = u1_infer_pt(u1, p, cfg)
        lab = read_bgr(os.path.splitext(p)[0] + ".png")
        gt = extract_red_mask(lab, cfg).astype(np.uint8)

        I = torch.from_numpy(img01[None,None].astype(np.float32)).to(cfg.device)
        Pt = torch.from_numpy(pt[None,None].astype(np.float32)).to(cfg.device)
        L0 = safe_logit(Pt)
        Pf = refine_logit_ema(u2, I, L0, cfg)[0,0].detach().cpu().numpy().astype(np.float32)

        P05 = (Pf >= cfg.thr_vis).astype(np.uint8)
        Pbest = (Pf >= float(thr_best)).astype(np.uint8)

        ov05 = overlay(img01, gt, P05)
        ovb  = overlay(img01, gt, Pbest)

        cv2.imwrite(os.path.join(out_dir, f"{base}_{tag}_I.png"), to_u8(img01))
        cv2.imwrite(os.path.join(out_dir, f"{base}_{tag}_GT.png"), (gt*255).astype(np.uint8))
        cv2.imwrite(os.path.join(out_dir, f"{base}_{tag}_Pt.png"), to_u8(pt))
        cv2.imwrite(os.path.join(out_dir, f"{base}_{tag}_Pf.png"), to_u8(Pf))

        cv2.imwrite(os.path.join(out_dir, f"{base}_{tag}_P_05.png"), (P05*255).astype(np.uint8))
        cv2.imwrite(os.path.join(out_dir, f"{base}_{tag}_overlay_05.png"), ov05)

        cv2.imwrite(os.path.join(out_dir, f"{base}_{tag}_P_best.png"), (Pbest*255).astype(np.uint8))
        cv2.imwrite(os.path.join(out_dir, f"{base}_{tag}_overlay_best.png"), ovb)


# =========================
# Plotting
# =========================
def plot_curves(df: pd.DataFrame, out_png: str, title: str, cfg: CFG):
    plt.figure()
    plt.plot(df["epoch"], df["train_loss"], label="train loss")
    plt.plot(df["epoch"], df["u2val_ap"], label="U2-val AP")
    plt.plot(df["epoch"], df["u2val_softdice"], label="U2-val softdice")
    r = cfg.region_select_r
    plt.plot(df["epoch"], df[f"u2val_region_iou_r{r}"], label=f"U2-val region IoU r={r}")
    plt.plot(df["epoch"], df["test_ap"], label="TEST AP (monitor)")
    plt.xlabel("epoch"); plt.legend(); plt.title(title); plt.tight_layout()
    plt.savefig(out_png, dpi=200)
    plt.close()


# =========================
# Main
# =========================
def main():
    cfg = CFG()
    set_seed(cfg.seed)

    if cfg.u1_best_ckpt.strip() == "" or (not os.path.isfile(cfg.u1_best_ckpt)):
        raise FileNotFoundError("CFG.u1_best_ckpt is empty or invalid. Set it to U1 best checkpoint path.")

    train2_folder = os.path.join(cfg.root, cfg.train2_dir)
    test_folder = os.path.join(cfg.root, cfg.test_dir)

    train2_jpgs_all = list_jpg(train2_folder)
    test_jpgs = list_jpg(test_folder)
    assert len(train2_jpgs_all) > 1, "TRAIN_2 folder too small. Run train_unet1.py first."
    assert len(test_jpgs) > 0, "TEST folder empty."

    # ---- split TRAIN_2 into train / U2-val (last N)
    n_val = int(cfg.u2_val_count)
    n_val = max(1, min(n_val, len(train2_jpgs_all) - 1))
    train2_train = train2_jpgs_all[:-n_val]
    train2_val   = train2_jpgs_all[-n_val:]

    # ---- load U1 best checkpoint and adopt its preprocessing config
    ckpt_u1 = torch.load(cfg.u1_best_ckpt, map_location=cfg.device)
    cfg_u1 = ckpt_u1.get("cfg", None)
    if cfg_u1 is not None:
        for k in ["crop_h","crop_w","use_clahe","clahe_clip","clahe_tile","bg_kernel","robust_p_lo","robust_p_hi",
                  "r_thr","g_thr","b_thr","patch","stride","rot_aug"]:
            if k in cfg_u1:
                setattr(cfg, k, cfg_u1[k])

    # ---- sanity: lambda length
    lam = list(cfg.lambda_schedule)
    if len(lam) != cfg.K:
        raise ValueError(f"lambda_schedule length({len(lam)}) must equal K({cfg.K}).")

    # U1 model (for TEST Pt)
    u1_base = int(cfg_u1["base"]) if (cfg_u1 is not None and "base" in cfg_u1) else 64
    u1_groups = int(cfg_u1["gn_groups"]) if (cfg_u1 is not None and "gn_groups" in cfg_u1) else 16
    u1 = ResUNet(in_ch=1, out_ch=1, base=u1_base, groups=u1_groups).to(cfg.device)
    u1.load_state_dict(ckpt_u1["model"])
    u1.eval()

    # ---- U2 run dir
    stamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    run_dir = os.path.join("runs", cfg.exp_name, stamp)
    ckpt_dir = os.path.join(run_dir, "checkpoints")
    ensure_dir(run_dir); ensure_dir(ckpt_dir)

    with open(os.path.join(run_dir, "config.json"), "w", encoding="utf-8") as f:
        json.dump(asdict(cfg), f, indent=2)

    with open(os.path.join(run_dir, "split_u2.json"), "w", encoding="utf-8") as f:
        json.dump({
            "train2_train": [os.path.basename(x) for x in train2_train],
            "train2_val":   [os.path.basename(x) for x in train2_val],
            "test":         [os.path.basename(x) for x in test_jpgs],
        }, f, indent=2)

    print(f"[U2v3] TRAIN_2 total={len(train2_jpgs_all)} | train={len(train2_train)} val={len(train2_val)} | TEST={len(test_jpgs)}")
    print("[U2v3] Run dir:", run_dir)

    # ---- data loader
    ds_tr = Train2PatchDS(train2_folder, train2_train, cfg)
    dl_tr = torch.utils.data.DataLoader(
        ds_tr, batch_size=cfg.batch, shuffle=True,
        num_workers=cfg.num_workers, pin_memory=True
    )

    # ---- U2 model
    u2 = TResUNet(in_ch=2, out_ch=1, base=cfg.base_u2, groups=cfg.gn_groups, time_dim=cfg.time_dim).to(cfg.device)
    opt = torch.optim.AdamW(u2.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)

    # ---- best selection (region-focused)
    best_score = -1e9
    best_thr_at_best = cfg.thr_vis
    best_epoch = -1

    rows = []

    for epoch in range(1, cfg.epochs + 1):
        u2.train()
        losses = []

        for I, Pt, Ybin, Yfield in dl_tr:
            I = I.to(cfg.device)
            Pt = Pt.to(cfg.device)
            Yf = Yfield.to(cfg.device)
            Yb = Ybin.to(cfg.device)

            # sample timestep for teacher-forced intermediate state
            t_int = torch.randint(low=0, high=cfg.K, size=(I.size(0),), device=cfg.device)
            lam_t = torch.tensor([lam[int(x)] for x in t_int.tolist()], device=cfg.device).view(-1,1,1,1)
            Pmix = (1.0 - lam_t) * Pt + lam_t * Yf

            # forward (field head)
            x = torch.cat([I, Pmix], dim=1)
            opt.zero_grad(set_to_none=True)
            out_logits = u2(x, t_int.float())

            # field loss (supervised on field target)
            loss_field = bce_dice_loss_from_logits(out_logits, Yf)

            # thin loss (supervised on refinement output Pf vs thin GT)
            # build L0 from Pt and do refinement in-graph (small K=6, batch=4 ok on A6000)
            L0 = safe_logit(Pt)
            # run refinement with gradients
            L = L0
            for t in range(cfg.K):
                P = torch.sigmoid(L)
                xref = torch.cat([I, P], dim=1)
                tt = torch.full((I.size(0),), float(t), device=cfg.device)
                dL = u2(xref, tt)
                L = (1.0 - cfg.beta_ema) * L + cfg.beta_ema * dL
            Pf = torch.sigmoid(L)
            loss_thin = bce_softdice_from_prob(Pf, Yb)

            loss = cfg.w_field * loss_field + cfg.w_thin * loss_thin
            loss.backward()
            opt.step()

            losses.append(float(loss.item()))

        train_loss = float(np.mean(losses))

        # ---- U2-val eval (full images with stored Pt)
        u2val_m = eval_u2_on_train2_images(u2, train2_folder, train2_val, cfg)

        # ---- TEST monitoring (Pt from U1; not for selection)
        test_m = eval_pipeline_on_test(u1, u2, test_jpgs, cfg)

        # selection score: region IoU @ r=select + boundary softdice (보조)
        rsel = cfg.region_select_r
        region_iou = float(u2val_m.get(f"region_iou_r{rsel}", 0.0))
        boundary_softdice = float(u2val_m["softdice"])
        score = 0.7 * region_iou + 0.3 * boundary_softdice

        row = {
            "epoch": epoch,
            "train_loss": train_loss,
            "u2val_ap": u2val_m["ap"],
            "u2val_softdice": u2val_m["softdice"],
            "u2val_bestF1": u2val_m["best_f1"],
            "u2val_bestThr": u2val_m["best_thr"],
            "test_ap": test_m["ap"],
            "test_softdice": test_m["softdice"],
            "test_bestF1": test_m["best_f1"],
            "test_bestThr": test_m["best_thr"],
            "u2val_score": score,
        }
        # log region metrics for r=2/3/4
        for r in cfg.region_r_list:
            row[f"u2val_region_iou_r{r}"] = u2val_m.get(f"region_iou_r{r}", np.nan)
            row[f"u2val_region_dice_r{r}"] = u2val_m.get(f"region_dice_r{r}", np.nan)
            row[f"test_region_iou_r{r}"] = test_m.get(f"region_iou_r{r}", np.nan)
            row[f"test_region_dice_r{r}"] = test_m.get(f"region_dice_r{r}", np.nan)

        rows.append(row)

        # save last
        torch.save({
            "model": u2.state_dict(),
            "cfg": asdict(cfg),
            "epoch": epoch,
            "train_loss": train_loss,
            "u2val_metric": u2val_m,
            "test_metric_monitor": test_m,
            "selection_score": score,
        }, os.path.join(ckpt_dir, "unet2_last.pth"))

        # best selection
        if score > best_score:
            best_score = score
            best_thr_at_best = float(u2val_m["best_thr"])
            best_epoch = epoch

            torch.save({
                "model": u2.state_dict(),
                "cfg": asdict(cfg),
                "epoch": epoch,
                "train_loss": train_loss,
                "u2val_metric": u2val_m,
                "test_metric_monitor": test_m,
                "selection_score": score,
                "best_thr_for_outputs": best_thr_at_best,
            }, os.path.join(ckpt_dir, "unet2_best.pth"))

            save_test_outputs(
                u1, u2, test_jpgs,
                os.path.join(run_dir, "paper_u2_test_best"),
                cfg,
                tag=f"U2_ep{epoch:03d}",
                thr_best=best_thr_at_best
            )

        print(
            f"[U2v3][{epoch:03d}] train_loss={train_loss:.4f} | "
            f"U2VAL(ap={u2val_m['ap']:.4f}, softdice={u2val_m['softdice']:.4f}, "
            f"regionIoU@r{rsel}={region_iou:.4f}, bestF1={u2val_m['best_f1']:.4f}@{u2val_m['best_thr']:.2f}) | "
            f"TEST(ap={test_m['ap']:.4f}, softdice={test_m['softdice']:.4f}, bestF1={test_m['best_f1']:.4f}@{test_m['best_thr']:.2f}) | "
            f"score={score:.4f} best={best_score:.4f}@ep{best_epoch:03d}"
        )

    # logs/plots
    df = pd.DataFrame(rows)
    df.to_csv(os.path.join(run_dir, "unet2_log.csv"), index=False)
    plot_curves(df, os.path.join(run_dir, "unet2_curves.png"), "U-Net2 v3 (mixed loss + region metric + K=6)", cfg)

    # final export with best
    ck = torch.load(os.path.join(ckpt_dir, "unet2_best.pth"), map_location=cfg.device)
    u2.load_state_dict(ck["model"])
    u2.eval()
    thr_best = float(ck.get("best_thr_for_outputs", cfg.thr_vis))

    save_test_outputs(u1, u2, test_jpgs, os.path.join(run_dir, "paper_u2_test"), cfg, tag="U2", thr_best=thr_best)

    print("[U2v3] Done.")
    print("[U2v3] Artifacts:", run_dir)
    print(f"[U2v3] Best score={best_score:.4f} at epoch={best_epoch}, best_thr_for_outputs={thr_best:.2f}")


if __name__ == "__main__":
    main()




뉴뉴 트레인 유넷2

import os, json, math, random
from dataclasses import dataclass, asdict
from datetime import datetime
from typing import List, Tuple, Dict

import cv2
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.nn.functional as F
import matplotlib.pyplot as plt
from scipy.ndimage import distance_transform_edt


# =========================
# Config (edit here)
# =========================
@dataclass
class CFG:
    root: str = "./DATA"
    train2_dir: str = "TRAIN_2_SEM_IMAGE"   # created by U1 best on VALID
    test_dir: str = "TEST_SEM_IMAGE"       # for monitoring + final report only

    # U1 best checkpoint (needed to create Pt for TEST monitoring)
    u1_best_ckpt: str = ""  # <- 반드시 실제 경로로 채워줘. 예: runs/exp_unet1/20260207_203012/checkpoints/unet1_best.pth

    device: str = "cuda" if torch.cuda.is_available() else "cpu"
    seed: int = 42

    # preprocessing params (will be overwritten from U1 ckpt cfg if available)
    crop_h: int = 896
    crop_w: int = 1280
    use_clahe: bool = True
    clahe_clip: float = 2.0
    clahe_tile: int = 8
    bg_kernel: int = 49
    robust_p_lo: float = 1.0
    robust_p_hi: float = 99.0

    # patch / aug
    patch: int = 384
    stride: int = 48
    rot_aug: bool = True

    # label extraction (red)
    r_thr: int = 150
    g_thr: int = 120
    b_thr: int = 120

    # U2 training
    batch: int = 4
    epochs: int = 40
    lr: float = 1e-3
    weight_decay: float = 1e-5

    # U2 model
    base_u2: int = 32
    gn_groups: int = 16
    time_dim: int = 128

    # t-schedule / refinement
    K: int = 5
    lambda_schedule: Tuple[float, ...] = (0.0, 0.25, 0.5, 0.75, 0.9)
    beta_ema: float = 0.7

    # field target
    field_sigma: float = 2.0

    # eval
    thr_vis: float = 0.5
    sweep_thr_min: float = 0.05
    sweep_thr_max: float = 0.95
    sweep_thr_steps: int = 19

    # dataloader
    num_workers: int = 0

    # U2-val split from TRAIN_2
    u2_val_count: int = 2  # file sort 후 마지막 n장을 U2-val

    # output
    exp_name: str = "exp_unet2"


# =========================
# Repro
# =========================
def set_seed(seed: int):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


# =========================
# Utils / preprocessing
# =========================
def ensure_dir(p: str):
    os.makedirs(p, exist_ok=True)

def list_jpg(folder: str) -> List[str]:
    files = [f for f in os.listdir(folder) if f.lower().endswith(".jpg")]
    files.sort()
    return [os.path.join(folder, f) for f in files]

def read_gray(path: str) -> np.ndarray:
    im = cv2.imread(path, cv2.IMREAD_GRAYSCALE)
    if im is None: raise FileNotFoundError(path)
    return im

def read_bgr(path: str) -> np.ndarray:
    im = cv2.imread(path, cv2.IMREAD_COLOR)
    if im is None: raise FileNotFoundError(path)
    return im

def crop_valid(img: np.ndarray, H: int, W: int) -> np.ndarray:
    return img[:H, :W]

def robust_rescale(x: np.ndarray, p_lo: float, p_hi: float) -> np.ndarray:
    lo = np.percentile(x, p_lo)
    hi = np.percentile(x, p_hi)
    if hi <= lo + 1e-8:
        return np.clip(x, 0, 1)
    y = (x - lo) / (hi - lo)
    return np.clip(y, 0, 1)

def apply_clahe(img01: np.ndarray, clip: float, tile: int) -> np.ndarray:
    img8 = np.clip(img01 * 255.0, 0, 255).astype(np.uint8)
    clahe = cv2.createCLAHE(clipLimit=clip, tileGridSize=(tile, tile))
    out = clahe.apply(img8).astype(np.float32) / 255.0
    return out

def estimate_background(img01: np.ndarray, k: int) -> np.ndarray:
    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (k, k))
    img8 = np.clip(img01 * 255.0, 0, 255).astype(np.uint8)
    bg8 = cv2.morphologyEx(img8, cv2.MORPH_OPEN, kernel)
    return bg8.astype(np.float32) / 255.0

def preprocess_sem(gray: np.ndarray, cfg: CFG) -> np.ndarray:
    x = crop_valid(gray, cfg.crop_h, cfg.crop_w).astype(np.float32) / 255.0
    if cfg.use_clahe:
        x = apply_clahe(x, cfg.clahe_clip, cfg.clahe_tile)
    bg = estimate_background(x, cfg.bg_kernel)
    x = x - bg
    x = robust_rescale(x, cfg.robust_p_lo, cfg.robust_p_hi)
    return x.astype(np.float32)

def extract_red_mask(label_bgr: np.ndarray, cfg: CFG) -> np.ndarray:
    lab = crop_valid(label_bgr, cfg.crop_h, cfg.crop_w)
    B, G, R = lab[...,0], lab[...,1], lab[...,2]
    m = (R > cfg.r_thr) & (G < cfg.g_thr) & (B < cfg.b_thr)
    return m.astype(np.uint8)

def boundary_field_from_mask(mask01: np.ndarray, sigma: float) -> np.ndarray:
    boundary = (mask01 > 0)
    inv = ~boundary
    d = distance_transform_edt(inv).astype(np.float32)
    field = np.exp(-(d*d) / (2.0*sigma*sigma)).astype(np.float32)
    return field

def compute_offsets(L: int, patch: int, stride: int) -> List[int]:
    max0 = L - patch
    if max0 < 0:
        return [0]
    offs = list(range(0, max0 + 1, stride))
    if offs[-1] != max0:
        offs.append(max0)
    return offs

def rot_k(a: np.ndarray, k: int) -> np.ndarray:
    return np.rot90(a, k).copy()

def to_u8(x01: np.ndarray) -> np.ndarray:
    return np.clip(x01 * 255.0, 0, 255).astype(np.uint8)

def overlay(gray01: np.ndarray, gt01: np.ndarray, pred01: np.ndarray) -> np.ndarray:
    g = to_u8(gray01)
    rgb = cv2.cvtColor(g, cv2.COLOR_GRAY2BGR)
    rgb[gt01 > 0] = (0,0,255)     # GT red
    rgb[pred01 > 0] = (0,255,0)   # Pred green
    return rgb


# =========================
# Metrics
# =========================
def soft_dice_score_np(prob: np.ndarray, gt: np.ndarray, eps: float = 1e-6) -> float:
    prob = prob.astype(np.float64)
    gt = gt.astype(np.float64)
    num = 2.0 * np.sum(prob * gt)
    den = np.sum(prob) + np.sum(gt) + eps
    return float(num / den)

def average_precision_np(y_true: np.ndarray, y_score: np.ndarray, eps: float = 1e-12) -> float:
    y_true = y_true.astype(np.uint8)
    y_score = y_score.astype(np.float64)
    pos = int(y_true.sum())
    if pos == 0: return 0.0
    order = np.argsort(-y_score)
    y_true_sorted = y_true[order]
    tp = np.cumsum(y_true_sorted == 1)
    fp = np.cumsum(y_true_sorted == 0)
    precision = tp / np.maximum(tp + fp, 1)
    ap = float(np.sum(precision[y_true_sorted == 1]) / (pos + eps))
    return ap

def sweep_best_f1(y_true: np.ndarray, y_score: np.ndarray, thresholds: np.ndarray, eps: float = 1e-12):
    y_true = y_true.astype(np.uint8)
    y_score = y_score.astype(np.float64)
    pos = y_true.sum()
    if pos == 0:
        return 0.0, float(thresholds[0])
    best_f1, best_thr = 0.0, float(thresholds[0])
    for thr in thresholds:
        y_pred = (y_score >= thr).astype(np.uint8)
        tp = int((y_pred & y_true).sum())
        fp = int((y_pred & (1-y_true)).sum())
        fn = int(((1-y_pred) & y_true).sum())
        precision = tp / (tp + fp + eps)
        recall = tp / (tp + fn + eps)
        f1 = 2.0 * precision * recall / (precision + recall + eps)
        if f1 > best_f1:
            best_f1, best_thr = float(f1), float(thr)
    return best_f1, best_thr


# =========================
# U1 model (for TEST Pt generation)
# must match train_unet1.py architecture
# =========================
class ResBlock(nn.Module):
    def __init__(self, in_ch, out_ch, groups=16):
        super().__init__()
        self.c1 = nn.Conv2d(in_ch, out_ch, 3, padding=1, bias=False)
        self.g1 = nn.GroupNorm(num_groups=min(groups, out_ch), num_channels=out_ch)
        self.c2 = nn.Conv2d(out_ch, out_ch, 3, padding=1, bias=False)
        self.g2 = nn.GroupNorm(num_groups=min(groups, out_ch), num_channels=out_ch)
        self.act = nn.SiLU(inplace=True)
        self.skip = None
        if in_ch != out_ch:
            self.skip = nn.Conv2d(in_ch, out_ch, 1, bias=False)
    def forward(self, x):
        h = self.act(self.g1(self.c1(x)))
        h = self.g2(self.c2(h))
        s = x if self.skip is None else self.skip(x)
        return self.act(h + s)

class Down(nn.Module):
    def __init__(self, in_ch, out_ch, groups=16):
        super().__init__()
        self.pool = nn.MaxPool2d(2)
        self.block = ResBlock(in_ch, out_ch, groups)
    def forward(self, x):
        return self.block(self.pool(x))

class Up(nn.Module):
    def __init__(self, in_ch, out_ch, groups=16):
        super().__init__()
        self.up = nn.Upsample(scale_factor=2, mode="bilinear", align_corners=False)
        self.block = ResBlock(in_ch, out_ch, groups)
    def forward(self, x1, x2):
        x1 = self.up(x1)
        dy = x2.size(2) - x1.size(2)
        dx = x2.size(3) - x1.size(3)
        x1 = F.pad(x1, [dx//2, dx-dx//2, dy//2, dy-dy//2])
        return self.block(torch.cat([x2, x1], dim=1))

class ResUNet(nn.Module):
    def __init__(self, in_ch=1, out_ch=1, base=64, groups=16):
        super().__init__()
        self.inc = ResBlock(in_ch, base, groups)
        self.d1 = Down(base, base*2, groups)
        self.d2 = Down(base*2, base*4, groups)
        self.d3 = Down(base*4, base*8, groups)
        self.d4 = Down(base*8, base*16, groups)
        self.u1 = Up(base*16 + base*8, base*8, groups)
        self.u2 = Up(base*8 + base*4, base*4, groups)
        self.u3 = Up(base*4 + base*2, base*2, groups)
        self.u4 = Up(base*2 + base, base, groups)
        self.outc = nn.Conv2d(base, out_ch, 1)
    def forward(self, x):
        x1 = self.inc(x)
        x2 = self.d1(x1)
        x3 = self.d2(x2)
        x4 = self.d3(x3)
        x5 = self.d4(x4)
        x = self.u1(x5, x4)
        x = self.u2(x, x3)
        x = self.u3(x, x2)
        x = self.u4(x, x1)
        return self.outc(x)


# =========================
# U2 model: t-embedding ResUNet (FiLM)
# =========================
class SinusoidalTimeEmbedding(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        self.dim = dim
    def forward(self, t: torch.Tensor) -> torch.Tensor:
        if t.dim() != 1:
            t = t.view(-1)
        half = self.dim // 2
        freqs = torch.exp(-math.log(10000) * torch.arange(0, half, device=t.device).float() / max(half-1, 1))
        ang = t.float()[:, None] * freqs[None, :]
        emb = torch.cat([torch.sin(ang), torch.cos(ang)], dim=1)
        if self.dim % 2 == 1:
            emb = F.pad(emb, (0,1))
        return emb

class FiLM(nn.Module):
    def __init__(self, time_dim: int, channels: int):
        super().__init__()
        self.mlp = nn.Sequential(nn.SiLU(), nn.Linear(time_dim, channels*2))
    def forward(self, x: torch.Tensor, t_emb: torch.Tensor) -> torch.Tensor:
        gb = self.mlp(t_emb)
        g, b = gb.chunk(2, dim=1)
        g = g[:, :, None, None]
        b = b[:, :, None, None]
        return x * (1.0 + g) + b

class ResBlockT(nn.Module):
    def __init__(self, in_ch, out_ch, groups: int, time_dim: int):
        super().__init__()
        self.c1 = nn.Conv2d(in_ch, out_ch, 3, padding=1, bias=False)
        self.g1 = nn.GroupNorm(num_groups=min(groups, out_ch), num_channels=out_ch)
        self.f1 = FiLM(time_dim, out_ch)
        self.c2 = nn.Conv2d(out_ch, out_ch, 3, padding=1, bias=False)
        self.g2 = nn.GroupNorm(num_groups=min(groups, out_ch), num_channels=out_ch)
        self.f2 = FiLM(time_dim, out_ch)
        self.act = nn.SiLU(inplace=True)
        self.skip = None
        if in_ch != out_ch:
            self.skip = nn.Conv2d(in_ch, out_ch, 1, bias=False)
    def forward(self, x, t_emb):
        h = self.c1(x)
        h = self.act(self.f1(self.g1(h), t_emb))
        h = self.c2(h)
        h = self.f2(self.g2(h), t_emb)
        s = x if self.skip is None else self.skip(x)
        return self.act(h + s)

class DownT(nn.Module):
    def __init__(self, in_ch, out_ch, groups: int, time_dim: int):
        super().__init__()
        self.pool = nn.MaxPool2d(2)
        self.block = ResBlockT(in_ch, out_ch, groups, time_dim)
    def forward(self, x, t_emb):
        return self.block(self.pool(x), t_emb)

class UpT(nn.Module):
    def __init__(self, in_ch, out_ch, groups: int, time_dim: int):
        super().__init__()
        self.up = nn.Upsample(scale_factor=2, mode="bilinear", align_corners=False)
        self.block = ResBlockT(in_ch, out_ch, groups, time_dim)
    def forward(self, x1, x2, t_emb):
        x1 = self.up(x1)
        dy = x2.size(2) - x1.size(2)
        dx = x2.size(3) - x1.size(3)
        x1 = F.pad(x1, [dx//2, dx-dx//2, dy//2, dy-dy//2])
        return self.block(torch.cat([x2, x1], dim=1), t_emb)

class TResUNet(nn.Module):
    def __init__(self, in_ch=2, out_ch=1, base=32, groups=16, time_dim=128):
        super().__init__()
        self.time_mlp = nn.Sequential(
            SinusoidalTimeEmbedding(time_dim),
            nn.Linear(time_dim, time_dim),
            nn.SiLU(),
            nn.Linear(time_dim, time_dim),
        )
        self.inc = ResBlockT(in_ch, base, groups, time_dim)
        self.d1 = DownT(base, base*2, groups, time_dim)
        self.d2 = DownT(base*2, base*4, groups, time_dim)
        self.d3 = DownT(base*4, base*8, groups, time_dim)
        self.d4 = DownT(base*8, base*16, groups, time_dim)
        self.u1 = UpT(base*16 + base*8, base*8, groups, time_dim)
        self.u2 = UpT(base*8 + base*4, base*4, groups, time_dim)
        self.u3 = UpT(base*4 + base*2, base*2, groups, time_dim)
        self.u4 = UpT(base*2 + base, base, groups, time_dim)
        self.outc = nn.Conv2d(base, out_ch, 1)

    def forward(self, x, t):
        if t.dim() != 1:
            t = t.view(-1)
        t_emb = self.time_mlp(t)
        x1 = self.inc(x, t_emb)
        x2 = self.d1(x1, t_emb)
        x3 = self.d2(x2, t_emb)
        x4 = self.d3(x3, t_emb)
        x5 = self.d4(x4, t_emb)
        x = self.u1(x5, x4, t_emb)
        x = self.u2(x, x3, t_emb)
        x = self.u3(x, x2, t_emb)
        x = self.u4(x, x1, t_emb)
        return self.outc(x)


# =========================
# TRAIN_2: Pt loader
# =========================
def load_pt_for_base(folder: str, base: str) -> np.ndarray:
    """Return Pt as float32 [0,1], prefer npz(pt) then png."""
    npz_path = os.path.join(folder, f"{base}_pt.npz")
    png_path = os.path.join(folder, f"{base}_pt.png")
    if os.path.isfile(npz_path):
        z = np.load(npz_path)
        pt = z["pt"].astype(np.float32)
        return np.clip(pt, 0, 1)
    if os.path.isfile(png_path):
        im = cv2.imread(png_path, cv2.IMREAD_GRAYSCALE)
        if im is None:
            raise FileNotFoundError(png_path)
        return (im.astype(np.float32) / 255.0)
    raise FileNotFoundError(f"No Pt found for {base} in {folder}")


# =========================
# Dataset: TRAIN_2 patches (train subset only)
# =========================
class Train2PatchDS(torch.utils.data.Dataset):
    def __init__(self, train2_folder: str, jpg_paths: List[str], cfg: CFG):
        self.folder = train2_folder
        self.cfg = cfg
        self.jpgs = jpg_paths
        assert len(self.jpgs) > 0, "Empty train subset for TRAIN_2."

        self.cache: Dict[str, Dict[str, np.ndarray]] = {}
        self.index: List[Tuple[int,int,int,int]] = []

        for i, p in enumerate(self.jpgs):
            img = preprocess_sem(read_gray(p), cfg)
            ys = compute_offsets(img.shape[0], cfg.patch, cfg.stride)
            xs = compute_offsets(img.shape[1], cfg.patch, cfg.stride)
            for y in ys:
                for x in xs:
                    rots = (0,1,2,3) if cfg.rot_aug else (0,)
                    for rk in rots:
                        self.index.append((i,y,x,rk))

    def __len__(self): return len(self.index)

    def _load(self, i: int):
        p = self.jpgs[i]
        if p in self.cache:
            return
        base = os.path.splitext(os.path.basename(p))[0]

        img01 = preprocess_sem(read_gray(p), self.cfg)

        lab = read_bgr(os.path.join(self.folder, f"{base}.png"))
        gt = extract_red_mask(lab, self.cfg).astype(np.uint8)
        field = boundary_field_from_mask(gt, sigma=float(self.cfg.field_sigma)).astype(np.float32)

        pt = load_pt_for_base(self.folder, base).astype(np.float32)
        pt = pt[:self.cfg.crop_h, :self.cfg.crop_w]

        self.cache[p] = {"img": img01, "gt": gt, "field": field, "pt": pt}

    def __getitem__(self, idx: int):
        i,y,x,rk = self.index[idx]
        self._load(i)
        p = self.jpgs[i]
        c = self.cache[p]

        img = c["img"][y:y+self.cfg.patch, x:x+self.cfg.patch]
        pt  = c["pt"][y:y+self.cfg.patch, x:x+self.cfg.patch]
        gt  = c["gt"][y:y+self.cfg.patch, x:x+self.cfg.patch]
        field = c["field"][y:y+self.cfg.patch, x:x+self.cfg.patch]

        img = rot_k(img, rk)
        pt  = rot_k(pt, rk)
        gt  = rot_k(gt, rk)
        field = rot_k(field, rk)

        I = torch.from_numpy(img[None].astype(np.float32))
        Pt = torch.from_numpy(pt[None].astype(np.float32))
        Ybin = torch.from_numpy(gt[None].astype(np.float32))
        Yfield = torch.from_numpy(field[None].astype(np.float32))
        return I, Pt, Ybin, Yfield


# =========================
# Loss
# =========================
def bce_dice_loss_from_logits(logits: torch.Tensor, target: torch.Tensor) -> torch.Tensor:
    bce = F.binary_cross_entropy_with_logits(logits, target)
    prob = torch.sigmoid(logits)
    num = 2.0 * (prob * target).sum(dim=(2,3))
    den = (prob * prob).sum(dim=(2,3)) + (target * target).sum(dim=(2,3)) + 1e-6
    dice_loss = (1.0 - (num / den)).mean()
    return bce + dice_loss


# =========================
# Refinement (logit-EMA)
# =========================
def safe_logit(p: torch.Tensor, eps: float = 1e-6) -> torch.Tensor:
    p = p.clamp(eps, 1-eps)
    return torch.log(p / (1-p))

@torch.no_grad()
def refine_logit_ema(u2: nn.Module, I: torch.Tensor, L0: torch.Tensor, cfg: CFG) -> torch.Tensor:
    L = L0.clone()
    for t in range(cfg.K):
        P = torch.sigmoid(L)
        x = torch.cat([I, P], dim=1)
        tt = torch.full((I.size(0),), float(t), device=I.device)
        dL = u2(x, tt)
        L = (1.0 - cfg.beta_ema) * L + cfg.beta_ema * dL
    return torch.sigmoid(L)  # Pf


# =========================
# U2-val eval on TRAIN_2 full images (uses stored Pt)
# =========================
@torch.no_grad()
def eval_u2_on_train2_images(u2: nn.Module, train2_folder: str, jpg_paths: List[str], cfg: CFG) -> Dict[str, float]:
    u2.eval()
    all_pf = []
    all_gt = []

    thresholds = np.linspace(cfg.sweep_thr_min, cfg.sweep_thr_max, cfg.sweep_thr_steps)

    for p in jpg_paths:
        base = os.path.splitext(os.path.basename(p))[0]
        img01 = preprocess_sem(read_gray(p), cfg)
        pt = load_pt_for_base(train2_folder, base).astype(np.float32)
        pt = pt[:cfg.crop_h, :cfg.crop_w]

        lab = read_bgr(os.path.join(train2_folder, f"{base}.png"))
        gt = extract_red_mask(lab, cfg).astype(np.uint8)

        I = torch.from_numpy(img01[None,None].astype(np.float32)).to(cfg.device)
        Pt = torch.from_numpy(pt[None,None].astype(np.float32)).to(cfg.device)
        L0 = safe_logit(Pt)

        Pf = refine_logit_ema(u2, I, L0, cfg)[0,0].detach().cpu().numpy().astype(np.float32)

        all_pf.append(Pf.reshape(-1))
        all_gt.append(gt.reshape(-1).astype(np.uint8))

    pf = np.concatenate(all_pf, 0)
    gt = np.concatenate(all_gt, 0)

    ap = average_precision_np(gt, pf)
    softdice = soft_dice_score_np(pf, gt)
    best_f1, best_thr = sweep_best_f1(gt, pf, thresholds)

    pred05 = (pf >= cfg.thr_vis).astype(np.uint8)
    inter = 2.0 * float(np.sum(pred05 * gt))
    den = float(np.sum(pred05) + np.sum(gt) + 1e-6)
    dice05 = inter / den

    return {"ap": ap, "softdice": softdice, "dice05": dice05, "best_f1": best_f1, "best_thr": best_thr}


# =========================
# TEST monitoring (Pt from U1 on the fly)
# =========================
@torch.no_grad()
def u1_infer_pt(u1: nn.Module, sem_jpg: str, cfg: CFG) -> Tuple[np.ndarray, np.ndarray]:
    img01 = preprocess_sem(read_gray(sem_jpg), cfg)
    t = torch.from_numpy(img01[None,None].astype(np.float32)).to(cfg.device)
    logits = u1(t)[0,0].detach().cpu().numpy().astype(np.float32)
    pt = 1.0 / (1.0 + np.exp(-logits))
    return img01, pt

@torch.no_grad()
def eval_pipeline_on_test(u1: nn.Module, u2: nn.Module, test_jpgs: List[str], cfg: CFG) -> Dict[str, float]:
    u1.eval(); u2.eval()
    all_pf = []
    all_gt = []

    thresholds = np.linspace(cfg.sweep_thr_min, cfg.sweep_thr_max, cfg.sweep_thr_steps)

    for p in test_jpgs:
        img01, pt = u1_infer_pt(u1, p, cfg)
        lab = read_bgr(os.path.splitext(p)[0] + ".png")
        gt = extract_red_mask(lab, cfg).astype(np.uint8)

        I = torch.from_numpy(img01[None,None].astype(np.float32)).to(cfg.device)
        Pt = torch.from_numpy(pt[None,None].astype(np.float32)).to(cfg.device)
        L0 = safe_logit(Pt)

        Pf = refine_logit_ema(u2, I, L0, cfg)[0,0].detach().cpu().numpy().astype(np.float32)

        all_pf.append(Pf.reshape(-1))
        all_gt.append(gt.reshape(-1).astype(np.uint8))

    pf = np.concatenate(all_pf, 0)
    gt = np.concatenate(all_gt, 0)

    ap = average_precision_np(gt, pf)
    softdice = soft_dice_score_np(pf, gt)
    best_f1, best_thr = sweep_best_f1(gt, pf, thresholds)

    pred05 = (pf >= cfg.thr_vis).astype(np.uint8)
    inter = 2.0 * float(np.sum(pred05 * gt))
    den = float(np.sum(pred05) + np.sum(gt) + 1e-6)
    dice05 = inter / den

    return {"ap": ap, "softdice": softdice, "dice05": dice05, "best_f1": best_f1, "best_thr": best_thr}


@torch.no_grad()
def save_test_outputs(u1: nn.Module, u2: nn.Module, test_jpgs: List[str], out_dir: str, cfg: CFG, tag: str, thr_best: float):
    ensure_dir(out_dir)
    u1.eval(); u2.eval()

    for p in test_jpgs:
        base = os.path.splitext(os.path.basename(p))[0]
        img01, pt = u1_infer_pt(u1, p, cfg)
        lab = read_bgr(os.path.splitext(p)[0] + ".png")
        gt = extract_red_mask(lab, cfg).astype(np.uint8)

        I = torch.from_numpy(img01[None,None].astype(np.float32)).to(cfg.device)
        Pt = torch.from_numpy(pt[None,None].astype(np.float32)).to(cfg.device)
        L0 = safe_logit(Pt)

        Pf = refine_logit_ema(u2, I, L0, cfg)[0,0].detach().cpu().numpy().astype(np.float32)

        # two binaries: fixed 0.5 and best_thr
        P05 = (Pf >= cfg.thr_vis).astype(np.uint8)
        Pbest = (Pf >= float(thr_best)).astype(np.uint8)

        ov05 = overlay(img01, gt, P05)
        ovb  = overlay(img01, gt, Pbest)

        cv2.imwrite(os.path.join(out_dir, f"{base}_{tag}_I.png"), to_u8(img01))
        cv2.imwrite(os.path.join(out_dir, f"{base}_{tag}_GT.png"), (gt*255).astype(np.uint8))
        cv2.imwrite(os.path.join(out_dir, f"{base}_{tag}_Pt.png"), to_u8(pt))
        cv2.imwrite(os.path.join(out_dir, f"{base}_{tag}_Pf.png"), to_u8(Pf))

        cv2.imwrite(os.path.join(out_dir, f"{base}_{tag}_P_05.png"), (P05*255).astype(np.uint8))
        cv2.imwrite(os.path.join(out_dir, f"{base}_{tag}_overlay_05.png"), ov05)

        cv2.imwrite(os.path.join(out_dir, f"{base}_{tag}_P_best.png"), (Pbest*255).astype(np.uint8))
        cv2.imwrite(os.path.join(out_dir, f"{base}_{tag}_overlay_best.png"), ovb)


# =========================
# Plotting
# =========================
def plot_curves(df: pd.DataFrame, out_png: str, title: str):
    plt.figure()
    plt.plot(df["epoch"], df["train_loss"], label="train loss")
    plt.plot(df["epoch"], df["u2val_ap"], label="U2-val AP")
    plt.plot(df["epoch"], df["u2val_softdice"], label="U2-val softdice")
    plt.plot(df["epoch"], df["test_ap"], label="TEST AP (monitor)")
    plt.xlabel("epoch"); plt.legend(); plt.title(title); plt.tight_layout()
    plt.savefig(out_png, dpi=200)
    plt.close()


# =========================
# Main
# =========================
def main():
    cfg = CFG()
    set_seed(cfg.seed)

    if cfg.u1_best_ckpt.strip() == "" or (not os.path.isfile(cfg.u1_best_ckpt)):
        raise FileNotFoundError("CFG.u1_best_ckpt is empty or invalid. Set it to U1 best checkpoint path.")

    train2_folder = os.path.join(cfg.root, cfg.train2_dir)
    test_folder = os.path.join(cfg.root, cfg.test_dir)

    train2_jpgs_all = list_jpg(train2_folder)
    test_jpgs = list_jpg(test_folder)
    assert len(train2_jpgs_all) > 0, "TRAIN_2 folder empty. Run train_unet1.py first."
    assert len(test_jpgs) > 0, "TEST folder empty."

    # ---- split TRAIN_2 into train / U2-val (last N)
    n_val = int(cfg.u2_val_count)
    n_val = max(1, min(n_val, len(train2_jpgs_all) - 1))
    train2_train = train2_jpgs_all[:-n_val]
    train2_val   = train2_jpgs_all[-n_val:]

    # ---- load U1 best checkpoint and adopt its preprocessing config (match U1)
    ckpt_u1 = torch.load(cfg.u1_best_ckpt, map_location=cfg.device)
    cfg_u1 = ckpt_u1.get("cfg", None)
    if cfg_u1 is not None:
        for k in ["crop_h","crop_w","use_clahe","clahe_clip","clahe_tile","bg_kernel","robust_p_lo","robust_p_hi",
                  "r_thr","g_thr","b_thr","patch","stride","rot_aug"]:
            if k in cfg_u1:
                setattr(cfg, k, cfg_u1[k])

    # U1 model (for TEST Pt)
    u1_base = int(cfg_u1["base"]) if (cfg_u1 is not None and "base" in cfg_u1) else 64
    u1_groups = int(cfg_u1["gn_groups"]) if (cfg_u1 is not None and "gn_groups" in cfg_u1) else 16
    u1 = ResUNet(in_ch=1, out_ch=1, base=u1_base, groups=u1_groups).to(cfg.device)
    u1.load_state_dict(ckpt_u1["model"])
    u1.eval()

    # ---- U2 run dir
    stamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    run_dir = os.path.join("runs", cfg.exp_name, stamp)
    ckpt_dir = os.path.join(run_dir, "checkpoints")
    ensure_dir(run_dir); ensure_dir(ckpt_dir)

    with open(os.path.join(run_dir, "config.json"), "w", encoding="utf-8") as f:
        json.dump(asdict(cfg), f, indent=2)

    # store split for reproducibility
    with open(os.path.join(run_dir, "split_u2.json"), "w", encoding="utf-8") as f:
        json.dump({
            "train2_train": [os.path.basename(x) for x in train2_train],
            "train2_val":   [os.path.basename(x) for x in train2_val],
            "test":         [os.path.basename(x) for x in test_jpgs],
        }, f, indent=2)

    print(f"[U2] TRAIN_2 total={len(train2_jpgs_all)} | train={len(train2_train)} val={len(train2_val)} | TEST={len(test_jpgs)}")
    print("[U2] Run dir:", run_dir)

    # ---- data loader (train subset only)
    ds_tr = Train2PatchDS(train2_folder, train2_train, cfg)
    dl_tr = torch.utils.data.DataLoader(
        ds_tr, batch_size=cfg.batch, shuffle=True,
        num_workers=cfg.num_workers, pin_memory=True
    )

    # ---- U2 model
    u2 = TResUNet(in_ch=2, out_ch=1, base=cfg.base_u2, groups=cfg.gn_groups, time_dim=cfg.time_dim).to(cfg.device)
    opt = torch.optim.AdamW(u2.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)

    lam = list(cfg.lambda_schedule)
    assert len(lam) == cfg.K, "lambda_schedule length must equal K"

    rows = []
    best_u2val_ap = -1.0
    best_thr_at_best = cfg.thr_vis

    for epoch in range(1, cfg.epochs + 1):
        u2.train()
        losses = []

        for I, Pt, Ybin, Yfield in dl_tr:
            I = I.to(cfg.device)
            Pt = Pt.to(cfg.device)
            Yf = Yfield.to(cfg.device)

            t_int = torch.randint(low=0, high=cfg.K, size=(I.size(0),), device=cfg.device)
            lam_t = torch.tensor([lam[int(x)] for x in t_int.tolist()], device=cfg.device).view(-1,1,1,1)

            # teacher-forced intermediate state
            Pmix = (1.0 - lam_t) * Pt + lam_t * Yf

            x = torch.cat([I, Pmix], dim=1)
            opt.zero_grad(set_to_none=True)
            out_logits = u2(x, t_int.float())

            loss = bce_dice_loss_from_logits(out_logits, Yf)
            loss.backward()
            opt.step()
            losses.append(float(loss.item()))

        train_loss = float(np.mean(losses))

        # ---- U2-val evaluation (uses stored Pt on holdout images)
        u2val_m = eval_u2_on_train2_images(u2, train2_folder, train2_val, cfg)

        # ---- TEST monitoring (Pt from U1; NOT for best selection)
        test_m = eval_pipeline_on_test(u1, u2, test_jpgs, cfg)

        rows.append({
            "epoch": epoch,
            "train_loss": train_loss,
            "u2val_ap": u2val_m["ap"],
            "u2val_softdice": u2val_m["softdice"],
            "u2val_dice05": u2val_m["dice05"],
            "u2val_bestF1": u2val_m["best_f1"],
            "u2val_bestThr": u2val_m["best_thr"],
            "test_ap": test_m["ap"],
            "test_softdice": test_m["softdice"],
            "test_dice05": test_m["dice05"],
            "test_bestF1": test_m["best_f1"],
            "test_bestThr": test_m["best_thr"],
        })

        # save last
        torch.save({
            "model": u2.state_dict(),
            "cfg": asdict(cfg),
            "epoch": epoch,
            "train_loss": train_loss,
            "u2val_metric": u2val_m,
            "test_metric_monitor": test_m,
        }, os.path.join(ckpt_dir, "unet2_last.pth"))

        # best selection: by U2-val AP
        if u2val_m["ap"] > best_u2val_ap:
            best_u2val_ap = float(u2val_m["ap"])
            best_thr_at_best = float(u2val_m["best_thr"])

            torch.save({
                "model": u2.state_dict(),
                "cfg": asdict(cfg),
                "epoch": epoch,
                "train_loss": train_loss,
                "u2val_metric": u2val_m,
                "test_metric_monitor": test_m,
                "best_thr_for_outputs": best_thr_at_best,
            }, os.path.join(ckpt_dir, "unet2_best.pth"))

            # best-time paper outputs (use best_thr from U2-val)
            save_test_outputs(
                u1, u2, test_jpgs,
                os.path.join(run_dir, "paper_u2_test_best"),
                cfg,
                tag=f"U2_ep{epoch:03d}",
                thr_best=best_thr_at_best
            )

        print(
            f"[U2][{epoch:03d}] train_loss={train_loss:.4f} | "
            f"U2VAL(ap={u2val_m['ap']:.4f}, softdice={u2val_m['softdice']:.4f}, bestF1={u2val_m['best_f1']:.4f}@{u2val_m['best_thr']:.2f}) | "
            f"TEST(ap={test_m['ap']:.4f}, softdice={test_m['softdice']:.4f}, bestF1={test_m['best_f1']:.4f}@{test_m['best_thr']:.2f})"
        )

    # logs/plots
    df = pd.DataFrame(rows)
    df.to_csv(os.path.join(run_dir, "unet2_log.csv"), index=False)
    plot_curves(df, os.path.join(run_dir, "unet2_curves.png"), "U-Net2 v2 (best by U2-val AP)")

    # load best and export final test outputs (use stored best_thr)
    ck = torch.load(os.path.join(ckpt_dir, "unet2_best.pth"), map_location=cfg.device)
    u2.load_state_dict(ck["model"])
    u2.eval()
    thr_best = float(ck.get("best_thr_for_outputs", cfg.thr_vis))

    save_test_outputs(u1, u2, test_jpgs, os.path.join(run_dir, "paper_u2_test"), cfg, tag="U2", thr_best=thr_best)

    print("[U2] Done.")
    print("[U2] Artifacts:", run_dir)
    print(f"[U2] Best selected by U2-val AP={best_u2val_ap:.4f}, best_thr_for_outputs={thr_best:.2f}")


if __name__ == "__main__":
    main()







뉴 트레인 유넷2

import os, json, math, random
from dataclasses import dataclass, asdict
from datetime import datetime
from typing import List, Tuple, Dict

import cv2
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.nn.functional as F
import matplotlib.pyplot as plt
from scipy.ndimage import distance_transform_edt


# =========================
# Config (edit here)
# =========================
@dataclass
class CFG:
    root: str = "./DATA"
    train2_dir: str = "TRAIN_2_SEM_IMAGE"   # created by U1 best on VALID
    test_dir: str = "TEST_SEM_IMAGE"       # for monitoring only

    # U1 best checkpoint (needed to create Pt for TEST monitoring)
    u1_best_ckpt: str = "./runs/exp_unet1/latest/checkpoints/unet1_best.pth"
    # ↑ 편하게 쓰려면 실행 전에 실제 경로로 바꿔줘.
    #   예: runs/exp_unet1/20260207_203012/checkpoints/unet1_best.pth

    device: str = "cuda" if torch.cuda.is_available() else "cpu"
    seed: int = 42

    # preprocessing params must match U1 config used to make TRAIN_2
    # (we will ALSO load U1 cfg from checkpoint when possible)
    # fallback values:
    crop_h: int = 896
    crop_w: int = 1280
    use_clahe: bool = True
    clahe_clip: float = 2.0
    clahe_tile: int = 8
    bg_kernel: int = 49
    robust_p_lo: float = 1.0
    robust_p_hi: float = 99.0

    # patch
    patch: int = 384
    stride: int = 48
    rot_aug: bool = True

    # label extraction (red)
    r_thr: int = 150
    g_thr: int = 120
    b_thr: int = 120

    # U2 training
    batch: int = 4
    epochs: int = 40
    lr: float = 1e-3
    weight_decay: float = 1e-5

    # U2 model
    base: int = 32
    gn_groups: int = 16
    time_dim: int = 128

    # t-schedule
    K: int = 5
    lambda_schedule: Tuple[float, ...] = (0.0, 0.25, 0.5, 0.75, 0.9)
    beta_ema: float = 0.7

    # field target
    field_sigma: float = 2.0

    # eval
    thr_vis: float = 0.5
    sweep_thr_min: float = 0.05
    sweep_thr_max: float = 0.95
    sweep_thr_steps: int = 19

    # dataloader
    num_workers: int = 0

    # output
    exp_name: str = "exp_unet2"


# =========================
# Repro
# =========================
def set_seed(seed: int):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


# =========================
# Utils / preprocessing
# =========================
def ensure_dir(p: str):
    os.makedirs(p, exist_ok=True)

def list_jpg(folder: str) -> List[str]:
    files = [f for f in os.listdir(folder) if f.lower().endswith(".jpg")]
    files.sort()
    return [os.path.join(folder, f) for f in files]

def read_gray(path: str) -> np.ndarray:
    im = cv2.imread(path, cv2.IMREAD_GRAYSCALE)
    if im is None: raise FileNotFoundError(path)
    return im

def read_bgr(path: str) -> np.ndarray:
    im = cv2.imread(path, cv2.IMREAD_COLOR)
    if im is None: raise FileNotFoundError(path)
    return im

def crop_valid(img: np.ndarray, H: int, W: int) -> np.ndarray:
    return img[:H, :W]

def robust_rescale(x: np.ndarray, p_lo: float, p_hi: float) -> np.ndarray:
    lo = np.percentile(x, p_lo)
    hi = np.percentile(x, p_hi)
    if hi <= lo + 1e-8:
        return np.clip(x, 0, 1)
    y = (x - lo) / (hi - lo)
    return np.clip(y, 0, 1)

def apply_clahe(img01: np.ndarray, clip: float, tile: int) -> np.ndarray:
    img8 = np.clip(img01 * 255.0, 0, 255).astype(np.uint8)
    clahe = cv2.createCLAHE(clipLimit=clip, tileGridSize=(tile, tile))
    out = clahe.apply(img8).astype(np.float32) / 255.0
    return out

def estimate_background(img01: np.ndarray, k: int) -> np.ndarray:
    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (k, k))
    img8 = np.clip(img01 * 255.0, 0, 255).astype(np.uint8)
    bg8 = cv2.morphologyEx(img8, cv2.MORPH_OPEN, kernel)
    return bg8.astype(np.float32) / 255.0

def preprocess_sem(gray: np.ndarray, cfg: CFG) -> np.ndarray:
    x = crop_valid(gray, cfg.crop_h, cfg.crop_w).astype(np.float32) / 255.0
    if cfg.use_clahe:
        x = apply_clahe(x, cfg.clahe_clip, cfg.clahe_tile)
    bg = estimate_background(x, cfg.bg_kernel)
    x = x - bg
    x = robust_rescale(x, cfg.robust_p_lo, cfg.robust_p_hi)
    return x.astype(np.float32)

def extract_red_mask(label_bgr: np.ndarray, cfg: CFG) -> np.ndarray:
    lab = crop_valid(label_bgr, cfg.crop_h, cfg.crop_w)
    B, G, R = lab[...,0], lab[...,1], lab[...,2]
    m = (R > cfg.r_thr) & (G < cfg.g_thr) & (B < cfg.b_thr)
    return m.astype(np.uint8)

def boundary_field_from_mask(mask01: np.ndarray, sigma: float) -> np.ndarray:
    boundary = (mask01 > 0)
    inv = ~boundary
    d = distance_transform_edt(inv).astype(np.float32)
    field = np.exp(-(d*d) / (2.0*sigma*sigma)).astype(np.float32)
    return field

def compute_offsets(L: int, patch: int, stride: int) -> List[int]:
    max0 = L - patch
    if max0 < 0:
        return [0]
    offs = list(range(0, max0 + 1, stride))
    if offs[-1] != max0:
        offs.append(max0)
    return offs

def rot_k(a: np.ndarray, k: int) -> np.ndarray:
    return np.rot90(a, k).copy()

def to_u8(x01: np.ndarray) -> np.ndarray:
    return np.clip(x01 * 255.0, 0, 255).astype(np.uint8)

def overlay(gray01: np.ndarray, gt01: np.ndarray, pred01: np.ndarray) -> np.ndarray:
    g = to_u8(gray01)
    rgb = cv2.cvtColor(g, cv2.COLOR_GRAY2BGR)
    rgb[gt01 > 0] = (0,0,255)
    rgb[pred01 > 0] = (0,255,0)
    return rgb


# =========================
# Metrics
# =========================
def soft_dice_score_np(prob: np.ndarray, gt: np.ndarray, eps: float = 1e-6) -> float:
    prob = prob.astype(np.float64)
    gt = gt.astype(np.float64)
    num = 2.0 * np.sum(prob * gt)
    den = np.sum(prob) + np.sum(gt) + eps
    return float(num / den)

def average_precision_np(y_true: np.ndarray, y_score: np.ndarray, eps: float = 1e-12) -> float:
    y_true = y_true.astype(np.uint8)
    y_score = y_score.astype(np.float64)
    pos = int(y_true.sum())
    if pos == 0: return 0.0
    order = np.argsort(-y_score)
    y_true_sorted = y_true[order]
    tp = np.cumsum(y_true_sorted == 1)
    fp = np.cumsum(y_true_sorted == 0)
    precision = tp / np.maximum(tp + fp, 1)
    ap = float(np.sum(precision[y_true_sorted == 1]) / (pos + eps))
    return ap

def sweep_best_f1(y_true: np.ndarray, y_score: np.ndarray, thresholds: np.ndarray, eps: float = 1e-12):
    y_true = y_true.astype(np.uint8)
    y_score = y_score.astype(np.float64)
    pos = y_true.sum()
    if pos == 0:
        return 0.0, float(thresholds[0])
    best_f1, best_thr = 0.0, float(thresholds[0])
    for thr in thresholds:
        y_pred = (y_score >= thr).astype(np.uint8)
        tp = int((y_pred & y_true).sum())
        fp = int((y_pred & (1-y_true)).sum())
        fn = int(((1-y_pred) & y_true).sum())
        precision = tp / (tp + fp + eps)
        recall = tp / (tp + fn + eps)
        f1 = 2.0 * precision * recall / (precision + recall + eps)
        if f1 > best_f1:
            best_f1, best_thr = float(f1), float(thr)
    return best_f1, best_thr


# =========================
# U1 model definition (for TEST Pt generation)
# - must match train_unet1.py architecture
# =========================
class ResBlock(nn.Module):
    def __init__(self, in_ch, out_ch, groups=16):
        super().__init__()
        self.c1 = nn.Conv2d(in_ch, out_ch, 3, padding=1, bias=False)
        self.g1 = nn.GroupNorm(num_groups=min(groups, out_ch), num_channels=out_ch)
        self.c2 = nn.Conv2d(out_ch, out_ch, 3, padding=1, bias=False)
        self.g2 = nn.GroupNorm(num_groups=min(groups, out_ch), num_channels=out_ch)
        self.act = nn.SiLU(inplace=True)
        self.skip = None
        if in_ch != out_ch:
            self.skip = nn.Conv2d(in_ch, out_ch, 1, bias=False)
    def forward(self, x):
        h = self.act(self.g1(self.c1(x)))
        h = self.g2(self.c2(h))
        s = x if self.skip is None else self.skip(x)
        return self.act(h + s)

class Down(nn.Module):
    def __init__(self, in_ch, out_ch, groups=16):
        super().__init__()
        self.pool = nn.MaxPool2d(2)
        self.block = ResBlock(in_ch, out_ch, groups)
    def forward(self, x):
        return self.block(self.pool(x))

class Up(nn.Module):
    def __init__(self, in_ch, out_ch, groups=16):
        super().__init__()
        self.up = nn.Upsample(scale_factor=2, mode="bilinear", align_corners=False)
        self.block = ResBlock(in_ch, out_ch, groups)
    def forward(self, x1, x2):
        x1 = self.up(x1)
        dy = x2.size(2) - x1.size(2)
        dx = x2.size(3) - x1.size(3)
        x1 = F.pad(x1, [dx//2, dx-dx//2, dy//2, dy-dy//2])
        return self.block(torch.cat([x2, x1], dim=1))

class ResUNet(nn.Module):
    def __init__(self, in_ch=1, out_ch=1, base=64, groups=16):
        super().__init__()
        self.inc = ResBlock(in_ch, base, groups)
        self.d1 = Down(base, base*2, groups)
        self.d2 = Down(base*2, base*4, groups)
        self.d3 = Down(base*4, base*8, groups)
        self.d4 = Down(base*8, base*16, groups)
        self.u1 = Up(base*16 + base*8, base*8, groups)
        self.u2 = Up(base*8 + base*4, base*4, groups)
        self.u3 = Up(base*4 + base*2, base*2, groups)
        self.u4 = Up(base*2 + base, base, groups)
        self.outc = nn.Conv2d(base, out_ch, 1)
    def forward(self, x):
        x1 = self.inc(x)
        x2 = self.d1(x1)
        x3 = self.d2(x2)
        x4 = self.d3(x3)
        x5 = self.d4(x4)
        x = self.u1(x5, x4)
        x = self.u2(x, x3)
        x = self.u3(x, x2)
        x = self.u4(x, x1)
        return self.outc(x)


# =========================
# U2 model: t-embedding ResUNet (FiLM)
# =========================
class SinusoidalTimeEmbedding(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        self.dim = dim
    def forward(self, t: torch.Tensor) -> torch.Tensor:
        if t.dim() != 1:
            t = t.view(-1)
        half = self.dim // 2
        freqs = torch.exp(-math.log(10000) * torch.arange(0, half, device=t.device).float() / max(half-1, 1))
        ang = t.float()[:, None] * freqs[None, :]
        emb = torch.cat([torch.sin(ang), torch.cos(ang)], dim=1)
        if self.dim % 2 == 1:
            emb = F.pad(emb, (0,1))
        return emb

class FiLM(nn.Module):
    def __init__(self, time_dim: int, channels: int):
        super().__init__()
        self.mlp = nn.Sequential(nn.SiLU(), nn.Linear(time_dim, channels*2))
    def forward(self, x: torch.Tensor, t_emb: torch.Tensor) -> torch.Tensor:
        gb = self.mlp(t_emb)
        g, b = gb.chunk(2, dim=1)
        g = g[:, :, None, None]
        b = b[:, :, None, None]
        return x * (1.0 + g) + b

class ResBlockT(nn.Module):
    def __init__(self, in_ch, out_ch, groups: int, time_dim: int):
        super().__init__()
        self.c1 = nn.Conv2d(in_ch, out_ch, 3, padding=1, bias=False)
        self.g1 = nn.GroupNorm(num_groups=min(groups, out_ch), num_channels=out_ch)
        self.f1 = FiLM(time_dim, out_ch)
        self.c2 = nn.Conv2d(out_ch, out_ch, 3, padding=1, bias=False)
        self.g2 = nn.GroupNorm(num_groups=min(groups, out_ch), num_channels=out_ch)
        self.f2 = FiLM(time_dim, out_ch)
        self.act = nn.SiLU(inplace=True)
        self.skip = None
        if in_ch != out_ch:
            self.skip = nn.Conv2d(in_ch, out_ch, 1, bias=False)
    def forward(self, x, t_emb):
        h = self.c1(x)
        h = self.act(self.f1(self.g1(h), t_emb))
        h = self.c2(h)
        h = self.f2(self.g2(h), t_emb)
        s = x if self.skip is None else self.skip(x)
        return self.act(h + s)

class DownT(nn.Module):
    def __init__(self, in_ch, out_ch, groups: int, time_dim: int):
        super().__init__()
        self.pool = nn.MaxPool2d(2)
        self.block = ResBlockT(in_ch, out_ch, groups, time_dim)
    def forward(self, x, t_emb):
        return self.block(self.pool(x), t_emb)

class UpT(nn.Module):
    def __init__(self, in_ch, out_ch, groups: int, time_dim: int):
        super().__init__()
        self.up = nn.Upsample(scale_factor=2, mode="bilinear", align_corners=False)
        self.block = ResBlockT(in_ch, out_ch, groups, time_dim)
    def forward(self, x1, x2, t_emb):
        x1 = self.up(x1)
        dy = x2.size(2) - x1.size(2)
        dx = x2.size(3) - x1.size(3)
        x1 = F.pad(x1, [dx//2, dx-dx//2, dy//2, dy-dy//2])
        return self.block(torch.cat([x2, x1], dim=1), t_emb)

class TResUNet(nn.Module):
    def __init__(self, in_ch=2, out_ch=1, base=32, groups=16, time_dim=128):
        super().__init__()
        self.time_mlp = nn.Sequential(
            SinusoidalTimeEmbedding(time_dim),
            nn.Linear(time_dim, time_dim),
            nn.SiLU(),
            nn.Linear(time_dim, time_dim),
        )
        self.inc = ResBlockT(in_ch, base, groups, time_dim)
        self.d1 = DownT(base, base*2, groups, time_dim)
        self.d2 = DownT(base*2, base*4, groups, time_dim)
        self.d3 = DownT(base*4, base*8, groups, time_dim)
        self.d4 = DownT(base*8, base*16, groups, time_dim)
        self.u1 = UpT(base*16 + base*8, base*8, groups, time_dim)
        self.u2 = UpT(base*8 + base*4, base*4, groups, time_dim)
        self.u3 = UpT(base*4 + base*2, base*2, groups, time_dim)
        self.u4 = UpT(base*2 + base, base, groups, time_dim)
        self.outc = nn.Conv2d(base, out_ch, 1)

    def forward(self, x, t):
        if t.dim() != 1:
            t = t.view(-1)
        t_emb = self.time_mlp(t)
        x1 = self.inc(x, t_emb)
        x2 = self.d1(x1, t_emb)
        x3 = self.d2(x2, t_emb)
        x4 = self.d3(x3, t_emb)
        x5 = self.d4(x4, t_emb)
        x = self.u1(x5, x4, t_emb)
        x = self.u2(x, x3, t_emb)
        x = self.u3(x, x2, t_emb)
        x = self.u4(x, x1, t_emb)
        return self.outc(x)  # logits update


# =========================
# Dataset: TRAIN_2 patches (SEM + Pt)
# =========================
def load_pt_for_base(folder: str, base: str) -> np.ndarray:
    """
    Return Pt as float32 [0,1], prefer npz(pt) then png.
    """
    npz_path = os.path.join(folder, f"{base}_pt.npz")
    png_path = os.path.join(folder, f"{base}_pt.png")
    if os.path.isfile(npz_path):
        z = np.load(npz_path)
        pt = z["pt"].astype(np.float32)
        return np.clip(pt, 0, 1)
    if os.path.isfile(png_path):
        im = cv2.imread(png_path, cv2.IMREAD_GRAYSCALE)
        if im is None:
            raise FileNotFoundError(png_path)
        return (im.astype(np.float32) / 255.0)
    raise FileNotFoundError(f"No Pt found for {base} in {folder}")

class Train2PatchDS(torch.utils.data.Dataset):
    def __init__(self, train2_folder: str, cfg: CFG):
        self.folder = train2_folder
        self.cfg = cfg
        self.jpgs = list_jpg(train2_folder)
        assert len(self.jpgs) > 0, f"Empty TRAIN_2 folder: {train2_folder}"

        self.cache: Dict[str, Dict[str, np.ndarray]] = {}
        self.index: List[Tuple[int,int,int,int]] = []

        for i, p in enumerate(self.jpgs):
            img = preprocess_sem(read_gray(p), cfg)
            ys = compute_offsets(img.shape[0], cfg.patch, cfg.stride)
            xs = compute_offsets(img.shape[1], cfg.patch, cfg.stride)
            for y in ys:
                for x in xs:
                    rots = (0,1,2,3) if cfg.rot_aug else (0,)
                    for rk in rots:
                        self.index.append((i,y,x,rk))

    def __len__(self): return len(self.index)

    def _load(self, i: int):
        p = self.jpgs[i]
        if p in self.cache:
            return
        base = os.path.splitext(os.path.basename(p))[0]

        img01 = preprocess_sem(read_gray(p), self.cfg)

        lab = read_bgr(os.path.join(self.folder, f"{base}.png"))
        gt = extract_red_mask(lab, self.cfg).astype(np.uint8)
        field = boundary_field_from_mask(gt, sigma=float(self.cfg.field_sigma)).astype(np.float32)

        pt = load_pt_for_base(self.folder, base).astype(np.float32)
        pt = pt[:self.cfg.crop_h, :self.cfg.crop_w]  # safety

        self.cache[p] = {"img": img01, "gt": gt, "field": field, "pt": pt}

    def __getitem__(self, idx: int):
        i,y,x,rk = self.index[idx]
        self._load(i)
        p = self.jpgs[i]
        c = self.cache[p]

        img = c["img"][y:y+self.cfg.patch, x:x+self.cfg.patch]
        pt  = c["pt"][y:y+self.cfg.patch, x:x+self.cfg.patch]
        gt  = c["gt"][y:y+self.cfg.patch, x:x+self.cfg.patch]
        field = c["field"][y:y+self.cfg.patch, x:x+self.cfg.patch]

        img = rot_k(img, rk)
        pt  = rot_k(pt, rk)
        gt  = rot_k(gt, rk)
        field = rot_k(field, rk)

        I = torch.from_numpy(img[None].astype(np.float32))      # (1,H,W)
        Pt = torch.from_numpy(pt[None].astype(np.float32))      # (1,H,W)
        Ybin = torch.from_numpy(gt[None].astype(np.float32))    # (1,H,W)
        Yfield = torch.from_numpy(field[None].astype(np.float32))
        return I, Pt, Ybin, Yfield


# =========================
# Loss
# =========================
def bce_dice_loss_from_logits(logits: torch.Tensor, target: torch.Tensor) -> torch.Tensor:
    bce = F.binary_cross_entropy_with_logits(logits, target)
    prob = torch.sigmoid(logits)
    num = 2.0 * (prob * target).sum(dim=(2,3))
    den = (prob * prob).sum(dim=(2,3)) + (target * target).sum(dim=(2,3)) + 1e-6
    dice_loss = (1.0 - (num / den)).mean()
    return bce + dice_loss


# =========================
# Refinement (logit-EMA)
# =========================
@torch.no_grad()
def refine_logit_ema(unet2: nn.Module, I: torch.Tensor, L0: torch.Tensor, cfg: CFG) -> torch.Tensor:
    """
    I: (B,1,H,W)
    L0: (B,1,H,W) initial logits (we will create from Pt by logit(Pt))
    Return Pf prob.
    """
    L = L0.clone()
    for t in range(cfg.K):
        P = torch.sigmoid(L)
        x = torch.cat([I, P], dim=1)
        tt = torch.full((I.size(0),), float(t), device=I.device)
        dL = unet2(x, tt)
        L = (1.0 - cfg.beta_ema) * L + cfg.beta_ema * dL
    return torch.sigmoid(L)


def safe_logit(p: torch.Tensor, eps: float = 1e-6) -> torch.Tensor:
    p = p.clamp(eps, 1-eps)
    return torch.log(p / (1-p))


# =========================
# TEST monitoring: generate Pt from U1 best on the fly
# =========================
@torch.no_grad()
def u1_infer_pt(u1: nn.Module, sem_jpg: str, cfg_u1_like: CFG) -> Tuple[np.ndarray, np.ndarray]:
    """
    Return (img01, pt_prob) for full image.
    """
    img01 = preprocess_sem(read_gray(sem_jpg), cfg_u1_like)
    t = torch.from_numpy(img01[None,None].astype(np.float32)).to(cfg_u1_like.device)
    logits = u1(t)[0,0].detach().cpu().numpy().astype(np.float32)
    pt = 1.0 / (1.0 + np.exp(-logits))
    return img01, pt


# =========================
# Validation metrics on full-image Pf vs thin GT
# =========================
@torch.no_grad()
def eval_pipeline_on_test(u1: nn.Module, u2: nn.Module, test_jpgs: List[str], cfg: CFG) -> Dict[str, float]:
    u1.eval(); u2.eval()
    all_pf = []
    all_gt = []

    thresholds = np.linspace(cfg.sweep_thr_min, cfg.sweep_thr_max, cfg.sweep_thr_steps)

    for p in test_jpgs:
        img01, pt = u1_infer_pt(u1, p, cfg)
        lab = read_bgr(os.path.splitext(p)[0] + ".png")
        gt = extract_red_mask(lab, cfg).astype(np.uint8)

        I = torch.from_numpy(img01[None,None].astype(np.float32)).to(cfg.device)
        Pt = torch.from_numpy(pt[None,None].astype(np.float32)).to(cfg.device)
        L0 = safe_logit(Pt)

        Pf = refine_logit_ema(u2, I, L0, cfg)[0,0].detach().cpu().numpy().astype(np.float32)

        all_pf.append(Pf.reshape(-1))
        all_gt.append(gt.reshape(-1).astype(np.uint8))

    pf = np.concatenate(all_pf, 0)
    gt = np.concatenate(all_gt, 0)

    ap = average_precision_np(gt, pf)
    softdice = soft_dice_score_np(pf, gt)
    best_f1, best_thr = sweep_best_f1(gt, pf, thresholds)

    pred05 = (pf >= cfg.thr_vis).astype(np.uint8)
    inter = 2.0 * float(np.sum(pred05 * gt))
    den = float(np.sum(pred05) + np.sum(gt) + 1e-6)
    dice05 = inter / den

    return {"ap": ap, "softdice": softdice, "dice05": dice05, "best_f1": best_f1, "best_thr": best_thr}


@torch.no_grad()
def save_test_outputs(u1: nn.Module, u2: nn.Module, test_jpgs: List[str], out_dir: str, cfg: CFG, tag: str):
    ensure_dir(out_dir)
    u1.eval(); u2.eval()

    for p in test_jpgs:
        base = os.path.splitext(os.path.basename(p))[0]
        img01, pt = u1_infer_pt(u1, p, cfg)
        lab = read_bgr(os.path.splitext(p)[0] + ".png")
        gt = extract_red_mask(lab, cfg).astype(np.uint8)

        I = torch.from_numpy(img01[None,None].astype(np.float32)).to(cfg.device)
        Pt = torch.from_numpy(pt[None,None].astype(np.float32)).to(cfg.device)
        L0 = safe_logit(Pt)

        Pf = refine_logit_ema(u2, I, L0, cfg)[0,0].detach().cpu().numpy().astype(np.float32)
        pred = (Pf >= cfg.thr_vis).astype(np.uint8)
        ov = overlay(img01, gt, pred)

        cv2.imwrite(os.path.join(out_dir, f"{base}_{tag}_I.png"), to_u8(img01))
        cv2.imwrite(os.path.join(out_dir, f"{base}_{tag}_GT.png"), (gt*255).astype(np.uint8))
        cv2.imwrite(os.path.join(out_dir, f"{base}_{tag}_Pt.png"), to_u8(pt))
        cv2.imwrite(os.path.join(out_dir, f"{base}_{tag}_Pf.png"), to_u8(Pf))
        cv2.imwrite(os.path.join(out_dir, f"{base}_{tag}_P.png"), (pred*255).astype(np.uint8))
        cv2.imwrite(os.path.join(out_dir, f"{base}_{tag}_overlay.png"), ov)


# =========================
# Plotting
# =========================
def plot_curves(df: pd.DataFrame, out_png: str, title: str):
    plt.figure()
    plt.plot(df["epoch"], df["train_loss"], label="train loss")
    if "test_ap" in df.columns:
        plt.plot(df["epoch"], df["test_ap"], label="test AP (monitor)")
    plt.xlabel("epoch"); plt.legend(); plt.title(title); plt.tight_layout()
    plt.savefig(out_png, dpi=200)
    plt.close()


# =========================
# Main
# =========================
def main():
    cfg = CFG()
    set_seed(cfg.seed)

    train2_folder = os.path.join(cfg.root, cfg.train2_dir)
    test_folder = os.path.join(cfg.root, cfg.test_dir)

    train2_jpgs = list_jpg(train2_folder)
    test_jpgs = list_jpg(test_folder)
    assert len(train2_jpgs) > 0, "TRAIN_2 folder empty. Run train_unet1.py first."
    assert len(test_jpgs) > 0, "TEST folder empty."

    # ---- load U1 best checkpoint (and adopt its preprocessing config if present)
    ckpt = torch.load(cfg.u1_best_ckpt, map_location=cfg.device)
    cfg_u1 = ckpt.get("cfg", None)
    if cfg_u1 is not None:
        # overwrite preprocessing-related fields to match U1 exactly
        for k in ["crop_h","crop_w","use_clahe","clahe_clip","clahe_tile","bg_kernel","robust_p_lo","robust_p_hi",
                  "r_thr","g_thr","b_thr","patch","stride","rot_aug","base","gn_groups"]:
            if k in cfg_u1:
                setattr(cfg, k, cfg_u1[k])

    u1 = ResUNet(in_ch=1, out_ch=1, base=int(getattr(cfg, "base", 64)), groups=int(getattr(cfg, "gn_groups", 16))).to(cfg.device)
    u1.load_state_dict(ckpt["model"])
    u1.eval()

    # ---- U2 run dir (timestamp)
    stamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    run_dir = os.path.join("runs", cfg.exp_name, stamp)
    ckpt_dir = os.path.join(run_dir, "checkpoints")
    ensure_dir(run_dir); ensure_dir(ckpt_dir)

    with open(os.path.join(run_dir, "config.json"), "w", encoding="utf-8") as f:
        json.dump(asdict(cfg), f, indent=2)

    print("[U2] TRAIN_2:", len(train2_jpgs), "TEST:", len(test_jpgs))
    print("[U2] Run dir:", run_dir)

    ds_tr = Train2PatchDS(train2_folder, cfg)
    dl_tr = torch.utils.data.DataLoader(
        ds_tr, batch_size=cfg.batch, shuffle=True,
        num_workers=cfg.num_workers, pin_memory=True
    )

    u2 = TResUNet(in_ch=2, out_ch=1, base=cfg.base, groups=cfg.gn_groups, time_dim=cfg.time_dim).to(cfg.device)
    opt = torch.optim.AdamW(u2.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)

    lam = list(cfg.lambda_schedule)
    assert len(lam) == cfg.K, "lambda_schedule length must equal K"

    rows = []
    best_by_trainloss = float("inf")

    for epoch in range(1, cfg.epochs + 1):
        u2.train()
        losses = []

        for I, Pt, Ybin, Yfield in dl_tr:
            I = I.to(cfg.device)
            Pt = Pt.to(cfg.device)
            Yf = Yfield.to(cfg.device)

            # sample timestep
            t_int = torch.randint(low=0, high=cfg.K, size=(I.size(0),), device=cfg.device)
            lam_t = torch.tensor([lam[int(x)] for x in t_int.tolist()], device=cfg.device).view(-1,1,1,1)

            # build P_t_mix = mix(Pt_input, Y_field) (teacher forcing to cover timesteps)
            Pmix = (1.0 - lam_t) * Pt + lam_t * Yf

            x = torch.cat([I, Pmix], dim=1)
            opt.zero_grad(set_to_none=True)
            out_logits = u2(x, t_int.float())

            loss = bce_dice_loss_from_logits(out_logits, Yf)
            loss.backward()
            opt.step()
            losses.append(float(loss.item()))

        train_loss = float(np.mean(losses))

        # test monitoring (NOT for model selection)
        test_m = eval_pipeline_on_test(u1, u2, test_jpgs, cfg)

        rows.append({
            "epoch": epoch,
            "train_loss": train_loss,
            "test_ap": test_m["ap"],
            "test_softdice": test_m["softdice"],
            "test_dice05": test_m["dice05"],
            "test_bestF1": test_m["best_f1"],
            "test_bestThr": test_m["best_thr"],
        })

        # save last
        torch.save({
            "model": u2.state_dict(),
            "cfg": asdict(cfg),
            "epoch": epoch,
            "train_loss": train_loss,
            "test_metric_monitor": test_m,
        }, os.path.join(ckpt_dir, "unet2_last.pth"))

        # best selection (conservative): by train_loss only (TEST is monitoring)
        if train_loss < best_by_trainloss:
            best_by_trainloss = train_loss
            torch.save({
                "model": u2.state_dict(),
                "cfg": asdict(cfg),
                "epoch": epoch,
                "train_loss": train_loss,
                "test_metric_monitor": test_m,
            }, os.path.join(ckpt_dir, "unet2_best.pth"))

            # save best-time test outputs (so even if crash later, have figures)
            save_test_outputs(u1, u2, test_jpgs, os.path.join(run_dir, "paper_u2_test_best"), cfg, tag=f"U2_ep{epoch:03d}")

        print(
            f"[U2][{epoch:03d}] train_loss={train_loss:.4f} | "
            f"TEST(ap={test_m['ap']:.4f}, softdice={test_m['softdice']:.4f}, dice05={test_m['dice05']:.4f}, "
            f"bestF1={test_m['best_f1']:.4f}@{test_m['best_thr']:.2f})"
        )

    # logs/plots
    df = pd.DataFrame(rows)
    df.to_csv(os.path.join(run_dir, "unet2_log.csv"), index=False)
    plot_curves(df, os.path.join(run_dir, "unet2_curves.png"), "U-Net2 (t-embedding)")

    # load best and export final test outputs
    ck = torch.load(os.path.join(ckpt_dir, "unet2_best.pth"), map_location=cfg.device)
    u2.load_state_dict(ck["model"])
    u2.eval()

    save_test_outputs(u1, u2, test_jpgs, os.path.join(run_dir, "paper_u2_test"), cfg, tag="U2")

    print("[U2] Done.")
    print("[U2] Artifacts:", run_dir)


if __name__ == "__main__":
    main()







new Train U-net2

import os, json, math, random
from dataclasses import dataclass, asdict
from typing import List, Dict, Tuple

import numpy as np
import pandas as pd
import cv2
import torch
import torch.nn as nn
import torch.nn.functional as F
import matplotlib.pyplot as plt
from scipy.ndimage import distance_transform_edt


# =========================
# Config (edit here)
# =========================
@dataclass
class CFG:
    # U1 run directory (contains split.json, config.json, cache_u1_logits)
    run_dir: str = "./runs/exp_unet1"

    device: str = "cuda" if torch.cuda.is_available() else "cpu"
    seed: int = 42

    # U2 training
    batch: int = 4
    epochs: int = 40
    lr: float = 1e-3
    weight_decay: float = 1e-5

    # U2 model capacity (보수적 default)
    base: int = 32
    gn_groups: int = 16

    # refinement / time embedding
    K: int = 5
    lambda_schedule: Tuple[float, ...] = (0.0, 0.25, 0.5, 0.75, 0.9)

    # logit-EMA refinement at evaluation/inference
    beta_ema: float = 0.7

    # visualization threshold (paper)
    thr_vis: float = 0.5

    # output
    exp_name: str = "exp_unet2"

    # debug export
    export_debug_samples: bool = True
    export_debug_max_images: int = 12
    export_debug_patches_per_image: int = 6


# =========================
# Repro
# =========================
def set_seed(seed: int):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True

def worker_init_fn(worker_id: int):
    seed = torch.initial_seed() % (2**32)
    np.random.seed(seed + worker_id)
    random.seed(seed + worker_id)


# =========================
# Utils / IO
# =========================
def ensure_dir(p: str):
    os.makedirs(p, exist_ok=True)

def read_gray(path: str) -> np.ndarray:
    im = cv2.imread(path, cv2.IMREAD_GRAYSCALE)
    if im is None:
        raise FileNotFoundError(path)
    return im

def read_label_bgr(path: str) -> np.ndarray:
    im = cv2.imread(path, cv2.IMREAD_COLOR)
    if im is None:
        raise FileNotFoundError(path)
    return im

def crop_valid(img: np.ndarray, crop_h: int, crop_w: int) -> np.ndarray:
    return img[:crop_h, :crop_w]

def to_u8(img01: np.ndarray) -> np.ndarray:
    return np.clip(img01 * 255.0, 0, 255).astype(np.uint8)

def overlay_boundary(gray01: np.ndarray, gt01: np.ndarray, pred01: np.ndarray) -> np.ndarray:
    g = to_u8(gray01)
    rgb = cv2.cvtColor(g, cv2.COLOR_GRAY2BGR)
    rgb[gt01 > 0] = (0, 0, 255)    # GT red
    rgb[pred01 > 0] = (0, 255, 0)  # Pred green
    return rgb


# =========================
# Preprocessing: MUST match U1 config (for reproducibility)
# =========================
def apply_clahe(img01: np.ndarray, clip: float, tile: int) -> np.ndarray:
    img8 = np.clip(img01 * 255.0, 0, 255).astype(np.uint8)
    clahe = cv2.createCLAHE(clipLimit=clip, tileGridSize=(tile, tile))
    out = clahe.apply(img8).astype(np.float32) / 255.0
    return out

def estimate_background(img01: np.ndarray, k: int) -> np.ndarray:
    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (k, k))
    img8 = np.clip(img01 * 255.0, 0, 255).astype(np.uint8)
    bg8 = cv2.morphologyEx(img8, cv2.MORPH_OPEN, kernel)
    return bg8.astype(np.float32) / 255.0

def robust_rescale(x: np.ndarray, p_lo: float, p_hi: float) -> np.ndarray:
    lo = np.percentile(x, p_lo)
    hi = np.percentile(x, p_hi)
    if hi <= lo + 1e-8:
        return np.clip(x, 0, 1)
    y = (x - lo) / (hi - lo)
    return np.clip(y, 0, 1)

def preprocess_sem(img_gray: np.ndarray, cfg_u1: dict) -> np.ndarray:
    H = int(cfg_u1["crop_h"]); W = int(cfg_u1["crop_w"])
    x = crop_valid(img_gray, H, W).astype(np.float32) / 255.0
    if bool(cfg_u1["use_clahe"]):
        x = apply_clahe(x, float(cfg_u1["clahe_clip"]), int(cfg_u1["clahe_tile"]))
    bg = estimate_background(x, int(cfg_u1["bg_kernel"]))
    x = x - bg
    x = robust_rescale(x, float(cfg_u1["robust_p_lo"]), float(cfg_u1["robust_p_hi"]))
    return x.astype(np.float32)

def extract_red_boundary_mask(label_bgr: np.ndarray, cfg_u1: dict) -> np.ndarray:
    H = int(cfg_u1["crop_h"]); W = int(cfg_u1["crop_w"])
    r_thr = int(cfg_u1["r_thr"]); g_thr = int(cfg_u1["g_thr"]); b_thr = int(cfg_u1["b_thr"])
    lab = crop_valid(label_bgr, H, W)
    B, G, R = lab[...,0], lab[...,1], lab[...,2]
    m = (R > r_thr) & (G < g_thr) & (B < b_thr)
    return m.astype(np.uint8)


# =========================
# Boundary field target (soft target, stable)
# =========================
def boundary_field_from_mask(mask01: np.ndarray, sigma: float) -> np.ndarray:
    boundary = (mask01 > 0)
    inv = ~boundary
    d = distance_transform_edt(inv).astype(np.float32)
    field = np.exp(-(d*d) / (2.0*sigma*sigma)).astype(np.float32)
    return field  # [0,1]


# =========================
# Patch utilities
# =========================
def compute_offsets(L: int, patch: int, stride: int) -> List[int]:
    max0 = L - patch
    if max0 < 0:
        return [0]
    offs = list(range(0, max0 + 1, stride))
    if offs[-1] != max0:
        offs.append(max0)
    return offs

def rot_k(a: np.ndarray, k: int) -> np.ndarray:
    return np.rot90(a, k).copy()


# =========================
# Loss / metric
# =========================
def soft_dice_loss(logits: torch.Tensor, target: torch.Tensor, eps: float = 1e-6) -> torch.Tensor:
    prob = torch.sigmoid(logits)
    num = 2.0 * (prob * target).sum(dim=(2,3))
    den = (prob * prob).sum(dim=(2,3)) + (target * target).sum(dim=(2,3)) + eps
    return (1.0 - (num / den)).mean()

def bce_dice_loss(logits: torch.Tensor, target: torch.Tensor) -> Tuple[torch.Tensor, float, float]:
    bce = F.binary_cross_entropy_with_logits(logits, target)
    d = soft_dice_loss(logits, target)
    return bce + d, float(bce.item()), float(d.item())

@torch.no_grad()
def dice_binary(pred: torch.Tensor, gt: torch.Tensor, eps: float = 1e-6) -> float:
    pred = pred.float()
    gt = gt.float()
    inter = (pred * gt).sum(dim=(2,3))
    den = pred.sum(dim=(2,3)) + gt.sum(dim=(2,3)) + eps
    return float(((2.0 * inter) / den).mean().item())


# =========================
# t-UNet (GroupNorm + FiLM)
# =========================
class SinusoidalTimeEmbedding(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        self.dim = dim
    def forward(self, t: torch.Tensor) -> torch.Tensor:
        if t.dim() != 1: t = t.view(-1)
        half = self.dim // 2
        freqs = torch.exp(-math.log(10000) * torch.arange(0, half, device=t.device).float() / max(half-1,1))
        ang = t.float()[:, None] * freqs[None, :]
        emb = torch.cat([torch.sin(ang), torch.cos(ang)], dim=1)
        if self.dim % 2 == 1:
            emb = F.pad(emb, (0,1))
        return emb

class FiLM(nn.Module):
    def __init__(self, time_dim: int, channels: int):
        super().__init__()
        self.mlp = nn.Sequential(nn.SiLU(), nn.Linear(time_dim, channels*2))
    def forward(self, x: torch.Tensor, t_emb: torch.Tensor) -> torch.Tensor:
        gb = self.mlp(t_emb)
        g, b = gb.chunk(2, dim=1)
        g = g[:, :, None, None]
        b = b[:, :, None, None]
        return x * (1.0 + g) + b

class ResBlockT(nn.Module):
    def __init__(self, in_ch, out_ch, groups: int, time_dim: int):
        super().__init__()
        self.c1 = nn.Conv2d(in_ch, out_ch, 3, padding=1, bias=False)
        self.g1 = nn.GroupNorm(num_groups=min(groups, out_ch), num_channels=out_ch)
        self.f1 = FiLM(time_dim, out_ch)

        self.c2 = nn.Conv2d(out_ch, out_ch, 3, padding=1, bias=False)
        self.g2 = nn.GroupNorm(num_groups=min(groups, out_ch), num_channels=out_ch)
        self.f2 = FiLM(time_dim, out_ch)

        self.act = nn.SiLU(inplace=True)
        self.skip = None
        if in_ch != out_ch:
            self.skip = nn.Conv2d(in_ch, out_ch, 1, bias=False)

    def forward(self, x, t_emb):
        h = self.c1(x)
        h = self.act(self.f1(self.g1(h), t_emb))
        h = self.c2(h)
        h = self.f2(self.g2(h), t_emb)
        s = x if self.skip is None else self.skip(x)
        return self.act(h + s)

class DownT(nn.Module):
    def __init__(self, in_ch, out_ch, groups: int, time_dim: int):
        super().__init__()
        self.pool = nn.MaxPool2d(2)
        self.block = ResBlockT(in_ch, out_ch, groups, time_dim)
    def forward(self, x, t_emb):
        return self.block(self.pool(x), t_emb)

class UpT(nn.Module):
    def __init__(self, in_ch, out_ch, groups: int, time_dim: int):
        super().__init__()
        self.up = nn.Upsample(scale_factor=2, mode="bilinear", align_corners=False)
        self.block = ResBlockT(in_ch, out_ch, groups, time_dim)
    def forward(self, x1, x2, t_emb):
        x1 = self.up(x1)
        dy = x2.size(2) - x1.size(2)
        dx = x2.size(3) - x1.size(3)
        x1 = F.pad(x1, [dx//2, dx-dx//2, dy//2, dy-dy//2])
        return self.block(torch.cat([x2, x1], dim=1), t_emb)

class TResUNet(nn.Module):
    def __init__(self, in_channels=2, out_channels=1, base=32, groups=16, time_dim=128):
        super().__init__()
        self.time_mlp = nn.Sequential(
            SinusoidalTimeEmbedding(time_dim),
            nn.Linear(time_dim, time_dim),
            nn.SiLU(),
            nn.Linear(time_dim, time_dim),
        )
        self.inc = ResBlockT(in_channels, base, groups, time_dim)
        self.d1 = DownT(base, base*2, groups, time_dim)
        self.d2 = DownT(base*2, base*4, groups, time_dim)
        self.d3 = DownT(base*4, base*8, groups, time_dim)
        self.d4 = DownT(base*8, base*16, groups, time_dim)

        self.u1 = UpT(base*16 + base*8, base*8, groups, time_dim)
        self.u2 = UpT(base*8 + base*4, base*4, groups, time_dim)
        self.u3 = UpT(base*4 + base*2, base*2, groups, time_dim)
        self.u4 = UpT(base*2 + base, base, groups, time_dim)

        self.outc = nn.Conv2d(base, out_channels, 1)

    def forward(self, x, t):
        if t.dim() != 1: t = t.view(-1)
        t_emb = self.time_mlp(t)
        x1 = self.inc(x, t_emb)
        x2 = self.d1(x1, t_emb)
        x3 = self.d2(x2, t_emb)
        x4 = self.d3(x3, t_emb)
        x5 = self.d4(x4, t_emb)
        x = self.u1(x5, x4, t_emb)
        x = self.u2(x, x3, t_emb)
        x = self.u3(x, x2, t_emb)
        x = self.u4(x, x1, t_emb)
        return self.outc(x)  # logits


# =========================
# Dataset (uses cached U1 logits)
# =========================
class U2PatchDS(torch.utils.data.Dataset):
    def __init__(self, jpg_paths: List[str], cache_dir: str, cfg_u1: dict, patch: int, stride: int, K: int, lam_sched: Tuple[float,...]):
        self.jpg_paths = jpg_paths
        self.cache_dir = cache_dir
        self.cfg_u1 = cfg_u1
        self.patch = patch
        self.stride = stride
        self.K = K
        self.lam = list(lam_sched)

        self.cache_img: Dict[str, Dict[str, np.ndarray]] = {}
        self.index: List[Tuple[int,int,int,int]] = []

        for i, p in enumerate(jpg_paths):
            img = preprocess_sem(read_gray(p), cfg_u1)
            ys = compute_offsets(img.shape[0], patch, stride)
            xs = compute_offsets(img.shape[1], patch, stride)
            for y in ys:
                for x in xs:
                    for rk in (0,1,2,3):
                        self.index.append((i,y,x,rk))

    def __len__(self): return len(self.index)

    def _load(self, i: int):
        p = self.jpg_paths[i]
        if p in self.cache_img:
            return
        base = os.path.splitext(os.path.basename(p))[0]
        npz = np.load(os.path.join(self.cache_dir, f"{base}.npz"))
        L0 = npz["logits"].astype(np.float32)  # stored float16 -> float32
        img01 = preprocess_sem(read_gray(p), self.cfg_u1)

        lab = read_label_bgr(os.path.splitext(p)[0] + ".png")
        gt = extract_red_boundary_mask(lab, self.cfg_u1).astype(np.uint8)

        sigma = 2.0  # conservative default
        field = boundary_field_from_mask(gt, sigma).astype(np.float32)

        self.cache_img[p] = {"img": img01, "L0": L0, "gt": gt, "field": field}

    def __getitem__(self, idx: int):
        i,y,x,rk = self.index[idx]
        self._load(i)
        p = self.jpg_paths[i]
        c = self.cache_img[p]

        img = c["img"][y:y+self.patch, x:x+self.patch]
        L0  = c["L0"][y:y+self.patch, x:x+self.patch]
        gt  = c["gt"][y:y+self.patch, x:x+self.patch]
        field = c["field"][y:y+self.patch, x:x+self.patch]

        img = rot_k(img, rk)
        L0  = rot_k(L0, rk)
        gt  = rot_k(gt, rk)
        field = rot_k(field, rk)

        I = torch.from_numpy(img[None].astype(np.float32))          # (1,H,W)
        L0t = torch.from_numpy(L0[None].astype(np.float32))         # (1,H,W)
        Ybin = torch.from_numpy(gt[None].astype(np.float32))        # (1,H,W)
        Yfield = torch.from_numpy(field[None].astype(np.float32))   # (1,H,W)
        return I, L0t, Ybin, Yfield


# =========================
# Refinement: logit-EMA
# =========================
@torch.no_grad()
def refine_logit_ema(unet2: nn.Module, I: torch.Tensor, L0: torch.Tensor, K: int, beta: float) -> torch.Tensor:
    """
    I:  (B,1,H,W)
    L0: (B,1,H,W) logits from U1
    Returns final probability Pf (B,1,H,W)
    """
    L = L0.clone()
    for t in range(K):
        P = torch.sigmoid(L)
        x = torch.cat([I, P], dim=1)
        tt = torch.full((I.size(0),), float(t), device=I.device)
        dL = unet2(x, tt)  # update logits
        L = (1.0 - beta) * L + beta * dL
    return torch.sigmoid(L)

@torch.no_grad()
def eval_unet2(unet2: nn.Module, loader, cfg: CFG) -> Dict[str, float]:
    unet2.eval()
    dices = []
    for I, L0, Ybin, Yfield in loader:
        I = I.to(cfg.device)
        L0 = L0.to(cfg.device)
        Y = Ybin.to(cfg.device)
        Pf = refine_logit_ema(unet2, I, L0, cfg.K, cfg.beta_ema)
        pred = (Pf > cfg.thr_vis).float()
        dices.append(dice_binary(pred, (Y > 0.5).float()))
    return {"dice": float(np.mean(dices))}


# =========================
# Paper-friendly exports (full image inference)
# =========================
@torch.no_grad()
def infer_full_prob_unet2(unet2: nn.Module, img01: np.ndarray, L0: np.ndarray, cfg: CFG) -> np.ndarray:
    """
    img01: (H,W) float [0,1]
    L0:    (H,W) float logits
    returns Pf: (H,W) float prob
    """
    I = torch.from_numpy(img01[None,None].astype(np.float32)).to(cfg.device)
    L0t = torch.from_numpy(L0[None,None].astype(np.float32)).to(cfg.device)
    Pf = refine_logit_ema(unet2, I, L0t, cfg.K, cfg.beta_ema)[0,0].detach().cpu().numpy()
    return Pf.astype(np.float32)

@torch.no_grad()
def save_paper_outputs_unet2(unet2: nn.Module, jpg_paths: List[str], cache_dir: str, out_dir: str, cfg_u1: dict, cfg: CFG, tag: str):
    """
    Save per image:
      - preprocessed input (I)
      - GT thin boundary mask (GT)
      - predicted binary mask (P) (thr_vis)
      - overlay (GT red, Pred green)
      - (optional) prob map as 8-bit for quick viewing
    """
    ensure_dir(out_dir)
    unet2.eval()

    for p in jpg_paths[:cfg.export_debug_max_images]:
        base = os.path.splitext(os.path.basename(p))[0]

        # load cached U1 logits
        npz = np.load(os.path.join(cache_dir, f"{base}.npz"))
        L0 = npz["logits"].astype(np.float32)

        # preprocess input exactly as U1
        img01 = preprocess_sem(read_gray(p), cfg_u1)

        # GT (thin expert label)
        lab = read_label_bgr(os.path.splitext(p)[0] + ".png")
        gt = extract_red_boundary_mask(lab, cfg_u1).astype(np.uint8)

        # inference
        Pf = infer_full_prob_unet2(unet2, img01, L0, cfg)
        pred = (Pf > cfg.thr_vis).astype(np.uint8)
        ov = overlay_boundary(img01, gt, pred)

        # save
        cv2.imwrite(os.path.join(out_dir, f"{base}_{tag}_I.png"), to_u8(img01))
        cv2.imwrite(os.path.join(out_dir, f"{base}_{tag}_GT.png"), (gt * 255).astype(np.uint8))
        cv2.imwrite(os.path.join(out_dir, f"{base}_{tag}_P.png"), (pred * 255).astype(np.uint8))
        cv2.imwrite(os.path.join(out_dir, f"{base}_{tag}_overlay.png"), ov)

        # prob map (for debugging; not mandatory for paper)
        cv2.imwrite(os.path.join(out_dir, f"{base}_{tag}_Pf.png"), to_u8(Pf))


# =========================
# Debug: export preproc inputs + patches (U2 view)
# =========================
def save_preproc_and_patches_u2(
    jpg_paths: List[str],
    cache_dir: str,
    out_dir: str,
    cfg_u1: dict,
    cfg: CFG,
    split_name: str,
    patches_per_image: int = 6,
):
    """
    Save:
      - full preprocessed SEM
      - full P0 = sigmoid(U1 logits)
      - full GT thin and GT field
      - random patches: (I, P0, field)
    """
    ensure_dir(out_dir)
    full_dir = os.path.join(out_dir, split_name, "full")
    patch_dir = os.path.join(out_dir, split_name, "patches")
    ensure_dir(full_dir); ensure_dir(patch_dir)

    rng = np.random.RandomState(cfg.seed + 999)

    H = int(cfg_u1["crop_h"]); W = int(cfg_u1["crop_w"])
    patch = int(cfg_u1["patch"]); stride = int(cfg_u1["stride"])

    for p in jpg_paths[:cfg.export_debug_max_images]:
        base = os.path.splitext(os.path.basename(p))[0]
        npz = np.load(os.path.join(cache_dir, f"{base}.npz"))
        L0 = npz["logits"].astype(np.float32)

        img01 = preprocess_sem(read_gray(p), cfg_u1)
        P0 = 1.0 / (1.0 + np.exp(-L0))

        lab = read_label_bgr(os.path.splitext(p)[0] + ".png")
        gt = extract_red_boundary_mask(lab, cfg_u1).astype(np.uint8)
        field = boundary_field_from_mask(gt, sigma=2.0)

        cv2.imwrite(os.path.join(full_dir, f"{base}_I.png"), to_u8(img01))
        cv2.imwrite(os.path.join(full_dir, f"{base}_P0.png"), to_u8(P0))
        cv2.imwrite(os.path.join(full_dir, f"{base}_GT_thin.png"), (gt * 255).astype(np.uint8))
        cv2.imwrite(os.path.join(full_dir, f"{base}_GT_field.png"), to_u8(field))

        ys = compute_offsets(H, patch, stride)
        xs = compute_offsets(W, patch, stride)
        coords = [(y,x) for y in ys for x in xs]
        if len(coords) == 0:
            continue

        sel = rng.choice(len(coords), size=min(patches_per_image, len(coords)), replace=False)
        for j, si in enumerate(sel):
            y, x = coords[int(si)]
            pI = img01[y:y+patch, x:x+patch]
            pP0 = P0[y:y+patch, x:x+patch]
            pF = field[y:y+patch, x:x+patch]
            cv2.imwrite(os.path.join(patch_dir, f"{base}_p{j:02d}_I.png"), to_u8(pI))
            cv2.imwrite(os.path.join(patch_dir, f"{base}_p{j:02d}_P0.png"), to_u8(pP0))
            cv2.imwrite(os.path.join(patch_dir, f"{base}_p{j:02d}_F.png"), to_u8(pF))


# =========================
# Plotting
# =========================
def plot_curves(df_tr: pd.DataFrame, df_va: pd.DataFrame, out_png: str, title: str):
    plt.figure()
    plt.plot(df_tr["epoch"], df_tr["loss"], label="train loss")
    plt.plot(df_va["epoch"], df_va["dice"], label="val dice")
    plt.xlabel("epoch"); plt.legend(); plt.title(title); plt.tight_layout()
    plt.savefig(out_png, dpi=200)
    plt.close()

def plot_scatter(df_tr: pd.DataFrame, df_va: pd.DataFrame, out_png: str):
    plt.figure()
    plt.scatter(df_tr["loss"], df_va["dice"])
    plt.xlabel("train loss"); plt.ylabel("val dice")
    plt.tight_layout()
    plt.savefig(out_png, dpi=200)
    plt.close()


# =========================
# Main
# =========================
def main():
    cfg = CFG()
    set_seed(cfg.seed)

    # --- load U1 run artifacts
    run1 = cfg.run_dir
    with open(os.path.join(run1, "split.json"), "r", encoding="utf-8") as f:
        split = json.load(f)
    with open(os.path.join(run1, "config.json"), "r", encoding="utf-8") as f:
        cfg_u1 = json.load(f)

    root = cfg_u1["root"]
    train_dir = cfg_u1["train_dir"]
    test_dir = cfg_u1["test_dir"]

    train_folder = os.path.join(root, train_dir)
    test_folder = os.path.join(root, test_dir)

    train_paths = [os.path.join(train_folder, fn) for fn in split["train_files"]]
    val_paths   = [os.path.join(train_folder, fn) for fn in split["val_files"]]
    test_paths  = [os.path.join(test_folder,  fn) for fn in split["test_files"]]

    cache_root = os.path.join(run1, "cache_u1_logits")
    cache_train = os.path.join(cache_root, "train")
    cache_val   = os.path.join(cache_root, "val")
    cache_test  = os.path.join(cache_root, "test")

    # --- create U2 run dir
    run2 = os.path.join("runs", cfg.exp_name)
    ckpt_dir = os.path.join(run2, "checkpoints")
    ensure_dir(run2)
    ensure_dir(ckpt_dir)

    # Save merged config for reproducibility
    merged = {"cfg_u2": asdict(cfg), "cfg_u1": cfg_u1, "split": split}
    with open(os.path.join(run2, "config_merged.json"), "w", encoding="utf-8") as f:
        json.dump(merged, f, indent=2)

    # Optional: export debug inputs for U2 (I/P0/field + patches)
    if cfg.export_debug_samples:
        dbg_dir = os.path.join(run2, "debug_inputs_u2")
        save_preproc_and_patches_u2(train_paths, cache_train, dbg_dir, cfg_u1, cfg, "train", cfg.export_debug_patches_per_image)
        save_preproc_and_patches_u2(val_paths,   cache_val,   dbg_dir, cfg_u1, cfg, "val",   cfg.export_debug_patches_per_image)
        save_preproc_and_patches_u2(test_paths,  cache_test,  dbg_dir, cfg_u1, cfg, "test",  cfg.export_debug_patches_per_image)

    # data
    g = torch.Generator().manual_seed(cfg.seed)

    patch = int(cfg_u1["patch"])
    stride = int(cfg_u1["stride"])

    ds_tr = U2PatchDS(train_paths, cache_train, cfg_u1, patch, stride, cfg.K, cfg.lambda_schedule)
    ds_va = U2PatchDS(val_paths,   cache_val,   cfg_u1, patch, stride, cfg.K, cfg.lambda_schedule)

    dl_tr = torch.utils.data.DataLoader(ds_tr, batch_size=cfg.batch, shuffle=True, num_workers=4,
                                        pin_memory=True, worker_init_fn=worker_init_fn, generator=g)
    dl_va = torch.utils.data.DataLoader(ds_va, batch_size=cfg.batch, shuffle=False, num_workers=2,
                                        pin_memory=True, worker_init_fn=worker_init_fn, generator=g)

    print("Train images:", len(train_paths), "Val images:", len(val_paths), "Test images:", len(test_paths))
    print("Train patches:", len(ds_tr), "Val patches:", len(ds_va))

    # model
    unet2 = TResUNet(in_channels=2, out_channels=1, base=cfg.base, groups=cfg.gn_groups, time_dim=128).to(cfg.device)
    opt = torch.optim.AdamW(unet2.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)

    lam = list(cfg.lambda_schedule)
    assert len(lam) == cfg.K, "lambda_schedule length must equal K"

    best_dice = -1.0
    rows_tr, rows_va = [], []

    for epoch in range(1, cfg.epochs + 1):
        unet2.train()
        losses, bces, dls = [], [], []

        for I, L0, Ybin, Yfield in dl_tr:
            I = I.to(cfg.device)
            L0 = L0.to(cfg.device)
            Yf = Yfield.to(cfg.device)

            # sample timestep
            t_int = torch.randint(low=0, high=cfg.K, size=(I.size(0),), device=cfg.device)
            lam_t = torch.tensor([lam[int(x)] for x in t_int.tolist()], device=cfg.device).view(-1,1,1,1)

            # build Pt = mix(P0, Y_field)
            P0 = torch.sigmoid(L0)
            Pt = (1.0 - lam_t) * P0 + lam_t * Yf

            x = torch.cat([I, Pt], dim=1)
            opt.zero_grad(set_to_none=True)
            out_logits = unet2(x, t_int.float())

            # supervise toward field (stable)
            loss, bce_v, dl_v = bce_dice_loss(out_logits, Yf)
            loss.backward()
            opt.step()

            losses.append(float(loss.item()))
            bces.append(float(bce_v))
            dls.append(float(dl_v))

        tr = {"epoch": epoch, "loss": float(np.mean(losses)), "bce": float(np.mean(bces)), "dice_loss": float(np.mean(dls))}
        va = eval_unet2(unet2, dl_va, cfg)
        va["epoch"] = epoch

        rows_tr.append(tr)
        rows_va.append(va)

        # save last
        last_path = os.path.join(ckpt_dir, "unet2_last.pth")
        torch.save({
            "model": unet2.state_dict(),
            "cfg_u2": asdict(cfg),
            "cfg_u1": cfg_u1,
            "epoch": epoch,
            "val_metric": va,
            "seed": cfg.seed,
        }, last_path)

        # save best by val dice (thin GT-based metric)
        if va["dice"] > best_dice:
            best_dice = va["dice"]
            best_path = os.path.join(ckpt_dir, "unet2_best.pth")
            torch.save({
                "model": unet2.state_dict(),
                "cfg_u2": asdict(cfg),
                "cfg_u1": cfg_u1,
                "epoch": epoch,
                "val_metric": va,
                "seed": cfg.seed,
            }, best_path)

        print(f"[U2][{epoch:03d}] train_loss={tr['loss']:.4f} val_dice={va['dice']:.4f}")

    # logs
    df_tr = pd.DataFrame(rows_tr)
    df_va = pd.DataFrame(rows_va)
    df_tr.to_csv(os.path.join(run2, "unet2_train.csv"), index=False)
    df_va.to_csv(os.path.join(run2, "unet2_val.csv"), index=False)

    plot_curves(df_tr, df_va, os.path.join(run2, "unet2_curves.png"), "U-Net2 (t-embedding)")
    plot_scatter(df_tr, df_va, os.path.join(run2, "unet2_scatter_loss_vs_valdice.png"))

    # --- load BEST and export paper-friendly outputs for VAL/TEST
    best_ckpt_path = os.path.join(ckpt_dir, "unet2_best.pth")
    ckpt = torch.load(best_ckpt_path, map_location=cfg.device)
    unet2.load_state_dict(ckpt["model"])
    unet2.eval()

    paper_val = os.path.join(run2, "paper_u2_val")
    paper_test = os.path.join(run2, "paper_u2_test")
    save_paper_outputs_unet2(unet2, val_paths,  cache_val,  paper_val,  cfg_u1, cfg, tag="U2")
    save_paper_outputs_unet2(unet2, test_paths, cache_test, paper_test, cfg_u1, cfg, tag="U2")

    print("U-Net2 done.")
    print("Artifacts saved to:", run2)


if __name__ == "__main__":
    main()





Train U-net2


import os, glob, json, math, random
from dataclasses import dataclass, asdict
from typing import List, Dict, Tuple

import numpy as np
import pandas as pd
import cv2
import torch
import torch.nn as nn
import torch.nn.functional as F
import matplotlib.pyplot as plt
from scipy.ndimage import distance_transform_edt


# =========================
# Config (keep aligned with U1 run_dir)
# =========================
@dataclass
class CFG:
    run_dir: str = "./runs/exp_unet1"   # <-- U1 run dir (contains split.json, config.json, cache_u1_logits)
    device: str = "cuda" if torch.cuda.is_available() else "cpu"
    seed: int = 42

    # U2 training
    batch: int = 4
    epochs: int = 40
    lr: float = 1e-3
    weight_decay: float = 1e-5

    # U2 model capacity (보수적으로 32 추천; 필요시 64로)
    base: int = 32
    gn_groups: int = 16

    # t embedding / refinement
    K: int = 5
    lambda_schedule: Tuple[float, ...] = (0.0, 0.25, 0.5, 0.75, 0.9)

    # logit-EMA in evaluation refinement
    beta_ema: float = 0.7

    # eval/visualize
    thr_vis: float = 0.5
    exp_name: str = "exp_unet2"


def set_seed(seed: int):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True

def worker_init_fn(worker_id: int):
    seed = torch.initial_seed() % (2**32)
    np.random.seed(seed + worker_id)
    random.seed(seed + worker_id)

def read_gray(path: str) -> np.ndarray:
    im = cv2.imread(path, cv2.IMREAD_GRAYSCALE)
    if im is None:
        raise FileNotFoundError(path)
    return im

def read_label_bgr(path: str) -> np.ndarray:
    im = cv2.imread(path, cv2.IMREAD_COLOR)
    if im is None:
        raise FileNotFoundError(path)
    return im

def overlay_boundary(gray01: np.ndarray, gt01: np.ndarray, pred01: np.ndarray) -> np.ndarray:
    g = np.clip(gray01 * 255.0, 0, 255).astype(np.uint8)
    rgb = cv2.cvtColor(g, cv2.COLOR_GRAY2BGR)
    rgb[gt01 > 0] = (0, 0, 255)   # GT red
    rgb[pred01 > 0] = (0, 255, 0) # Pred green
    return rgb

def crop_valid(img: np.ndarray, crop_h: int, crop_w: int) -> np.ndarray:
    return img[:crop_h, :crop_w]

def apply_clahe(img01: np.ndarray, clip: float, tile: int) -> np.ndarray:
    img8 = np.clip(img01 * 255.0, 0, 255).astype(np.uint8)
    clahe = cv2.createCLAHE(clipLimit=clip, tileGridSize=(tile, tile))
    out = clahe.apply(img8).astype(np.float32) / 255.0
    return out

def estimate_background(img01: np.ndarray, k: int) -> np.ndarray:
    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (k, k))
    img8 = np.clip(img01 * 255.0, 0, 255).astype(np.uint8)
    bg8 = cv2.morphologyEx(img8, cv2.MORPH_OPEN, kernel)
    return bg8.astype(np.float32) / 255.0

def robust_rescale(x: np.ndarray, p_lo: float, p_hi: float) -> np.ndarray:
    lo = np.percentile(x, p_lo)
    hi = np.percentile(x, p_hi)
    if hi <= lo + 1e-8:
        return np.clip(x, 0, 1)
    y = (x - lo) / (hi - lo)
    return np.clip(y, 0, 1)

def preprocess_sem(img_gray: np.ndarray, cfg_u1: dict) -> np.ndarray:
    # use EXACT U1 preprocessing settings for reproducibility
    H = int(cfg_u1["crop_h"]); W = int(cfg_u1["crop_w"])
    x = crop_valid(img_gray, H, W).astype(np.float32) / 255.0
    if bool(cfg_u1["use_clahe"]):
        x = apply_clahe(x, float(cfg_u1["clahe_clip"]), int(cfg_u1["clahe_tile"]))
    bg = estimate_background(x, int(cfg_u1["bg_kernel"]))
    x = x - bg
    x = robust_rescale(x, float(cfg_u1["robust_p_lo"]), float(cfg_u1["robust_p_hi"]))
    return x.astype(np.float32)

def extract_red_boundary_mask(label_bgr: np.ndarray, cfg_u1: dict) -> np.ndarray:
    H = int(cfg_u1["crop_h"]); W = int(cfg_u1["crop_w"])
    r_thr = int(cfg_u1["r_thr"]); g_thr = int(cfg_u1["g_thr"]); b_thr = int(cfg_u1["b_thr"])
    lab = crop_valid(label_bgr, H, W)
    B, G, R = lab[...,0], lab[...,1], lab[...,2]
    m = (R > r_thr) & (G < g_thr) & (B < b_thr)
    return m.astype(np.uint8)

def boundary_field_from_mask(mask01: np.ndarray, sigma: float) -> np.ndarray:
    boundary = (mask01 > 0)
    inv = ~boundary
    d = distance_transform_edt(inv).astype(np.float32)
    field = np.exp(-(d*d) / (2.0*sigma*sigma)).astype(np.float32)
    return field  # [0,1]

def compute_offsets(L: int, patch: int, stride: int) -> List[int]:
    max0 = L - patch
    if max0 < 0:
        return [0]
    offs = list(range(0, max0 + 1, stride))
    if offs[-1] != max0:
        offs.append(max0)
    return offs

def rot_k(a: np.ndarray, k: int) -> np.ndarray:
    return np.rot90(a, k).copy()

def soft_dice_loss(logits: torch.Tensor, target: torch.Tensor, eps: float = 1e-6) -> torch.Tensor:
    prob = torch.sigmoid(logits)
    num = 2.0 * (prob * target).sum(dim=(2,3))
    den = (prob * prob).sum(dim=(2,3)) + (target * target).sum(dim=(2,3)) + eps
    return (1.0 - (num / den)).mean()

def bce_dice_loss(logits: torch.Tensor, target: torch.Tensor) -> Tuple[torch.Tensor, float, float]:
    bce = F.binary_cross_entropy_with_logits(logits, target)
    d = soft_dice_loss(logits, target)
    return bce + d, float(bce.item()), float(d.item())

@torch.no_grad()
def dice_binary(pred: torch.Tensor, gt: torch.Tensor, eps: float = 1e-6) -> float:
    pred = pred.float()
    gt = gt.float()
    inter = (pred * gt).sum(dim=(2,3))
    den = pred.sum(dim=(2,3)) + gt.sum(dim=(2,3)) + eps
    return float(((2.0 * inter) / den).mean().item())


# =========================
# t-UNet (GroupNorm + FiLM)
# =========================
class SinusoidalTimeEmbedding(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        self.dim = dim
    def forward(self, t: torch.Tensor) -> torch.Tensor:
        if t.dim() != 1: t = t.view(-1)
        half = self.dim // 2
        freqs = torch.exp(-math.log(10000) * torch.arange(0, half, device=t.device).float() / max(half-1,1))
        ang = t.float()[:, None] * freqs[None, :]
        emb = torch.cat([torch.sin(ang), torch.cos(ang)], dim=1)
        if self.dim % 2 == 1:
            emb = F.pad(emb, (0,1))
        return emb

class FiLM(nn.Module):
    def __init__(self, time_dim: int, channels: int):
        super().__init__()
        self.mlp = nn.Sequential(nn.SiLU(), nn.Linear(time_dim, channels*2))
    def forward(self, x: torch.Tensor, t_emb: torch.Tensor) -> torch.Tensor:
        gb = self.mlp(t_emb)
        g, b = gb.chunk(2, dim=1)
        g = g[:, :, None, None]
        b = b[:, :, None, None]
        return x * (1.0 + g) + b

class ResBlockT(nn.Module):
    def __init__(self, in_ch, out_ch, groups: int, time_dim: int):
        super().__init__()
        self.c1 = nn.Conv2d(in_ch, out_ch, 3, padding=1, bias=False)
        self.g1 = nn.GroupNorm(num_groups=min(groups, out_ch), num_channels=out_ch)
        self.f1 = FiLM(time_dim, out_ch)

        self.c2 = nn.Conv2d(out_ch, out_ch, 3, padding=1, bias=False)
        self.g2 = nn.GroupNorm(num_groups=min(groups, out_ch), num_channels=out_ch)
        self.f2 = FiLM(time_dim, out_ch)

        self.act = nn.SiLU(inplace=True)
        self.skip = None
        if in_ch != out_ch:
            self.skip = nn.Conv2d(in_ch, out_ch, 1, bias=False)

    def forward(self, x, t_emb):
        h = self.c1(x)
        h = self.act(self.f1(self.g1(h), t_emb))
        h = self.c2(h)
        h = self.f2(self.g2(h), t_emb)
        s = x if self.skip is None else self.skip(x)
        return self.act(h + s)

class DownT(nn.Module):
    def __init__(self, in_ch, out_ch, groups: int, time_dim: int):
        super().__init__()
        self.pool = nn.MaxPool2d(2)
        self.block = ResBlockT(in_ch, out_ch, groups, time_dim)
    def forward(self, x, t_emb):
        return self.block(self.pool(x), t_emb)

class UpT(nn.Module):
    def __init__(self, in_ch, out_ch, groups: int, time_dim: int):
        super().__init__()
        self.up = nn.Upsample(scale_factor=2, mode="bilinear", align_corners=False)
        self.block = ResBlockT(in_ch, out_ch, groups, time_dim)
    def forward(self, x1, x2, t_emb):
        x1 = self.up(x1)
        dy = x2.size(2) - x1.size(2)
        dx = x2.size(3) - x1.size(3)
        x1 = F.pad(x1, [dx//2, dx-dx//2, dy//2, dy-dy//2])
        return self.block(torch.cat([x2, x1], dim=1), t_emb)

class TResUNet(nn.Module):
    def __init__(self, in_channels=2, out_channels=1, base=32, groups=16, time_dim=128):
        super().__init__()
        self.time_mlp = nn.Sequential(
            SinusoidalTimeEmbedding(time_dim),
            nn.Linear(time_dim, time_dim),
            nn.SiLU(),
            nn.Linear(time_dim, time_dim),
        )
        self.inc = ResBlockT(in_channels, base, groups, time_dim)
        self.d1 = DownT(base, base*2, groups, time_dim)
        self.d2 = DownT(base*2, base*4, groups, time_dim)
        self.d3 = DownT(base*4, base*8, groups, time_dim)
        self.d4 = DownT(base*8, base*16, groups, time_dim)

        self.u1 = UpT(base*16 + base*8, base*8, groups, time_dim)
        self.u2 = UpT(base*8 + base*4, base*4, groups, time_dim)
        self.u3 = UpT(base*4 + base*2, base*2, groups, time_dim)
        self.u4 = UpT(base*2 + base, base, groups, time_dim)

        self.outc = nn.Conv2d(base, out_channels, 1)

    def forward(self, x, t):
        if t.dim() != 1: t = t.view(-1)
        t_emb = self.time_mlp(t)
        x1 = self.inc(x, t_emb)
        x2 = self.d1(x1, t_emb)
        x3 = self.d2(x2, t_emb)
        x4 = self.d3(x3, t_emb)
        x5 = self.d4(x4, t_emb)
        x = self.u1(x5, x4, t_emb)
        x = self.u2(x, x3, t_emb)
        x = self.u3(x, x2, t_emb)
        x = self.u4(x, x1, t_emb)
        return self.outc(x)  # logits


# =========================
# Dataset (uses U1 logits cache)
# =========================
class U2PatchDS(torch.utils.data.Dataset):
    def __init__(self, jpg_paths: List[str], cache_dir: str, cfg_u1: dict, patch: int, stride: int, K: int, lam_sched: Tuple[float,...]):
        self.jpg_paths = jpg_paths
        self.cache_dir = cache_dir
        self.cfg_u1 = cfg_u1
        self.patch = patch
        self.stride = stride
        self.K = K
        self.lam = list(lam_sched)

        self.cache_img: Dict[str, Dict[str, np.ndarray]] = {}
        self.index: List[Tuple[int,int,int,int]] = []

        for i, p in enumerate(jpg_paths):
            img = preprocess_sem(read_gray(p), cfg_u1)
            ys = compute_offsets(img.shape[0], patch, stride)
            xs = compute_offsets(img.shape[1], patch, stride)
            for y in ys:
                for x in xs:
                    for rk in (0,1,2,3):
                        self.index.append((i,y,x,rk))

    def __len__(self): return len(self.index)

    def _load(self, i: int):
        p = self.jpg_paths[i]
        if p in self.cache_img:
            return
        base = os.path.splitext(os.path.basename(p))[0]
        npz = np.load(os.path.join(self.cache_dir, f"{base}.npz"))
        L0 = npz["logits"].astype(np.float32)  # (H,W) float16 stored -> float32 here
        img01 = preprocess_sem(read_gray(p), self.cfg_u1)

        lab = read_label_bgr(os.path.splitext(p)[0] + ".png")
        gt = extract_red_boundary_mask(lab, self.cfg_u1).astype(np.uint8)
        # soft target field (stable for thin labels)
        sigma = 2.0  # conservative default
        field = boundary_field_from_mask(gt, sigma).astype(np.float32)

        self.cache_img[p] = {"img": img01, "L0": L0, "gt": gt, "field": field}

    def __getitem__(self, idx: int):
        i,y,x,rk = self.index[idx]
        self._load(i)
        p = self.jpg_paths[i]
        c = self.cache_img[p]

        img = c["img"][y:y+self.patch, x:x+self.patch]
        L0  = c["L0"][y:y+self.patch, x:x+self.patch]
        gt  = c["gt"][y:y+self.patch, x:x+self.patch]
        field = c["field"][y:y+self.patch, x:x+self.patch]

        img = rot_k(img, rk)
        L0  = rot_k(L0, rk)
        gt  = rot_k(gt, rk)
        field = rot_k(field, rk)

        I = torch.from_numpy(img[None].astype(np.float32))      # (1,H,W)
        L0t = torch.from_numpy(L0[None].astype(np.float32))     # (1,H,W)
        Ybin = torch.from_numpy(gt[None].astype(np.float32))    # (1,H,W)
        Yfield = torch.from_numpy(field[None].astype(np.float32)) # (1,H,W)
        return I, L0t, Ybin, Yfield


# =========================
# Eval: refinement with logit-EMA
# =========================
@torch.no_grad()
def refine_logit_ema(unet2: nn.Module, I: torch.Tensor, L0: torch.Tensor, K: int, beta: float) -> torch.Tensor:
    """
    I: (B,1,H,W)
    L0: (B,1,H,W)
    returns P_final (B,1,H,W)
    """
    L = L0.clone()
    for t in range(K):
        P = torch.sigmoid(L)
        x = torch.cat([I, P], dim=1)
        tt = torch.full((I.size(0),), float(t), device=I.device)
        dL = unet2(x, tt)  # logits update (treated as "absolute-like" update)
        L = (1.0 - beta) * L + beta * dL
    return torch.sigmoid(L)

@torch.no_grad()
def eval_unet2(unet2: nn.Module, loader, cfg: CFG) -> Dict[str, float]:
    unet2.eval()
    dices = []
    for I, L0, Ybin, Yfield in loader:
        I = I.to(cfg.device)
        L0 = L0.to(cfg.device)
        Y = Ybin.to(cfg.device)
        Pf = refine_logit_ema(unet2, I, L0, cfg.K, cfg.beta_ema)
        pred = (Pf > cfg.thr_vis).float()
        dices.append(dice_binary(pred, (Y > 0.5).float()))
    return {"dice": float(np.mean(dices))}

@torch.no_grad()
def save_val_overlays(unet2: nn.Module, jpg_paths: List[str], cache_dir: str, out_dir: str, cfg_u1: dict, cfg: CFG):
    os.makedirs(out_dir, exist_ok=True)
    unet2.eval()
    for p in jpg_paths:
        base = os.path.splitext(os.path.basename(p))[0]
        npz = np.load(os.path.join(cache_dir, f"{base}.npz"))
        L0 = npz["logits"].astype(np.float32)
        img01 = preprocess_sem(read_gray(p), cfg_u1)

        lab = read_label_bgr(os.path.splitext(p)[0] + ".png")
        gt = extract_red_boundary_mask(lab, cfg_u1).astype(np.uint8)

        I = torch.from_numpy(img01[None,None].astype(np.float32)).to(cfg.device)
        L0t = torch.from_numpy(L0[None,None].astype(np.float32)).to(cfg.device)
        Pf = refine_logit_ema(unet2, I, L0t, cfg.K, cfg.beta_ema)[0,0].cpu().numpy()
        pred = (Pf > cfg.thr_vis).astype(np.uint8)

        ov = overlay_boundary(img01, gt, pred)
        cv2.imwrite(os.path.join(out_dir, f"{base}_U2_overlay.png"), ov)


# =========================
# Plotting
# =========================
def plot_curves(df_tr: pd.DataFrame, df_va: pd.DataFrame, out_png: str, title: str):
    plt.figure()
    plt.plot(df_tr["epoch"], df_tr["loss"], label="train loss")
    plt.plot(df_va["epoch"], df_va["dice"], label="val dice")
    plt.xlabel("epoch"); plt.legend(); plt.title(title); plt.tight_layout()
    plt.savefig(out_png, dpi=200)
    plt.close()

def plot_scatter(df_tr: pd.DataFrame, df_va: pd.DataFrame, out_png: str):
    plt.figure()
    plt.scatter(df_tr["loss"], df_va["dice"])
    plt.xlabel("train loss"); plt.ylabel("val dice")
    plt.tight_layout()
    plt.savefig(out_png, dpi=200)
    plt.close()


# =========================
# Main
# =========================
def main():
    cfg = CFG()
    set_seed(cfg.seed)

    # --- load U1 run artifacts
    run1 = cfg.run_dir
    with open(os.path.join(run1, "split.json"), "r", encoding="utf-8") as f:
        split = json.load(f)
    with open(os.path.join(run1, "config.json"), "r", encoding="utf-8") as f:
        cfg_u1 = json.load(f)

    root = cfg_u1["root"]
    train_dir = cfg_u1["train_dir"]
    test_dir = cfg_u1["test_dir"]

    train_folder = os.path.join(root, train_dir)
    test_folder = os.path.join(root, test_dir)

    train_paths = [os.path.join(train_folder, fn) for fn in split["train_files"]]
    val_paths   = [os.path.join(train_folder, fn) for fn in split["val_files"]]
    test_paths  = [os.path.join(test_folder, fn) for fn in split["test_files"]]

    cache_root = os.path.join(run1, "cache_u1_logits")
    cache_train = os.path.join(cache_root, "train")
    cache_val   = os.path.join(cache_root, "val")
    cache_test  = os.path.join(cache_root, "test")

    # --- create U2 run dir
    run2 = os.path.join("runs", cfg.exp_name)
    ckpt_dir = os.path.join(run2, "checkpoints")
    os.makedirs(ckpt_dir, exist_ok=True)
    os.makedirs(run2, exist_ok=True)

    # Save merged config for reproducibility
    merged = {"cfg_u2": asdict(cfg), "cfg_u1": cfg_u1, "split": split}
    with open(os.path.join(run2, "config_merged.json"), "w", encoding="utf-8") as f:
        json.dump(merged, f, indent=2)

    # data
    g = torch.Generator().manual_seed(cfg.seed)

    patch = int(cfg_u1["patch"])
    stride = int(cfg_u1["stride"])

    ds_tr = U2PatchDS(train_paths, cache_train, cfg_u1, patch, stride, cfg.K, cfg.lambda_schedule)
    ds_va = U2PatchDS(val_paths,   cache_val,   cfg_u1, patch, stride, cfg.K, cfg.lambda_schedule)

    dl_tr = torch.utils.data.DataLoader(ds_tr, batch_size=cfg.batch, shuffle=True, num_workers=4,
                                        pin_memory=True, worker_init_fn=worker_init_fn, generator=g)
    dl_va = torch.utils.data.DataLoader(ds_va, batch_size=cfg.batch, shuffle=False, num_workers=2,
                                        pin_memory=True, worker_init_fn=worker_init_fn, generator=g)

    print("Train images:", len(train_paths), "Val images:", len(val_paths), "Test images:", len(test_paths))
    print("Train patches:", len(ds_tr), "Val patches:", len(ds_va))

    # model
    unet2 = TResUNet(in_channels=2, out_channels=1, base=cfg.base, groups=cfg.gn_groups, time_dim=128).to(cfg.device)
    opt = torch.optim.AdamW(unet2.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)

    lam = list(cfg.lambda_schedule)
    assert len(lam) == cfg.K, "lambda_schedule length must equal K"

    best_dice = -1.0
    rows_tr, rows_va = [], []

    for epoch in range(1, cfg.epochs + 1):
        unet2.train()
        losses, bces, dices = [], [], []

        for I, L0, Ybin, Yfield in dl_tr:
            I = I.to(cfg.device)
            L0 = L0.to(cfg.device)
            Yf = Yfield.to(cfg.device)

            # sample timestep
            t_int = torch.randint(low=0, high=cfg.K, size=(I.size(0),), device=cfg.device)
            lam_t = torch.tensor([lam[int(x)] for x in t_int.tolist()], device=cfg.device).view(-1,1,1,1)

            # build Pt = mix(sigmoid(L0), Y_field) for stage-aware training
            P0 = torch.sigmoid(L0)
            Pt = (1.0 - lam_t) * P0 + lam_t * Yf

            x = torch.cat([I, Pt], dim=1)
            opt.zero_grad(set_to_none=True)
            out_logits = unet2(x, t_int.float())

            # conservative: supervise output logits toward field target
            loss, bce_v, dice_v = bce_dice_loss(out_logits, Yf)
            loss.backward()
            opt.step()

            losses.append(float(loss.item()))
            bces.append(bce_v)
            dices.append(dice_v)

        tr = {"epoch": epoch, "loss": float(np.mean(losses)), "bce": float(np.mean(bces)), "dice_loss": float(np.mean(dices))}
        va = eval_unet2(unet2, dl_va, cfg)
        va["epoch"] = epoch

        rows_tr.append(tr)
        rows_va.append(va)

        # save last
        last_path = os.path.join(ckpt_dir, "unet2_last.pth")
        torch.save({
            "model": unet2.state_dict(),
            "cfg_u2": asdict(cfg),
            "cfg_u1": cfg_u1,
            "epoch": epoch,
            "val_metric": va,
            "seed": cfg.seed,
        }, last_path)

        # save best
        if va["dice"] > best_dice:
            best_dice = va["dice"]
            best_path = os.path.join(ckpt_dir, "unet2_best.pth")
            torch.save({
                "model": unet2.state_dict(),
                "cfg_u2": asdict(cfg),
                "cfg_u1": cfg_u1,
                "epoch": epoch,
                "val_metric": va,
                "seed": cfg.seed,
            }, best_path)

        print(f"[U2][{epoch:03d}] train_loss={tr['loss']:.4f} val_dice={va['dice']:.4f}")

    # logs
    df_tr = pd.DataFrame(rows_tr)
    df_va = pd.DataFrame(rows_va)
    df_tr.to_csv(os.path.join(run2, "unet2_train.csv"), index=False)
    df_va.to_csv(os.path.join(run2, "unet2_val.csv"), index=False)

    plot_curves(df_tr, df_va, os.path.join(run2, "unet2_curves.png"), "U-Net2 (t-embedding)")
    plot_scatter(df_tr, df_va, os.path.join(run2, "unet2_scatter_loss_vs_valdice.png"))

    # overlays on val for qualitative check (best checkpoint)
    best_ckpt_path = os.path.join(ckpt_dir, "unet2_best.pth")
    ckpt = torch.load(best_ckpt_path, map_location=cfg.device)
    unet2.load_state_dict(ckpt["model"])
    unet2.eval()
    save_val_overlays(unet2, val_paths, cache_val, os.path.join(run2, "overlays_u2_val"), cfg_u1, cfg)

    print("U-Net2 done.")
    print("Artifacts saved to:", run2)


if __name__ == "__main__":
    main()