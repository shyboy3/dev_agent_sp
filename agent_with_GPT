# app_spectrum_agent_starter.py
# -----------------------------------------------------------------------------
# Spectrum Agent Starter (Single-file)
# - Text-only Ollama server
# - Preview (first file), multi-file analyze
# - Denoise (Hampel, Savitzky-Golay), Baseline (AsLS), Onset/Peaks
# - Manual mode & Agent mode (plan â†’ execute â†’ evaluate â†’ revise)
# - PDF report (Korean font fallback)
# - Simple chatbot (/chat)
# -----------------------------------------------------------------------------
# Requirements:
#   pip install fastapi uvicorn[standard] python-multipart matplotlib pillow requests
#   pip install pandas openpyxl scipy pyyaml
# -----------------------------------------------------------------------------

import os, io, re, json, base64, textwrap
from io import BytesIO
from typing import Optional, List, Tuple, Dict, Callable
from dataclasses import dataclass

import numpy as np
import pandas as pd
import requests

import matplotlib
matplotlib.use("Agg")
import matplotlib.pyplot as plt
from matplotlib.backends.backend_pdf import PdfPages
from matplotlib.font_manager import fontManager, findSystemFonts

from PIL import Image
from scipy.signal import find_peaks, savgol_filter
from scipy import sparse
from scipy.sparse.linalg import spsolve

from fastapi import FastAPI, UploadFile, File, Form, Header, HTTPException, Depends
from fastapi.responses import HTMLResponse, JSONResponse

# ===================== Config =====================
OLLAMA_HOST = os.environ.get("OLLAMA_HOST_TEXT", "http://localhost:11435")
TEXT_MODEL  = os.environ.get("TEXT_MODEL", "gpt-oss")  # e.g., "gpt-oss" or "gpt-oss:latest"
AUTH_TOKEN  = os.environ.get("LOCAL_AGENT_TOKEN")

MAX_AGENT_ITERS = int(os.environ.get("MAX_AGENT_ITERS", "3"))

app = FastAPI(title="Spectrum Agent Starter (Single-file)")

# ===================== Auth =====================
def require_auth(x_auth_token: Optional[str] = Header(None)):
    if AUTH_TOKEN and x_auth_token != AUTH_TOKEN:
        raise HTTPException(status_code=401, detail="Unauthorized")
    return True

# ===================== Ollama helpers =====================
def list_model_names(base: str) -> List[str]:
    try:
        r = requests.get(f"{base}/api/tags", timeout=10)
        r.raise_for_status()
        models = r.json().get("models", [])
        out=[]
        for m in models:
            if isinstance(m, dict):
                name = m.get("name") or m.get("model") or m.get("tag") or m.get("id")
                if name: out.append(name)
        return out
    except Exception:
        return []

def ensure_model_installed(model: str, base: str) -> str:
    """
    Return the concrete tag installed.
    If 'model' has no tag, match by prefix (e.g., 'gpt-oss' -> 'gpt-oss:latest').
    """
    names = list_model_names(base)
    if model in names:
        return model
    want = model.split(":", 1)[0]
    for n in names:
        if n.split(":",1)[0] == want:
            return n
    raise HTTPException(status_code=400, detail=f"Model '{model}' not found at {base}. Found: {names}")

def ollama_generate_once(model: str, prompt: str, base: str, num_predict=512) -> str:
    req = {
        "model": model,
        "prompt": prompt,
        "stream": False,
        "options": {"temperature": 0.2, "top_p": 0.8, "num_predict": num_predict},
        "keep_alive": "20m",
    }
    r = requests.post(f"{base}/api/generate", json=req, timeout=120)
    r.raise_for_status()
    return r.json().get("response","").strip()

# ===================== CSV/Excel parsing =====================
def smart_decode_csv(csv_bytes: bytes) -> str:
    if csv_bytes.startswith(b"\xff\xfe"): return csv_bytes.decode("utf-16le", errors="ignore")
    if csv_bytes.startswith(b"\xfe\xff"): return csv_bytes.decode("utf-16be", errors="ignore")
    if csv_bytes.startswith(b"\xef\xbb\xbf"): return csv_bytes.decode("utf-8-sig", errors="ignore")
    if b"\x00" in csv_bytes:
        for enc in ("utf-16le","utf-16be"):
            try: return csv_bytes.decode(enc, errors="ignore")
            except: pass
    try: return csv_bytes.decode("utf-8", errors="ignore")
    except: return csv_bytes.decode("latin-1", errors="ignore")

def parse_spectrum_csv(csv_bytes: bytes):
    text = smart_decode_csv(csv_bytes)
    lines = [ln.strip() for ln in text.splitlines() if ln.strip()]
    cand_delims = [",", ";", "\t", r"\s+"]
    xlabel = "X"; ylabel = "Y"
    x_vals, y_vals = [], []

    def to_float(tok: str):
        t = re.sub(r"(?<=\d),(?=\d)", ".", tok.strip())
        return float(t)

    for ln in lines:
        low = ln.lower()
        if low.startswith("#") or low.startswith("//"): continue
        if any(low.startswith(k) for k in ["xlabel","ylabel","x_label","y_label","x:","y:"]):
            parts = re.split(r"[:,]", ln, maxsplit=1)
            if len(parts)==2:
                key,val = parts[0].strip().lower(), parts[1].strip()
                if "x" in key: xlabel = val or xlabel
                elif "y" in key: ylabel = val or ylabel
            continue
        row=None
        for d in cand_delims:
            toks = re.split(r"\s+", ln) if d==r"\s+" else ln.split(d)
            if len(toks)>=2: row=toks; break
        if not row: continue
        nums=[]
        for tok in row:
            try: nums.append(to_float(tok))
            except: nums.append(None)
        found=False
        for i in range(len(nums)):
            if nums[i] is None: continue
            for j in range(i+1,len(nums)):
                if nums[j] is None: continue
                x_vals.append(nums[i]); y_vals.append(nums[j]); found=True
                break
            if found: break
    if not x_vals or not y_vals or len(x_vals)!=len(y_vals):
        raise ValueError("No numeric X,Y pairs found in CSV.")
    return x_vals, y_vals, xlabel, ylabel

def parse_excel(file_bytes: bytes):
    df = pd.read_excel(BytesIO(file_bytes), engine="openpyxl")
    if df.shape[1] < 2:
        raise ValueError("Excel íŒŒì¼ì—ì„œ ìµœì†Œ ë‘ ê°œì˜ ì—´(X,Y)ì´ í•„ìš”í•©ë‹ˆë‹¤.")
    x_series = pd.to_numeric(df.iloc[:,0], errors="coerce").dropna()
    y_series = pd.to_numeric(df.iloc[:,1], errors="coerce").dropna()
    idx = x_series.index.intersection(y_series.index)
    x = x_series.loc[idx].astype(float).tolist()
    y = y_series.loc[idx].astype(float).tolist()
    xlabel = str(df.columns[0]); ylabel = str(df.columns[1])
    return x, y, xlabel, ylabel

# ===================== Operators (registry) =====================
# Each op: (x:List[float], y:List[float], kw:dict) -> (x,y,meta)
OpFn = Callable[[List[float], List[float], Dict], Tuple[List[float], List[float], Dict]]
OP_REGISTRY: Dict[str, OpFn] = {}
def register(name: str):
    def deco(fn: OpFn):
        OP_REGISTRY[name] = fn; return fn
    return deco

@register("hampel")
def op_hampel(x, y, kw):
    window = int(float(kw.get("window", 7)))
    sigma  = float(kw.get("sigma", 3.0))
    arr = np.asarray(y, float)
    k = max(1, window//2)
    y_med = np.copy(arr)
    # rolling median
    for i in range(len(arr)):
        s = max(0, i-k); e = min(len(arr), i+k+1)
        y_med[i] = np.median(arr[s:e])
    diff = np.abs(arr - y_med)
    med_abs_dev = np.median(diff) + 1e-12
    thr = sigma * 1.4826 * med_abs_dev
    out = np.where(diff > thr, y_med, arr)
    return x, out.tolist(), {"op":"hampel", "window":window, "sigma":sigma}

@register("sg")
def op_sg(x, y, kw):
    win = int(float(kw.get("window", 11)))
    poly= int(float(kw.get("poly", 3)))
    if win % 2 == 0: win += 1
    win = max(win, poly + 3 if (poly + 3) % 2 == 1 else poly + 4)
    win = min(win, len(y) - 1 - (len(y) % 2 == 0))
    arr = np.asarray(y, float)
    out = savgol_filter(arr, window_length=max(5, win), polyorder=max(2, poly))
    return x, out.tolist(), {"op":"sg", "window":win, "poly":poly}

def baseline_asls(y: np.ndarray, lam: float = 1e5, p: float = 0.01, niter: int = 10) -> np.ndarray:
    y = np.asarray(y, dtype=float)
    L = len(y)
    D = sparse.diags([1, -2, 1], [0, -1, -2], shape=(L, L-2))
    DTD = (lam * D @ D.T).tocsc()
    w = np.ones(L)
    for _ in range(niter):
        W = sparse.diags(w, 0, shape=(L, L))
        Z = W + DTD
        z = spsolve(Z, w * y)
        w = p * (y > z) + (1 - p) * (y < z)
    return z

@register("baseline")
def op_baseline(x, y, kw):
    lam  = float(kw.get("lam", 1e5))
    p    = float(kw.get("p", 0.01))
    nite = int(float(kw.get("iter", 10)))
    arr = np.asarray(y, float)
    base = baseline_asls(arr, lam=lam, p=p, niter=nite)
    return x, (arr - base).tolist(), {"op":"baseline", "lam":lam, "p":p, "iter":nite}

@register("normalize")
def op_normalize(x, y, kw):
    kind = str(kw.get("kind","minmax")).lower()
    arr = np.asarray(y, float)
    if kind == "z":
        m, s = float(np.mean(arr)), float(np.std(arr)+1e-12)
        out = (arr - m) / s
        return x, out.tolist(), {"op":"normalize","kind":"z","mean":m,"std":s}
    lo, hi = float(np.min(arr)), float(np.max(arr))
    out = (arr - lo) / (hi - lo + 1e-12)
    return x, out.tolist(), {"op":"normalize","kind":"minmax","min":lo,"max":hi}

@register("resample")
def op_resample(x, y, kw):
    n = int(float(kw.get("n", 1024)))
    xp = np.linspace(float(np.min(x)), float(np.max(x)), n)
    yp = np.interp(xp, np.asarray(x, float), np.asarray(y, float))
    return xp.tolist(), yp.tolist(), {"op":"resample", "n":n}

# "peaks"ì™€ "onset"ì€ ë³´í†µ ê²€ì¶œë§Œ í•˜ë¯€ë¡œ ì—¬ê¸°ì„  metaë§Œ ë‚¨ê¸°ê³  ì›ë³¸ y ìœ ì§€
def detect_onset(x: List[float], y_corr: List[float], win: int = 11, poly: int = 2, z_thresh: float = 3.0) -> Optional[int]:
    x = np.asarray(x, dtype=float); y = np.asarray(y_corr, dtype=float)
    if len(y) < 7: return None
    if win % 2 == 0: win += 1
    win = max(win, poly + 3 if (poly + 3) % 2 == 1 else poly + 4)
    win = min(win, len(y) - 1 - (len(y) % 2 == 0))
    if win < 5: return None
    dy = savgol_filter(y, window_length=win, polyorder=poly, deriv=1)
    low_mask = y < np.quantile(y, 0.3)
    sigma = np.std(dy[low_mask]) if np.any(low_mask) else np.std(dy)
    if sigma <= 0: return None
    thr = z_thresh * sigma
    cand = np.where((dy > thr) & (y > 0))[0]
    return int(cand[0]) if cand.size else None

def detect_peaks_list(x: List[float], y: List[float], prominence_frac: float = 0.02, distance_pts: int = 5, width: Optional[float] = None) -> List[dict]:
    y_arr = np.asarray(y, dtype=float)
    y_min, y_max = float(np.nanmin(y_arr)), float(np.nanmax(y_arr))
    prom = max(prominence_frac * (y_max - y_min), 1e-12)
    kwargs = {"prominence": prom, "distance": max(distance_pts, 1)}
    if width is not None: kwargs["width"] = width
    idx, props = find_peaks(y_arr, **kwargs)
    peaks=[]
    for k,i in enumerate(idx):
        peaks.append({"i": int(i), "x": float(x[i]), "y": float(y[i]), "prom": float(props["prominences"][k])})
    peaks.sort(key=lambda d:d["x"])
    return peaks

ALIASES = {
    "ìŠ¤ë¬´ë”©":"sg","savitzky":"sg","savitzky-golay":"sg","sgfilter":"sg",
    "í•´ë°€":"hampel","hampel-filter":"hampel",
    "ë² ì´ìŠ¤ë¼ì¸":"baseline","bg":"baseline","asls":"baseline",
    "ì •ê·œí™”":"normalize","zscore":"normalize","minmax":"normalize",
    "ë¦¬ìƒ˜í”Œ":"resample","resampling":"resample",
    "í”¼í¬":"peaks","peak":"peaks",
    "ì˜¨ì…‹":"onset","threshold-onset":"onset",
}

def normalize_op_name(op:str) -> str:
    o = (op or "").strip().lower()
    return ALIASES.get(o, o)

def validate_and_normalize_steps(steps: List[dict]):
    warns=[]; norm=[]
    for i,s in enumerate(steps or []):
        if not isinstance(s, dict): warns.append(f"[step {i+1}] invalid"); continue
        op_raw = s.get("op","")
        op = normalize_op_name(op_raw)
        s = {**s, "op": op}
        if op not in OP_REGISTRY and not op.startswith("custom:"):
            warns.append(f"[step {i+1}] unknown op '{op_raw}'")
            s["__unknown__"]=True
        norm.append(s)
    return norm, warns

def execute_steps(x: List[float], y: List[float], steps: List[dict]):
    metas=[]
    for s in steps or []:
        if s.get("__unknown__"): continue
        op = s["op"]; kw = {k:v for k,v in s.items() if k!="op"}
        fn = OP_REGISTRY[op]
        x, y, meta = fn(x, y, kw)
        metas.append({"op":op, **(meta or {})})
    return x, y, metas

# ===================== Planner / Evaluator =====================
PLAN_SYS_PROMPT = """ë‹¹ì‹ ì€ ìŠ¤í™íŠ¸ëŸ¼ ë¶„ì„ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„°ì…ë‹ˆë‹¤.
ë‹¤ìŒ DSL YAMLë§Œ ì¶œë ¥í•˜ì„¸ìš”. ì„¤ëª…/ì½”ë“œë¸”ë¡ ê¸ˆì§€.
pipeline:
  - op: hampel|sg|normalize|resample|baseline|peaks|onset
"""

def plan_from_goal(goal: str, model: str, host: str):
    yml = ollama_generate_once(model, PLAN_SYS_PROMPT + "\nëª©í‘œ:\n" + goal + "\nì¶œë ¥:\n", host, num_predict=400)
    try:
        import yaml
        doc = yaml.safe_load(yml) or {}
        steps = doc.get("pipeline") or []
    except Exception:
        steps = []
    steps, warns = validate_and_normalize_steps(steps)
    return steps, warns, {"raw_yaml": yml}

def revise_plan(prev_steps: List[dict], feedback: dict, model: str, host: str):
    payload = json.dumps({"prev": prev_steps, "feedback": feedback}, ensure_ascii=False)
    yml = ollama_generate_once(model, PLAN_SYS_PROMPT + "\nìˆ˜ì •:\n" + payload + "\nì¶œë ¥:\n", host, num_predict=400)
    try:
        import yaml
        doc = yaml.safe_load(yml) or {}
        steps = doc.get("pipeline") or []
    except Exception:
        steps = prev_steps
    steps, warns = validate_and_normalize_steps(steps)
    return steps, warns, {"raw_yaml": yml}

def evaluate_result(x, y_corr, onset_idx, peaks: List[dict]):
    ok=True; fb={}
    if onset_idx is None:
        ok=False; fb["onset"]="not_found"
    n=len(peaks or []); fb["peaks_count"]=n
    if n==0: ok=False; fb["peaks"]="none"
    if n>50: ok=False; fb["peaks"]="too_many"
    arr=np.asarray(y_corr,float)
    if arr.size>16:
        snr=(arr.max()-arr.min())/(arr[:max(8,arr.size//20)].std()+1e-12)
        fb["snr"]=float(snr)
        if snr<2.0:
            ok=False; fb["snr_status"]="low"
    return ok, fb

# ===================== Plot / PDF =====================
def set_korean_font():
    # Try common Korean fonts
    candidates = [
        "Malgun Gothic",               # Windows
        "Apple SD Gothic Neo",         # macOS
        "Noto Sans CJK KR", "Noto Serif CJK KR",
        "NanumGothic", "NanumGothicCoding", "NanumMyeongjo",
    ]
    # Scan system fonts for candidates
    avail = {os.path.basename(p): p for p in findSystemFonts()}
    # Use family name
    fams = [f.name for f in fontManager.ttflist]
    for f in candidates:
        if f in fams:
            matplotlib.rcParams["font.family"] = [f]
            matplotlib.rcParams["axes.unicode_minus"] = False
            return
    # fallback: default (may not render Korean perfectly)
    matplotlib.rcParams["axes.unicode_minus"] = False

set_korean_font()

def plot_layers_png(x: List[float], y: List[float],
                    show_baseline: bool, baseline: Optional[np.ndarray],
                    show_onset: bool, onset_idx: Optional[int],
                    show_peaks: bool, peaks: Optional[List[dict]],
                    xlabel: str, ylabel: str,
                    show_corrected: bool = False, y_corr: Optional[np.ndarray] = None) -> bytes:
    x = np.asarray(x, dtype=float); y=np.asarray(y, dtype=float)
    fig = plt.figure()
    plt.plot(x, y, label="raw")
    if show_baseline and baseline is not None:
        plt.plot(x, baseline, linestyle="--", label="baseline")
    if show_corrected and (y_corr is not None):
        plt.plot(x, y_corr, linestyle="-.", label="corrected")
    if show_peaks and peaks:
        for p in peaks: plt.axvline(float(p["x"]), linestyle=":", linewidth=1)
    if show_onset and onset_idx is not None and 0 <= onset_idx < len(x):
        plt.axvline(float(x[onset_idx]), linestyle="-.", linewidth=1.2)
    plt.xlabel(xlabel); plt.ylabel(ylabel); plt.legend(loc="best")
    buf = BytesIO(); fig.savefig(buf, format="png", bbox_inches="tight", dpi=150); plt.close(fig)
    return buf.getvalue()

def plot_layers_dual_png(
    x: List[float], y: List[float],
    show_baseline: bool, baseline: Optional[np.ndarray],
    show_onset: bool, onset_idx: Optional[int],
    show_peaks: bool, peaks: Optional[List[dict]],
    xlabel: str, ylabel: str,
    y_corr: Optional[np.ndarray]
) -> bytes:
    x = np.asarray(x, dtype=float); y = np.asarray(y, dtype=float)
    if y_corr is None:
        y_corr = y if baseline is None else (y - baseline)
    fig, axes = plt.subplots(1, 2, figsize=(10, 4))
    ax = axes[0]
    ax.plot(x, y, label="raw")
    if show_baseline and (baseline is not None):
        ax.plot(x, baseline, linestyle="--", label="baseline")
    ax.set_xlabel(xlabel); ax.set_ylabel(ylabel); ax.legend(loc="best"); ax.set_title("Raw + Baseline")
    ax = axes[1]
    ax.plot(x, y_corr, label="corrected", linestyle="-.")
    if show_peaks and peaks:
        for p in peaks: ax.axvline(float(p["x"]), linestyle=":", linewidth=1)
    if show_onset and onset_idx is not None and 0 <= onset_idx < len(x):
        ax.axvline(float(x[onset_idx]), linestyle="-.", linewidth=1.2)
    ax.set_xlabel(xlabel); ax.set_ylabel(ylabel); ax.legend(loc="best"); ax.set_title("Corrected + Peaks/Onset")
    buf = BytesIO(); fig.tight_layout(); fig.savefig(buf, format="png", bbox_inches="tight", dpi=150); plt.close(fig)
    return buf.getvalue()

def save_pdf_report(filename: str, pages: List[Dict]):
    """
    pages: list of dict:
      {
        "title": str,
        "images": [png_bytes,...],
        "text_blocks": [str,...],
      }
    """
    with PdfPages(filename) as pdf:
        for page in pages:
            fig = plt.figure(figsize=(11.69, 8.27))  # A4 landscape (inches)
            y_cursor = 0.95
            title = page.get("title","")
            if title:
                plt.text(0.02, y_cursor, title, fontsize=14, weight="bold", transform=plt.gca().transAxes)
                y_cursor -= 0.05
            imgs = page.get("images",[]) or []
            # place up to two images side-by-side
            if imgs:
                cols = 2 if len(imgs)>=2 else 1
                for i,imgb in enumerate(imgs[:2]):
                    ax = plt.axes([0.05 + i*(0.45), 0.45, 0.42, 0.42]) if cols==2 else plt.axes([0.05, 0.45, 0.9, 0.42])
                    im = Image.open(BytesIO(imgb))
                    ax.imshow(im); ax.axis("off")
                y_cursor = 0.42
            blocks = page.get("text_blocks",[]) or []
            # print text blocks stacked
            for blk in blocks:
                txt = textwrap.fill(str(blk), width=110)
                plt.text(0.03, y_cursor, txt, fontsize=10, transform=plt.gca().transAxes, va='top')
                y_cursor -= 0.08 + 0.015*(len(txt)//80)
            plt.axis("off"); pdf.savefig(fig, bbox_inches="tight"); plt.close(fig)

# ===================== HTML =====================
def html_form():
    host_info = f"Ollama host: <code>{OLLAMA_HOST}</code> Â· TEXT_MODEL: <code>{TEXT_MODEL}</code>"
    token_hint = f"(Auth ON)" if AUTH_TOKEN else "(Auth OFF)"
    return f"""<!doctype html>
<html><head><meta charset="utf-8"><title>Spectrum Agent Starter</title>
<style>
body{{max-width:1100px;margin:20px auto;font-family:system-ui,-apple-system,Segoe UI,Roboto}}
.flex{{display:flex;gap:12px;flex-wrap:wrap}}
.preview img{{max-height:240px;border:1px solid #ddd;padding:4px;background:#fff}}
.result{{white-space:pre-wrap;border:1px solid #ddd;padding:12px;background:#fafafa}}
.small{{color:#666;font-size:12px}}
.tablabel{{padding:6px 12px;cursor:pointer;background:#eee;margin-right:4px;border:1px solid #ddd;display:inline-block}}
.tabpanel{{display:none;padding:8px;border:1px solid #ddd;margin-top:4px}}
.tabpanel.active{{display:block}}
</style></head><body>
<h1>ğŸ“ˆ Spectrum Agent Starter</h1>
<div class="small">{host_info} Â· {token_hint}</div>

<form action="/analyze" method="post" enctype="multipart/form-data">
  <label>CSV/Excel (multi)</label><br>
  <input id="fileInput" type="file" name="csv" accept=".csv,.tsv,application/vnd.openxmlformats-officedocument.spreadsheetml.sheet,application/vnd.ms-excel" multiple required />

  <fieldset style="margin-top:10px;">
    <legend>ì‹¤í–‰ ëª¨ë“œ</legend>
    <label><input type="radio" name="run_mode" value="manual" checked> ìˆ˜ë™(ì²´í¬ë°•ìŠ¤)</label>
    <label><input type="radio" name="run_mode" value="agent"> ëª©í‘œ ê¸°ë°˜(Agent)</label>
  </fieldset>

  <label>Prompt / Goal</label>
  <textarea name="prompt" rows="5" style="width:100%;">ì‘ë‹µì€ ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ. ì£¼ìš” í”¼í¬ì™€ ë¬¼ë¦¬ì  ì˜ë¯¸ë¥¼ ìš”ì•½í•˜ë¼.</textarea>

  <fieldset style="margin-top:10px;">
    <legend>ì „ì²˜ë¦¬ ì˜µì…˜ (ìˆ˜ë™ ëª¨ë“œ)</legend>
    <div class="flex">
      <label><input type="checkbox" name="do_hampel" value="1"> Hampel</label>
      <label><input type="checkbox" name="do_sg" value="1" checked> Savitzkyâ€“Golay</label>
      <label><input type="checkbox" name="do_bg" value="1" checked> Baseline (AsLS)</label>
      <label><input type="checkbox" name="do_onset" value="1"> Onset</label>
      <label><input type="checkbox" name="do_peaks" value="1" checked> Peaks</label>
      <label><input type="checkbox" name="show_corrected" value="1" checked> Show corrected</label>
    </div>
    <div class="flex">
      <div style="flex:1"><label>Hampel window</label><input name="hampel_win" value="7"></div>
      <div style="flex:1"><label>Hampel sigma</label><input name="hampel_sigma" value="3.0"></div>
      <div style="flex:1"><label>SG window</label><input name="sg_win" value="11"></div>
      <div style="flex:1"><label>SG poly</label><input name="sg_poly" value="3"></div>
    </div>
    <div class="flex">
      <div style="flex:1"><label>AsLS Î»</label><input name="bg_lam" value="100000"></div>
      <div style="flex:1"><label>AsLS p</label><input name="bg_p" value="0.01"></div>
      <div style="flex:1"><label>AsLS iter</label><input name="bg_iter" value="10"></div>
    </div>
    <div class="flex">
      <div style="flex:1"><label>Onset win</label><input name="on_win" value="11"></div>
      <div style="flex:1"><label>Onset poly</label><input name="on_poly" value="2"></div>
      <div style="flex:1"><label>Onset z</label><input name="on_z" value="3.0"></div>
    </div>
    <div class="flex">
      <div style="flex:1"><label>peaks.prominence_frac</label><input name="prom_frac" value="0.02"></div>
      <div style="flex:1"><label>peaks.distance_pts</label><input name="dist_pts" value="5"></div>
      <div style="flex:1"><label>peaks.width</label><input name="width" value=""></div>
    </div>
  </fieldset>

  <div class="flex" style="margin-top:10px;">
    <button type="button" onclick="doPreview()">Preview First File</button>
    <button type="submit">Analyze</button>
  </div>
</form>

<h3>Data Preview</h3>
<div id="preview" class="preview"></div>
<div id="previewStats" class="small"></div>

<hr>
<h2>Chatbot</h2>
<div class="flex">
  <input id="chatInput" style="flex:1" placeholder="ëª¨ë¸ì—ê²Œ ì§ˆë¬¸í•˜ì„¸ìš”â€¦">
  <button onclick="sendChat()">Send</button>
</div>
<pre id="chatLog" class="result" style="margin-top:8px;min-height:140px;"></pre>

<script>
const AUTH_TOKEN = {"on": %s, "val": "%s"};

async function doPreview(){
  const fi = document.getElementById('fileInput');
  const preview = document.getElementById('preview');
  const stats = document.getElementById('previewStats');
  preview.innerHTML = ''; stats.textContent='';
  if(!fi.files || !fi.files.length){ alert('íŒŒì¼ì„ ì„ íƒí•˜ì„¸ìš”'); return; }
  const fd = new FormData(); fd.append('csv', fi.files[0]);
  try{
    const res = await fetch('/preview', {
      method: 'POST',
      headers: AUTH_TOKEN.on ? {'X-Auth-Token': AUTH_TOKEN.val} : {},
      body: fd
    });
    if(!res.ok){ throw new Error(await res.text()); }
    const data = await res.json();
    preview.innerHTML = '<img src="'+data.png+'">';
    const s = data.stats || {};
    stats.textContent = `points=${s.n_points} | ${s.xlabel}: ${s.x_min}~${s.x_max} | ${s.ylabel}: ${s.y_min}~${s.y_max}`;
  }catch(e){
    preview.innerHTML = '<div class="small" style="color:#b00;">ë¯¸ë¦¬ë³´ê¸° ì‹¤íŒ¨</div>';
    console.error(e);
  }
}

async function sendChat(){
  const input = document.getElementById('chatInput');
  const log = document.getElementById('chatLog');
  const text = input.value.trim(); if(!text) return;
  log.textContent += "\\n[You] "+text+"\\n";
  input.value='';
  try{
    const res = await fetch('/chat', {
      method:'POST',
      headers: Object.assign({'Content-Type':'application/json'}, AUTH_TOKEN.on ? {'X-Auth-Token': AUTH_TOKEN.val} : {}),
      body: JSON.stringify({message: text})
    });
    if(!res.ok){ throw new Error(await res.text()); }
    const data = await res.json();
    log.textContent += "[Model] "+data.reply+"\\n";
  }catch(e){
    log.textContent += "[Error] "+e.message+"\\n";
  }
}
</script>

</body></html>
""" % ("true" if AUTH_TOKEN else "false", (AUTH_TOKEN or ""))
# end html_form()

# ===================== Routes =====================
@app.get("/", response_class=HTMLResponse)
def index(_: bool = Depends(require_auth)):
    return html_form()

@app.get("/debug/models")
def debug_models(_: bool = Depends(require_auth)):
    return {
        "host": OLLAMA_HOST,
        "available_models": list_model_names(OLLAMA_HOST),
        "text_model_fixed": TEXT_MODEL,
    }

@app.post("/preview")
def preview(csv: UploadFile = File(...), _ok: bool = Depends(require_auth)):
    b = csv.file.read()
    csv.file.close()
    low = (csv.filename or "").lower()
    if low.endswith((".xlsx",".xls")):
        x, y, xlabel, ylabel = parse_excel(b)
    else:
        x, y, xlabel, ylabel = parse_spectrum_csv(b)
    # simple plot
    fig = plt.figure()
    plt.plot(x, y); plt.xlabel(xlabel); plt.ylabel(ylabel)
    buf = BytesIO(); fig.savefig(buf, format="png", bbox_inches="tight", dpi=150); plt.close(fig)
    img64 = base64.b64encode(buf.getvalue()).decode()
    stats = {
        "n_points": len(x),
        "xlabel": xlabel, "ylabel": ylabel,
        "x_min": float(np.min(x)), "x_max": float(np.max(x)),
        "y_min": float(np.min(y)), "y_max": float(np.max(y)),
    }
    return JSONResponse({"png": "data:image/png;base64,"+img64, "stats": stats})

@app.post("/chat")
def chat(message: Dict[str,str], _ok: bool = Depends(require_auth)):
    model_chosen = ensure_model_installed(TEXT_MODEL, OLLAMA_HOST)
    user_text = (message.get("message") or "").strip()
    if not user_text:
        raise HTTPException(status_code=400, detail="empty message")
    reply = ollama_generate_once(model_chosen, user_text, OLLAMA_HOST, num_predict=600)
    return {"reply": reply}

@app.post("/analyze", response_class=HTMLResponse)
def analyze(
    csv: List[UploadFile] = File(...),
    prompt: str = Form(""),
    run_mode: str = Form("manual"),

    # manual options
    do_hampel: Optional[str] = Form(None),
    do_sg: Optional[str] = Form(None),
    do_bg: Optional[str] = Form(None),
    do_onset: Optional[str] = Form(None),
    do_peaks: Optional[str] = Form(None),
    show_corrected: Optional[str] = Form(None),

    hampel_win: str = Form("7"),
    hampel_sigma: str = Form("3.0"),
    sg_win: str = Form("11"),
    sg_poly: str = Form("3"),

    bg_lam: str = Form("100000"),
    bg_p: str = Form("0.01"),
    bg_iter: str = Form("10"),

    on_win: str = Form("11"),
    on_poly: str = Form("2"),
    on_z: str = Form("3.0"),

    prom_frac: str = Form("0.02"),
    dist_pts: str = Form("5"),
    width: str = Form(""),

    _ok: bool = Depends(require_auth)
):
    def f2(v,d):
        try: return float(v)
        except: return d
    def i2(v,d):
        try: return int(float(v))
        except: return d

    use_hampel = (do_hampel=="1")
    use_sg     = (do_sg=="1")
    use_bg     = (do_bg=="1")
    use_onset  = (do_onset=="1")
    use_peaks  = (do_peaks=="1")
    show_corr  = (show_corrected=="1")

    hampel_win_v   = max(i2(hampel_win,7),3)
    hampel_sigma_v = max(f2(hampel_sigma,3.0),0.1)
    sg_win_v       = max(i2(sg_win,11),5)
    sg_poly_v      = max(i2(sg_poly,3),2)

    bg_lam_v  = max(f2(bg_lam,1e5),1.0)
    bg_p_v    = min(max(f2(bg_p,0.01),1e-5),0.5)
    bg_iter_v = max(i2(bg_iter,10),1)

    on_win_v  = max(i2(on_win,11),5)
    on_poly_v = max(i2(on_poly,2),1)
    on_z_v    = max(f2(on_z,3.0),0.1)

    prom_frac_v = f2(prom_frac,0.02)
    dist_pts_v  = max(i2(dist_pts,5),1)
    width_v     = f2(width,None) if (width or "").strip() else None

    model_chosen = ensure_model_installed(TEXT_MODEL, OLLAMA_HOST)

    # read all uploaded files (sync)
    file_payloads=[]
    for k, uf in enumerate(csv,1):
        name = uf.filename or f"file_{k}"
        b = uf.file.read()
        uf.file.close()
        file_payloads.append({"name":name,"bytes":b})

    tabs_labels=[]; tabs_panes=[]
    pdf_pages=[]

    for fp in file_payloads:
        name = fp["name"]; b = fp["bytes"]
        low = name.lower()
        if low.endswith((".xlsx",".xls")):
            x, y, xlabel, ylabel = parse_excel(b)
        else:
            x, y, xlabel, ylabel = parse_spectrum_csv(b)

        # ---------- AGENT MODE ----------
        if run_mode == "agent":
            steps, warns, meta0 = plan_from_goal(prompt, model_chosen, OLLAMA_HOST)
            history=[]
            x2, y2 = x, y
            for it in range(1, MAX_AGENT_ITERS+1):
                x2, y2, metas = execute_steps(x2, y2, steps)
                y_work = np.asarray(y2, float)
                baseline = None
                # Onset/Peaks for evaluation
                onset_idx = detect_onset(x2, y_work.tolist(), win=on_win_v, poly=on_poly_v, z_thresh=on_z_v)
                peaks = detect_peaks_list(x2, y_work.tolist(), prominence_frac=prom_frac_v, distance_pts=dist_pts_v, width=width_v)
                ok, fb = evaluate_result(x2, y_work, onset_idx, peaks)
                history.append({"iter":it, "steps":steps, "feedback":fb})
                if ok: break
                steps, warns2, meta1 = revise_plan(steps, fb, model_chosen, OLLAMA_HOST)

            # plots
            overlay_png = plot_layers_dual_png(
                x2, y,
                show_baseline=False, baseline=None,
                show_onset=True, onset_idx=onset_idx,
                show_peaks=True, peaks=peaks,
                xlabel=xlabel, ylabel=ylabel,
                y_corr=np.asarray(y2,float)
            )
            img64 = base64.b64encode(overlay_png).decode()

            # llm interpretation on final
            base_prompt = (
                (prompt or "ì‘ë‹µì€ ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ. ìŠ¤í™íŠ¸ëŸ¼ë§Œ í•´ì„í•˜ë¼.").strip()
                + "\n\n[Agent ìµœì¢… ë‹¨ê³„]\n"
                + json.dumps(steps, ensure_ascii=False)
                + "\n[ê²€ì¶œ]\n"
                + (f"onset @ {x2[onset_idx]:.6g}" if onset_idx is not None else "(no onset)")
                + "\npeaks:\n"
                + "\n".join([f"- peak @ {p['x']:.6g} (y={p['y']:.6g}, prom={p['prom']:.6g})" for p in peaks]) or "(no peaks)"
                + "\nì´ë¯¸ì§€/í˜•íƒœ ì–¸ê¸‰ ê¸ˆì§€."
            )
            out_text = ollama_generate_once(model_chosen, base_prompt, OLLAMA_HOST, num_predict=800)

            # tab pane
            pane_inner = (
                f"<h3>{name} â€” Agent</h3>"
                f"<div class='preview'><img src='data:image/png;base64,{img64}'></div>"
                "<details class='small'><summary>Agent steps</summary><pre>"
                + json.dumps(steps, indent=2, ensure_ascii=False) + "</pre></details>"
                "<details class='small'><summary>History</summary><pre>"
                + json.dumps(history, indent=2, ensure_ascii=False) + "</pre></details>"
                "<h3>LLM í•´ì„</h3><div class='result'>"+ out_text + "</div>"
            )
            tabs_labels.append(f"<span class='tablabel'>{name}</span>")
            tabs_panes.append(f"<div class='tabpanel'>{pane_inner}</div>")

            # pdf
            pdf_pages.append({
                "title": f"{name} â€” Agent ê²°ê³¼",
                "images": [overlay_png],
                "text_blocks": [
                    "ìµœì¢… Steps:\n" + json.dumps(steps, ensure_ascii=False),
                    "History ìš”ì•½:\n" + json.dumps(history[-1] if history else {}, ensure_ascii=False),
                    "LLM í•´ì„:\n" + out_text
                ]
            })

            continue

        # ---------- MANUAL MODE ----------
        y_work = np.asarray(y, float)
        if use_hampel:
            x_, y_work, _ = op_hampel(x, y_work.tolist(), {"window":hampel_win_v,"sigma":hampel_sigma_v})
            y_work = np.asarray(y_work, float)
        if use_sg:
            x_, y_work, _ = op_sg(x, y_work.tolist(), {"window":sg_win_v,"poly":sg_poly_v})
            y_work = np.asarray(y_work, float)

        baseline = None
        y_corr = y_work.copy()
        if use_bg:
            baseline = baseline_asls(y_corr, lam=bg_lam_v, p=bg_p_v, niter=bg_iter_v)
            y_corr = y_corr - baseline

        onset_idx = detect_onset(x, y_corr.tolist(), win=on_win_v, poly=on_poly_v, z_thresh=on_z_v) if use_onset else None
        peaks = detect_peaks_list(x, y_corr.tolist(), prominence_frac=prom_frac_v, distance_pts=dist_pts_v, width=width_v) if use_peaks else []

        if show_corr:
            overlay_png = plot_layers_dual_png(
                x, y,
                show_baseline=use_bg, baseline=baseline,
                show_onset=use_onset, onset_idx=onset_idx,
                show_peaks=use_peaks, peaks=peaks,
                xlabel=xlabel, ylabel=ylabel,
                y_corr=y_corr
            )
        else:
            overlay_png = plot_layers_png(
                x, y,
                show_baseline=use_bg, baseline=baseline,
                show_onset=use_onset, onset_idx=onset_idx,
                show_peaks=use_peaks, peaks=peaks,
                xlabel=xlabel, ylabel=ylabel,
                show_corrected=False, y_corr=None
            )
        img64 = base64.b64encode(overlay_png).decode()

        top_peaks_text = "\n".join([f"- peak @ {p['x']:.6g} (y={p['y']:.6g}, prom={p['prom']:.6g})" for p in peaks]) or "(no peaks)"
        onset_line = f"onset @ {x[onset_idx]:.6g}" if (onset_idx is not None) else "(no onset)"
        base_prompt = (
            (prompt or "ì‘ë‹µì€ ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ. ìŠ¤í™íŠ¸ëŸ¼ë§Œ í•´ì„í•˜ë¼.").strip()
            + "\n\n[ì „ì²˜ë¦¬ ì˜µì…˜] DN(Hampel="+("Y" if use_hampel else "N")
            + ", SG="+("Y" if use_sg else "N")+") | BG="+("Y" if use_bg else "N")
            + " | ONSET="+("Y" if use_onset else "N")+" | PEAKS="+("Y" if use_peaks else "N")
            + f"\n[ì¶•] X={xlabel}, Y={ylabel}"
            + f"\n[onset] {onset_line}\n[peaks]\n{top_peaks_text}\nì´ë¯¸ì§€/í˜•íƒœ ì–¸ê¸‰ ê¸ˆì§€."
        )
        out_text = ollama_generate_once(model_chosen, base_prompt, OLLAMA_HOST, num_predict=800)

        params = {
            "Hampel": f"window={hampel_win_v}, sigma={hampel_sigma_v}" if use_hampel else "OFF",
            "Savitzkyâ€“Golay": f"window={sg_win_v}, poly={sg_poly_v}" if use_sg else "OFF",
            "AsLS": f"Î»={bg_lam_v}, p={bg_p_v}, iter={bg_iter_v}" if use_bg else "OFF",
            "Onset": f"win={on_win_v}, poly={on_poly_v}, z={on_z_v}" if use_onset else "OFF",
            "Peaks": f"prom_frac={prom_frac_v}, dist={dist_pts_v}, width={width_v}" if use_peaks else "OFF"
        }
        param_lines = "\n".join([f"{k}: {v}" for k, v in params.items()])
        param_html = (
            "<details class='small' style='margin:8px 0;'><summary>ì „ì²˜ë¦¬ íŒŒë¼ë¯¸í„°</summary>"
            "<pre style='white-space:pre-wrap;'>" + param_lines + "</pre></details>"
        )

        pane_inner = (
            f"<h3>{name} â€” Manual</h3>"
            f"<div class='preview'><img src='data:image/png;base64,{img64}'></div>"
            "<h3>Server Summary (text fed to LLM)</h3>"
            "<pre>"+ base_prompt + "</pre>"
            + param_html +
            "<h3>LLM í•´ì„</h3><div class='result'>"+ out_text + "</div>"
        )
        tabs_labels.append(f"<span class='tablabel'>{name}</span>")
        tabs_panes.append(f"<div class='tabpanel'>{pane_inner}</div>")

        # PDF pages
        pdf_pages.append({
            "title": f"{name} â€” Manual ê²°ê³¼",
            "images": [overlay_png],
            "text_blocks": [
                "ì „ì²˜ë¦¬ íŒŒë¼ë¯¸í„°:\n"+param_lines,
                "LLM í•´ì„:\n"+out_text
            ]
        })

    # Build tabs HTML
    tabs_html = (
        "<div id='tablabels'>" + "".join(tabs_labels) + "</div>"
        + "<div id='tabpanes'>" + "".join(tabs_panes) + "</div>"
        + "<script>(function(){const ls=[...document.querySelectorAll('.tablabel')];const ps=[...document.querySelectorAll('.tabpanel')];function act(i){ls.forEach((el,k)=>el.style.fontWeight=k===i?'bold':'normal');ps.forEach((el,k)=>el.className='tabpanel'+(k===i?' active':''));}ls.forEach((el,i)=>el.addEventListener('click',()=>act(i)));if(ls.length)act(0);})();</script>"
    )

    # Save PDF to memory and offer a download link
    pdf_buf = BytesIO()
    save_pdf_report(pdf_buf, pdf_pages)
    pdf_bytes = pdf_buf.getvalue()
    pdf_b64 = base64.b64encode(pdf_bytes).decode()

    html = f"""<!doctype html><html><head><meta charset="utf-8"><title>Results</title>
<style>
.preview img{{max-height:240px;border:1px solid #ddd;padding:4px;background:#fff}}
.result{{white-space:pre-wrap;border:1px solid #ddd;padding:12px;background:#fafafa}}
.small{{color:#666;font-size:12px}}
.tablabel{{padding:6px 12px;cursor:pointer;background:#eee;margin-right:4px;border:1px solid #ddd;display:inline-block}}
.tabpanel{{display:none;padding:8px;border:1px solid #ddd;margin-top:4px}}
.tabpanel.active{{display:block}}
</style></head><body>
<h2>Results ({ensure_model_installed(TEXT_MODEL, OLLAMA_HOST)} @ {OLLAMA_HOST})</h2>
<p><a download="spectrum_report.pdf" href="data:application/pdf;base64,{pdf_b64}">ğŸ“„ PDF ë‹¤ìš´ë¡œë“œ</a></p>
{tabs_html}
<p class="small">* Analyze (ë™ê¸°) Â· ìŠ¤íŠ¸ë¦¬ë° ë¯¸ì‚¬ìš© Â· Agent/Manual ì§€ì›</p>
<p><a href="/">â† Back</a></p>
</body></html>"""
    return HTMLResponse(html)

# ===================== main =====================
if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="127.0.0.1", port=7862, log_level="info")