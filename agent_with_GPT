# app_spectrum_agent_starter.py
# -----------------------------------------------------------------------------
# Spectrum Agent Starter (Single-file)
# - Text-only Ollama server
# - Preview (first file), multi-file analyze
# - Denoise (Hampel, Savitzky-Golay), Baseline (AsLS), Onset/Peaks
# - Manual mode & Agent mode (plan → execute → evaluate → revise)
# - PDF report (Korean font fallback)
# - Simple chatbot (/chat)
# -----------------------------------------------------------------------------
# Requirements:
#   pip install fastapi uvicorn[standard] python-multipart matplotlib pillow requests
#   pip install pandas openpyxl scipy pyyaml
# -----------------------------------------------------------------------------

import os, io, re, json, base64, textwrap
from io import BytesIO
from typing import Optional, List, Tuple, Dict, Callable
from dataclasses import dataclass

import numpy as np
import pandas as pd
import requests

import matplotlib
matplotlib.use("Agg")
import matplotlib.pyplot as plt
from matplotlib.backends.backend_pdf import PdfPages
from matplotlib.font_manager import fontManager, findSystemFonts

from PIL import Image
from scipy.signal import find_peaks, savgol_filter
from scipy import sparse
from scipy.sparse.linalg import spsolve

from fastapi import FastAPI, UploadFile, File, Form, Header, HTTPException, Depends
from fastapi.responses import HTMLResponse, JSONResponse

# ===================== Config =====================
OLLAMA_HOST = os.environ.get("OLLAMA_HOST_TEXT", "http://localhost:11435")
TEXT_MODEL  = os.environ.get("TEXT_MODEL", "gpt-oss")  # e.g., "gpt-oss" or "gpt-oss:latest"
AUTH_TOKEN  = os.environ.get("LOCAL_AGENT_TOKEN")

MAX_AGENT_ITERS = int(os.environ.get("MAX_AGENT_ITERS", "3"))

app = FastAPI(title="Spectrum Agent Starter (Single-file)")

# ===================== Auth =====================
def require_auth(x_auth_token: Optional[str] = Header(None)):
    if AUTH_TOKEN and x_auth_token != AUTH_TOKEN:
        raise HTTPException(status_code=401, detail="Unauthorized")
    return True

# ===================== Ollama helpers =====================
def list_model_names(base: str) -> List[str]:
    try:
        r = requests.get(f"{base}/api/tags", timeout=10)
        r.raise_for_status()
        models = r.json().get("models", [])
        out=[]
        for m in models:
            if isinstance(m, dict):
                name = m.get("name") or m.get("model") or m.get("tag") or m.get("id")
                if name: out.append(name)
        return out
    except Exception:
        return []

def ensure_model_installed(model: str, base: str) -> str:
    """
    Return the concrete tag installed.
    If 'model' has no tag, match by prefix (e.g., 'gpt-oss' -> 'gpt-oss:latest').
    """
    names = list_model_names(base)
    if model in names:
        return model
    want = model.split(":", 1)[0]
    for n in names:
        if n.split(":",1)[0] == want:
            return n
    raise HTTPException(status_code=400, detail=f"Model '{model}' not found at {base}. Found: {names}")

def ollama_generate_once(model: str, prompt: str, base: str, num_predict=512) -> str:
    req = {
        "model": model,
        "prompt": prompt,
        "stream": False,
        "options": {"temperature": 0.2, "top_p": 0.8, "num_predict": num_predict},
        "keep_alive": "20m",
    }
    r = requests.post(f"{base}/api/generate", json=req, timeout=120)
    r.raise_for_status()
    return r.json().get("response","").strip()

# ===================== CSV/Excel parsing =====================
def smart_decode_csv(csv_bytes: bytes) -> str:
    if csv_bytes.startswith(b"\xff\xfe"): return csv_bytes.decode("utf-16le", errors="ignore")
    if csv_bytes.startswith(b"\xfe\xff"): return csv_bytes.decode("utf-16be", errors="ignore")
    if csv_bytes.startswith(b"\xef\xbb\xbf"): return csv_bytes.decode("utf-8-sig", errors="ignore")
    if b"\x00" in csv_bytes:
        for enc in ("utf-16le","utf-16be"):
            try: return csv_bytes.decode(enc, errors="ignore")
            except: pass
    try: return csv_bytes.decode("utf-8", errors="ignore")
    except: return csv_bytes.decode("latin-1", errors="ignore")

def parse_spectrum_csv(csv_bytes: bytes):
    text = smart_decode_csv(csv_bytes)
    lines = [ln.strip() for ln in text.splitlines() if ln.strip()]
    cand_delims = [",", ";", "\t", r"\s+"]
    xlabel = "X"; ylabel = "Y"
    x_vals, y_vals = [], []

    def to_float(tok: str):
        t = re.sub(r"(?<=\d),(?=\d)", ".", tok.strip())
        return float(t)

    for ln in lines:
        low = ln.lower()
        if low.startswith("#") or low.startswith("//"): continue
        if any(low.startswith(k) for k in ["xlabel","ylabel","x_label","y_label","x:","y:"]):
            parts = re.split(r"[:,]", ln, maxsplit=1)
            if len(parts)==2:
                key,val = parts[0].strip().lower(), parts[1].strip()
                if "x" in key: xlabel = val or xlabel
                elif "y" in key: ylabel = val or ylabel
            continue
        row=None
        for d in cand_delims:
            toks = re.split(r"\s+", ln) if d==r"\s+" else ln.split(d)
            if len(toks)>=2: row=toks; break
        if not row: continue
        nums=[]
        for tok in row:
            try: nums.append(to_float(tok))
            except: nums.append(None)
        found=False
        for i in range(len(nums)):
            if nums[i] is None: continue
            for j in range(i+1,len(nums)):
                if nums[j] is None: continue
                x_vals.append(nums[i]); y_vals.append(nums[j]); found=True
                break
            if found: break
    if not x_vals or not y_vals or len(x_vals)!=len(y_vals):
        raise ValueError("No numeric X,Y pairs found in CSV.")
    return x_vals, y_vals, xlabel, ylabel

def parse_excel(file_bytes: bytes):
    df = pd.read_excel(BytesIO(file_bytes), engine="openpyxl")
    if df.shape[1] < 2:
        raise ValueError("Excel 파일에서 최소 두 개의 열(X,Y)이 필요합니다.")
    x_series = pd.to_numeric(df.iloc[:,0], errors="coerce").dropna()
    y_series = pd.to_numeric(df.iloc[:,1], errors="coerce").dropna()
    idx = x_series.index.intersection(y_series.index)
    x = x_series.loc[idx].astype(float).tolist()
    y = y_series.loc[idx].astype(float).tolist()
    xlabel = str(df.columns[0]); ylabel = str(df.columns[1])
    return x, y, xlabel, ylabel

# ===================== Operators (registry) =====================
# Each op: (x:List[float], y:List[float], kw:dict) -> (x,y,meta)
OpFn = Callable[[List[float], List[float], Dict], Tuple[List[float], List[float], Dict]]
OP_REGISTRY: Dict[str, OpFn] = {}
def register(name: str):
    def deco(fn: OpFn):
        OP_REGISTRY[name] = fn; return fn
    return deco

@register("hampel")
def op_hampel(x, y, kw):
    window = int(float(kw.get("window", 7)))
    sigma  = float(kw.get("sigma", 3.0))
    arr = np.asarray(y, float)
    k = max(1, window//2)
    y_med = np.copy(arr)
    # rolling median
    for i in range(len(arr)):
        s = max(0, i-k); e = min(len(arr), i+k+1)
        y_med[i] = np.median(arr[s:e])
    diff = np.abs(arr - y_med)
    med_abs_dev = np.median(diff) + 1e-12
    thr = sigma * 1.4826 * med_abs_dev
    out = np.where(diff > thr, y_med, arr)
    return x, out.tolist(), {"op":"hampel", "window":window, "sigma":sigma}

@register("sg")
def op_sg(x, y, kw):
    win = int(float(kw.get("window", 11)))
    poly= int(float(kw.get("poly", 3)))
    if win % 2 == 0: win += 1
    win = max(win, poly + 3 if (poly + 3) % 2 == 1 else poly + 4)
    win = min(win, len(y) - 1 - (len(y) % 2 == 0))
    arr = np.asarray(y, float)
    out = savgol_filter(arr, window_length=max(5, win), polyorder=max(2, poly))
    return x, out.tolist(), {"op":"sg", "window":win, "poly":poly}

def baseline_asls(y: np.ndarray, lam: float = 1e5, p: float = 0.01, niter: int = 10) -> np.ndarray:
    y = np.asarray(y, dtype=float)
    L = len(y)
    D = sparse.diags([1, -2, 1], [0, -1, -2], shape=(L, L-2))
    DTD = (lam * D @ D.T).tocsc()
    w = np.ones(L)
    for _ in range(niter):
        W = sparse.diags(w, 0, shape=(L, L))
        Z = W + DTD
        z = spsolve(Z, w * y)
        w = p * (y > z) + (1 - p) * (y < z)
    return z

@register("baseline")
def op_baseline(x, y, kw):
    lam  = float(kw.get("lam", 1e5))
    p    = float(kw.get("p", 0.01))
    nite = int(float(kw.get("iter", 10)))
    arr = np.asarray(y, float)
    base = baseline_asls(arr, lam=lam, p=p, niter=nite)
    return x, (arr - base).tolist(), {"op":"baseline", "lam":lam, "p":p, "iter":nite}

@register("normalize")
def op_normalize(x, y, kw):
    kind = str(kw.get("kind","minmax")).lower()
    arr = np.asarray(y, float)
    if kind == "z":
        m, s = float(np.mean(arr)), float(np.std(arr)+1e-12)
        out = (arr - m) / s
        return x, out.tolist(), {"op":"normalize","kind":"z","mean":m,"std":s}
    lo, hi = float(np.min(arr)), float(np.max(arr))
    out = (arr - lo) / (hi - lo + 1e-12)
    return x, out.tolist(), {"op":"normalize","kind":"minmax","min":lo,"max":hi}

@register("resample")
def op_resample(x, y, kw):
    n = int(float(kw.get("n", 1024)))
    xp = np.linspace(float(np.min(x)), float(np.max(x)), n)
    yp = np.interp(xp, np.asarray(x, float), np.asarray(y, float))
    return xp.tolist(), yp.tolist(), {"op":"resample", "n":n}

# "peaks"와 "onset"은 보통 검출만 하므로 여기선 meta만 남기고 원본 y 유지
def detect_onset(x: List[float], y_corr: List[float], win: int = 11, poly: int = 2, z_thresh: float = 3.0) -> Optional[int]:
    x = np.asarray(x, dtype=float); y = np.asarray(y_corr, dtype=float)
    if len(y) < 7: return None
    if win % 2 == 0: win += 1
    win = max(win, poly + 3 if (poly + 3) % 2 == 1 else poly + 4)
    win = min(win, len(y) - 1 - (len(y) % 2 == 0))
    if win < 5: return None
    dy = savgol_filter(y, window_length=win, polyorder=poly, deriv=1)
    low_mask = y < np.quantile(y, 0.3)
    sigma = np.std(dy[low_mask]) if np.any(low_mask) else np.std(dy)
    if sigma <= 0: return None
    thr = z_thresh * sigma
    cand = np.where((dy > thr) & (y > 0))[0]
    return int(cand[0]) if cand.size else None

def detect_peaks_list(x: List[float], y: List[float], prominence_frac: float = 0.02, distance_pts: int = 5, width: Optional[float] = None) -> List[dict]:
    y_arr = np.asarray(y, dtype=float)
    y_min, y_max = float(np.nanmin(y_arr)), float(np.nanmax(y_arr))
    prom = max(prominence_frac * (y_max - y_min), 1e-12)
    kwargs = {"prominence": prom, "distance": max(distance_pts, 1)}
    if width is not None: kwargs["width"] = width
    idx, props = find_peaks(y_arr, **kwargs)
    peaks=[]
    for k,i in enumerate(idx):
        peaks.append({"i": int(i), "x": float(x[i]), "y": float(y[i]), "prom": float(props["prominences"][k])})
    peaks.sort(key=lambda d:d["x"])
    return peaks

ALIASES = {
    "스무딩":"sg","savitzky":"sg","savitzky-golay":"sg","sgfilter":"sg",
    "해밀":"hampel","hampel-filter":"hampel",
    "베이스라인":"baseline","bg":"baseline","asls":"baseline",
    "정규화":"normalize","zscore":"normalize","minmax":"normalize",
    "리샘플":"resample","resampling":"resample",
    "피크":"peaks","peak":"peaks",
    "온셋":"onset","threshold-onset":"onset",
}

def normalize_op_name(op:str) -> str:
    o = (op or "").strip().lower()
    return ALIASES.get(o, o)

def validate_and_normalize_steps(steps: List[dict]):
    warns=[]; norm=[]
    for i,s in enumerate(steps or []):
        if not isinstance(s, dict): warns.append(f"[step {i+1}] invalid"); continue
        op_raw = s.get("op","")
        op = normalize_op_name(op_raw)
        s = {**s, "op": op}
        if op not in OP_REGISTRY and not op.startswith("custom:"):
            warns.append(f"[step {i+1}] unknown op '{op_raw}'")
            s["__unknown__"]=True
        norm.append(s)
    return norm, warns

def execute_steps(x: List[float], y: List[float], steps: List[dict]):
    metas=[]
    for s in steps or []:
        if s.get("__unknown__"): continue
        op = s["op"]; kw = {k:v for k,v in s.items() if k!="op"}
        fn = OP_REGISTRY[op]
        x, y, meta = fn(x, y, kw)
        metas.append({"op":op, **(meta or {})})
    return x, y, metas

# ===================== Planner / Evaluator =====================
PLAN_SYS_PROMPT = """당신은 스펙트럼 분석 오케스트레이터입니다.
다음 DSL YAML만 출력하세요. 설명/코드블록 금지.
pipeline:
  - op: hampel|sg|normalize|resample|baseline|peaks|onset
"""

def plan_from_goal(goal: str, model: str, host: str):
    yml = ollama_generate_once(model, PLAN_SYS_PROMPT + "\n목표:\n" + goal + "\n출력:\n", host, num_predict=400)
    try:
        import yaml
        doc = yaml.safe_load(yml) or {}
        steps = doc.get("pipeline") or []
    except Exception:
        steps = []
    steps, warns = validate_and_normalize_steps(steps)
    return steps, warns, {"raw_yaml": yml}

def revise_plan(prev_steps: List[dict], feedback: dict, model: str, host: str):
    payload = json.dumps({"prev": prev_steps, "feedback": feedback}, ensure_ascii=False)
    yml = ollama_generate_once(model, PLAN_SYS_PROMPT + "\n수정:\n" + payload + "\n출력:\n", host, num_predict=400)
    try:
        import yaml
        doc = yaml.safe_load(yml) or {}
        steps = doc.get("pipeline") or []
    except Exception:
        steps = prev_steps
    steps, warns = validate_and_normalize_steps(steps)
    return steps, warns, {"raw_yaml": yml}

def evaluate_result(x, y_corr, onset_idx, peaks: List[dict]):
    ok=True; fb={}
    if onset_idx is None:
        ok=False; fb["onset"]="not_found"
    n=len(peaks or []); fb["peaks_count"]=n
    if n==0: ok=False; fb["peaks"]="none"
    if n>50: ok=False; fb["peaks"]="too_many"
    arr=np.asarray(y_corr,float)
    if arr.size>16:
        snr=(arr.max()-arr.min())/(arr[:max(8,arr.size//20)].std()+1e-12)
        fb["snr"]=float(snr)
        if snr<2.0:
            ok=False; fb["snr_status"]="low"
    return ok, fb

# ===================== Plot / PDF =====================
def set_korean_font():
    # Try common Korean fonts
    candidates = [
        "Malgun Gothic",               # Windows
        "Apple SD Gothic Neo",         # macOS
        "Noto Sans CJK KR", "Noto Serif CJK KR",
        "NanumGothic", "NanumGothicCoding", "NanumMyeongjo",
    ]
    # Scan system fonts for candidates
    avail = {os.path.basename(p): p for p in findSystemFonts()}
    # Use family name
    fams = [f.name for f in fontManager.ttflist]
    for f in candidates:
        if f in fams:
            matplotlib.rcParams["font.family"] = [f]
            matplotlib.rcParams["axes.unicode_minus"] = False
            return
    # fallback: default (may not render Korean perfectly)
    matplotlib.rcParams["axes.unicode_minus"] = False

set_korean_font()

def plot_layers_png(x: List[float], y: List[float],
                    show_baseline: bool, baseline: Optional[np.ndarray],
                    show_onset: bool, onset_idx: Optional[int],
                    show_peaks: bool, peaks: Optional[List[dict]],
                    xlabel: str, ylabel: str,
                    show_corrected: bool = False, y_corr: Optional[np.ndarray] = None) -> bytes:
    x = np.asarray(x, dtype=float); y=np.asarray(y, dtype=float)
    fig = plt.figure()
    plt.plot(x, y, label="raw")
    if show_baseline and baseline is not None:
        plt.plot(x, baseline, linestyle="--", label="baseline")
    if show_corrected and (y_corr is not None):
        plt.plot(x, y_corr, linestyle="-.", label="corrected")
    if show_peaks and peaks:
        for p in peaks: plt.axvline(float(p["x"]), linestyle=":", linewidth=1)
    if show_onset and onset_idx is not None and 0 <= onset_idx < len(x):
        plt.axvline(float(x[onset_idx]), linestyle="-.", linewidth=1.2)
    plt.xlabel(xlabel); plt.ylabel(ylabel); plt.legend(loc="best")
    buf = BytesIO(); fig.savefig(buf, format="png", bbox_inches="tight", dpi=150); plt.close(fig)
    return buf.getvalue()

def plot_layers_dual_png(
    x: List[float], y: List[float],
    show_baseline: bool, baseline: Optional[np.ndarray],
    show_onset: bool, onset_idx: Optional[int],
    show_peaks: bool, peaks: Optional[List[dict]],
    xlabel: str, ylabel: str,
    y_corr: Optional[np.ndarray]
) -> bytes:
    x = np.asarray(x, dtype=float); y = np.asarray(y, dtype=float)
    if y_corr is None:
        y_corr = y if baseline is None else (y - baseline)
    fig, axes = plt.subplots(1, 2, figsize=(10, 4))
    ax = axes[0]
    ax.plot(x, y, label="raw")
    if show_baseline and (baseline is not None):
        ax.plot(x, baseline, linestyle="--", label="baseline")
    ax.set_xlabel(xlabel); ax.set_ylabel(ylabel); ax.legend(loc="best"); ax.set_title("Raw + Baseline")
    ax = axes[1]
    ax.plot(x, y_corr, label="corrected", linestyle="-.")
    if show_peaks and peaks:
        for p in peaks: ax.axvline(float(p["x"]), linestyle=":", linewidth=1)
    if show_onset and onset_idx is not None and 0 <= onset_idx < len(x):
        ax.axvline(float(x[onset_idx]), linestyle="-.", linewidth=1.2)
    ax.set_xlabel(xlabel); ax.set_ylabel(ylabel); ax.legend(loc="best"); ax.set_title("Corrected + Peaks/Onset")
    buf = BytesIO(); fig.tight_layout(); fig.savefig(buf, format="png", bbox_inches="tight", dpi=150); plt.close(fig)
    return buf.getvalue()

def save_pdf_report(filename: str, pages: List[Dict]):
    """
    pages: list of dict:
      {
        "title": str,
        "images": [png_bytes,...],
        "text_blocks": [str,...],
      }
    """
    with PdfPages(filename) as pdf:
        for page in pages:
            fig = plt.figure(figsize=(11.69, 8.27))  # A4 landscape (inches)
            y_cursor = 0.95
            title = page.get("title","")
            if title:
                plt.text(0.02, y_cursor, title, fontsize=14, weight="bold", transform=plt.gca().transAxes)
                y_cursor -= 0.05
            imgs = page.get("images",[]) or []
            # place up to two images side-by-side
            if imgs:
                cols = 2 if len(imgs)>=2 else 1
                for i,imgb in enumerate(imgs[:2]):
                    ax = plt.axes([0.05 + i*(0.45), 0.45, 0.42, 0.42]) if cols==2 else plt.axes([0.05, 0.45, 0.9, 0.42])
                    im = Image.open(BytesIO(imgb))
                    ax.imshow(im); ax.axis("off")
                y_cursor = 0.42
            blocks = page.get("text_blocks",[]) or []
            # print text blocks stacked
            for blk in blocks:
                txt = textwrap.fill(str(blk), width=110)
                plt.text(0.03, y_cursor, txt, fontsize=10, transform=plt.gca().transAxes, va='top')
                y_cursor -= 0.08 + 0.015*(len(txt)//80)
            plt.axis("off"); pdf.savefig(fig, bbox_inches="tight"); plt.close(fig)

# ===================== HTML =====================
def html_form():
    host_info = f"Ollama host: <code>{OLLAMA_HOST}</code> · TEXT_MODEL: <code>{TEXT_MODEL}</code>"
    token_hint = f"(Auth ON)" if AUTH_TOKEN else "(Auth OFF)"
    return f"""<!doctype html>
<html><head><meta charset="utf-8"><title>Spectrum Agent Starter</title>
<style>
body{{max-width:1100px;margin:20px auto;font-family:system-ui,-apple-system,Segoe UI,Roboto}}
.flex{{display:flex;gap:12px;flex-wrap:wrap}}
.preview img{{max-height:240px;border:1px solid #ddd;padding:4px;background:#fff}}
.result{{white-space:pre-wrap;border:1px solid #ddd;padding:12px;background:#fafafa}}
.small{{color:#666;font-size:12px}}
.tablabel{{padding:6px 12px;cursor:pointer;background:#eee;margin-right:4px;border:1px solid #ddd;display:inline-block}}
.tabpanel{{display:none;padding:8px;border:1px solid #ddd;margin-top:4px}}
.tabpanel.active{{display:block}}
</style></head><body>
<h1>📈 Spectrum Agent Starter</h1>
<div class="small">{host_info} · {token_hint}</div>

<form action="/analyze" method="post" enctype="multipart/form-data">
  <label>CSV/Excel (multi)</label><br>
  <input id="fileInput" type="file" name="csv" accept=".csv,.tsv,application/vnd.openxmlformats-officedocument.spreadsheetml.sheet,application/vnd.ms-excel" multiple required />

  <fieldset style="margin-top:10px;">
    <legend>실행 모드</legend>
    <label><input type="radio" name="run_mode" value="manual" checked> 수동(체크박스)</label>
    <label><input type="radio" name="run_mode" value="agent"> 목표 기반(Agent)</label>
  </fieldset>

  <label>Prompt / Goal</label>
  <textarea name="prompt" rows="5" style="width:100%;">응답은 반드시 한국어로. 주요 피크와 물리적 의미를 요약하라.</textarea>

  <fieldset style="margin-top:10px;">
    <legend>전처리 옵션 (수동 모드)</legend>
    <div class="flex">
      <label><input type="checkbox" name="do_hampel" value="1"> Hampel</label>
      <label><input type="checkbox" name="do_sg" value="1" checked> Savitzky–Golay</label>
      <label><input type="checkbox" name="do_bg" value="1" checked> Baseline (AsLS)</label>
      <label><input type="checkbox" name="do_onset" value="1"> Onset</label>
      <label><input type="checkbox" name="do_peaks" value="1" checked> Peaks</label>
      <label><input type="checkbox" name="show_corrected" value="1" checked> Show corrected</label>
    </div>
    <div class="flex">
      <div style="flex:1"><label>Hampel window</label><input name="hampel_win" value="7"></div>
      <div style="flex:1"><label>Hampel sigma</label><input name="hampel_sigma" value="3.0"></div>
      <div style="flex:1"><label>SG window</label><input name="sg_win" value="11"></div>
      <div style="flex:1"><label>SG poly</label><input name="sg_poly" value="3"></div>
    </div>
    <div class="flex">
      <div style="flex:1"><label>AsLS λ</label><input name="bg_lam" value="100000"></div>
      <div style="flex:1"><label>AsLS p</label><input name="bg_p" value="0.01"></div>
      <div style="flex:1"><label>AsLS iter</label><input name="bg_iter" value="10"></div>
    </div>
    <div class="flex">
      <div style="flex:1"><label>Onset win</label><input name="on_win" value="11"></div>
      <div style="flex:1"><label>Onset poly</label><input name="on_poly" value="2"></div>
      <div style="flex:1"><label>Onset z</label><input name="on_z" value="3.0"></div>
    </div>
    <div class="flex">
      <div style="flex:1"><label>peaks.prominence_frac</label><input name="prom_frac" value="0.02"></div>
      <div style="flex:1"><label>peaks.distance_pts</label><input name="dist_pts" value="5"></div>
      <div style="flex:1"><label>peaks.width</label><input name="width" value=""></div>
    </div>
  </fieldset>

  <div class="flex" style="margin-top:10px;">
    <button type="button" onclick="doPreview()">Preview First File</button>
    <button type="submit">Analyze</button>
  </div>
</form>

<h3>Data Preview</h3>
<div id="preview" class="preview"></div>
<div id="previewStats" class="small"></div>

<hr>
<h2>Chatbot</h2>
<div class="flex">
  <input id="chatInput" style="flex:1" placeholder="모델에게 질문하세요…">
  <button onclick="sendChat()">Send</button>
</div>
<pre id="chatLog" class="result" style="margin-top:8px;min-height:140px;"></pre>

<script>
const AUTH_TOKEN = {"on": %s, "val": "%s"};

async function doPreview(){
  const fi = document.getElementById('fileInput');
  const preview = document.getElementById('preview');
  const stats = document.getElementById('previewStats');
  preview.innerHTML = ''; stats.textContent='';
  if(!fi.files || !fi.files.length){ alert('파일을 선택하세요'); return; }
  const fd = new FormData(); fd.append('csv', fi.files[0]);
  try{
    const res = await fetch('/preview', {
      method: 'POST',
      headers: AUTH_TOKEN.on ? {'X-Auth-Token': AUTH_TOKEN.val} : {},
      body: fd
    });
    if(!res.ok){ throw new Error(await res.text()); }
    const data = await res.json();
    preview.innerHTML = '<img src="'+data.png+'">';
    const s = data.stats || {};
    stats.textContent = `points=${s.n_points} | ${s.xlabel}: ${s.x_min}~${s.x_max} | ${s.ylabel}: ${s.y_min}~${s.y_max}`;
  }catch(e){
    preview.innerHTML = '<div class="small" style="color:#b00;">미리보기 실패</div>';
    console.error(e);
  }
}

async function sendChat(){
  const input = document.getElementById('chatInput');
  const log = document.getElementById('chatLog');
  const text = input.value.trim(); if(!text) return;
  log.textContent += "\\n[You] "+text+"\\n";
  input.value='';
  try{
    const res = await fetch('/chat', {
      method:'POST',
      headers: Object.assign({'Content-Type':'application/json'}, AUTH_TOKEN.on ? {'X-Auth-Token': AUTH_TOKEN.val} : {}),
      body: JSON.stringify({message: text})
    });
    if(!res.ok){ throw new Error(await res.text()); }
    const data = await res.json();
    log.textContent += "[Model] "+data.reply+"\\n";
  }catch(e){
    log.textContent += "[Error] "+e.message+"\\n";
  }
}
</script>

</body></html>
""" % ("true" if AUTH_TOKEN else "false", (AUTH_TOKEN or ""))
# end html_form()

# ===================== Routes =====================
@app.get("/", response_class=HTMLResponse)
def index(_: bool = Depends(require_auth)):
    return html_form()

@app.get("/debug/models")
def debug_models(_: bool = Depends(require_auth)):
    return {
        "host": OLLAMA_HOST,
        "available_models": list_model_names(OLLAMA_HOST),
        "text_model_fixed": TEXT_MODEL,
    }

@app.post("/preview")
def preview(csv: UploadFile = File(...), _ok: bool = Depends(require_auth)):
    b = csv.file.read()
    csv.file.close()
    low = (csv.filename or "").lower()
    if low.endswith((".xlsx",".xls")):
        x, y, xlabel, ylabel = parse_excel(b)
    else:
        x, y, xlabel, ylabel = parse_spectrum_csv(b)
    # simple plot
    fig = plt.figure()
    plt.plot(x, y); plt.xlabel(xlabel); plt.ylabel(ylabel)
    buf = BytesIO(); fig.savefig(buf, format="png", bbox_inches="tight", dpi=150); plt.close(fig)
    img64 = base64.b64encode(buf.getvalue()).decode()
    stats = {
        "n_points": len(x),
        "xlabel": xlabel, "ylabel": ylabel,
        "x_min": float(np.min(x)), "x_max": float(np.max(x)),
        "y_min": float(np.min(y)), "y_max": float(np.max(y)),
    }
    return JSONResponse({"png": "data:image/png;base64,"+img64, "stats": stats})

@app.post("/chat")
def chat(message: Dict[str,str], _ok: bool = Depends(require_auth)):
    model_chosen = ensure_model_installed(TEXT_MODEL, OLLAMA_HOST)
    user_text = (message.get("message") or "").strip()
    if not user_text:
        raise HTTPException(status_code=400, detail="empty message")
    reply = ollama_generate_once(model_chosen, user_text, OLLAMA_HOST, num_predict=600)
    return {"reply": reply}

@app.post("/analyze", response_class=HTMLResponse)
def analyze(
    csv: List[UploadFile] = File(...),
    prompt: str = Form(""),
    run_mode: str = Form("manual"),

    # manual options
    do_hampel: Optional[str] = Form(None),
    do_sg: Optional[str] = Form(None),
    do_bg: Optional[str] = Form(None),
    do_onset: Optional[str] = Form(None),
    do_peaks: Optional[str] = Form(None),
    show_corrected: Optional[str] = Form(None),

    hampel_win: str = Form("7"),
    hampel_sigma: str = Form("3.0"),
    sg_win: str = Form("11"),
    sg_poly: str = Form("3"),

    bg_lam: str = Form("100000"),
    bg_p: str = Form("0.01"),
    bg_iter: str = Form("10"),

    on_win: str = Form("11"),
    on_poly: str = Form("2"),
    on_z: str = Form("3.0"),

    prom_frac: str = Form("0.02"),
    dist_pts: str = Form("5"),
    width: str = Form(""),

    _ok: bool = Depends(require_auth)
):
    def f2(v,d):
        try: return float(v)
        except: return d
    def i2(v,d):
        try: return int(float(v))
        except: return d

    use_hampel = (do_hampel=="1")
    use_sg     = (do_sg=="1")
    use_bg     = (do_bg=="1")
    use_onset  = (do_onset=="1")
    use_peaks  = (do_peaks=="1")
    show_corr  = (show_corrected=="1")

    hampel_win_v   = max(i2(hampel_win,7),3)
    hampel_sigma_v = max(f2(hampel_sigma,3.0),0.1)
    sg_win_v       = max(i2(sg_win,11),5)
    sg_poly_v      = max(i2(sg_poly,3),2)

    bg_lam_v  = max(f2(bg_lam,1e5),1.0)
    bg_p_v    = min(max(f2(bg_p,0.01),1e-5),0.5)
    bg_iter_v = max(i2(bg_iter,10),1)

    on_win_v  = max(i2(on_win,11),5)
    on_poly_v = max(i2(on_poly,2),1)
    on_z_v    = max(f2(on_z,3.0),0.1)

    prom_frac_v = f2(prom_frac,0.02)
    dist_pts_v  = max(i2(dist_pts,5),1)
    width_v     = f2(width,None) if (width or "").strip() else None

    model_chosen = ensure_model_installed(TEXT_MODEL, OLLAMA_HOST)

    # read all uploaded files (sync)
    file_payloads=[]
    for k, uf in enumerate(csv,1):
        name = uf.filename or f"file_{k}"
        b = uf.file.read()
        uf.file.close()
        file_payloads.append({"name":name,"bytes":b})

    tabs_labels=[]; tabs_panes=[]
    pdf_pages=[]

    for fp in file_payloads:
        name = fp["name"]; b = fp["bytes"]
        low = name.lower()
        if low.endswith((".xlsx",".xls")):
            x, y, xlabel, ylabel = parse_excel(b)
        else:
            x, y, xlabel, ylabel = parse_spectrum_csv(b)

        # ---------- AGENT MODE ----------
        if run_mode == "agent":
            steps, warns, meta0 = plan_from_goal(prompt, model_chosen, OLLAMA_HOST)
            history=[]
            x2, y2 = x, y
            for it in range(1, MAX_AGENT_ITERS+1):
                x2, y2, metas = execute_steps(x2, y2, steps)
                y_work = np.asarray(y2, float)
                baseline = None
                # Onset/Peaks for evaluation
                onset_idx = detect_onset(x2, y_work.tolist(), win=on_win_v, poly=on_poly_v, z_thresh=on_z_v)
                peaks = detect_peaks_list(x2, y_work.tolist(), prominence_frac=prom_frac_v, distance_pts=dist_pts_v, width=width_v)
                ok, fb = evaluate_result(x2, y_work, onset_idx, peaks)
                history.append({"iter":it, "steps":steps, "feedback":fb})
                if ok: break
                steps, warns2, meta1 = revise_plan(steps, fb, model_chosen, OLLAMA_HOST)

            # plots
            overlay_png = plot_layers_dual_png(
                x2, y,
                show_baseline=False, baseline=None,
                show_onset=True, onset_idx=onset_idx,
                show_peaks=True, peaks=peaks,
                xlabel=xlabel, ylabel=ylabel,
                y_corr=np.asarray(y2,float)
            )
            img64 = base64.b64encode(overlay_png).decode()

            # llm interpretation on final
            base_prompt = (
                (prompt or "응답은 반드시 한국어로. 스펙트럼만 해석하라.").strip()
                + "\n\n[Agent 최종 단계]\n"
                + json.dumps(steps, ensure_ascii=False)
                + "\n[검출]\n"
                + (f"onset @ {x2[onset_idx]:.6g}" if onset_idx is not None else "(no onset)")
                + "\npeaks:\n"
                + "\n".join([f"- peak @ {p['x']:.6g} (y={p['y']:.6g}, prom={p['prom']:.6g})" for p in peaks]) or "(no peaks)"
                + "\n이미지/형태 언급 금지."
            )
            out_text = ollama_generate_once(model_chosen, base_prompt, OLLAMA_HOST, num_predict=800)

            # tab pane
            pane_inner = (
                f"<h3>{name} — Agent</h3>"
                f"<div class='preview'><img src='data:image/png;base64,{img64}'></div>"
                "<details class='small'><summary>Agent steps</summary><pre>"
                + json.dumps(steps, indent=2, ensure_ascii=False) + "</pre></details>"
                "<details class='small'><summary>History</summary><pre>"
                + json.dumps(history, indent=2, ensure_ascii=False) + "</pre></details>"
                "<h3>LLM 해석</h3><div class='result'>"+ out_text + "</div>"
            )
            tabs_labels.append(f"<span class='tablabel'>{name}</span>")
            tabs_panes.append(f"<div class='tabpanel'>{pane_inner}</div>")

            # pdf
            pdf_pages.append({
                "title": f"{name} — Agent 결과",
                "images": [overlay_png],
                "text_blocks": [
                    "최종 Steps:\n" + json.dumps(steps, ensure_ascii=False),
                    "History 요약:\n" + json.dumps(history[-1] if history else {}, ensure_ascii=False),
                    "LLM 해석:\n" + out_text
                ]
            })

            continue

        # ---------- MANUAL MODE ----------
        y_work = np.asarray(y, float)
        if use_hampel:
            x_, y_work, _ = op_hampel(x, y_work.tolist(), {"window":hampel_win_v,"sigma":hampel_sigma_v})
            y_work = np.asarray(y_work, float)
        if use_sg:
            x_, y_work, _ = op_sg(x, y_work.tolist(), {"window":sg_win_v,"poly":sg_poly_v})
            y_work = np.asarray(y_work, float)

        baseline = None
        y_corr = y_work.copy()
        if use_bg:
            baseline = baseline_asls(y_corr, lam=bg_lam_v, p=bg_p_v, niter=bg_iter_v)
            y_corr = y_corr - baseline

        onset_idx = detect_onset(x, y_corr.tolist(), win=on_win_v, poly=on_poly_v, z_thresh=on_z_v) if use_onset else None
        peaks = detect_peaks_list(x, y_corr.tolist(), prominence_frac=prom_frac_v, distance_pts=dist_pts_v, width=width_v) if use_peaks else []

        if show_corr:
            overlay_png = plot_layers_dual_png(
                x, y,
                show_baseline=use_bg, baseline=baseline,
                show_onset=use_onset, onset_idx=onset_idx,
                show_peaks=use_peaks, peaks=peaks,
                xlabel=xlabel, ylabel=ylabel,
                y_corr=y_corr
            )
        else:
            overlay_png = plot_layers_png(
                x, y,
                show_baseline=use_bg, baseline=baseline,
                show_onset=use_onset, onset_idx=onset_idx,
                show_peaks=use_peaks, peaks=peaks,
                xlabel=xlabel, ylabel=ylabel,
                show_corrected=False, y_corr=None
            )
        img64 = base64.b64encode(overlay_png).decode()

        top_peaks_text = "\n".join([f"- peak @ {p['x']:.6g} (y={p['y']:.6g}, prom={p['prom']:.6g})" for p in peaks]) or "(no peaks)"
        onset_line = f"onset @ {x[onset_idx]:.6g}" if (onset_idx is not None) else "(no onset)"
        base_prompt = (
            (prompt or "응답은 반드시 한국어로. 스펙트럼만 해석하라.").strip()
            + "\n\n[전처리 옵션] DN(Hampel="+("Y" if use_hampel else "N")
            + ", SG="+("Y" if use_sg else "N")+") | BG="+("Y" if use_bg else "N")
            + " | ONSET="+("Y" if use_onset else "N")+" | PEAKS="+("Y" if use_peaks else "N")
            + f"\n[축] X={xlabel}, Y={ylabel}"
            + f"\n[onset] {onset_line}\n[peaks]\n{top_peaks_text}\n이미지/형태 언급 금지."
        )
        out_text = ollama_generate_once(model_chosen, base_prompt, OLLAMA_HOST, num_predict=800)

        params = {
            "Hampel": f"window={hampel_win_v}, sigma={hampel_sigma_v}" if use_hampel else "OFF",
            "Savitzky–Golay": f"window={sg_win_v}, poly={sg_poly_v}" if use_sg else "OFF",
            "AsLS": f"λ={bg_lam_v}, p={bg_p_v}, iter={bg_iter_v}" if use_bg else "OFF",
            "Onset": f"win={on_win_v}, poly={on_poly_v}, z={on_z_v}" if use_onset else "OFF",
            "Peaks": f"prom_frac={prom_frac_v}, dist={dist_pts_v}, width={width_v}" if use_peaks else "OFF"
        }
        param_lines = "\n".join([f"{k}: {v}" for k, v in params.items()])
        param_html = (
            "<details class='small' style='margin:8px 0;'><summary>전처리 파라미터</summary>"
            "<pre style='white-space:pre-wrap;'>" + param_lines + "</pre></details>"
        )

        pane_inner = (
            f"<h3>{name} — Manual</h3>"
            f"<div class='preview'><img src='data:image/png;base64,{img64}'></div>"
            "<h3>Server Summary (text fed to LLM)</h3>"
            "<pre>"+ base_prompt + "</pre>"
            + param_html +
            "<h3>LLM 해석</h3><div class='result'>"+ out_text + "</div>"
        )
        tabs_labels.append(f"<span class='tablabel'>{name}</span>")
        tabs_panes.append(f"<div class='tabpanel'>{pane_inner}</div>")

        # PDF pages
        pdf_pages.append({
            "title": f"{name} — Manual 결과",
            "images": [overlay_png],
            "text_blocks": [
                "전처리 파라미터:\n"+param_lines,
                "LLM 해석:\n"+out_text
            ]
        })

    # Build tabs HTML
    tabs_html = (
        "<div id='tablabels'>" + "".join(tabs_labels) + "</div>"
        + "<div id='tabpanes'>" + "".join(tabs_panes) + "</div>"
        + "<script>(function(){const ls=[...document.querySelectorAll('.tablabel')];const ps=[...document.querySelectorAll('.tabpanel')];function act(i){ls.forEach((el,k)=>el.style.fontWeight=k===i?'bold':'normal');ps.forEach((el,k)=>el.className='tabpanel'+(k===i?' active':''));}ls.forEach((el,i)=>el.addEventListener('click',()=>act(i)));if(ls.length)act(0);})();</script>"
    )

    # Save PDF to memory and offer a download link
    pdf_buf = BytesIO()
    save_pdf_report(pdf_buf, pdf_pages)
    pdf_bytes = pdf_buf.getvalue()
    pdf_b64 = base64.b64encode(pdf_bytes).decode()

    html = f"""<!doctype html><html><head><meta charset="utf-8"><title>Results</title>
<style>
.preview img{{max-height:240px;border:1px solid #ddd;padding:4px;background:#fff}}
.result{{white-space:pre-wrap;border:1px solid #ddd;padding:12px;background:#fafafa}}
.small{{color:#666;font-size:12px}}
.tablabel{{padding:6px 12px;cursor:pointer;background:#eee;margin-right:4px;border:1px solid #ddd;display:inline-block}}
.tabpanel{{display:none;padding:8px;border:1px solid #ddd;margin-top:4px}}
.tabpanel.active{{display:block}}
</style></head><body>
<h2>Results ({ensure_model_installed(TEXT_MODEL, OLLAMA_HOST)} @ {OLLAMA_HOST})</h2>
<p><a download="spectrum_report.pdf" href="data:application/pdf;base64,{pdf_b64}">📄 PDF 다운로드</a></p>
{tabs_html}
<p class="small">* Analyze (동기) · 스트리밍 미사용 · Agent/Manual 지원</p>
<p><a href="/">← Back</a></p>
</body></html>"""
    return HTMLResponse(html)

# ===================== main =====================
if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="127.0.0.1", port=7862, log_level="info")