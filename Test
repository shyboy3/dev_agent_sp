index.html

<!DOCTYPE html>
<html lang="ko">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>A6000 AI Lab Analyst</title>
    
    <!-- 1. ìŠ¤íƒ€ì¼ ë„êµ¬ (Tailwind CSS) -->
    <script src="https://cdn.tailwindcss.com"></script>
    <!-- 2. ë°˜ì‘í˜• ë¡œì§ ë„êµ¬ (Vue.js 3) -->
    <script src="https://unpkg.com/vue@3/dist/vue.global.js"></script>
    <!-- 3. ì°¨íŠ¸ ë„êµ¬ (Plotly.js) -->
    <script src="https://cdn.plot.ly/plotly-2.27.0.min.js"></script>
    <!-- 4. ë§ˆí¬ë‹¤ìš´ ë Œë”ëŸ¬ (Marked) -->
    <script src="https://cdn.jsdelivr.net/npm/marked/marked.min.js"></script>
    <!-- 5. ì•„ì´ì½˜ (Lucide) -->
    <script src="https://unpkg.com/lucide@latest"></script>

    <style>
        .loader {
            border-top-color: #3498db;
            -webkit-animation: spinner 1.5s linear infinite;
            animation: spinner 1.5s linear infinite;
        }
        @keyframes spinner {
            0% { transform: rotate(0deg); }
            100% { transform: rotate(360deg); }
        }
    </style>
</head>
<body class="bg-slate-100 text-slate-800 font-sans">
    <div id="app" class="min-h-screen p-6">
        
        <!-- Header -->
        <header class="max-w-7xl mx-auto mb-8 flex items-center justify-between">
            <div class="flex items-center gap-3">
                <div class="bg-indigo-600 p-2 rounded-lg text-white">
                    <i data-lucide="microscope" class="w-8 h-8"></i>
                </div>
                <div>
                    <h1 class="text-2xl font-bold text-slate-900">A6000 Multi-modal Analyst</h1>
                    <p class="text-sm text-slate-500">Dual GPU Powered Local Analysis System</p>
                </div>
            </div>
            <div class="flex gap-2 text-xs font-mono bg-slate-200 px-3 py-1 rounded">
                <span class="text-green-600">â— System Ready</span>
                <span>|</span>
                <span>GPU: A6000 x2</span>
            </div>
        </header>

        <main class="max-w-7xl mx-auto grid grid-cols-1 lg:grid-cols-3 gap-6">
            
            <!-- Left: Upload & Status -->
            <div class="space-y-6">
                <!-- Dropzone -->
                <div 
                    class="bg-white p-8 rounded-xl shadow-sm border-2 border-dashed border-indigo-100 hover:border-indigo-400 transition-colors text-center cursor-pointer"
                    @dragover.prevent
                    @drop.prevent="handleDrop"
                    @click="$refs.fileInput.click()"
                >
                    <input type="file" ref="fileInput" multiple class="hidden" @change="handleFileSelect">
                    <i data-lucide="upload-cloud" class="w-12 h-12 mx-auto text-indigo-300 mb-4"></i>
                    <p class="font-medium text-slate-700">íŒŒì¼ì„ ë“œë˜ê·¸í•˜ê±°ë‚˜ í´ë¦­í•˜ì„¸ìš”</p>
                    <p class="text-xs text-slate-400 mt-2">CSV, PNG, JPG, PDF ì§€ì›</p>
                </div>

                <!-- File List -->
                <div v-if="files.length > 0" class="bg-white rounded-xl shadow-sm border border-slate-200 overflow-hidden">
                    <div class="p-4 bg-slate-50 border-b border-slate-100 font-semibold text-sm">
                        ì—…ë¡œë“œ ëŒ€ê¸° ëª©ë¡ ({{ files.length }})
                    </div>
                    <ul class="max-h-60 overflow-y-auto">
                        <li v-for="(file, index) in files" :key="index" class="p-3 border-b last:border-0 flex justify-between items-center text-sm">
                            <span class="truncate max-w-[200px]">{{ file.name }}</span>
                            <button @click.stop="removeFile(index)" class="text-red-400 hover:text-red-600">Ã—</button>
                        </li>
                    </ul>
                    <div class="p-4">
                        <button 
                            @click="analyze" 
                            :disabled="isAnalyzing"
                            class="w-full py-3 bg-indigo-600 hover:bg-indigo-700 text-white rounded-lg font-bold flex justify-center items-center gap-2 disabled:opacity-50 disabled:cursor-not-allowed"
                        >
                            <span v-if="isAnalyzing" class="loader ease-linear rounded-full border-2 border-t-2 border-gray-200 h-5 w-5"></span>
                            {{ isAnalyzing ? 'ë¶„ì„ ì¤‘...' : 'ì¢…í•© ë¶„ì„ ì‹¤í–‰' }}
                        </button>
                    </div>
                </div>
            </div>

            <!-- Right: Results -->
            <div class="lg:col-span-2 space-y-6">
                
                <!-- Initial State -->
                <div v-if="!result && !isAnalyzing" class="h-96 bg-white rounded-xl shadow-sm border border-slate-200 flex flex-col items-center justify-center text-slate-400">
                    <i data-lucide="bar-chart-2" class="w-16 h-16 mb-4 opacity-50"></i>
                    <p>ë°ì´í„°ë¥¼ ì—…ë¡œë“œí•˜ë©´ ë¶„ì„ ê²°ê³¼ê°€ ì—¬ê¸°ì— í‘œì‹œë©ë‹ˆë‹¤.</p>
                </div>

                <!-- Analysis Results -->
                <div v-if="result" class="space-y-6 animate-fade-in">
                    
                    <!-- 1. Final Report -->
                    <div class="bg-white p-6 rounded-xl shadow-sm border border-slate-200">
                        <h2 class="text-xl font-bold mb-4 flex items-center gap-2 text-indigo-800">
                            <i data-lucide="file-text"></i> ì¢…í•© ë¦¬í¬íŠ¸
                        </h2>
                        <div class="prose prose-slate max-w-none" v-html="renderMarkdown(result.final_report)"></div>
                    </div>

                    <!-- 2. Individual Results -->
                    <div v-for="(item, idx) in result.results" :key="idx" class="bg-white p-6 rounded-xl shadow-sm border border-slate-200">
                        
                        <!-- Spectrum Type -->
                        <div v-if="item.type === 'spectrum'">
                            <h3 class="font-bold text-slate-700 mb-2 flex items-center gap-2">
                                <i data-lucide="activity"></i> {{ item.filename }}
                            </h3>
                            <p class="text-sm text-slate-500 mb-4">{{ item.summary }}</p>
                            <!-- Chart Container -->
                            <div :id="'chart-' + idx" class="w-full h-64 bg-slate-50 rounded"></div>
                        </div>

                        <!-- Image Type -->
                        <div v-if="item.type === 'image'">
                            <h3 class="font-bold text-slate-700 mb-2 flex items-center gap-2">
                                <i data-lucide="image"></i> {{ item.filename }}
                            </h3>
                            <div class="grid grid-cols-2 gap-4">
                                <img :src="'data:image/jpeg;base64,' + item.image_b64" class="rounded border border-slate-200 max-h-64 object-contain">
                                <div class="text-sm text-slate-600 bg-slate-50 p-4 rounded">
                                    <strong>AI Analysis:</strong><br>
                                    {{ item.summary }}
                                </div>
                            </div>
                        </div>

                    </div>
                </div>
            </div>
        </main>
    </div>

    <script>
        const { createApp, ref, nextTick } = Vue;

        createApp({
            setup() {
                const files = ref([]);
                const isAnalyzing = ref(false);
                const result = ref(null);

                const handleFileSelect = (e) => {
                    files.value.push(...Array.from(e.target.files));
                };

                const handleDrop = (e) => {
                    files.value.push(...Array.from(e.dataTransfer.files));
                };

                const removeFile = (index) => {
                    files.value.splice(index, 1);
                };

                const renderMarkdown = (text) => {
                    return marked.parse(text || '');
                };

                const analyze = async () => {
                    if (files.value.length === 0) return;
                    
                    isAnalyzing.value = true;
                    result.value = null;
                    
                    const formData = new FormData();
                    files.value.forEach(f => formData.append('files', f));

                    try {
                        const response = await fetch('/api/analyze', {
                            method: 'POST',
                            body: formData
                        });
                        const data = await response.json();
                        result.value = data;

                        // Wait for DOM update then draw charts
                        await nextTick();
                        drawCharts(data.results);
                        
                        // Re-initialize icons
                        lucide.createIcons();

                    } catch (e) {
                        alert("ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: " + e);
                    } finally {
                        isAnalyzing.value = false;
                    }
                };

                const drawCharts = (results) => {
                    results.forEach((item, idx) => {
                        if (item.type === 'spectrum' && item.chart_data) {
                            const trace = {
                                x: item.chart_data.map(d => d.x),
                                y: item.chart_data.map(d => d.y),
                                mode: 'lines',
                                line: {color: '#4f46e5'},
                                name: 'Signal'
                            };
                            
                            // Peaks
                            const peaks = {
                                x: item.peaks.map(p => p.x),
                                y: item.peaks.map(p => p.y),
                                mode: 'markers',
                                marker: {color: 'red', size: 8},
                                name: 'Peaks'
                            };

                            const layout = {
                                margin: {t: 20, b: 40, l: 40, r: 20},
                                showlegend: false,
                                xaxis: {title: 'Wavelength/Index'},
                                yaxis: {title: 'Intensity'}
                            };

                            Plotly.newPlot('chart-' + idx, [trace, peaks], layout);
                        }
                    });
                };

                // Initialize icons on mount
                setTimeout(() => lucide.createIcons(), 100);

                return {
                    files, isAnalyzing, result,
                    handleFileSelect, handleDrop, removeFile, analyze, renderMarkdown
                };
            }
        }).mount('#app');
    </script>
</body>
</html>



main.py

import os
import io
import asyncio
import base64
import json
from typing import List, Dict, Any
from contextlib import asynccontextmanager

from fastapi import FastAPI, UploadFile, File
from fastapi.responses import HTMLResponse, FileResponse
from fastapi.middleware.cors import CORSMiddleware

import pandas as pd
import numpy as np
from scipy.signal import find_peaks
import cv2
from ollama import Client  # â˜… ìˆ˜ì •ë¨: Client í´ë˜ìŠ¤ import
import torch
from PIL import Image
from pdf2image import convert_from_bytes
from transformers import Qwen2VLForConditionalGeneration, AutoProcessor
from qwen_vl_utils import process_vision_info

# --- 0. ì„¤ì • ---
# â˜… ìˆ˜ì •ë¨: í˜¸ìŠ¤íŠ¸ ì£¼ì†Œë¥¼ ëª…ì‹œì ìœ¼ë¡œ ì§€ì •
OLLAMA_HOST = "http://127.0.0.1:11434"
ollama_client = Client(host=OLLAMA_HOST)

# --- 1. ëª¨ë¸ ë° ë¦¬ì†ŒìŠ¤ ê´€ë¦¬ ---
ml_models = {}

@asynccontextmanager
async def lifespan(app: FastAPI):
    print(f"ğŸš€ Initializing A6000 Backend Server...")
    
    # 1. Ollama ì—°ê²° í…ŒìŠ¤íŠ¸
    try:
        print(f"ğŸ“¡ Testing connection to Ollama at {OLLAMA_HOST}...")
        ollama_client.list() # ì—°ê²° í…ŒìŠ¤íŠ¸ìš© í˜¸ì¶œ
        print("âœ… Ollama Connected Successfully!")
    except Exception as e:
        print(f"âŒ Ollama Connection FAILED! Make sure 'ollama serve' is running.")
        print(f"   Error: {e}")

    # 2. HuggingFace Qwen2-VL ë¡œë“œ (A6000ìš©)
    print("â³ Loading Qwen2-VL Model (This may take a while)...")
    try:
        model_path = "Qwen/Qwen2-VL-7B-Instruct" 
        
        ml_models["qwen_model"] = Qwen2VLForConditionalGeneration.from_pretrained(
            model_path,
            torch_dtype=torch.bfloat16,
            device_map="auto",
            trust_remote_code=True
        )
        ml_models["qwen_processor"] = AutoProcessor.from_pretrained(model_path, min_pixels=256*28*28, max_pixels=1280*28*28)
        print("âœ… Qwen2-VL Model Loaded!")
    except Exception as e:
        print(f"âš ï¸ Qwen2-VL Load Failed: {e}")
        print("   -> PDF Vision Analysis will be skipped.")
    
    yield
    
    ml_models.clear()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()

app = FastAPI(title="A6000 Final Analyst", lifespan=lifespan)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_methods=["*"],
    allow_headers=["*"],
)

# --- 2. ë¶„ì„ ì—”ì§„ ---

def run_qwen_inference(image: Image.Image, prompt: str) -> str:
    """Qwen2-VL Inference"""
    if "qwen_model" not in ml_models:
        return "[Error] Vision Model not loaded."

    model = ml_models["qwen_model"]
    processor = ml_models["qwen_processor"]

    messages = [
        {
            "role": "user",
            "content": [
                {"type": "image", "image": image},
                {"type": "text", "text": prompt},
            ],
        }
    ]

    text_input = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    image_inputs, video_inputs = process_vision_info(messages)
    
    inputs = processor(
        text=[text_input],
        images=image_inputs,
        videos=video_inputs,
        padding=True,
        return_tensors="pt",
    ).to(model.device)

    with torch.no_grad():
        generated_ids = model.generate(**inputs, max_new_tokens=1024)
    
    generated_ids_trimmed = [
        out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
    ]
    output_text = processor.batch_decode(
        generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False
    )[0]
    
    return output_text

async def analyze_pdf_advanced(content: bytes, filename: str):
    try:
        # Poppler ê²½ë¡œ ë¬¸ì œì‹œ ì—ëŸ¬ ë°œìƒ ê°€ëŠ¥
        images = convert_from_bytes(content, first_page=1, last_page=3, fmt='jpeg')
        
        full_text = ""
        preview_image_b64 = None

        for i, img in enumerate(images):
            if i == 0:
                buffered = io.BytesIO()
                img.save(buffered, format="JPEG")
                preview_image_b64 = base64.b64encode(buffered.getvalue()).decode('utf-8')

            page_content = run_qwen_inference(
                img, 
                "Transcribe this page into Markdown. Describe charts/tables in detail."
            )
            full_text += f"\n--- Page {i+1} ---\n{page_content}\n"

        return {
            "type": "image",
            "filename": filename,
            "summary": f"**PDF Vision Analysis**\n\n{full_text}",
            "image_b64": preview_image_b64
        }
    except Exception as e:
        return {"type": "error", "filename": filename, "msg": f"PDF Error: {str(e)}"}

async def analyze_spectrum(content: bytes, filename: str):
    try:
        try:
            df = pd.read_csv(io.BytesIO(content))
        except:
            df = pd.read_csv(io.BytesIO(content), delimiter='\t')
            
        x = df.iloc[:, 0].values
        y = df.iloc[:, 1].values if df.shape[1] > 1 else df.iloc[:, 0].values
        
        peaks, _ = find_peaks(y, height=np.mean(y), distance=20)
        
        step = max(1, len(x) // 1000)
        chart_data = [{"x": float(xi), "y": float(yi)} for xi, yi in zip(x[::step], y[::step])]
        peak_vals = [{"x": float(x[p]), "y": float(y[p])} for p in peaks]

        return {
            "type": "spectrum",
            "filename": filename,
            "summary": f"Detected {len(peaks)} major peaks.",
            "chart_data": chart_data,
            "peaks": peak_vals
        }
    except Exception as e:
        return {"type": "error", "filename": filename, "msg": str(e)}

async def analyze_image_vision(content: bytes, filename: str):
    try:
        img = Image.open(io.BytesIO(content)).convert("RGB")
        description = run_qwen_inference(img, "Analyze this scientific image.")
        
        buffered = io.BytesIO()
        img.save(buffered, format="JPEG")
        img_b64 = base64.b64encode(buffered.getvalue()).decode('utf-8')
        
        return {
            "type": "image",
            "filename": filename,
            "summary": description,
            "image_b64": img_b64
        }
    except Exception as e:
        return {"type": "error", "filename": filename, "msg": str(e)}

# --- 3. ë¼ìš°í„° ---

@app.post("/api/analyze")
async def analyze_orchestrator(files: List[UploadFile] = File(...)):
    tasks = []
    
    for file in files:
        content = await file.read()
        fname = file.filename.lower()
        
        if fname.endswith('.pdf'):
            tasks.append(analyze_pdf_advanced(content, file.filename))
        elif fname.endswith(('.csv', '.txt', '.tsv')):
            tasks.append(analyze_spectrum(content, file.filename))
        elif fname.endswith(('.png', '.jpg', '.jpeg', '.tif')):
            tasks.append(analyze_image_vision(content, file.filename))
            
    results = await asyncio.gather(*tasks)
    
    # ì¢…í•© ë¦¬í¬íŠ¸ ìƒì„± (Ollama)
    valid_results = [r for r in results if r.get("type") != "error"]
    context_texts = []
    
    for r in valid_results:
        summary = r.get("summary", "")
        context_texts.append(f"--- File: {r['filename']} ---\n{summary[:2000]}")
    
    final_context = "\n".join(context_texts)
    final_report = "ë¶„ì„ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."
    
    if final_context:
        try:
            # â˜… ìˆ˜ì •ë¨: ëª…ì‹œì  Client ì‚¬ìš© ë° ì—ëŸ¬ ë¡œê¹… ê°•í™”
            # ë‚´ PCì— ì„¤ì¹˜ëœ ëª¨ë¸ëª…ì„ ì—¬ê¸°ì— ì •í™•íˆ ì ì–´ì£¼ì„¸ìš”! (ì˜ˆ: llama3, llama3:70b, mistral)
            TARGET_MODEL = "llama3:70b" 
            
            print(f"ğŸ” Sending synthesis request to Ollama ({TARGET_MODEL})...")
            
            res = await asyncio.to_thread(
                ollama_client.chat,  # client ê°ì²´ ì‚¬ìš©
                model=TARGET_MODEL,
                messages=[{
                    'role': 'user', 
                    'content': f"Synthesize these scientific results into a report:\n\n{final_context}"
                }]
            )
            final_report = res['message']['content']
            print("âœ… Synthesis Complete!")
            
        except Exception as e:
            err_msg = f"âŒ Ollama Call Failed: {str(e)}\n(Check if model '{TARGET_MODEL}' is pulled and 'ollama serve' is running)"
            print(err_msg)
            final_report = err_msg

    return {
        "results": results,
        "final_report": final_report
    }

@app.get("/", response_class=HTMLResponse)
async def serve_index():
    if os.path.exists("index.html"):
        return FileResponse("index.html")
    return "<h1>index.html not found</h1>"

if __name__ == "__main__":
    import uvicorn
    # 0.0.0.0ìœ¼ë¡œ ì—´ì–´ì•¼ 127.0.0.1 ë°”ì¸ë”© ë¬¸ì œ íšŒí”¼ ê°€ëŠ¥
    uvicorn.run(app, host="0.0.0.0", port=8000)



