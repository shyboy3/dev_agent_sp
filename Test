ë¸Œì´15
import os
import io
import asyncio
import base64
import json
import re
import math
from typing import List, Dict, Any, Optional
from pydantic import BaseModel

# [1] ë³´ì•ˆ ì„¤ì •
os.environ['NO_PROXY'] = '127.0.0.1,localhost,0.0.0.0,::1'
if 'HTTP_PROXY' in os.environ: del os.environ['HTTP_PROXY']
if 'HTTPS_PROXY' in os.environ: del os.environ['HTTPS_PROXY']

from fastapi import FastAPI, UploadFile, File, Form
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import HTMLResponse, FileResponse
import pandas as pd
import numpy as np

# ê³¼í•™ ì—°ì‚°
from scipy.signal import find_peaks, savgol_filter
from scipy.optimize import curve_fit
from scipy.ndimage import label, find_objects, gaussian_filter, rotate
from skimage.feature import blob_log
import cv2

from ollama import Client 
from PIL import Image
from pdf2image import convert_from_bytes
from pptx import Presentation
from pptx.enum.shapes import MSO_SHAPE_TYPE

# [2] ì„¤ì •
OLLAMA_HOST = "http://127.0.0.1:11434"
ollama_client = Client(host=OLLAMA_HOST)
VISION_MODEL = "qwen3-vl:30b-a3b-instruct" 
TEXT_MODEL = "llama3:70b"

# ==========================================
# [3] V15 ê³¼í•™ ì—”ì§„ (Interface Metrology)
# ==========================================
class ScienceProcessorV15:
    
    @staticmethod
    def crop_valid_rotated_region(img_rotated, angle_deg):
        h, w = img_rotated.shape[:2]
        if abs(angle_deg) < 1.0: return img_rotated # ì‘ì€ ê°ë„ëŠ” ë¬´ì‹œ
        
        # ê°„ë‹¨í•œ ì¤‘ì•™ë¶€ Crop (ë§ˆë¦„ëª¨ ë¬¸ì œ íšŒí”¼)
        # ì•ˆì „í•˜ê²Œ ì¤‘ì•™ 70% ì˜ì—­ë§Œ ì‚¬ìš© (ë³µì¡í•œ ê¸°í•˜í•™ ê³„ì‚° ëŒ€ì‹  ì•ˆì •ì„± íƒí•¨)
        cy, cx = h // 2, w // 2
        crop_h, crop_w = int(h * 0.7), int(w * 0.7)
        y1 = max(0, cy - crop_h // 2)
        x1 = max(0, cx - crop_w // 2)
        y2 = min(h, y1 + crop_h)
        x2 = min(w, x1 + crop_w)
        return img_rotated[y1:y2, x1:x2]

    @staticmethod
    def analyze_thin_film(img_gray):
        """
        [V15] ê³„ë©´ ì¶”ì  ë° ê±°ì¹ ê¸°/ë‘ê»˜ ì •ë°€ ë¶„ì„
        """
        h_orig, w_orig = img_gray.shape
        log = []
        
        # 1. Auto-Rotation
        scale = min(1.0, 512 / max(h_orig, w_orig))
        small = cv2.resize(img_gray, None, fx=scale, fy=scale)
        best_angle = 0
        max_var = -1
        
        for ang in range(-90, 91, 2):
            rot = rotate(small, ang, reshape=False, order=1)
            prof = np.mean(rot, axis=1)
            var = np.var(np.abs(np.gradient(prof)))
            if var > max_var: max_var = var; best_angle = ang
            
        log.append(f"Auto-Rotation: {best_angle}Â°")
        
        # 2. Rotate & Valid Crop
        img_rot = rotate(img_gray, best_angle, reshape=True, order=3, mode='constant', cval=np.mean(img_gray))
        img_crop = ScienceProcessorV15.crop_valid_rotated_region(img_rot, best_angle)
        h, w = img_crop.shape
        
        # 3. Global Peak Search (Approximate Locations)
        img_blur = gaussian_filter(img_crop, sigma=3)
        profile = np.mean(img_blur, axis=1)
        gradient = np.abs(np.gradient(profile))
        # ì£¼ìš” ê³„ë©´ ì°¾ê¸°
        interfaces_approx, _ = find_peaks(gradient, height=np.max(gradient)*0.25, distance=30)
        
        log.append(f"Global Layers Found: {len(interfaces_approx)}")
        
        # 4. Local Interface Tracking (Pixel-wise)
        # ê° xì¢Œí‘œë§ˆë‹¤ gradientê°€ ìµœëŒ€ì¸ yë¥¼ ì°¾ìŒ (Window Search)
        interface_paths = [] # List of [y_0, y_1, ..., y_w]
        
        grad_img = np.abs(np.gradient(img_blur, axis=0)) # Yì¶• ë°©í–¥ ë¯¸ë¶„ ì´ë¯¸ì§€
        
        for y_approx in interfaces_approx:
            path = []
            search_range = 15 # ìƒí•˜ 15í”½ì…€ íƒìƒ‰
            
            for x in range(w):
                # ê²€ìƒ‰ ë²”ìœ„ ì„¤ì •
                y_start = max(0, y_approx - search_range)
                y_end = min(h, y_approx + search_range)
                
                # í•´ë‹¹ ì»¬ëŸ¼(x)ì˜ ë¡œì»¬ ì˜ì—­ì—ì„œ ìµœëŒ€ Gradient ìœ„ì¹˜ ì°¾ê¸°
                local_col = grad_img[y_start:y_end, x]
                if len(local_col) == 0: 
                    path.append(y_approx)
                    continue
                    
                local_max_idx = np.argmax(local_col)
                real_y = y_start + local_max_idx
                path.append(real_y)
            
            # ë…¸ì´ì¦ˆ ì œê±° (Median Filter on Path)
            path_smooth = savgol_filter(path, window_length=31, polyorder=2)
            interface_paths.append(path_smooth)

        # 5. Analysis: Roughness & Thickness
        roughness_stats = []
        thickness_stats = {}
        overlay = cv2.cvtColor(img_crop, cv2.COLOR_GRAY2BGR)
        
        # Draw Paths
        for idx, path in enumerate(interface_paths):
            # A. Polynomial Fit (Trend) - 3ì°¨ í•¨ìˆ˜
            x_axis = np.arange(len(path))
            coeffs = np.polyfit(x_axis, path, 3)
            poly_func = np.poly1d(coeffs)
            trend_line = poly_func(x_axis)
            
            # B. Roughness (Actual - Trend)
            residuals = path - trend_line
            rms = np.sqrt(np.mean(residuals**2)) # Rq
            ra = np.mean(np.abs(residuals))      # Ra
            roughness_stats.append(f"Layer {idx+1}: Rq={rms:.2f}px, Ra={ra:.2f}px")
            
            # C. Draw
            # Actual Path (Green)
            pts = np.array([np.transpose(np.vstack([x_axis, path]))], np.int32)
            cv2.polylines(overlay, pts, isClosed=False, color=(0, 255, 0), thickness=2)
            # Trend Line (Red Dashed - Simulated by drawing line)
            pts_trend = np.array([np.transpose(np.vstack([x_axis, trend_line]))], np.int32)
            cv2.polylines(overlay, pts_trend, isClosed=False, color=(0, 0, 255), thickness=1)

        # Thickness Distribution
        if len(interface_paths) >= 2:
            # ë‘ ê³„ë©´ ì‚¬ì´ì˜ ê±°ë¦¬ (Point-to-Point)
            t_dist = []
            for i in range(len(interface_paths) - 1):
                diff = np.array(interface_paths[i+1]) - np.array(interface_paths[i])
                mean_t = np.mean(diff)
                std_t = np.std(diff)
                t_dist.append(f"L{i+1}-L{i+2}: Avg={mean_t:.1f}px (Â±{std_t:.2f})")
                
                # ì‹œê°í™” (ì¤‘ê°„ ì§€ì ì— í…ìŠ¤íŠ¸)
                mid_y = int((np.mean(interface_paths[i]) + np.mean(interface_paths[i+1])) / 2)
                cv2.putText(overlay, f"{mean_t:.1f}px", (w//2, mid_y), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 0), 1)
            
            thickness_stats["distribution"] = t_dist
            
        stats = {
            "type": "Thin Film Metrology",
            "angle": best_angle,
            "roughness": roughness_stats,
            "thickness": thickness_stats
        }
        
        return overlay, stats, log

    @staticmethod
    def detect_footer_boundary(img_array):
        # (ê¸°ì¡´ V14 ë™ì¼)
        try:
            h, w = img_array.shape[:2]
            if h < 100 or w < 100: return None
            search_h = int(h * 0.25)
            roi = img_array[h-search_h:, :]
            gray = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY) if len(roi.shape)==3 else roi
            sobel_y = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=3)
            row_sums = np.sum(np.uint8(np.absolute(sobel_y)), axis=1)
            if np.max(row_sums) < (w * 30): return None
            split_idx = np.argmax(row_sums)
            if split_idx > search_h * 0.98: return None
            return (h - search_h) + split_idx
        except: return None

    @staticmethod
    def process_image(img_bytes, equipment, mode, goal):
        nparr = np.frombuffer(img_bytes, np.uint8)
        img_raw = cv2.imdecode(nparr, cv2.IMREAD_COLOR)
        if img_raw is None: return None, {}
        
        process_log = []
        
        # 1. Smart Crop
        body_img, footer_img, split_y = img_raw, None, ScienceProcessorV15.detect_footer_boundary(img_raw)
        if split_y:
            body_img = img_raw[:split_y, :]
            footer_img = img_raw[split_y:, :]
            process_log.append("Smart Crop: Footer removed")
        
        # Helper Encoders
        def to_b64(im): return base64.b64encode(cv2.imencode('.jpg', im)[1]).decode('utf-8')
        b64_raw = to_b64(body_img)
        b64_footer = to_b64(footer_img) if footer_img is not None else None

        if mode == "None":
            return {"raw_b64": b64_raw, "proc_b64": b64_raw, "footer_b64": b64_footer, "stats": {"info": "Raw"}, "log": process_log}

        # 2. Logic Branch
        gray = cv2.cvtColor(body_img, cv2.COLOR_BGR2GRAY)
        kwd = (equipment + " " + goal).lower()
        
        processed_overlay = body_img.copy()
        stats = {}
        
        # [V15] Thin Film Metrology
        is_film = any(x in kwd for x in ["film", "layer", "thick", "ë°•ë§‰", "ë‘ê»˜", "ê³„ë©´", "roughness", "ê±°ì¹ ê¸°"])
        
        if is_film:
            processed_overlay, f_stats, f_log = ScienceProcessorV15.analyze_thin_film(gray)
            stats.update(f_stats)
            process_log.extend(f_log)
        else:
            # Particle / Atom
            is_bright = np.mean(gray) > 127
            gray_proc = cv2.bitwise_not(gray) if is_bright else gray
            gray_blur = cv2.GaussianBlur(gray_proc, (5, 5), 0)
            
            detect_atoms = False
            detect_particles = False
            
            if mode == "AI-Adaptive":
                if any(x in kwd for x in ["atom", "ì›ì"]): detect_atoms = True
                if any(x in kwd for x in ["particle", "ì…ì"]): detect_particles = True
            elif mode == "Auto":
                if equipment in ["STEM", "TEM"]: detect_atoms = True
                if equipment in ["SEM", "Optical"]: detect_particles = True

            if detect_atoms:
                blobs = blob_log(gray_blur, min_sigma=3, max_sigma=15, threshold=0.1)
                for y, x, r in blobs: cv2.circle(processed_overlay, (int(x), int(y)), int(r*1.414), (0, 0, 255), 2)
                stats["count"] = len(blobs)
                process_log.append(f"Atoms: {len(blobs)}")
            elif detect_particles:
                _, th = cv2.threshold(gray_blur, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)
                cnts, _ = cv2.findContours(th, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
                cv2.drawContours(processed_overlay, cnts, -1, (0, 255, 0), 2)
                stats["count"] = len(cnts)
                process_log.append(f"Particles: {len(cnts)}")

        return {
            "raw_b64": b64_raw,
            "proc_b64": to_b64(processed_overlay),
            "footer_b64": b64_footer,
            "stats": stats,
            "log": process_log
        }

    # --- Spectrum & Helper ---
    @staticmethod
    def gaussian(x, amp, cen, wid): return amp * np.exp(-(x-cen)**2 / (2*wid**2))

    @staticmethod
    def fit_peaks(x, y, peaks):
        fits = []
        for p in peaks:
            try:
                p0 = [y[p], x[p], 1.0]
                win = 20
                s, e = max(0, p-win), min(len(x), p+win)
                if len(x[s:e]) > 3:
                    popt, _ = curve_fit(ScienceProcessorV15.gaussian, x[s:e], y[s:e], p0=p0, maxfev=1000)
                    fits.append({"amp": popt[0], "center": popt[1], "sigma": popt[2]})
            except: pass
        return fits

    @staticmethod
    def simple_baseline(y):
        w = max(5, len(y)//10)
        s = pd.Series(y).rolling(window=w, center=True).min().bfill().ffill().values
        if len(s) > 51: s = savgol_filter(s, 51, 3)
        return s

    @staticmethod
    def process_spectrum(x, y, mode, goal):
        log = []
        y_raw = y.copy()
        if mode == "None":
            peaks, _ = find_peaks(y, height=np.max(y)*0.05)
            return {"x": x, "y_raw": y, "y_proc": y, "y_base": np.zeros_like(y), "peaks": [{"x": float(x[p]), "y": float(y[p])} for p in peaks], "log": ["Raw"], "fits": []}
        
        win = 15
        do_fit = False
        if mode == "AI-Adaptive":
            if "noise" in goal.lower(): win = 31
            if "fit" in goal.lower(): do_fit = True
        
        base = ScienceProcessorV15.simple_baseline(y)
        y_proc = np.maximum(y - base, 0)
        if len(y_proc) > win: y_proc = savgol_filter(y_proc, win, 3)
        peaks, _ = find_peaks(y_proc, height=np.max(y_proc)*0.05, distance=10)
        fits = ScienceProcessorV15.fit_peaks(x, y_proc, peaks) if do_fit else []
        
        return {"x": x, "y_raw": y_raw, "y_proc": y_proc, "y_base": base, "peaks": [{"x": float(x[p]), "y": float(y_proc[p])} for p in peaks], "log": log, "fits": fits}

def detect_excel_blocks(df):
    blocks = []; mask = ~df.isnull().to_numpy()
    if df.empty: return blocks
    labeled, n = label(mask, structure=[[0,1,0],[1,1,1],[0,1,0]])
    for sl in find_objects(labeled):
        b = df.iloc[sl]
        if b.shape[0]>=3 and b.shape[1]>=2: blocks.append(b.reset_index(drop=True))
    return blocks

async def analyze_vision_ollama(image_bytes: bytes, prompt: str) -> str:
    try:
        b64 = base64.b64encode(image_bytes).decode('utf-8')
        res = await asyncio.to_thread(ollama_client.chat, model=VISION_MODEL, messages=[{'role':'user','content':prompt,'images':[b64]}])
        c = res['message']['content']
        if "<script" in c: return "Blocked"
        return c
    except Exception as e: return str(e)

# --- App ---
app = FastAPI(title="Analyst V15")
app.add_middleware(CORSMiddleware, allow_origins=["*"], allow_methods=["*"], allow_headers=["*"])

class TranslateReq(BaseModel): text: str

@app.post("/api/translate")
async def translate_text(req: TranslateReq):
    try:
        res = await asyncio.to_thread(ollama_client.chat, model=TEXT_MODEL, messages=[{'role':'system','content':'Translate to English.'}, {'role':'user','content':req.text}])
        return {"translated": res['message']['content']}
    except Exception as e: return {"translated": str(e)}

@app.post("/api/analyze")
async def analyze_orchestrator(files: List[UploadFile] = File(...), file_configs: str = Form(...)):
    configs = json.loads(file_configs)
    print(f"ğŸš€ V15 Start. Files: {len(files)}")
    
    tasks = []
    final_results = []
    file_map = {f.filename: await f.read() for f in files}
    
    for filename, config in configs.items():
        if filename not in file_map: continue
        content = file_map[filename]
        eq, goal, mode = config.get("equipment", "General"), config.get("goal", ""), config.get("mode", "Auto")
        fname_lower = filename.lower()
        
        # [A] Document
        if eq == "Document" or fname_lower.endswith(('.pdf', '.ppt', '.pptx')):
            async def proc_doc(c, n, e):
                try:
                    if n.lower().endswith('.pdf'):
                        images = convert_from_bytes(c, dpi=150, fmt='jpeg')[:10]
                        full_txt = ""; pages = []
                        for idx, img in enumerate(images):
                            buf = io.BytesIO(); img.save(buf, format="JPEG")
                            b64 = base64.b64encode(buf.getvalue()).decode()
                            desc = await analyze_vision_ollama(buf.getvalue(), "í•œê¸€ ìš”ì•½.")
                            full_txt += f"\nPage {idx+1}: {desc}"
                            pages.append({"page_num": idx+1, "image_b64": b64, "desc": desc})
                        return {"type": "pdf", "filename": n, "equipment": "Lit", "raw_context": full_txt, "pages": pages}
                    elif n.lower().endswith(('.ppt', '.pptx')):
                        prs = Presentation(io.BytesIO(c))
                        slides = []; full_txt = ""
                        for i, slide in enumerate(prs.slides):
                            txt = ""; imgs = []
                            for s in slide.shapes:
                                if hasattr(s, "text"): txt += s.text + "\n"
                                if s.shape_type == MSO_SHAPE_TYPE.PICTURE:
                                    try:
                                        ib64 = base64.b64encode(s.image.blob).decode('utf-8')
                                        desc = await analyze_vision_ollama(s.image.blob, "í•œê¸€ ì„¤ëª….")
                                        imgs.append({"b64": ib64, "desc": desc})
                                    except: pass
                            full_txt += f"Slide {i+1}: {txt}\n"
                            slides.append({"slide_num": i+1, "text": txt, "images": imgs})
                        return {"type": "ppt", "filename": n, "equipment": "Lit", "raw_context": full_txt, "slides": slides}
                except Exception as e: return {"type": "error", "filename": n, "msg": str(e)}
            tasks.append(proc_doc(content, filename, eq))
            continue

        # [B] Data (Spectrum)
        if fname_lower.endswith(('.xlsx', '.xls', '.csv', '.txt')):
            async def proc_spec(c, n, e, g, m):
                try:
                    if n.endswith(('.csv', '.txt')):
                        try: df = pd.read_csv(io.BytesIO(c))
                        except: df = pd.read_csv(io.BytesIO(c), delimiter='\t')
                        blocks = [df]; sheet_name="CSV"
                    else:
                        xls = pd.ExcelFile(io.BytesIO(c))
                        df = pd.read_excel(xls, sheet_name=xls.sheet_names[0], header=None)
                        blocks = detect_excel_blocks(df); sheet_name=xls.sheet_names[0]

                    res_list = []
                    for i, block in enumerate(blocks):
                        try:
                            num_block = block.apply(pd.to_numeric, errors='coerce').dropna(how='all').dropna(how='all', axis=1)
                            if not num_block.empty and num_block.shape[0] > 5:
                                x = num_block.iloc[:, 0].fillna(0).values; y = num_block.iloc[:, 1].fillna(0).values
                                res = ScienceProcessorV15.process_spectrum(x, y, mode, g)
                                step = max(1, len(x)//100)
                                chart = [{"x": float(res["x"][k]), "y_proc": float(res["y_proc"][k]), "y_raw": float(res["y_raw"][k]), "y_base": float(res["y_base"][k])} for k in range(0, len(x), step)]
                                res_list.append({
                                    "type": "spectrum", "filename": f"{n} ({sheet_name}-{i+1})", "equipment": e,
                                    "raw_context": f"Peaks: {len(res['peaks'])}", "chart_data": chart, "log": res["log"], "fits": res["fits"]
                                })
                        except: pass
                    return res_list[0] if len(res_list)==1 else res_list
                except Exception as ex: return [{"type": "error", "filename": n, "msg": str(ex)}]
            pass

        # [C] Data (Image)
        elif fname_lower.endswith(('.png', '.jpg', '.jpeg', '.tif', '.bmp')):
            async def proc_img(c, n, e, g, m):
                try:
                    vis_res = ScienceProcessorV15.process_image(c, e, m, g)
                    if not vis_res: return {"type": "error", "filename": n, "msg": "Fail"}
                    meta = ""
                    if vis_res.get("footer_b64"):
                        fb = base64.b64decode(vis_res["footer_b64"])
                        meta = await analyze_vision_ollama(fb, "Read scale/HV.")
                    body = base64.b64decode(vis_res["proc_b64"])
                    desc = await analyze_vision_ollama(body, f"í•œê¸€ ë¶„ì„. {e}. ëª©í‘œ:{g}. í†µê³„:{vis_res.get('stats',{})}")
                    return {
                        "type": "image", "filename": n, "equipment": e, "summary": desc,
                        "raw_context": f"Img {e}: {desc}\nMeta: {meta}",
                        "raw_b64": vis_res["raw_b64"], "proc_b64": vis_res["proc_b64"], "footer_b64": vis_res.get("footer_b64"),
                        "stats": vis_res.get("stats",{}), "log": vis_res.get("log", [])
                    }
                except Exception as ex: return {"type": "error", "filename": n, "msg": str(ex)}
            tasks.append(proc_img(content, filename, eq, goal, mode))

    if tasks:
        results = await asyncio.gather(*tasks, return_exceptions=True)
        for r in results:
            if isinstance(r, list): final_results.extend(r)
            elif isinstance(r, dict): final_results.append(r)

    data_ctx, lit_ctx = "", ""
    for r in final_results:
        if r.get("type") != "error":
            raw = r.get("raw_context") or ""
            blk = f"\n=== File: {r['filename']} ===\n{raw[:3000]}\n"
            if r.get("equipment") in ["Literature","Lit"]: lit_ctx += blk
            else: data_ctx += blk
    
    final_report = "Fail"
    if data_ctx or lit_ctx:
        try:
            res = await asyncio.to_thread(ollama_client.chat, model=TEXT_MODEL, messages=[{'role':'system','content':'ìˆ˜ì„ ì—°êµ¬ì›. í•œê¸€. í‘œ í™œìš©.'}, {'role':'user','content':f"Data:\n{data_ctx}\n\nLit:\n{lit_ctx}"}])
            final_report = res['message']['content']
        except Exception as e: final_report = str(e)

    return {"results": final_results, "final_report": final_report}

@app.get("/", response_class=HTMLResponse)
async def serve_index():
    if os.path.exists("index.html"): return FileResponse("index.html")
    return "<h1>Upload index.html</h1>"

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)



ë¸Œì´14

import os
import io
import asyncio
import base64
import json
import re
import math
from typing import List, Dict, Any, Optional
from pydantic import BaseModel

# [1] ë³´ì•ˆ ì„¤ì •
os.environ['NO_PROXY'] = '127.0.0.1,localhost,0.0.0.0,::1'
if 'HTTP_PROXY' in os.environ: del os.environ['HTTP_PROXY']
if 'HTTPS_PROXY' in os.environ: del os.environ['HTTPS_PROXY']

from fastapi import FastAPI, UploadFile, File, Form
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import HTMLResponse, FileResponse
import pandas as pd
import numpy as np

# ê³¼í•™ ì—°ì‚°
from scipy.signal import find_peaks, savgol_filter
from scipy.optimize import curve_fit
from scipy.ndimage import label, find_objects, gaussian_filter, rotate
from skimage.feature import blob_log
import cv2

from ollama import Client 
from PIL import Image
from pdf2image import convert_from_bytes
from pptx import Presentation
from pptx.enum.shapes import MSO_SHAPE_TYPE

# [2] ì„¤ì •
OLLAMA_HOST = "http://127.0.0.1:11434"
ollama_client = Client(host=OLLAMA_HOST)
VISION_MODEL = "qwen3-vl:30b-a3b-instruct" 
TEXT_MODEL = "llama3:70b"

# [3] V14 ê³¼í•™ ì—”ì§„ (Lossless Projection)
class ScienceProcessorV14:
    
    # --- [A] Geometry Helpers ---
    @staticmethod
    def crop_valid_rotated_region(img_rotated, angle_deg):
        """
        [V14 New] íšŒì „ëœ ì´ë¯¸ì§€ì—ì„œ ê²€ì€ìƒ‰(ë¹ˆ ê³µê°„)ì„ ì œì™¸í•œ
        'ê°€ì¥ í° ë‚´ë¶€ ì§ì‚¬ê°í˜•(Largest Inscribed Rectangle)'ì„ ì°¾ì•„ ì˜ë¼ëƒ…ë‹ˆë‹¤.
        """
        h, w = img_rotated.shape[:2]
        angle_rad = math.radians(abs(angle_deg))
        
        # ê¸°í•˜í•™ì  ê³„ì‚° (ì›ë³¸ ë¹„ìœ¨ ìœ ì§€ ê°€ì •)
        # sin/cos ê°’ì— ë”°ë¼ ìœ íš¨ ì˜ì—­ì˜ ë„ˆë¹„/ë†’ì´ ê³„ì‚°
        sin_a = math.sin(angle_rad)
        cos_a = math.cos(angle_rad)
        
        # íšŒì „ í›„ ì¤‘ì‹¬ ê¸°ì¤€ ìœ íš¨ ë°•ìŠ¤ í¬ê¸° ì¶”ì • (ê°„ì†Œí™”ëœ ë¡œì§)
        # ì‹¤ì œë¡œëŠ” ì›ë³¸ h0, w0ë¥¼ ì•Œì•„ì•¼ ì •í™•í•˜ì§€ë§Œ, ì—¬ê¸°ì„œëŠ” íšŒì „ëœ ì´ë¯¸ì§€ ë‚´ì—ì„œ
        # ê²€ì€ìƒ‰ì´ ì•„ë‹Œ ì˜ì—­ì„ ì°¾ëŠ” ë°©ì‹ìœ¼ë¡œ êµ¬í˜„ (ë” ë²”ìš©ì )
        
        # 1. Grayscale & Threshold to find content
        if len(img_rotated.shape) == 3:
            gray = cv2.cvtColor(img_rotated, cv2.COLOR_BGR2GRAY)
        else:
            gray = img_rotated
            
        # íšŒì „ ì‹œ ë¹ˆ ê³µê°„ì€ ë³´í†µ 0(ê²€ì€ìƒ‰) ë˜ëŠ” ë³´ê°„ëœ ê°’
        # ì•ˆì „í•˜ê²Œ 0ì´ ì•„ë‹Œ í”½ì…€ì˜ Bounding Boxë¥¼ êµ¬í•¨ (í•˜ì§€ë§Œ ì´ê±´ ë§ˆë¦„ëª¨ì„)
        # ë§ˆë¦„ëª¨ ì•ˆì˜ ì§ì‚¬ê°í˜•ì„ êµ¬í•´ì•¼ í•¨.
        
        # ê°„ë‹¨í•˜ê³  ê°•ë ¥í•œ ë°©ë²•: ì¤‘ì‹¬ì—ì„œ ì¡°ê¸ˆì”© ë°–ìœ¼ë¡œ ë‚˜ê°€ë©° ê²€ì€ìƒ‰ì„ ë§Œë‚  ë•Œê¹Œì§€ í™•ì¥
        cy, cx = h // 2, w // 2
        
        # Xì¶• íƒìƒ‰
        valid_w = 0
        for x in range(cx, w):
            if gray[cy, x] == 0: break
            valid_w += 1
        
        # Yì¶• íƒìƒ‰
        valid_h = 0
        for y in range(cy, h):
            if gray[y, cx] == 0: break
            valid_h += 1
            
        # ì•ˆì „ ë§ˆì§„ (90%)
        crop_w = int(valid_w * 2 * 0.9)
        crop_h = int(valid_h * 2 * 0.9)
        
        # Crop
        x1 = cx - crop_w // 2
        y1 = cy - crop_h // 2
        
        # ë²”ìœ„ ì²´í¬
        x1, y1 = max(0, x1), max(0, y1)
        x2, y2 = min(w, x1 + crop_w), min(h, y1 + crop_h)
        
        return img_rotated[y1:y2, x1:x2]

    # --- [B] Analysis Engines ---
    @staticmethod
    def analyze_thin_film(img_gray):
        """
        ë°•ë§‰ ë¶„ì„ (íšŒì „ -> ìœ íš¨ í¬ë¡­ -> í”„ë¡œíŒŒì¼ë§)
        """
        h_orig, w_orig = img_gray.shape
        log = []
        
        # 1. Angle Detection (Low Res for Speed)
        scale = min(1.0, 512 / max(h_orig, w_orig))
        small = cv2.resize(img_gray, None, fx=scale, fy=scale)
        best_angle = 0
        max_var = -1
        
        # ê°ë„ ì •ë°€ íƒìƒ‰
        for ang in range(-90, 91, 2):
            # íšŒì „ ì‹œ bicubic ì‚¬ìš© (íƒìƒ‰ìš©)
            rot = rotate(small, ang, reshape=False, order=1)
            prof = np.mean(rot, axis=1)
            var = np.var(np.abs(np.gradient(prof)))
            if var > max_var:
                max_var = var
                best_angle = ang
        
        log.append(f"Detected Angle: {best_angle}Â°")
        
        # 2. High-Quality Rotation (Lanczos4)
        # LanczosëŠ” Sinc í•¨ìˆ˜ ê¸°ë°˜ì´ë¼ Edge ë³´ì¡´ë ¥ì´ ê°€ì¥ ì¢‹ìŒ
        img_rot = rotate(img_gray, best_angle, reshape=True, order=4, mode='constant', cval=0)
        
        # 3. [V14] Valid Area Crop (Remove Black Borders)
        img_crop = ScienceProcessorV14.crop_valid_rotated_region(img_rot, best_angle)
        h_c, w_c = img_crop.shape
        log.append(f"Valid Region Cropped: {w_c}x{h_c}")
        
        # 4. Profile & Interface
        # ì´ì œ ë§ˆë¦„ëª¨ê°€ ì•„ë‹Œ ì§ì‚¬ê°í˜• ë°ì´í„°ì´ë¯€ë¡œ ìˆ˜ì§ í‰ê· ì´ ì •í™•í•¨
        img_blur = gaussian_filter(img_crop, sigma=3)
        profile = np.mean(img_blur, axis=1)
        gradient = np.abs(np.gradient(profile))
        
        interfaces, _ = find_peaks(gradient, height=np.max(gradient)*0.25, distance=30)
        
        # 5. Visualization (Overlay on the Cropped Image)
        overlay = cv2.cvtColor(img_crop, cv2.COLOR_GRAY2BGR)
        for y_pos in interfaces:
            cv2.line(overlay, (0, y_pos), (w_c, y_pos), (0, 0, 255), 2)
            
        # Thickness Calculation
        thicknesses = []
        if len(interfaces) >= 2:
            thicknesses = np.diff(interfaces).tolist()
            
        stats = {
            "type": "Thin Film",
            "angle": best_angle,
            "layers": len(interfaces),
            "thickness_px": thicknesses,
            "avg_thickness": float(np.mean(thicknesses)) if thicknesses else 0
        }
        
        # Encode cropped image for display
        return overlay, stats, log

    @staticmethod
    def detect_footer_boundary(img_array):
        # (ê¸°ì¡´ê³¼ ë™ì¼)
        try:
            h, w = img_array.shape[:2]
            if h < 100 or w < 100: return None
            search_h = int(h * 0.25)
            roi = img_array[h-search_h:, :]
            gray = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY) if len(roi.shape)==3 else roi
            sobel_y = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=3)
            row_sums = np.sum(np.uint8(np.absolute(sobel_y)), axis=1)
            if np.max(row_sums) < (w * 30): return None
            split_idx = np.argmax(row_sums)
            if split_idx > search_h * 0.98: return None
            return (h - search_h) + split_idx
        except: return None

    @staticmethod
    def process_image(img_bytes, equipment, mode, goal):
        nparr = np.frombuffer(img_bytes, np.uint8)
        img_raw = cv2.imdecode(nparr, cv2.IMREAD_COLOR)
        if img_raw is None: return None, {}
        
        process_log = []
        
        # 1. Smart Crop (Footer Removal)
        body_img = img_raw
        footer_img = None
        split_y = ScienceProcessorV14.detect_footer_boundary(img_raw)
        
        if split_y:
            body_img = img_raw[:split_y, :]
            footer_img = img_raw[split_y:, :]
            process_log.append("Smart Crop: Footer removed")
        
        # Encoding Helpers
        def encode_img(img):
            _, buf = cv2.imencode('.jpg', img)
            return base64.b64encode(buf).decode('utf-8')

        b64_raw = encode_img(body_img)
        b64_footer = encode_img(footer_img) if footer_img is not None else None

        if mode == "None":
            return {"raw_b64": b64_raw, "proc_b64": b64_raw, "footer_b64": b64_footer, "stats": {"info": "Raw"}, "log": process_log}

        # 2. Analysis Branch
        gray = cv2.cvtColor(body_img, cv2.COLOR_BGR2GRAY)
        kwd = (equipment + " " + goal).lower()
        
        processed_overlay = body_img.copy()
        stats = {}
        
        # [V14] Thin Film Check
        is_film = any(x in kwd for x in ["film", "layer", "thick", "ë°•ë§‰", "ë‘ê»˜", "ê³„ë©´"])
        
        if is_film:
            # V14 Engine Call (Returns Cropped & Rotated Image)
            processed_overlay, f_stats, f_log = ScienceProcessorV14.analyze_thin_film(gray)
            stats.update(f_stats)
            process_log.extend(f_log)
            # Thin Film ëª¨ë“œì—ì„œëŠ” ì›ë³¸(Raw) ëŒ€ì‹  ì²˜ë¦¬ëœ(Rotated) ì´ë¯¸ì§€ë¥¼ ë©”ì¸ìœ¼ë¡œ ë³´ì—¬ì£¼ëŠ”ê²Œ ë‚˜ìŒ
            # í•˜ì§€ë§Œ ë¹„êµë¥¼ ìœ„í•´ RawëŠ” ê·¸ëŒ€ë¡œ ë‘ 
        else:
            # Particle / Atom Check
            is_bright = np.mean(gray) > 127
            gray_proc = cv2.bitwise_not(gray) if is_bright else gray
            gray_blur = cv2.GaussianBlur(gray_proc, (5, 5), 0)
            
            detect_atoms = False
            detect_particles = False
            
            if mode == "AI-Adaptive":
                if any(x in kwd for x in ["atom", "ì›ì"]): detect_atoms = True
                if any(x in kwd for x in ["particle", "ì…ì"]): detect_particles = True
            elif mode == "Auto":
                if equipment in ["STEM", "TEM"]: detect_atoms = True
                if equipment in ["SEM", "Optical"]: detect_particles = True

            if detect_atoms:
                blobs = blob_log(gray_blur, min_sigma=3, max_sigma=15, threshold=0.1)
                for y, x, r in blobs: cv2.circle(processed_overlay, (int(x), int(y)), int(r*1.414), (0, 0, 255), 2)
                stats["count"] = len(blobs)
                process_log.append(f"Atoms: {len(blobs)}")
            elif detect_particles:
                _, th = cv2.threshold(gray_blur, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)
                cnts, _ = cv2.findContours(th, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
                cv2.drawContours(processed_overlay, cnts, -1, (0, 255, 0), 2)
                stats["count"] = len(cnts)
                process_log.append(f"Particles: {len(cnts)}")

        b64_proc = encode_img(processed_overlay)
        
        return {
            "raw_b64": b64_raw,
            "proc_b64": b64_proc,
            "footer_b64": b64_footer,
            "stats": stats,
            "log": process_log
        }

    # --- Spectrum & Helper ---
    @staticmethod
    def gaussian(x, amp, cen, wid): return amp * np.exp(-(x-cen)**2 / (2*wid**2))

    @staticmethod
    def fit_peaks(x, y, peaks):
        fits = []
        for p in peaks:
            try:
                p0 = [y[p], x[p], 1.0]
                win = 20
                s, e = max(0, p-win), min(len(x), p+win)
                if len(x[s:e]) > 3:
                    popt, _ = curve_fit(ScienceProcessorV14.gaussian, x[s:e], y[s:e], p0=p0, maxfev=1000)
                    fits.append({"amp": popt[0], "center": popt[1], "sigma": popt[2]})
            except: pass
        return fits

    @staticmethod
    def simple_baseline(y):
        w = max(5, len(y)//10)
        s = pd.Series(y).rolling(window=w, center=True).min().bfill().ffill().values
        if len(s) > 51: s = savgol_filter(s, 51, 3)
        return s

    @staticmethod
    def process_spectrum(x, y, mode, goal):
        log = []
        y_raw = y.copy()
        if mode == "None":
            peaks, _ = find_peaks(y, height=np.max(y)*0.05)
            return {"x": x, "y_raw": y, "y_proc": y, "y_base": np.zeros_like(y), "peaks": [{"x": float(x[p]), "y": float(y[p])} for p in peaks], "log": ["Raw"], "fits": []}
        
        win = 15
        do_fit = False
        if mode == "AI-Adaptive":
            if "noise" in goal.lower(): win = 31
            if "fit" in goal.lower(): do_fit = True
        
        base = ScienceProcessorV14.simple_baseline(y)
        y_proc = np.maximum(y - base, 0)
        if len(y_proc) > win: y_proc = savgol_filter(y_proc, win, 3)
        
        peaks, _ = find_peaks(y_proc, height=np.max(y_proc)*0.05, distance=10)
        fits = ScienceProcessorV14.fit_peaks(x, y_proc, peaks) if do_fit else []
        
        return {"x": x, "y_raw": y_raw, "y_proc": y_proc, "y_base": base, "peaks": [{"x": float(x[p]), "y": float(y_proc[p])} for p in peaks], "log": log, "fits": fits}

def detect_excel_blocks(df):
    blocks = []; mask = ~df.isnull().to_numpy()
    if df.empty: return blocks
    labeled, n = label(mask, structure=[[0,1,0],[1,1,1],[0,1,0]])
    for sl in find_objects(labeled):
        b = df.iloc[sl]
        if b.shape[0]>=3 and b.shape[1]>=2: blocks.append(b.reset_index(drop=True))
    return blocks

async def analyze_vision_ollama(image_bytes: bytes, prompt: str) -> str:
    try:
        b64 = base64.b64encode(image_bytes).decode('utf-8')
        res = await asyncio.to_thread(ollama_client.chat, model=VISION_MODEL, messages=[{'role':'user','content':prompt,'images':[b64]}])
        c = res['message']['content']
        if "<script" in c: return "Blocked"
        return c
    except Exception as e: return str(e)

# --- App ---
app = FastAPI(title="Analyst V14")
app.add_middleware(CORSMiddleware, allow_origins=["*"], allow_methods=["*"], allow_headers=["*"])

class TranslateReq(BaseModel): text: str

@app.post("/api/translate")
async def translate_text(req: TranslateReq):
    try:
        res = await asyncio.to_thread(ollama_client.chat, model=TEXT_MODEL, messages=[{'role':'system','content':'Translate to English.'}, {'role':'user','content':req.text}])
        return {"translated": res['message']['content']}
    except Exception as e: return {"translated": str(e)}

@app.post("/api/analyze")
async def analyze_orchestrator(files: List[UploadFile] = File(...), file_configs: str = Form(...)):
    configs = json.loads(file_configs)
    print(f"ğŸš€ V14 Start. Files: {len(files)}")
    
    tasks = []
    final_results = []
    file_map = {f.filename: await f.read() for f in files}
    
    for filename, config in configs.items():
        if filename not in file_map: continue
        content = file_map[filename]
        eq, goal, mode = config.get("equipment", "General"), config.get("goal", ""), config.get("mode", "Auto")
        fname_lower = filename.lower()
        
        # [A] Document
        if eq == "Document" or fname_lower.endswith(('.pdf', '.ppt', '.pptx')):
            async def proc_doc(c, n, e):
                try:
                    if n.lower().endswith('.pdf'):
                        images = convert_from_bytes(c, dpi=150, fmt='jpeg')[:10]
                        full_txt = ""; pages = []
                        for idx, img in enumerate(images):
                            buf = io.BytesIO(); img.save(buf, format="JPEG")
                            b64 = base64.b64encode(buf.getvalue()).decode()
                            desc = await analyze_vision_ollama(buf.getvalue(), "í•œê¸€ ìš”ì•½.")
                            full_txt += f"\nPage {idx+1}: {desc}"
                            pages.append({"page_num": idx+1, "image_b64": b64, "desc": desc})
                        return {"type": "pdf", "filename": n, "equipment": "Lit", "raw_context": full_txt, "pages": pages}
                    elif n.lower().endswith(('.ppt', '.pptx')):
                        prs = Presentation(io.BytesIO(c))
                        slides = []; full_txt = ""
                        for i, slide in enumerate(prs.slides):
                            txt = ""; imgs = []
                            for s in slide.shapes:
                                if hasattr(s, "text"): txt += s.text + "\n"
                                if s.shape_type == MSO_SHAPE_TYPE.PICTURE:
                                    try:
                                        ib64 = base64.b64encode(s.image.blob).decode('utf-8')
                                        desc = await analyze_vision_ollama(s.image.blob, "í•œê¸€ ì„¤ëª….")
                                        imgs.append({"b64": ib64, "desc": desc})
                                    except: pass
                            full_txt += f"Slide {i+1}: {txt}\n"
                            slides.append({"slide_num": i+1, "text": txt, "images": imgs})
                        return {"type": "ppt", "filename": n, "equipment": "Lit", "raw_context": full_txt, "slides": slides}
                except Exception as e: return {"type": "error", "filename": n, "msg": str(e)}
            tasks.append(proc_doc(content, filename, eq))
            continue

        # [B] Data (Spectrum)
        if fname_lower.endswith(('.xlsx', '.xls', '.csv', '.txt')):
            async def proc_spec(c, n, e, g, m):
                try:
                    if n.endswith(('.csv', '.txt')):
                        try: df = pd.read_csv(io.BytesIO(c))
                        except: df = pd.read_csv(io.BytesIO(c), delimiter='\t')
                        blocks = [df]; sheet_name="CSV"
                    else:
                        xls = pd.ExcelFile(io.BytesIO(c))
                        df = pd.read_excel(xls, sheet_name=xls.sheet_names[0], header=None)
                        blocks = detect_excel_blocks(df); sheet_name=xls.sheet_names[0]

                    res_list = []
                    for i, block in enumerate(blocks):
                        try:
                            num_block = block.apply(pd.to_numeric, errors='coerce').dropna(how='all').dropna(how='all', axis=1)
                            if not num_block.empty and num_block.shape[0] > 5:
                                x = num_block.iloc[:, 0].fillna(0).values; y = num_block.iloc[:, 1].fillna(0).values
                                res = ScienceProcessorV14.process_spectrum(x, y, mode, g)
                                step = max(1, len(x)//100)
                                chart = [{"x": float(res["x"][k]), "y_proc": float(res["y_proc"][k]), "y_raw": float(res["y_raw"][k]), "y_base": float(res["y_base"][k])} for k in range(0, len(x), step)]
                                res_list.append({
                                    "type": "spectrum", "filename": f"{n} ({sheet_name}-{i+1})", "equipment": e,
                                    "raw_context": f"Peaks: {len(res['peaks'])}", "chart_data": chart, "log": res["log"], "fits": res["fits"]
                                })
                        except: pass
                    return res_list[0] if len(res_list)==1 else res_list
                except Exception as ex: return [{"type": "error", "filename": n, "msg": str(ex)}]
            pass # (ìŠ¤í™íŠ¸ëŸ¼ ë¡œì§ì€ gather ì²˜ë¦¬ë¥¼ ìœ„í•´ ë˜í•‘ í•„ìš” - V13ê³¼ ë™ì¼)

        # [C] Data (Image)
        elif fname_lower.endswith(('.png', '.jpg', '.jpeg', '.tif', '.bmp')):
            async def proc_img(c, n, e, g, m):
                try:
                    vis_res = ScienceProcessorV14.process_image(c, e, m, g)
                    if not vis_res: return {"type": "error", "filename": n, "msg": "Fail"}
                    meta = ""
                    if vis_res.get("footer_b64"):
                        fb = base64.b64decode(vis_res["footer_b64"])
                        meta = await analyze_vision_ollama(fb, "Read scale/HV.")
                    body = base64.b64decode(vis_res["proc_b64"])
                    desc = await analyze_vision_ollama(body, f"í•œê¸€ ë¶„ì„. {e}. ëª©í‘œ:{g}. í†µê³„:{vis_res.get('stats',{})}")
                    return {
                        "type": "image", "filename": n, "equipment": e, "summary": desc,
                        "raw_context": f"Img {e}: {desc}\nMeta: {meta}",
                        "raw_b64": vis_res["raw_b64"], "proc_b64": vis_res["proc_b64"], "footer_b64": vis_res.get("footer_b64"),
                        "stats": vis_res.get("stats",{}), "log": vis_res.get("log", [])
                    }
                except Exception as ex: return {"type": "error", "filename": n, "msg": str(ex)}
            tasks.append(proc_img(content, filename, eq, goal, mode))

    if tasks:
        results = await asyncio.gather(*tasks, return_exceptions=True)
        for r in results:
            if isinstance(r, list): final_results.extend(r)
            elif isinstance(r, dict): final_results.append(r)

    data_ctx, lit_ctx = "", ""
    for r in final_results:
        if r.get("type") != "error":
            raw = r.get("raw_context") or ""
            blk = f"\n=== File: {r['filename']} ===\n{raw[:3000]}\n"
            if r.get("equipment") in ["Literature","Lit"]: lit_ctx += blk
            else: data_ctx += blk
    
    final_report = "Fail"
    if data_ctx or lit_ctx:
        try:
            res = await asyncio.to_thread(ollama_client.chat, model=TEXT_MODEL, messages=[{'role':'system','content':'ìˆ˜ì„ ì—°êµ¬ì›. í•œê¸€. í‘œ í™œìš©.'}, {'role':'user','content':f"Data:\n{data_ctx}\n\nLit:\n{lit_ctx}"}])
            final_report = res['message']['content']
        except Exception as e: final_report = str(e)

    return {"results": final_results, "final_report": final_report}

@app.get("/", response_class=HTMLResponse)
async def serve_index():
    if os.path.exists("index.html"): return FileResponse("index.html")
    return "<h1>Upload index.html</h1>"

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)



ë¸Œì´14
<!DOCTYPE html>
<html lang="ko">
<head>
    <meta charset="UTF-8">
    <title>AI Research Center V14.1</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <script src="https://unpkg.com/vue@3/dist/vue.global.js"></script>
    <script src="https://cdn.plot.ly/plotly-2.27.0.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/marked/marked.min.js"></script>
    <script src="https://unpkg.com/lucide@latest"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/html2pdf.js/0.10.1/html2pdf.bundle.min.js"></script>
    <style>
        @import url('https://fonts.googleapis.com/css2?family=Inter:wght@400;600;700&display=swap');
        body { font-family: 'Inter', sans-serif; }
        .prose table { border-collapse: collapse; width: 100%; margin: 1em 0; }
        .prose th, .prose td { border: 1px solid #cbd5e1; padding: 8px; font-size: 0.9rem; }
        .prose th { background-color: #f1f5f9; }
        .loader { border: 3px solid #f3f3f3; border-top: 3px solid #0ea5e9; border-radius: 50%; width: 20px; height: 20px; animation: spin 1s linear infinite; }
        @keyframes spin { 0% { transform: rotate(0deg); } 100% { transform: rotate(360deg); } }
    </style>
</head>
<body class="bg-slate-100 text-slate-800">
    <div id="app" class="min-h-screen p-6">
        <header class="max-w-7xl mx-auto mb-6 flex items-center gap-3">
            <div class="bg-indigo-600 p-2 rounded-lg shadow"><i data-lucide="microscope" class="w-6 h-6 text-white"></i></div>
            <div><h1 class="text-2xl font-bold">AI Research Center V14.1</h1><p class="text-xs text-slate-500">Lossless Projection â€¢ Smart UX</p></div>
        </header>

        <main class="max-w-7xl mx-auto grid grid-cols-1 lg:grid-cols-12 gap-6">
            
            <!-- Staging Area -->
            <div class="lg:col-span-4 space-y-4">
                <div class="bg-white p-5 rounded-xl shadow-sm border border-slate-200">
                    <h2 class="font-bold mb-3 flex items-center gap-2"><i data-lucide="files" class="w-4 h-4"></i> Data Staging</h2>
                    <div class="border-2 border-dashed border-slate-300 rounded-lg p-6 text-center hover:bg-indigo-50 cursor-pointer" @click="$refs.f.click()">
                        <input type="file" ref="f" multiple class="hidden" @change="addFiles">
                        <p class="text-sm font-semibold text-slate-600">Add Files</p>
                    </div>
                    <div v-if="stagedFiles.length" class="mt-4 space-y-3 max-h-[60vh] overflow-y-auto pr-1">
                        <div v-for="(item, i) in stagedFiles" :key="i" class="p-3 bg-slate-50 border rounded-lg text-sm">
                            <div class="flex justify-between items-center mb-2">
                                <span class="font-bold truncate w-2/3" :title="item.file.name">{{ item.file.name }}</span>
                                <button @click="removeFile(i)" class="text-red-400 hover:text-red-600"><i data-lucide="trash-2" class="w-4 h-4"></i></button>
                            </div>
                            <div class="grid grid-cols-1 gap-2">
                                <select v-model="item.equipment" class="w-full p-1.5 border rounded text-xs bg-white font-semibold text-indigo-700">
                                    <optgroup label="Literature">
                                        <option value="Document">Document (PDF/PPT)</option>
                                    </optgroup>
                                    <optgroup label="Spectroscopy">
                                        <option value="XRD">XRD</option>
                                        <option value="EDS">EDS / EDX</option>
                                        <option value="EELS">EELS</option>
                                        <option value="Raman">Raman</option>
                                        <option value="FT-IR">FT-IR</option>
                                        <option value="PL">PL</option>
                                    </optgroup>
                                    <optgroup label="Microscopy">
                                        <option value="SEM">SEM</option>
                                        <option value="TEM">TEM (Thin Film)</option>
                                        <option value="STEM">STEM (Thin Film)</option>
                                        <option value="AFM">AFM</option>
                                        <option value="Optical BF">Optical (BF)</option>
                                    </optgroup>
                                </select>
                                <select v-if="item.equipment !== 'Document'" v-model="item.mode" class="w-full p-1.5 border rounded text-xs bg-white text-indigo-700">
                                    <option value="Auto">âœ¨ Auto Process</option>
                                    <option value="None">ğŸš« None (Raw)</option>
                                    <option value="AI-Adaptive">ğŸ§  AI-Adaptive</option>
                                </select>
                                <input v-if="item.equipment !== 'Document'" v-model="item.goal" type="text" placeholder="Goal (e.g. Measure thickness)" class="w-full p-1.5 border rounded text-xs">
                            </div>
                        </div>
                    </div>
                    <button @click="analyze" :disabled="isAnalyzing || stagedFiles.length===0" class="w-full mt-4 py-3 bg-indigo-600 text-white rounded-lg font-bold flex justify-center gap-2 shadow hover:bg-indigo-700 disabled:opacity-50">
                        <div v-if="isAnalyzing" class="loader"></div>
                        <span>Generate Synthesis Report</span>
                    </button>
                </div>
            </div>

            <!-- Report Area -->
            <div class="lg:col-span-8">
                <div v-if="result" class="space-y-6 animate-fade-in">
                    
                    <!-- Tools -->
                    <div class="flex justify-between items-center bg-white p-2 rounded-xl shadow-sm border border-slate-200">
                        <div class="flex gap-2">
                            <button @click="setLang('ko')" :class="lang==='ko'?'bg-indigo-100 text-indigo-700 font-bold':'text-slate-500 hover:bg-slate-50'" class="px-4 py-2 rounded-lg text-sm transition-all flex items-center gap-2">ğŸ‡°ğŸ‡· í•œê¸€</button>
                            <button @click="setLang('en')" :class="lang==='en'?'bg-blue-100 text-blue-700 font-bold':'text-slate-500 hover:bg-slate-50'" class="px-4 py-2 rounded-lg text-sm transition-all flex items-center gap-2">
                                <div v-if="isTranslating" class="w-3 h-3 border-2 border-blue-600 border-t-transparent rounded-full animate-spin"></div>
                                <span v-else>ğŸ‡ºğŸ‡¸</span> English
                            </button>
                        </div>
                        <button @click="dlPDF" class="px-3 py-1.5 bg-slate-800 text-white text-xs rounded">PDF Export</button>
                    </div>

                    <div id="report-view" class="bg-white p-8 rounded-xl shadow-sm border border-slate-200">
                        
                        <!-- Synthesis -->
                        <section class="mb-10">
                            <h2 class="text-lg font-bold text-indigo-700 mb-3 border-b pb-1">1. Synthesis</h2>
                            <div class="prose prose-sm text-slate-700 max-w-none" v-html="md(displayReport)"></div>
                        </section>

                        <!-- Details -->
                        <section>
                            <h2 class="text-lg font-bold text-slate-800 mb-5 border-b pb-1">2. Data Evidence</h2>
                            
                            <div v-for="(item, idx) in result.results" :key="idx" class="mb-8 p-4 bg-slate-50 rounded-lg border break-inside-avoid">
                                <div class="flex items-center justify-between mb-3">
                                    <div class="flex items-center gap-2">
                                        <span class="px-2 py-0.5 bg-white border rounded text-[10px] font-bold uppercase text-slate-500">{{ item.equipment }}</span>
                                        <h3 class="font-bold text-sm">{{ item.filename }}</h3>
                                    </div>
                                    <!-- Toggle Button -->
                                    <button @click="item.showDocs = !item.showDocs" class="text-xs text-indigo-600 font-bold hover:underline flex items-center gap-1">
                                        <i :data-lucide="item.showDocs ? 'eye-off' : 'eye'" class="w-3 h-3"></i>
                                        {{ item.showDocs ? 'Hide Text' : 'Show Text' }}
                                    </button>
                                </div>

                                <!-- Spectrum Chart -->
                                <div v-if="item.chart_data" class="bg-white p-2 rounded border mb-3">
                                    <div :id="'chart-'+idx" class="w-full h-60"></div>
                                </div>

                                <!-- Image Viewer -->
                                <div v-if="item.raw_b64" class="bg-white p-2 rounded border mb-3">
                                    <div class="flex gap-2 mb-2 text-xs justify-center">
                                        <button @click="item.view='proc'" :class="item.view!=='raw'?'font-bold text-indigo-600':''">Processed</button>
                                        <span class="text-slate-300">|</span>
                                        <button @click="item.view='raw'" :class="item.view==='raw'?'font-bold text-indigo-600':''">Original</button>
                                        <span class="text-slate-300" v-if="item.footer_b64">|</span>
                                        <button v-if="item.footer_b64" @click="item.view='footer'" :class="item.view==='footer'?'font-bold text-indigo-600':''">Info Bar</button>
                                    </div>
                                    <img v-if="item.view!=='footer'" :src="'data:image/jpeg;base64,' + (item.view==='raw' ? item.raw_b64 : item.proc_b64)" class="max-h-80 mx-auto object-contain bg-black">
                                    <img v-if="item.view==='footer'" :src="'data:image/jpeg;base64,' + item.footer_b64" class="max-h-24 mx-auto object-contain border mt-2">
                                </div>

                                <!-- [Fix] PDF Pages -->
                                <div v-if="item.pages" class="space-y-4">
                                    <div v-for="p in item.pages" :key="p.page_num" class="flex gap-4 bg-white p-3 rounded border">
                                        <!-- Image always visible -->
                                        <div :class="item.showDocs ? 'w-1/3' : 'w-full text-center'">
                                            <img :src="'data:image/jpeg;base64,'+p.image_b64" class="max-h-60 object-contain border shadow-sm mx-auto">
                                            <p class="text-center text-[10px] text-slate-400 mt-1">Page {{p.page_num}}</p>
                                        </div>
                                        <!-- Text toggled -->
                                        <div v-if="item.showDocs" class="w-2/3 text-xs prose" v-html="md(lang === 'en' ? (p.desc_en || p.desc) : p.desc)"></div>
                                    </div>
                                </div>

                                <!-- [Fix] PPT Slides -->
                                <div v-if="item.slides" class="space-y-4">
                                    <div v-for="s in item.slides" :key="s.slide_num" class="bg-white p-3 rounded border">
                                        <div class="flex justify-between mb-2">
                                            <div class="text-xs font-bold text-indigo-600">Slide {{s.slide_num}}</div>
                                        </div>
                                        
                                        <!-- Extracted Images (Always Visible) -->
                                        <div v-if="s.images && s.images.length" class="grid grid-cols-2 gap-2 mb-2">
                                            <div v-for="(img, imx) in s.images" :key="imx" class="bg-slate-50 p-2 text-center">
                                                <img :src="'data:image/jpeg;base64,'+img.b64" class="max-h-40 mx-auto object-contain bg-white border">
                                                <!-- Image Description Toggle -->
                                                <p v-if="item.showDocs" class="text-[10px] text-slate-500 mt-1 bg-white p-1 rounded">{{ img.desc }}</p>
                                            </div>
                                        </div>

                                        <!-- Slide Text (Toggled) -->
                                        <div v-if="item.showDocs" class="text-xs prose border-t pt-2 mt-2 whitespace-pre-wrap max-h-32 overflow-y-auto bg-slate-50 p-2 rounded">
                                            {{ s.text }}
                                        </div>
                                    </div>
                                </div>

                                <!-- Summary Text (Toggled) -->
                                <div v-if="item.showDocs" class="text-xs text-slate-600 mt-2 prose bg-slate-50 p-3 rounded" v-html="md(lang === 'en' ? (item.summary_en || item.summary) : item.summary)"></div>
                                
                                <!-- Log (Toggled) -->
                                <div v-if="item.log && item.log.length > 0 && item.showDocs" class="mt-2 p-2 bg-gray-100 rounded border text-[10px] font-mono text-gray-600">
                                    <strong>âš™ï¸ Log:</strong> {{ item.log.join(' â†’ ') }}
                                </div>

                            </div>
                        </section>
                    </div>
                </div>
            </div>
        </main>
    </div>
    <script>
        const { createApp, ref, computed, nextTick } = Vue;
        createApp({
            setup() {
                const stagedFiles = ref([]);
                const isAnalyzing = ref(false);
                const isTranslating = ref(false);
                const result = ref(null);
                const lang = ref('ko'); 
                const translatedReport = ref('');
                const md = (t) => marked.parse(t||'');

                const displayReport = computed(() => lang.value === 'en' && translatedReport.value ? translatedReport.value : (result.value ? result.value.final_report : ''));

                const addFiles = (e) => {
                    Array.from(e.target.files).forEach(f => {
                        let eq = 'General';
                        if(f.name.match(/\.(pdf|ppt|pptx)$/i)) eq = 'Document';
                        stagedFiles.value.push({ file: f, equipment: eq, goal: '', mode: 'Auto', view: 'proc', showDocs: true });
                    });
                    e.target.value = ''; 
                };
                const removeFile = (i) => stagedFiles.value.splice(i, 1);

                const analyze = async () => {
                    isAnalyzing.value = true;
                    lang.value = 'ko'; 
                    translatedReport.value = '';
                    const fd = new FormData();
                    const configs = {};
                    stagedFiles.value.forEach(item => {
                        fd.append('files', item.file);
                        configs[item.file.name] = { equipment: item.equipment, goal: item.goal, mode: item.mode };
                    });
                    fd.append('file_configs', JSON.stringify(configs));

                    try {
                        const res = await fetch('/api/analyze', { method: 'POST', body: fd });
                        const data = await res.json();
                        // Initialize showDocs to true
                        if(data.results) data.results.forEach(r => r.showDocs = true);
                        result.value = data;
                        await nextTick();
                        
                        data.results.forEach((item, i) => {
                            if(item.chart_data) {
                                const raw = { x: item.chart_data.map(d=>d.x), y: item.chart_data.map(d=>d.y_raw), mode:'lines', name:'Raw', line:{color:'#cbd5e1', width:1} };
                                const proc = { x: item.chart_data.map(d=>d.x), y: item.chart_data.map(d=>d.y_proc), mode:'lines', name:'Proc', line:{color:'#4f46e5', width:2} };
                                const base = { x: item.chart_data.map(d=>d.x), y: item.chart_data.map(d=>d.y_base), mode:'lines', name:'Base', line:{color:'#f59e0b', width:1, dash:'dot'} };
                                Plotly.newPlot('chart-'+i, [raw, base, proc], {margin:{t:10,b:30,l:40,r:10}, showlegend:false}, {displayModeBar:false});
                            }
                        });
                        lucide.createIcons();
                    } catch(e) { alert(e); } 
                    finally { isAnalyzing.value = false; }
                };

                const setLang = async (target) => {
                    if (target === 'ko') { lang.value = 'ko'; return; }
                    if (target === 'en') {
                        if (translatedReport.value) { lang.value = 'en'; return; }
                        isTranslating.value = true;
                        try {
                            const res = await fetch('/api/translate', {
                                method: 'POST', headers: {'Content-Type': 'application/json'},
                                body: JSON.stringify({ text: result.value.final_report })
                            });
                            const data = await res.json();
                            translatedReport.value = data.translated;
                            lang.value = 'en';
                        } catch(e) { alert("Trans Error: " + e); }
                        finally { isTranslating.value = false; }
                    }
                };
                
                const dlPDF = () => html2pdf().set({ margin:10, filename:'V14_1_Report.pdf', image:{type:'jpeg',quality:0.98}, html2canvas:{scale:2}, jsPDF:{unit:'mm',format:'a4'} }).from(document.getElementById('report-view')).save();

                setTimeout(()=>lucide.createIcons(), 100);
                return { stagedFiles, isAnalyzing, isTranslating, result, lang, displayReport, addFiles, removeFile, analyze, md, dlPDF, setLang };
            }
        }).mount('#app');
    </script>
</body>
</html>


