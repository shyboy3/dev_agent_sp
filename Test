model_clients.py

# model_clients.py

from __future__ import annotations
from typing import Optional, Dict, Any, List
import base64
import json
import os
import httpx

FigureAnalysis = Dict[str, Any]


class TextModelClient:
    """
    텍스트 전용 LLM 클라이언트.
    - 기본은 OpenAI 호환 /v1/chat/completions 스타일 가정.
    - gpt-oss:20b, qwen3-vl:30b-a3b-instruct(텍스트만) 등에 사용.
    """

    def __init__(
        self,
        api_url: str,
        model: str,
        api_key: Optional[str] = None,
        backend: str = "openai_compat",
        timeout: float = 120.0,
    ):
        self.api_url = api_url.rstrip("/")
        self.model = model
        self.api_key = api_key
        self.backend = backend
        self.timeout = timeout

    async def chat_completion(
        self,
        system_prompt: str,
        user_content: str | List[Dict[str, Any]],
        temperature: float = 0.2,
        max_tokens: Optional[int] = None,
    ) -> str:
        """
        system + user 메시지를 던지고, 텍스트 응답만 문자열로 받는 헬퍼.
        user_content가 string이면 그대로, list면 content로 사용.
        """

        # OpenAI 호환 backend 기준
        if self.backend == "openai_compat":
            messages: List[Dict[str, Any]] = []

            if system_prompt:
                messages.append({"role": "system", "content": system_prompt})

            if isinstance(user_content, str):
                messages.append({"role": "user", "content": user_content})
            else:
                # 이미 content 배열이 준비된 경우 (예: 다중 파트)
                messages.append({"role": "user", "content": user_content})

            payload: Dict[str, Any] = {
                "model": self.model,
                "messages": messages,
                "temperature": temperature,
            }
            if max_tokens is not None:
                payload["max_tokens"] = max_tokens

            headers: Dict[str, str] = {}
            if self.api_key:
                headers["Authorization"] = f"Bearer {self.api_key}"

            async with httpx.AsyncClient(timeout=self.timeout) as client:
                r = await client.post(f"{self.api_url}/v1/chat/completions", json=payload, headers=headers)
            r.raise_for_status()
            data = r.json()
            return data["choices"][0]["message"]["content"]

        # 다른 backend 타입을 쓰고 싶으면 여기서 분기 추가
        raise NotImplementedError(f"Unsupported text backend: {self.backend}")
    

class VisionModelClient:
    """
    비전(LMV) 클라이언트.
    - 이미지 + 텍스트를 넣고, FigureAnalysis JSON을 받도록 설계.
    - backend / model 설정만 바꿔주면 Qwen2.5-VL, Qwen3-VL 등 교체 가능.
    """

    def __init__(
        self,
        api_url: str,
        model: str,
        api_key: Optional[str] = None,
        backend: str = "openai_compat",
        timeout: float = 120.0,
    ):
        self.api_url = api_url.rstrip("/")
        self.model = model
        self.api_key = api_key
        self.backend = backend
        self.timeout = timeout

    def _figure_prompt(self) -> str:
        # 여기 프롬프트는 공통. 모델이 바뀌어도 그대로 사용.
        return """You are an expert scientific figure interpreter.

You will receive ONE scientific image (plot, spectrum, table screenshot, or microscopy image).
Carefully analyze it and respond ONLY with a single JSON object, no extra text.

The JSON schema MUST be:

{
  "kind": "plot" | "spectrum" | "table" | "microscopy" | "logo_or_decorative" | "other",
  "short_caption": "one-sentence description in Korean",
  "key_findings": [
    "bullet 1 in Korean",
    "bullet 2 in Korean"
  ],
  "axes": {
    "x_label": "string or null",
    "y_label": "string or null",
    "x_unit": "string or null",
    "y_unit": "string or null"
  },
  "table": {
    "header": [ "col1", "col2", ... ],
    "rows": [
      ["r1c1","r1c2",...],
      ["r2c1","r2c2",...]
    ]
  } | null
}

Rules:
- If the image is NOT a table, set "table" to null.
- If the image is a table screenshot, try to reconstruct header/rows in "table".
- If it is clearly just a company logo, icon, or UI decoration, set "kind": "logo_or_decorative".
- Do not add any fields beyond this schema.
- Respond with VALID JSON only (no comments, no markdown, no explanation).
"""

    async def analyze_figure(self, image_path: str) -> Optional[FigureAnalysis]:
        """
        이미지 1장을 분석해서 FigureAnalysis dict로 반환.
        실패 시 None.
        """
        if self.backend == "openai_compat":
            return await self._analyze_openai_compat(image_path)

        # 다른 backend (예: ollama, custom) 쓰고 싶으면 여기 분기 추가
        raise NotImplementedError(f"Unsupported vision backend: {self.backend}")

    async def _analyze_openai_compat(self, image_path: str) -> Optional[FigureAnalysis]:
        try:
            with open(image_path, "rb") as f:
                img_bytes = f.read()
            img_b64 = base64.b64encode(img_bytes).decode("utf-8")

            prompt = self._figure_prompt()

            # OpenAI 호환 멀티모달 payload
            payload = {
                "model": self.model,
                "messages": [
                    {
                        "role": "user",
                        "content": [
                            {"type": "text", "text": prompt},
                            {
                                "type": "image_url",
                                "image_url": {
                                    "url": f"data:image/png;base64,{img_b64}"
                                },
                            },
                        ],
                    }
                ],
                "temperature": 0.1,
            }

            headers: Dict[str, str] = {}
            if self.api_key:
                headers["Authorization"] = f"Bearer {self.api_key}"

            async with httpx.AsyncClient(timeout=self.timeout) as client:
                r = await client.post(f"{self.api_url}/v1/chat/completions", json=payload, headers=headers)
            r.raise_for_status()
            data = r.json()
            content = data["choices"][0]["message"]["content"]

            # 모델이 JSON string으로 응답하도록 프롬프트를 강제했기 때문에, 여기서 파싱
            return json.loads(content)

        except Exception as e:
            print(f"[WARN] Vision model call failed ({image_path}): {e}")
            return None




research_report_server_all_in_one.py (혹은 메인 서버 파일) 상단에서:


import os
from model_clients import TextModelClient, VisionModelClient

# 예시: 환경변수에서 읽기 (없으면 기본값)
TEXT_API_URL = os.getenv("TEXT_API_URL", "http://localhost:8000")
TEXT_MODEL   = os.getenv("TEXT_MODEL_NAME", "gpt-oss:20b")
TEXT_API_KEY = os.getenv("TEXT_API_KEY", "")

VISION_API_URL = os.getenv("VISION_API_URL", "http://localhost:8000")
VISION_MODEL   = os.getenv("VISION_MODEL_NAME", "qwen2.5-vl:7b")
VISION_API_KEY = os.getenv("VISION_API_KEY", "")

# 텍스트 모델 클라이언트 (기본: gpt-oss:20b)
text_client = TextModelClient(
    api_url=TEXT_API_URL,
    model=TEXT_MODEL,
    api_key=TEXT_API_KEY or None,
    backend="openai_compat",
)

# 비전 모델 클라이언트 (기본: qwen2.5-vl:7b)
vision_client = VisionModelClient(
    api_url=VISION_API_URL,
    model=VISION_MODEL,
    api_key=VISION_API_KEY or None,
    backend="openai_compat",
)



TEXT_MODEL_NAME=qwen3-vl:30b-a3b-instruct 이런 식으로 환경변수만 갈아끼우면 됨.
	•	비전 모델을 Qwen3-VL로 바꾸고 싶으면:
VISION_MODEL_NAME=qwen3-vl:30b-a3b-instruct

즉, 코드는 그대로 두고 설정만 바꾸는 구조.






3. 텍스트 리포트 생성 부분 바꾸기 (generate_report_text 예시)

지금 generate_report_text(meta, parsed_files) 안에서
직접 httpx로 LLM을 부르고 있을 텐데, 그 부분을 text_client로 바꾸면 돼.


예를들어:
# 예시: 기존에 있던 것 (대략)
# async def generate_report_text(meta, parsed_files):
#     ctx = build_context(meta, parsed_files)
#     system_prompt = SYSTEM_PROMPT
#     payload = {...}
#     async with httpx.AsyncClient() as client:
#         ...
#     return content


이렇게 바꾸는 느낌:
import json

SYSTEM_PROMPT = """
당신은 재료/소자 분석 연구 리포트를 작성하는 전문 리포트 작성 어시스턴트입니다.
(중략: 이미 쓰고 있던 SYSTEM 프롬프트 내용)
"""

async def generate_report_text(meta, parsed_files):
    ctx = build_context(meta, parsed_files)  # meta + files + headings + tables + mm_summaries 등
    user_content = json.dumps(ctx, ensure_ascii=False)

    # text_client는 전역으로 만들어둔 TextModelClient 인스턴스
    report_text = await text_client.chat_completion(
        system_prompt=SYSTEM_PROMPT,
        user_content=user_content,
        temperature=0.3,
        max_tokens=None,  # 필요하면 제한
    )
    return report_text


이제 텍스트 LLM을 변경하고 싶으면 text_client 생성 시 model만 바꾸면 됨.


4. 비전 해석 붙이는 부분 바꾸기 (enrich_items_with_mm)

이전에는 Qwen 전용 함수를 썼다면,
이제는 vision_client.analyze_figure(image_path)만 부르면 됨.

from PIL import Image as PILImage
import os
from typing import List, Dict, Any

async def enrich_items_with_mm(parsed_files: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """
    - parsed_files 안의 각 item에서 extracted_images를 찾아
      적당한 이미지들만 골라 VisionModelClient로 해석.
    - 결과는 item["mm_summaries"]에 누적.
    """
    if vision_client is None:
        return parsed_files

    tasks = []

    for item in parsed_files:
        imgs = item.get("extracted_images") or []
        if not imgs:
            continue

        selected: List[str] = []

        for path in imgs:
            if not path or not os.path.exists(path):
                continue

            try:
                with PILImage.open(path) as im:
                    w, h = im.size
                area = w * h
                ratio = max(w, h) / max(1, min(w, h))
            except Exception:
                area = 0
                ratio = 1

            # 너무 작은 거나 길쭉한 배너는 로고/장식으로 보고 스킵
            if area < 50_000 or ratio > 8:
                continue

            selected.append(path)
            if len(selected) >= 6:  # 파일당 최대 6장 정도만
                break

        for path in selected:
            tasks.append((item, path))

    async def worker(itm: Dict[str, Any], img_path: str):
        analysis = await vision_client.analyze_figure(img_path)
        if not analysis:
            return
        mm_list = itm.setdefault("mm_summaries", [])
        mm_list.append({
            "image_path": img_path,
            "analysis": analysis,
        })

    # 비동기로 병렬 처리
    import asyncio
    await asyncio.gather(*(worker(itm, p) for itm, p in tasks))
    return parsed_files


여기서도 Qwen이라는 단어가 한 번도 안 나와:
	•	Vision 모델을 Qwen2.5-vl:7b → Qwen3-vl:30b 로 바꾸고 싶으면
VISION_MODEL_NAME만 갈아끼우면 끝.

⸻

5. 전체 흐름 요약
	1.	모델 클라이언트 모듈 (model_clients.py)에
	•	TextModelClient
	•	VisionModelClient
구현
	2.	서버 시작할 때:
	•	text_client = TextModelClient(...)
	•	vision_client = VisionModelClient(...)
를 전역/DI로 만들어둠
	3.	리포트 생성 플로우:
	•	파일 파싱 → parsed_files
	•	parsed_files = await enrich_items_with_mm(parsed_files)
	•	report_text = await generate_report_text(meta, parsed_files)
	•	render_pdf(report_text, parsed_files, ...)
	4.	모델 교체가 필요하면:
	•	.env나 설정에서
	•	TEXT_MODEL_NAME
	•	VISION_MODEL_NAME
	•	(필요 시 URL/API_KEY)
만 수정하면 됨.

이 정도 구조면:
	•	지금 말한
	•	텍스트: gpt-oss:20b (또는 qwen3-vl:30b-a3b-instruct)
	•	비전: qwen2.5-vl:7b 또는 qwen3-vl:30b-a3b-instruct
	•	이후에 다른 멀티모달 모델 추가할 때도
adapter 함수 한 군데만 손보면 되니까 꽤 오래 버틸 거야.

이제 네 쪽 엔드포인트(URL, 경로)가 만약 /v1/chat/completions랑 조금 다르면
그 부분만 같이 맞춰보면 돼.
필요하면, 너가 실제로 쓰는 curl 예시 한 줄만 알려줘도
그 형식에 딱 맞게 TextModelClient / VisionModelClient 내부를 구체적으로 맞춰 줄게.


