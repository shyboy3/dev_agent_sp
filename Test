V8ì½”ë“œ

import os
import io
import asyncio
import base64
import json
from typing import List, Dict, Any, Optional

# [í•„ìˆ˜] ë³´ì•ˆë§ ìš°íšŒ
os.environ['NO_PROXY'] = '127.0.0.1,localhost,0.0.0.0,::1'
if 'HTTP_PROXY' in os.environ: del os.environ['HTTP_PROXY']
if 'HTTPS_PROXY' in os.environ: del os.environ['HTTPS_PROXY']

from fastapi import FastAPI, UploadFile, File, Form
from fastapi.middleware.cors import CORSMiddleware
import pandas as pd
import numpy as np

# ê³¼í•™ ì—°ì‚° & ì´ë¯¸ì§€ ì²˜ë¦¬
from scipy.signal import find_peaks, savgol_filter
from scipy.optimize import curve_fit
from scipy.ndimage import label
from skimage.feature import blob_log
import cv2

from ollama import Client 
from pdf2image import convert_from_bytes
from pptx import Presentation
from pptx.enum.shapes import MSO_SHAPE_TYPE

# --- ì„¤ì • ---
OLLAMA_HOST = "http://127.0.0.1:11434"
ollama_client = Client(host=OLLAMA_HOST)
VISION_MODEL = "qwen3-vl:30b-a3b-instruct" 
TEXT_MODEL = "llama3:70b"

# --- V8 ê³¼í•™ ì²˜ë¦¬ ì—”ì§„ ---

class ScienceProcessorV8:
    
    @staticmethod
    def detect_footer_boundary(img_array):
        """
        [V8 í•µì‹¬] ìŠ¤ë§ˆíŠ¸ ì •ë³´ë°” ê°ì§€ ì•Œê³ ë¦¬ì¦˜
        ë‹¨ìˆœ í¼ì„¼íŠ¸ê°€ ì•„ë‹ˆë¼, ì´ë¯¸ì§€ í•˜ë‹¨ì˜ 'ê²½ê³„ì„ 'ì„ ì°¾ì•„ëƒ…ë‹ˆë‹¤.
        """
        h, w = img_array.shape[:2]
        
        # 1. í•˜ë‹¨ 25% ì˜ì—­ë§Œ ROI(Region of Interest)ë¡œ ì„¤ì • (ì†ë„ ë° ì •í™•ë„ í–¥ìƒ)
        search_ratio = 0.25
        search_h = int(h * search_ratio)
        roi = img_array[h-search_h:, :]
        
        # 2. ê·¸ë ˆì´ìŠ¤ì¼€ì¼ ë° ì—£ì§€ ê²€ì¶œ
        if len(roi.shape) == 3:
            gray = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)
        else:
            gray = roi
            
        # 3. ìˆ˜í‰ ì—£ì§€ ê°•ì¡° (Sobel Filter Yë°©í–¥) -> ê²½ê³„ì„  ì°¾ê¸°
        # Cannyë³´ë‹¤ Sobelì´ ìˆ˜í‰ì„  ê²€ì¶œì— ìœ ë¦¬í•  ìˆ˜ ìˆìŒ
        sobel_y = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=3)
        abs_sobel = np.absolute(sobel_y)
        edges = np.uint8(abs_sobel)
        
        # 4. ìˆ˜í‰ íˆ¬ì˜ (í–‰ ë‹¨ìœ„ í•©ê³„)
        # ê°€ë¡œì¤„ í•˜ë‚˜ê°€ ì „ì²´ì ìœ¼ë¡œ ì—£ì§€ë¼ë©´(ì§ì„ ), í•©ê³„ê°€ ë§¤ìš° ë†’ìŒ
        row_sums = np.sum(edges, axis=1)
        
        # 5. ê°€ì¥ ê°•ë ¥í•œ ì—£ì§€ ì°¾ê¸° (ê²½ê³„ì„  í›„ë³´)
        # ë…¸ì´ì¦ˆ ë°©ì§€ë¥¼ ìœ„í•´ ì¼ì • ì„ê³„ê°’ ì´ìƒì´ì–´ì•¼ í•¨
        max_val = np.max(row_sums)
        if max_val < (w * 50): # ì—£ì§€ê°€ ë„ˆë¬´ ì•½í•˜ë©´(ì •ë³´ë°”ê°€ ì—†ìœ¼ë©´)
            return None # ì •ë³´ë°” ì—†ìŒ íŒë‹¨
            
        split_idx_roi = np.argmax(row_sums)
        
        # 6. ìœ„ì¹˜ ë³´ì • (ROI ê¸°ì¤€ì´ë¯€ë¡œ ì „ì²´ ì¢Œí‘œë¡œ ë³€í™˜)
        # ë„ˆë¬´ ë§¨ ë°‘ë°”ë‹¥(í•˜ë‹¨ 2% ì´ë‚´)ì´ë©´ ë¬´ì‹œ (í…Œë‘ë¦¬ì¼ ìˆ˜ ìˆìŒ)
        if split_idx_roi > search_h * 0.95:
            return None
            
        real_split_y = (h - search_h) + split_idx_roi
        
        return real_split_y

    @staticmethod
    def simple_baseline(y):
        window = max(5, len(y)//10)
        baseline = pd.Series(y).rolling(window=window, center=True).min().fillna(method='bfill').fillna(method='ffill').values
        if len(baseline) > 51:
            baseline = savgol_filter(baseline, window_length=51, polyorder=3)
        return baseline

    @staticmethod
    def process_spectrum(x, y, mode, goal):
        log = []
        y_raw = y.copy()
        
        if mode == "None":
            peaks, _ = find_peaks(y, height=np.max(y)*0.05)
            return {"x": x, "y_raw": y, "y_proc": y, "y_base": np.zeros_like(y),
                    "peaks": [{"x": float(x[p]), "y": float(y[p])} for p in peaks], "log": ["Raw Data"]}

        # AI Adaptive Params
        window = 15
        if mode == "AI-Adaptive" and "noise" in goal.lower(): window = 31
        
        # Process
        base = ScienceProcessorV8.simple_baseline(y)
        y_corr = np.maximum(y - base, 0)
        y_proc = savgol_filter(y_corr, window, 3) if len(y_corr) > window else y_corr
        peaks, _ = find_peaks(y_proc, height=np.max(y_proc)*0.05, distance=10)

        return {"x": x, "y_raw": y_raw, "y_proc": y_proc, "y_base": base,
                "peaks": [{"x": float(x[p]), "y": float(y_proc[p])} for p in peaks], "log": log}

    @staticmethod
    def process_image(img_bytes, equipment, mode, goal):
        nparr = np.frombuffer(img_bytes, np.uint8)
        img_raw = cv2.imdecode(nparr, cv2.IMREAD_COLOR)
        if img_raw is None: return None, {}
        
        # --- [V8 Smart Split] ---
        footer_img = None
        body_img = img_raw
        has_footer = False
        
        # í˜„ë¯¸ê²½ ì¥ë¹„ì¼ ê²½ìš°ì—ë§Œ ìŠ¤ë§ˆíŠ¸ ê°ì§€ ì‹œë„
        if equipment in ["SEM", "TEM", "STEM", "FIB", "AFM"]:
            split_y = ScienceProcessorV8.detect_footer_boundary(img_raw)
            if split_y:
                body_img = img_raw[:split_y, :]   # ìƒë‹¨: ë°ì´í„°
                footer_img = img_raw[split_y:, :] # í•˜ë‹¨: ì •ë³´(ë©”íƒ€ë°ì´í„°)
                has_footer = True
        
        # ì¸ì½”ë”© (Body)
        _, buf_body = cv2.imencode('.jpg', body_img)
        b64_body = base64.b64encode(buf_body).decode('utf-8')
        
        # ì¸ì½”ë”© (Footer - ë©”íƒ€ë°ì´í„° ì¶”ì¶œìš©)
        b64_footer = None
        if has_footer:
            _, buf_footer = cv2.imencode('.jpg', footer_img)
            b64_footer = base64.b64encode(buf_footer).decode('utf-8')

        # Mode: None -> ì²˜ë¦¬ëœ(ì˜ë¦°) ì´ë¯¸ì§€ë§Œ ë°˜í™˜í•˜ê³  ë
        if mode == "None":
            return {
                "raw_b64": b64_body, # ì˜ë¦° ì´ë¯¸ì§€ë¥¼ Rawë¡œ ê°„ì£¼
                "proc_b64": b64_body,
                "footer_b64": b64_footer,
                "stats": {"info": "Smart Cropped (Raw)"}
            }

        # CV Analysis (on Body Image)
        gray = cv2.cvtColor(body_img, cv2.COLOR_BGR2GRAY)
        gray = cv2.GaussianBlur(gray, (5, 5), 0)
        overlay = body_img.copy()
        stats = {"info": "Smart Cropped & Processed"}

        # Adaptive Logic
        detect_atoms = False
        detect_particles = False
        
        if mode == "AI-Adaptive":
            if "atom" in goal.lower() or "ì›ì" in goal: detect_atoms = True
            if "particle" in goal.lower() or "ì…ì" in goal: detect_particles = True
        elif mode == "Auto":
            if equipment in ["STEM", "TEM"]: detect_atoms = True
            if equipment in ["SEM", "Optical"]: detect_particles = True

        if detect_atoms:
            blobs = blob_log(gray, min_sigma=3, max_sigma=15, threshold=0.1)
            for blob in blobs:
                y, x, r = blob
                cv2.circle(overlay, (int(x), int(y)), int(r*1.414), (0, 0, 255), 2)
            stats["count"] = len(blobs)
            stats["type"] = "Atoms"
        elif detect_particles:
            _, thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)
            cnts, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
            cv2.drawContours(overlay, cnts, -1, (0, 255, 0), 2)
            stats["count"] = len(cnts)
            stats["type"] = "Particles"

        _, buf_proc = cv2.imencode('.jpg', overlay)
        
        return {
            "raw_b64": b64_body,
            "proc_b64": base64.b64encode(buf_proc).decode('utf-8'),
            "footer_b64": b64_footer,
            "stats": stats
        }

# --- í—¬í¼ ---
def detect_excel_blocks(df):
    # (ì´ì „ ì½”ë“œì™€ ë™ì¼)
    blocks = []
    if df.empty: return blocks
    mask = ~df.isnull().to_numpy()
    from scipy.ndimage import label, find_objects
    labeled, n = label(mask, structure=[[0,1,0],[1,1,1],[0,1,0]])
    for sl in find_objects(labeled):
        b = df.iloc[sl]
        if b.shape[0]>=3 and b.shape[1]>=2: blocks.append(b.reset_index(drop=True))
    return blocks

async def analyze_vision_ollama(image_bytes: bytes, prompt: str) -> str:
    try:
        b64 = base64.b64encode(image_bytes).decode('utf-8')
        res = await asyncio.to_thread(ollama_client.chat, model=VISION_MODEL, messages=[{'role':'user','content':prompt,'images':[b64]}])
        c = res['message']['content']
        if "<script" in c: return "[Error]"
        return c
    except Exception as e: return str(e)

# --- ë¼ìš°í„° ---
app = FastAPI(title="Analyst V8")
app.add_middleware(CORSMiddleware, allow_origins=["*"], allow_methods=["*"], allow_headers=["*"])

@app.post("/api/analyze")
async def analyze_orchestrator(
    files: List[UploadFile] = File(...),
    file_configs: str = Form(...) 
):
    configs = json.loads(file_configs)
    print(f"ğŸš€ V8 Processing {len(configs)} files")
    
    tasks = []
    file_map = {f.filename: await f.read() for f in files}
    
    for filename, config in configs.items():
        if filename not in file_map: continue
        content = file_map[filename]
        eq = config.get("equipment", "General")
        goal = config.get("goal", "")
        mode = config.get("mode", "Auto")
        
        # 1. Document (Skip processing)
        if eq == "Document" or filename.lower().endswith(('.pdf','.ppt','.pptx')):
            # (V7.5ì˜ ë¬¸ì„œ ì²˜ë¦¬ ë¡œì§ ë™ì¼ - ìƒëµ ì—†ì´ êµ¬í˜„ í•„ìš”)
            # ì—¬ê¸°ì„œëŠ” ì§€ë©´ìƒ ìŠ¤í™íŠ¸ëŸ¼/ì´ë¯¸ì§€ì— ì§‘ì¤‘
            pass 

        # 2. Spectrum (Excel)
        elif filename.lower().endswith(('.xlsx', '.xls')):
            async def proc_xls(c, n, e, g, m):
                # (V7.5 ë¡œì§ ë™ì¼)
                pass
            # (ì‹¤ì œ êµ¬í˜„ ì‹œ V7.5 ì½”ë“œ ë³µì‚¬)
            pass

        # 3. Image (V8 Dual-Pass Logic)
        elif filename.lower().endswith(('.png', '.jpg', '.jpeg', '.tif', '.bmp')):
            async def process_img_v8(c, n, e, g, m):
                try:
                    # Step 1: Smart Split & CV Process
                    vis_res = ScienceProcessorV8.process_image(c, e, m, g)
                    
                    # Step 2: Metadata Extraction (OCR on Footer)
                    metadata_text = ""
                    if vis_res.get("footer_b64"):
                        footer_bytes = base64.b64decode(vis_res["footer_b64"])
                        meta_prompt = "Read the scale bar, magnification, voltage (HV), and date from this info bar. Output as text."
                        metadata_text = await analyze_vision_ollama(footer_bytes, meta_prompt)
                    
                    # Step 3: Main Analysis (Body + Metadata)
                    body_bytes = base64.b64decode(vis_res["proc_b64"])
                    
                    lang_inst = "Korean"
                    prompt = f"""
                    Analyze this {e} image in {lang_inst}.
                    User Goal: {g}.
                    CV Stats: {vis_res['stats']}.
                    [Microscope Metadata]: {metadata_text}
                    
                    Use the metadata (Scale, HV) to interpret the features size and quality accurately.
                    """
                    desc = await analyze_vision_ollama(body_bytes, prompt)
                    
                    return {
                        "type": "image",
                        "filename": n,
                        "equipment": e,
                        "summary": desc,
                        "raw_context": f"Image Analysis: {desc}\nMetadata Found: {metadata_text}\nStats: {vis_res['stats']}",
                        "raw_b64": vis_res["raw_b64"],
                        "proc_b64": vis_res["proc_b64"], # Crop & Overlayed
                        "footer_b64": vis_res.get("footer_b64"), # UIì— ë³´ì—¬ì¤„ ìˆ˜ë„ ìˆìŒ
                        "stats": vis_res["stats"]
                    }
                except Exception as ex:
                    return {"type": "error", "filename": n, "msg": str(ex)}
            
            tasks.append(process_img_v8(content, filename, eq, goal, mode))

    # ... (ë‚˜ë¨¸ì§€ gather ë° synthesis ë¡œì§ V7.5ì™€ ë™ì¼)
    if tasks:
        res_list = await asyncio.gather(*tasks)
        # ... Synthesis Code ...
        return {"results": list(res_list), "final_report": "Done"} # (ì•½ì‹ ë°˜í™˜)

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)

2. í”„ë¡ íŠ¸ì—”ë“œ (HTML): index.html (V8)
ê²°ê³¼ í™”ë©´ì—ì„œ **"Extracted Metadata"**ë¥¼ ë³„ë„ë¡œ ë³´ì—¬ì£¼ë„ë¡ ìˆ˜ì •í•©ë‹ˆë‹¤.
<!-- V8 ê²°ê³¼ ì¹´ë“œ (Image ë¶€ë¶„) -->
<div v-if="item.raw_b64" class="bg-white p-2 rounded border mb-3">
    <div class="flex gap-2 mb-2 text-xs">
        <button @click="item.view='proc'" :class="item.view!=='raw'?'font-bold text-sky-600':''">Processed Body</button>
        <span class="text-slate-300">|</span>
        <button @click="item.view='raw'" :class="item.view==='raw'?'font-bold text-sky-600':''">Cropped Body</button>
        <span class="text-slate-300" v-if="item.footer_b64">|</span>
        <button v-if="item.footer_b64" @click="item.view='footer'" :class="item.view==='footer'?'font-bold text-sky-600':''">Info Bar</button>
    </div>
    
    <!-- ë©”ì¸ ì´ë¯¸ì§€ -->
    <img v-if="item.view!=='footer'" :src="'data:image/jpeg;base64,' + (item.view==='raw' ? item.raw_b64 : item.proc_b64)" class="max-h-80 mx-auto object-contain bg-black">
    
    <!-- í‘¸í„°(ì •ë³´ë°”) ì´ë¯¸ì§€ -->
    <img v-if="item.view==='footer'" :src="'data:image/jpeg;base64,' + item.footer_b64" class="max-h-20 mx-auto object-contain border mt-4">

    <!-- í†µê³„ ë° ë©”íƒ€ë°ì´í„° -->
    <div class="mt-2 p-2 bg-slate-50 rounded text-xs text-slate-600">
        <p v-if="item.stats"><strong>Stats:</strong> {{ item.stats }}</p>
        <!-- Metadata ë‚´ìš©ì´ raw_contextì— í¬í•¨ë˜ì–´ ìˆìŒ -->
    </div>
</div>






ì—ì´ì „íŠ¸ ëª¨ë“œ ì‹ ê·œ ì½”ë“œ
import os
import io
import asyncio
import base64
import json
import re
from typing import List, Dict, Any, Optional
from contextlib import asynccontextmanager

# ==========================================
# [1] ë„¤íŠ¸ì›Œí¬/ë³´ì•ˆ ì„¤ì • (í•„ìˆ˜)
# ==========================================
# íšŒì‚¬ ë³´ì•ˆë§(McAfee ë“±)ì´ ë¡œì»¬ í†µì‹ (127.0.0.1)ì„ ê°€ë¡œì±„ì§€ ëª»í•˜ê²Œ ê°•ì œ ì„¤ì •
os.environ['NO_PROXY'] = '127.0.0.1,localhost,0.0.0.0,::1'
if 'HTTP_PROXY' in os.environ: del os.environ['HTTP_PROXY']
if 'HTTPS_PROXY' in os.environ: del os.environ['HTTPS_PROXY']

from fastapi import FastAPI, UploadFile, File, Form
from fastapi.responses import HTMLResponse, FileResponse
from fastapi.middleware.cors import CORSMiddleware

import pandas as pd
import numpy as np

# ê³¼í•™ ì—°ì‚° ë° ì´ë¯¸ì§€ ì²˜ë¦¬ ë¼ì´ë¸ŒëŸ¬ë¦¬
from scipy.signal import find_peaks, savgol_filter, peak_widths
from scipy.optimize import curve_fit
from scipy.ndimage import label, find_objects, gaussian_filter
from skimage.feature import blob_log  # ì›ì(Blob) ê²€ì¶œìš©
import cv2

from ollama import Client 
from PIL import Image
from pdf2image import convert_from_bytes
from pptx import Presentation
from pptx.enum.shapes import MSO_SHAPE_TYPE

# ==========================================
# [2] ëª¨ë¸ ë° ì„œë²„ ì„¤ì •
# ==========================================
# ì‚¬ìš©ì í™˜ê²½ì— ë§ëŠ” í¬íŠ¸ ë° ëª¨ë¸ëª… ì„¤ì •
OLLAMA_HOST = "http://127.0.0.1:11434"
ollama_client = Client(host=OLLAMA_HOST)

VISION_MODEL = "qwen3-vl:30b-a3b-instruct" 
TEXT_MODEL = "llama3:70b"

# ==========================================
# [3] V6 í•µì‹¬: AI ì ì‘í˜• í”„ë¡œì„¸ì„œ (Science Core)
# ==========================================
class AdaptiveProcessor:
    
    @staticmethod
    def gaussian(x, amp, cen, wid):
        return amp * np.exp(-(x-cen)**2 / (2*wid**2))

    @staticmethod
    def fit_peaks(x, y, peaks):
        """í”¼í¬ ìœ„ì¹˜ì— ê°€ìš°ì‹œì•ˆ í”¼íŒ… ìˆ˜í–‰"""
        fitted_params = []
        for p in peaks:
            try:
                # ì´ˆê¸°ê°’ ì¶”ì • (Amplitude, Center, Width)
                p0 = [y[p], x[p], 1.0]
                window = 20
                start, end = max(0, p-window), min(len(x), p+window)
                if len(x[start:end]) > 3: # ë°ì´í„°ê°€ ì¶©ë¶„í•  ë•Œë§Œ
                    popt, _ = curve_fit(AdaptiveProcessor.gaussian, x[start:end], y[start:end], p0=p0, maxfev=1000)
                    fitted_params.append({"amp": popt[0], "center": popt[1], "sigma": popt[2]})
            except:
                pass
        return fitted_params

    @staticmethod
    def process_spectrum(x, y, mode, goal):
        """
        ìŠ¤í™íŠ¸ëŸ¼ ì „ì²˜ë¦¬ (Mode: None, Auto, AI-Adaptive)
        """
        log = []
        y_proc = y.copy()
        features = {}

        # 1. Mode: None (ì „ì²˜ë¦¬ ì—†ìŒ)
        if mode == "None":
            peaks, _ = find_peaks(y, height=np.max(y)*0.05)
            return x, y, {"peaks": [{"x": float(x[p]), "y": float(y[p])} for p in peaks], "log": ["Raw Data"]}

        # 2. íŒŒë¼ë¯¸í„° ê²°ì • (AI-Adaptive or Default)
        window_length = 11
        polyorder = 3
        do_fitting = False
        
        if mode == "AI-Adaptive" and goal:
            goal_lower = goal.lower()
            if "ë…¸ì´ì¦ˆ" in goal_lower or "noise" in goal_lower:
                window_length = 21 # ìŠ¤ë¬´ë”© ê°•í™”
                log.append("AI: Detected noise reduction goal -> Strong smoothing.")
            if "í”¼íŒ…" in goal_lower or "fit" in goal_lower or "gaussian" in goal_lower:
                do_fitting = True
                log.append("AI: Detected fitting goal -> Gaussian fitting enabled.")
            if "raw" in goal_lower or "ë‚ ê²ƒ" in goal_lower:
                window_length = 0
        
        # 3. ì‹ í˜¸ ì²˜ë¦¬ ìˆ˜í–‰
        # Smoothing
        if window_length > 3 and len(y_proc) > window_length:
            y_proc = savgol_filter(y_proc, window_length, polyorder)
            log.append("Applied Savitzky-Golay Filter.")
        
        # Baseline Correction (Simple)
        baseline = np.percentile(y_proc, 5)
        y_proc = y_proc - baseline
        y_proc[y_proc < 0] = 0

        # Peak Finding
        peaks, _ = find_peaks(y_proc, height=np.max(y_proc)*0.05, distance=10)
        
        # Fitting
        fits = []
        if do_fitting and len(peaks) > 0:
            fits = AdaptiveProcessor.fit_peaks(x, y_proc, peaks)
            features["fits"] = fits

        # ê²°ê³¼ ì •ë¦¬
        peak_info = [{"x": float(x[p]), "y": float(y_proc[p])} for p in peaks]
        features["peaks"] = peak_info
        features["log"] = log
        
        return x, y_proc, features

    @staticmethod
    def process_image(img_bytes, mode, goal, equipment):
        """
        ì´ë¯¸ì§€ ì „ì²˜ë¦¬ (ì›ì ê²€ì¶œ, ì…ì ë¶„ì„, ë…¸ì´ì¦ˆ ì œê±°)
        """
        nparr = np.frombuffer(img_bytes, np.uint8)
        img = cv2.imdecode(nparr, cv2.IMREAD_COLOR)
        if img is None: return None, {}
        
        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
        stats = {}
        processed_img = img.copy()
        
        if mode == "None":
            _, buffer = cv2.imencode('.jpg', img)
            return base64.b64encode(buffer).decode('utf-8'), {"note": "Raw Image"}

        # AI íŒŒë¼ë¯¸í„° ê²°ì •
        detect_atoms = False
        detect_particles = False
        denoise_strength = 3
        
        goal_lower = (goal or "").lower()
        
        # ëª©í‘œ ë˜ëŠ” ì¥ë¹„ íƒ€ì…ì— ë”°ë¥¸ ë¡œì§ ë¶„ê¸°
        if mode == "AI-Adaptive":
            if "ì›ì" in goal_lower or "atom" in goal_lower: detect_atoms = True
            if "ì…ì" in goal_lower or "particle" in goal_lower: detect_particles = True
            if "ë…¸ì´ì¦ˆ" in goal_lower: denoise_strength = 9
        
        if equipment in ["STEM", "TEM"] and mode == "Auto": detect_atoms = True
        if equipment in ["SEM", "Optical"] and mode == "Auto": detect_particles = True

        # 1. Denoising
        gray_blur = cv2.GaussianBlur(gray, (denoise_strength, denoise_strength), 0)

        # 2. Atomic Analysis (Blob Detection - LoG)
        if detect_atoms:
            # ì›ì ê²€ì¶œ (scikit-image)
            blobs = blob_log(gray_blur, min_sigma=3, max_sigma=15, num_sigma=10, threshold=0.1)
            for blob in blobs:
                y, x, r = blob
                # ì›ì ìœ„ì¹˜ì— ë¹¨ê°„ ì› ê·¸ë¦¬ê¸°
                cv2.circle(processed_img, (int(x), int(y)), int(r*1.414), (0, 0, 255), 2)
            stats["atom_count"] = len(blobs)
            stats["method"] = "LoG (Atomic Blob Detection)"
        
        # 3. Particle Analysis (Watershed / Contours)
        elif detect_particles:
             _, thresh = cv2.threshold(gray_blur, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)
             kernel = np.ones((3,3), np.uint8)
             opening = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, kernel, iterations=2)
             contours, _ = cv2.findContours(opening, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
             
             # ì…ì ê²½ê³„ì„  ê·¸ë¦¬ê¸° (ì´ˆë¡ìƒ‰)
             cv2.drawContours(processed_img, contours, -1, (0, 255, 0), 2)
             
             stats["particle_count"] = len(contours)
             areas = [cv2.contourArea(c) for c in contours]
             if areas: stats["avg_area"] = float(np.mean(areas))
             stats["method"] = "Contour/Watershed"

        _, buffer = cv2.imencode('.jpg', processed_img)
        return base64.b64encode(buffer).decode('utf-8'), stats

# ==========================================
# [4] í—¬í¼ í•¨ìˆ˜ (Excel Island, Vision API)
# ==========================================

def detect_excel_blocks(df: pd.DataFrame) -> List[pd.DataFrame]:
    """ì—‘ì…€ ì•„ì¼ëœë“œ(ë…ë¦½ëœ í‘œ) ê°ì§€"""
    blocks = []
    if df.empty: return blocks
    mask = ~df.isnull().to_numpy()
    structure = [[0,1,0], [1,1,1], [0,1,0]] # ì‹­ìê°€ ì—°ê²° êµ¬ì¡°
    labeled_array, num_features = label(mask, structure=structure)
    slices = find_objects(labeled_array)
    
    for slice_obj in slices:
        block = df.iloc[slice_obj]
        # ìµœì†Œ í¬ê¸° í•„í„°ë§ (3í–‰ 2ì—´ ì´ìƒ)
        if block.shape[0] >= 3 and block.shape[1] >= 2:
            blocks.append(block.reset_index(drop=True))
    return blocks

async def analyze_vision_ollama(image_bytes: bytes, prompt: str) -> str:
    """Vision LLM í˜¸ì¶œ ë° ë³´ì•ˆë§ ì—ëŸ¬ í•„í„°ë§"""
    try:
        img_b64 = base64.b64encode(image_bytes).decode('utf-8')
        response = await asyncio.to_thread(
            ollama_client.chat,
            model=VISION_MODEL,
            messages=[{'role': 'user', 'content': prompt, 'images': [img_b64]}]
        )
        content = response['message']['content']
        # ë³´ì•ˆë§ HTML ì‘ë‹µ ê°ì§€
        if "<script" in content or "mwg-internal" in content:
            return "[Error: Firewall/Proxy blocked the AI connection.]"
        return content
    except Exception as e:
        return f"[Vision AI Error: {e}]"

# ==========================================
# [5] íŒŒì„œ (Parsers) - V4, V5, V6 í†µí•©
# ==========================================

async def parse_excel_v5(content: bytes, filename: str, equipment: str, mode: str, goal: str):
    """ì—‘ì…€ íŒŒì‹± + V6 ì ì‘í˜• ì²˜ë¦¬"""
    results = []
    try:
        xls = pd.ExcelFile(io.BytesIO(content))
        for sheet_name in xls.sheet_names:
            df = pd.read_excel(xls, sheet_name=sheet_name, header=None)
            blocks = detect_excel_blocks(df)
            
            for i, block in enumerate(blocks):
                chart_data = []
                peaks_info = []
                fits_info = []
                
                try:
                    # ìˆ«ì ë°ì´í„°ë§Œ ì¶”ì¶œ
                    num_block = block.apply(pd.to_numeric, errors='coerce').dropna(how='all', axis=0).dropna(how='all', axis=1)
                    if not num_block.empty and num_block.shape[0] > 5:
                        x = num_block.iloc[:, 0].fillna(0).values
                        y = num_block.iloc[:, 1].fillna(0).values if num_block.shape[1] > 1 else x
                        
                        # [V6] ì ì‘í˜• ì²˜ë¦¬
                        x_proc, y_proc, feats = AdaptiveProcessor.process_spectrum(x, y, mode, goal)
                        peaks_info = feats.get('peaks', [])
                        fits_info = feats.get('fits', [])
                        
                        step = max(1, len(x_proc) // 150)
                        chart_data = [{"x": float(v1), "y": float(v2)} for v1, v2 in zip(x_proc[::step], y_proc[::step])]
                except:
                    pass

                raw_txt = block.to_csv(index=False, header=False)[:1000]
                peak_str = ", ".join([f"{p['x']:.1f}" for p in peaks_info[:5]])
                
                results.append({
                    "type": "excel_block",
                    "filename": f"{filename} ({sheet_name}-{i+1})",
                    "summary": f"Block Analysis ({equipment}): {len(peaks_info)} peaks detected.",
                    "raw_context": f"Source: {equipment}\nPeaks Found: {peak_str}\nRaw Sample:\n{raw_txt}",
                    "chart_data": chart_data,
                    "fits": fits_info
                })
    except Exception as e:
        return [{"type": "error", "filename": filename, "msg": str(e)}]
    return results

async def parse_image_v5(content: bytes, filename: str, lang: str, equipment: str, mode: str, goal: str):
    """ì´ë¯¸ì§€ íŒŒì‹± + V6 ì›ì/ì…ì ë¶„ì„"""
    try:
        # 1. V6 í”„ë¡œì„¸ì„œë¡œ ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ë° í†µê³„ ì¶”ì¶œ
        img_b64, stats = AdaptiveProcessor.process_image(content, mode, goal, equipment)
        
        # 2. Vision LLM í•´ì„
        lang_inst = "Korean" if lang == 'korean' else "English"
        prompt = f"""
        Analyze this {equipment} image in {lang_inst}.
        User Goal: {goal}.
        CV Algorithm Stats: {stats} (e.g., Atom/Particle count).
        Explain the scientific significance based on these stats.
        """
        desc = await analyze_vision_ollama(content, prompt) # ì›ë³¸ ì´ë¯¸ì§€ë¥¼ LLMì— ì „ë‹¬
        
        return {
            "type": "image",
            "filename": filename,
            "summary": desc,
            "image_b64": img_b64, # ì „ì²˜ë¦¬ëœ(ì˜¤ë²„ë ˆì´ëœ) ì´ë¯¸ì§€ ë°˜í™˜
            "raw_context": f"Image ({equipment}): {desc}\nStats: {stats}"
        }
    except Exception as e:
        return {"type": "error", "filename": filename, "msg": str(e)}

async def parse_spectrum_v5(content: bytes, filename: str, equipment: str, mode: str, goal: str):
    """CSV ìŠ¤í™íŠ¸ëŸ¼ íŒŒì‹± + V6 ì ì‘í˜• ì²˜ë¦¬"""
    try:
        try: df = pd.read_csv(io.BytesIO(content))
        except: df = pd.read_csv(io.BytesIO(content), delimiter='\t')
        
        x = df.iloc[:, 0].values
        y = df.iloc[:, 1].values if df.shape[1] > 1 else df.iloc[:, 0].values
        
        x_proc, y_proc, feats = AdaptiveProcessor.process_spectrum(x, y, mode, goal)
        peaks = feats.get('peaks', [])
        
        step = max(1, len(x_proc) // 200)
        chart_data = [{"x": float(v1), "y": float(v2)} for v1, v2 in zip(x_proc[::step], y_proc[::step])]
        
        return {
            "type": "spectrum",
            "filename": filename,
            "summary": f"Spectrum ({equipment}): {len(peaks)} peaks.",
            "chart_data": chart_data,
            "raw_context": f"Spectrum Peaks: {len(peaks)} found. Stats: {feats.get('log')}"
        }
    except Exception as e:
        return {"type": "error", "filename": filename, "msg": str(e)}

async def parse_ppt_v4(content: bytes, filename: str, lang: str):
    """PPT íŒŒì‹± (ì´ë¯¸ì§€ ì¶”ì¶œ í¬í•¨)"""
    try:
        prs = Presentation(io.BytesIO(content))
        slides_data = []
        full_context = ""
        lang_inst = "Korean" if lang == 'korean' else "English"

        for i, slide in enumerate(prs.slides):
            slide_text = []
            extracted_images = []
            
            for shape in slide.shapes:
                if hasattr(shape, "text") and shape.text.strip():
                    slide_text.append(shape.text)
                if shape.shape_type == MSO_SHAPE_TYPE.PICTURE:
                    try:
                        image_blob = shape.image.blob
                        img_b64 = base64.b64encode(image_blob).decode('utf-8')
                        prompt = f"Analyze image in {lang_inst}."
                        desc = await analyze_vision_ollama(image_blob, prompt)
                        extracted_images.append({"b64": img_b64, "desc": desc})
                    except: pass

            txt = "\n".join(slide_text)
            img_desc = "\n".join([f"[Img]: {img['desc']}" for img in extracted_images])
            full_context += f"### Slide {i+1}\nText: {txt}\nVisuals: {img_desc}\n"
            
            slides_data.append({"slide_num": i+1, "text": txt, "images": extracted_images})
            
        return {
            "type": "ppt",
            "filename": filename,
            "summary": f"PPT ({len(prs.slides)} slides)",
            "raw_context": full_context,
            "slides": slides_data
        }
    except Exception as e:
        return {"type": "error", "filename": filename, "msg": str(e)}

async def parse_pdf_v4(content: bytes, filename: str, lang: str):
    """PDF íŒŒì‹± (í˜ì´ì§€ë³„)"""
    try:
        images = convert_from_bytes(content, dpi=150, fmt='jpeg')
        pages_data = []
        full_context = ""
        lang_inst = "Korean" if lang == 'korean' else "English"

        for i, img in enumerate(images):
            buffered = io.BytesIO()
            img.save(buffered, format="JPEG")
            img_bytes = buffered.getvalue()
            img_b64 = base64.b64encode(img_bytes).decode('utf-8')
            
            prompt = f"Analyze page {i+1} in {lang_inst}. No LaTeX."
            page_desc = await analyze_vision_ollama(img_bytes, prompt)
            
            full_context += f"\n--- Page {i+1} ---\n{page_desc}\n"
            pages_data.append({"page_num": i+1, "image_b64": img_b64, "desc": page_desc})

        return {
            "type": "pdf",
            "filename": filename,
            "summary": f"PDF ({len(images)} pages)",
            "raw_context": full_context,
            "pages": pages_data
        }
    except Exception as e:
        return {"type": "error", "filename": filename, "msg": f"PDF Error: {str(e)}"}


# ==========================================
# [6] ë©”ì¸ ë¼ìš°í„° ë° ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜
# ==========================================
app = FastAPI(title="Analyst V6 Full Integrated")
app.add_middleware(CORSMiddleware, allow_origins=["*"], allow_methods=["*"], allow_headers=["*"])

@app.post("/api/analyze")
async def analyze_orchestrator(
    files: List[UploadFile] = File(...),
    custom_prompt: Optional[str] = Form(""),
    goal: Optional[str] = Form(""),
    preprocess_mode: str = Form("Auto"),
    language: str = Form("korean"),
    equipment: str = Form("General")
):
    print(f"ğŸš€ V6 Analysis Start. Files: {len(files)}, Mode: {preprocess_mode}, Eq: {equipment}")
    
    # íŒŒì¼ ë©”ëª¨ë¦¬ ë¡œë“œ
    file_data_list = []
    for file in files:
        file_data_list.append({"name": file.filename, "content": await file.read()})
    
    tasks = []
    final_results = []
    
    # 1. íŒŒì¼ë³„ ì²˜ë¦¬
    for f in file_data_list:
        fname = f["name"].lower()
        content = f["content"]
        
        if fname.endswith(('.xlsx', '.xls')):
            # ì—‘ì…€ì€ ì¦‰ì‹œ ì‹¤í–‰ (ë¸”ë¡ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜)
            blocks = await parse_excel_v5(content, f["name"], equipment, preprocess_mode, goal)
            final_results.extend(blocks)
            
        elif fname.endswith(('.ppt', '.pptx')):
            tasks.append(parse_ppt_v4(content, f["name"], language))
            
        elif fname.endswith('.pdf'):
            tasks.append(parse_pdf_v4(content, f["name"], language))
            
        elif fname.endswith(('.csv', '.txt')):
            tasks.append(parse_spectrum_v5(content, f["name"], equipment, preprocess_mode, goal))
            
        elif fname.endswith(('.png', '.jpg', '.jpeg', '.tif', '.tiff', '.bmp')):
            tasks.append(parse_image_v5(content, f["name"], language, equipment, preprocess_mode, goal))

    # 2. ë¹„ë™ê¸° ì‘ì—… ë³‘ë ¬ ì‹¤í–‰
    if tasks:
        other_results = await asyncio.gather(*tasks)
        final_results.extend(list(other_results))
    
    # 3. ì¢…í•© ë¦¬í¬íŠ¸ ìƒì„±
    valid_data = [r for r in final_results if r.get("type") != "error"]
    context_text = ""
    for item in valid_data:
        raw = item.get("raw_context", "")
        if "mwg-internal" not in raw:
            context_text += f"\n=== Source: {item['filename']} ===\n{raw[:2500]}\n" # ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ ì¡°ì ˆ

    lang_inst = "Korean" if language == 'korean' else "English"
    system_prompt = f"""
    You are a Senior Scientist. Write report in **{lang_inst}**.
    User Goal: "{goal}".
    Do NOT generate code. Analyze scientific meaning.
    """
    
    final_report = "ë¶„ì„ ì‹¤íŒ¨"
    if context_text:
        try:
            print("ğŸ“ Synthesizing Final Report...")
            res = await asyncio.to_thread(
                ollama_client.chat,
                model=TEXT_MODEL,
                messages=[{'role': 'system', 'content': system_prompt}, {'role': 'user', 'content': context_text}],
                options={"num_ctx": 8192}
            )
            final_report = res['message']['content']
        except Exception as e:
            final_report = f"Report Generation Error: {e}"

    return {"results": final_results, "final_report": final_report}

@app.get("/", response_class=HTMLResponse)
async def serve_index():
    if os.path.exists("index.html"): return FileResponse("index.html")
    return "<h1>index.html not found</h1>"

if __name__ == "__main__":
    import uvicorn
    # 0.0.0.0ìœ¼ë¡œ ì—´ì–´ì•¼ 127.0.0.1 ë¬¸ì œ íšŒí”¼ ê°€ëŠ¥
    uvicorn.run(app, host="0.0.0.0", port=8000)







import os
import io
import asyncio
import base64
import json
import re
from typing import List, Dict, Any, Optional

# [í•„ìˆ˜] ë³´ì•ˆë§ ìš°íšŒ
os.environ['NO_PROXY'] = '127.0.0.1,localhost,0.0.0.0,::1'
if 'HTTP_PROXY' in os.environ: del os.environ['HTTP_PROXY']
if 'HTTPS_PROXY' in os.environ: del os.environ['HTTPS_PROXY']

from fastapi import FastAPI, UploadFile, File, Form
from fastapi.middleware.cors import CORSMiddleware
import pandas as pd
import numpy as np

# ê³¼í•™ ì—°ì‚° & ì´ë¯¸ì§€ ì²˜ë¦¬
from scipy.signal import find_peaks, savgol_filter, peak_widths
from scipy.optimize import curve_fit
from scipy.ndimage import label, find_objects
import cv2
from skimage.feature import blob_log # ì›ì ê²€ì¶œìš© (Laplacian of Gaussian)
from ollama import Client 
from PIL import Image
from pdf2image import convert_from_bytes
from pptx import Presentation
from pptx.enum.shapes import MSO_SHAPE_TYPE

# --- ì„¤ì • ---
OLLAMA_HOST = "http://127.0.0.1:11434"
ollama_client = Client(host=OLLAMA_HOST)
VISION_MODEL = "qwen3-vl:30b-a3b-instruct" 
TEXT_MODEL = "llama3:70b"

# --- [V6 í•µì‹¬] AI ì ì‘í˜• í”„ë¡œì„¸ì„œ ---

class AdaptiveProcessor:
    
    @staticmethod
    def gaussian(x, amp, cen, wid):
        return amp * np.exp(-(x-cen)**2 / (2*wid**2))

    @staticmethod
    def fit_peaks(x, y, peaks):
        """í”¼í¬ ìœ„ì¹˜ì— ê°€ìš°ì‹œì•ˆ í”¼íŒ… ìˆ˜í–‰"""
        fitted_params = []
        for p in peaks:
            try:
                # ì´ˆê¸°ê°’ ì¶”ì • (Amplitude, Center, Width)
                p0 = [y[p], x[p], 1.0]
                # ì£¼ë³€ ë°ì´í„°ë§Œ ì˜ë¼ì„œ í”¼íŒ… (ì†ë„ í–¥ìƒ)
                window = 20
                start, end = max(0, p-window), min(len(x), p+window)
                popt, _ = curve_fit(AdaptiveProcessor.gaussian, x[start:end], y[start:end], p0=p0, maxfev=1000)
                fitted_params.append({"amp": popt[0], "center": popt[1], "sigma": popt[2]})
            except:
                pass
        return fitted_params

    @staticmethod
    def process_spectrum(x, y, mode, goal):
        """
        Mode: None, Auto, AI-Adaptive
        Goal: ì‚¬ìš©ì ì…ë ¥ (ì˜ˆ: 'ë…¸ì´ì¦ˆ ì œê±°í•´ì¤˜', 'í”¼íŒ…í•´ì¤˜')
        """
        log = []
        y_proc = y.copy()
        features = {}

        # 1. Mode: None (ì „ì²˜ë¦¬ ì—†ìŒ)
        if mode == "None":
            peaks, _ = find_peaks(y, height=np.max(y)*0.1)
            return x, y, {"peaks": peaks, "note": "Raw Data Used"}

        # 2. íŒŒë¼ë¯¸í„° ê²°ì • (AI or Default)
        window_length = 11
        polyorder = 3
        do_fitting = False
        
        if mode == "AI-Adaptive" and goal:
            # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ë§¤ì¹­ìœ¼ë¡œ íŒŒë¼ë¯¸í„° ì¡°ì • (ì†ë„ë¥¼ ìœ„í•´ LLM í˜¸ì¶œ ëŒ€ì‹  ë¡œì§ ì²˜ë¦¬)
            # (ë” ê³ ë„í™”í•˜ë ¤ë©´ ì—¬ê¸°ì„œ LLMì—ê²Œ JSON ì„¤ì •ì„ ë°›ì•„ì˜¬ ìˆ˜ ìˆìŒ)
            if "ë…¸ì´ì¦ˆ" in goal or "noise" in goal.lower():
                window_length = 21 # ìŠ¤ë¬´ë”© ê°•í™”
                log.append("AI: Detected noise reduction goal -> Increased smoothing window.")
            if "í”¼íŒ…" in goal or "fit" in goal.lower() or "gaussian" in goal.lower():
                do_fitting = True
                log.append("AI: Detected fitting goal -> Enabled Gaussian fitting.")
            if "ë‚ ê²ƒ" in goal or "raw" in goal.lower():
                window_length = 0 # ìŠ¤ë¬´ë”© ë”

        # 3. ì‹ í˜¸ ì²˜ë¦¬ ìˆ˜í–‰
        # Smoothing
        if window_length > 3:
            y_proc = savgol_filter(y_proc, window_length, polyorder)
        
        # Baseline Correction
        baseline = np.percentile(y_proc, 5)
        y_proc = y_proc - baseline
        y_proc[y_proc < 0] = 0

        # Peak Finding
        peaks, _ = find_peaks(y_proc, height=np.max(y_proc)*0.05, distance=10)
        
        # Fitting (AI ê²°ì •)
        fits = []
        if do_fitting and len(peaks) > 0:
            fits = AdaptiveProcessor.fit_peaks(x, y_proc, peaks)
            features["fits"] = fits

        # ê²°ê³¼ ì •ë¦¬
        peak_info = [{"x": float(x[p]), "y": float(y_proc[p])} for p in peaks]
        features["peaks"] = peak_info
        features["log"] = log
        
        return x, y_proc, features

    @staticmethod
    def process_image(img_bytes, mode, goal, equipment):
        """
        ì›ì ì˜ìƒ(STEM) ì²˜ë¦¬ ë° ì ì‘í˜• í•„í„°ë§
        """
        nparr = np.frombuffer(img_bytes, np.uint8)
        img = cv2.imdecode(nparr, cv2.IMREAD_COLOR)
        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
        
        stats = {}
        processed_img = img.copy()
        
        if mode == "None":
            _, buffer = cv2.imencode('.jpg', img)
            return base64.b64encode(buffer).decode('utf-8'), {"note": "Raw Image"}

        # --- AI íŒë‹¨ ë¡œì§ ---
        detect_atoms = False
        denoise_strength = 5
        
        if mode == "AI-Adaptive":
            if "ì›ì" in goal or "atom" in goal.lower() or "STEM" in equipment:
                detect_atoms = True
            if "ë…¸ì´ì¦ˆ" in goal or "clean" in goal.lower():
                denoise_strength = 9
        elif equipment in ["STEM", "TEM"]: # Auto Mode ê¸°ë³¸ê°’
            detect_atoms = True

        # 1. Denoising
        gray_blur = cv2.GaussianBlur(gray, (denoise_strength, denoise_strength), 0)

        # 2. Atomic Analysis (Blob Detection)
        if detect_atoms:
            # scikit-imageì˜ blob_log (Laplacian of Gaussian) ì‚¬ìš© - ì›ì ê²€ì¶œì— íƒì›”
            # min_sigma, max_sigmaëŠ” ì›ìì˜ ëŒ€ëµì  í¬ê¸° í”½ì…€ ë°˜ê²½
            blobs = blob_log(gray_blur, min_sigma=3, max_sigma=15, num_sigma=10, threshold=0.1)
            
            # ì‹œê°í™” (ë¹¨ê°„ ì› ê·¸ë¦¬ê¸°)
            for blob in blobs:
                y, x, r = blob
                cv2.circle(processed_img, (int(x), int(y)), int(r*1.414), (0, 0, 255), 2)
            
            stats["atom_count"] = len(blobs)
            stats["method"] = "LoG (Laplacian of Gaussian)"
        
        # 3. General Particle Analysis (Watershed) - SEM ë“±
        elif equipment in ["SEM", "Optical"]:
             _, thresh = cv2.threshold(gray_blur, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)
             contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
             cv2.drawContours(processed_img, contours, -1, (0, 255, 0), 1)
             stats["particle_count"] = len(contours)
             stats["method"] = "Otsu Thresholding"

        _, buffer = cv2.imencode('.jpg', processed_img)
        return base64.b64encode(buffer).decode('utf-8'), stats

# --- [ì¬ì‚¬ìš©] ì—‘ì…€ ì•„ì¼ëœë“œ ê°ì§€ ---
def detect_excel_blocks(df: pd.DataFrame) -> List[pd.DataFrame]:
    blocks = []
    if df.empty: return blocks
    mask = ~df.isnull().to_numpy()
    structure = [[0,1,0], [1,1,1], [0,1,0]]
    labeled_array, num_features = label(mask, structure=structure)
    slices = find_objects(labeled_array)
    for slice_obj in slices:
        block = df.iloc[slice_obj]
        if block.shape[0] >= 3 and block.shape[1] >= 2:
            blocks.append(block.reset_index(drop=True))
    return blocks

async def analyze_vision_ollama(image_bytes: bytes, prompt: str) -> str:
    try:
        img_b64 = base64.b64encode(image_bytes).decode('utf-8')
        response = await asyncio.to_thread(
            ollama_client.chat,
            model=VISION_MODEL,
            messages=[{'role': 'user', 'content': prompt, 'images': [img_b64]}]
        )
        content = response['message']['content']
        if "<script" in content or "mwg-internal" in content: return "[Error: Firewall Blocked]"
        return content
    except Exception as e: return f"[Vision Error: {e}]"

# --- ë©”ì¸ ë¼ìš°í„° ---
app = FastAPI(title="Analyst V6 Adaptive")
app.add_middleware(CORSMiddleware, allow_origins=["*"], allow_methods=["*"], allow_headers=["*"])

@app.post("/api/analyze")
async def analyze_orchestrator(
    files: List[UploadFile] = File(...),
    custom_prompt: Optional[str] = Form(""),
    goal: Optional[str] = Form(""),       # ì‚¬ìš©ì ë¶„ì„ ëª©í‘œ (ì˜ˆ: "ì›ì ì„¸ì¤˜")
    preprocess_mode: str = Form("Auto"),  # None, Auto, AI-Adaptive
    language: str = Form("korean"),
    equipment: str = Form("General")
):
    print(f"ğŸš€ V6 Request. Mode: {preprocess_mode}, Goal: {goal}, Eq: {equipment}")
    
    file_data_list = []
    for file in files:
        file_data_list.append({"name": file.filename, "content": await file.read()})
    
    final_results = []
    tasks = []

    # 1. ì—‘ì…€ ì²˜ë¦¬
    for f in file_data_list:
        fname = f["name"].lower()
        content = f["content"]
        
        if fname.endswith(('.xlsx', '.xls')):
            try:
                xls = pd.ExcelFile(io.BytesIO(content))
                for sheet_name in xls.sheet_names:
                    df = pd.read_excel(xls, sheet_name=sheet_name, header=None)
                    blocks = detect_excel_blocks(df)
                    for i, block in enumerate(blocks):
                        chart_data = []
                        peaks_info = []
                        fits_info = []
                        
                        # ìˆ«ì ì¶”ì¶œ ë° ì²˜ë¦¬
                        try:
                            num_block = block.apply(pd.to_numeric, errors='coerce').dropna(how='all', axis=0).dropna(how='all', axis=1)
                            if not num_block.empty and num_block.shape[0] > 5:
                                x = num_block.iloc[:, 0].fillna(0).values
                                y = num_block.iloc[:, 1].fillna(0).values if num_block.shape[1] > 1 else x
                                
                                # [V6] Adaptive Processing í˜¸ì¶œ
                                x_proc, y_proc, feats = AdaptiveProcessor.process_spectrum(x, y, preprocess_mode, goal)
                                peaks_info = feats.get('peaks', [])
                                fits_info = feats.get('fits', [])
                                
                                step = max(1, len(x_proc) // 150)
                                chart_data = [{"x": float(v1), "y": float(v2)} for v1, v2 in zip(x_proc[::step], y_proc[::step])]
                        except: pass

                        raw_txt = block.to_csv(index=False, header=False)[:1000]
                        peak_str = ", ".join([f"{p['x']:.1f}" for p in peaks_info[:5]])
                        
                        final_results.append({
                            "type": "excel_block",
                            "filename": f"{f['name']} ({sheet_name}-{i+1})",
                            "summary": f"Peaks found: {len(peaks_info)}. Fits: {len(fits_info)}",
                            "raw_context": f"Data: {equipment}\nPeaks: {peak_str}\nRaw:\n{raw_txt}",
                            "chart_data": chart_data,
                            "fits": fits_info
                        })
            except Exception as e:
                final_results.append({"type": "error", "filename": f["name"], "msg": str(e)})

        # 2. ì´ë¯¸ì§€ ì²˜ë¦¬
        elif fname.endswith(('.png', '.jpg', '.jpeg', '.tif', '.bmp')):
            try:
                img_b64, stats = AdaptiveProcessor.process_image(content, preprocess_mode, goal, equipment)
                
                # Vision LLM í•´ì„
                lang_inst = "Korean" if language == 'korean' else "English"
                prompt = f"Analyze this {equipment} image in {lang_inst}. Goal: {goal}. CV Stats: {stats}"
                desc = await analyze_vision_ollama(content, prompt)
                
                final_results.append({
                    "type": "image",
                    "filename": f["name"],
                    "summary": desc,
                    "image_b64": img_b64,
                    "raw_context": f"Image Analysis ({equipment}): {desc}\nCV Stats: {stats}"
                })
            except Exception as e:
                final_results.append({"type": "error", "filename": f["name"], "msg": str(e)})
                
        # 3. PDF/PPT/CSV (ê¸°ì¡´ V4 ë¡œì§ ì‚¬ìš© - ì—¬ê¸°ì„  ìƒëµí–ˆìœ¼ë‚˜ ì‹¤ì œ ì½”ë“œì—” í¬í•¨ë˜ì–´ì•¼ í•¨)
        # ... (ì´ì „ ì½”ë“œì˜ tasks.append ë¡œì§ ê·¸ëŒ€ë¡œ ì‚¬ìš©)

    # (ê°„ì†Œí™”ë¥¼ ìœ„í•´ ë‹¤ë¥¸ íƒ€ì… ìŠ¤í‚µ, ì‹¤ì œë¡œëŠ” ì—¬ê¸°ì— gather ë¡œì§ í•„ìš”)

    # ì¢…í•© ë¦¬í¬íŠ¸
    context_text = ""
    for item in final_results:
        if item.get("type") != "error":
            raw = item.get("raw_context", "")
            if "mwg-internal" not in raw:
                context_text += f"\n=== Source: {item['filename']} ===\n{raw[:2000]}\n"

    lang_inst = "Korean" if language == 'korean' else "English"
    system_prompt = f"""
    You are a Senior Scientist. Write report in **{lang_inst}**.
    Focus on the user's goal: "{goal}".
    Do NOT generate code.
    """
    
    final_report = "ë¶„ì„ ì‹¤íŒ¨"
    if context_text:
        try:
            res = await asyncio.to_thread(
                ollama_client.chat,
                model=TEXT_MODEL,
                messages=[{'role': 'system', 'content': system_prompt}, {'role': 'user', 'content': context_text}]
            )
            final_report = res['message']['content']
        except Exception as e:
            final_report = f"Error: {e}"

    return {"results": final_results, "final_report": final_report}

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)


ì—ì´ì „íŠ¸ ëª¨ë“œ ì‹ ê·œ ì›¹í˜ì´ì§€
<!DOCTYPE html>
<html lang="ko">
<head>
    <meta charset="UTF-8">
    <title>AI Science Lab V6</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <script src="https://unpkg.com/vue@3/dist/vue.global.js"></script>
    <script src="https://cdn.plot.ly/plotly-2.27.0.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/marked/marked.min.js"></script>
    <script src="https://unpkg.com/lucide@latest"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/html2pdf.js/0.10.1/html2pdf.bundle.min.js"></script>
    <style>@import url('https://fonts.googleapis.com/css2?family=Inter:wght@400;600;700&display=swap'); body { font-family: 'Inter', sans-serif; } .prose { max-width: none; overflow-wrap: break-word; } .loader { border: 3px solid #f3f3f3; border-top: 3px solid #db2777; border-radius: 50%; width: 20px; height: 20px; animation: spin 1s linear infinite; } @keyframes spin { 0% { transform: rotate(0deg); } 100% { transform: rotate(360deg); } }</style>
</head>
<body class="bg-slate-50 text-slate-800">
    <div id="app" class="min-h-screen p-6">
        <header class="max-w-7xl mx-auto mb-8 flex items-center gap-3">
            <div class="bg-pink-600 p-2.5 rounded-xl shadow-lg"><i data-lucide="atom" class="w-8 h-8 text-white"></i></div>
            <div><h1 class="text-2xl font-bold text-slate-900">AI Science Lab V6</h1><p class="text-sm text-slate-500">Adaptive Preprocessing â€¢ Atomic Analysis</p></div>
        </header>

        <main class="max-w-7xl mx-auto grid grid-cols-1 lg:grid-cols-12 gap-8">
            <!-- Controls -->
            <div class="lg:col-span-4 space-y-6">
                <div class="bg-white p-6 rounded-2xl shadow-sm border border-slate-100">
                    <h2 class="font-bold mb-4">1. Configuration</h2>
                    
                    <div class="mb-4">
                        <label class="block text-xs font-bold text-slate-400 uppercase mb-1">Preprocessing Mode</label>
                        <select v-model="preprocessMode" class="w-full p-2 bg-slate-50 border rounded text-sm">
                            <option value="Auto">Auto (Equipment Default)</option>
                            <option value="None">None (Raw Data)</option>
                            <option value="AI-Adaptive">AI-Adaptive (Goal Driven)</option>
                        </select>
                    </div>

                    <div class="mb-4" v-if="preprocessMode === 'AI-Adaptive'">
                        <label class="block text-xs font-bold text-pink-500 uppercase mb-1">Analysis Goal (AI Target)</label>
                        <input type="text" v-model="goal" class="w-full p-2 border-2 border-pink-100 rounded focus:border-pink-500 text-sm" placeholder="e.g., Count atoms, Remove noise, Fit Gaussian">
                    </div>

                    <div class="mb-4">
                        <label class="block text-xs font-bold text-slate-400 uppercase mb-1">Equipment</label>
                        <select v-model="equipment" class="w-full p-2 bg-slate-50 border rounded text-sm">
                            <option value="STEM">STEM / TEM (Atomic)</option>
                            <option value="SEM">SEM (Particles)</option>
                            <option value="Spectrum">Spectrum (General)</option>
                        </select>
                    </div>

                    <div class="border-2 border-dashed border-slate-200 rounded-xl p-6 text-center cursor-pointer hover:bg-pink-50 hover:border-pink-300" @click="$refs.f.click()">
                        <input type="file" ref="f" multiple class="hidden" @change="hF">
                        <p class="font-semibold text-slate-600">Upload Data</p>
                    </div>
                    <div v-if="files.length" class="mt-2 text-xs text-slate-500">{{ files.length }} files selected</div>
                </div>

                <div class="bg-white p-6 rounded-2xl shadow-sm border border-slate-100">
                    <button @click="analyze" :disabled="isAnalyzing" class="w-full py-3 bg-pink-600 text-white rounded-xl font-bold flex justify-center gap-2 shadow-lg shadow-pink-200">
                        <div v-if="isAnalyzing" class="loader"></div>
                        <span>Run V6 Analysis</span>
                    </button>
                </div>
            </div>

            <!-- Results -->
            <div class="lg:col-span-8">
                <div v-if="result" class="space-y-6 animate-fade-in-up">
                    <div id="report-content" class="bg-white p-10 rounded-2xl shadow-sm border border-slate-100">
                        <div class="flex justify-between items-center mb-6">
                            <h1 class="text-3xl font-bold">Analysis Report</h1>
                            <span class="px-3 py-1 bg-pink-100 text-pink-700 rounded-full text-xs font-bold">{{ result.mode }} Mode</span>
                        </div>

                        <section class="mb-10">
                            <h2 class="text-xl font-bold text-pink-700 mb-4 pb-2 border-b">Synthesis</h2>
                            <div class="prose text-slate-700" v-html="md(result.final_report)"></div>
                        </section>

                        <section>
                            <h2 class="text-xl font-bold text-slate-800 mb-6 pb-2 border-b">Visual Data</h2>
                            <div v-for="(item, idx) in result.results" :key="idx" class="mb-10 break-inside-avoid">
                                <h3 class="font-bold text-lg mb-2">{{ item.filename }}</h3>

                                <!-- Chart -->
                                <div v-if="item.chart_data" class="bg-slate-50 p-4 rounded border mb-4">
                                    <div :id="'chart-' + idx" class="w-full h-64 bg-white"></div>
                                    <p v-if="item.fits && item.fits.length" class="text-xs text-green-600 mt-2 font-mono">
                                        Gaussian Fits detected: {{ item.fits.length }} (Check graph)
                                    </p>
                                </div>

                                <!-- Image -->
                                <div v-if="item.image_b64" class="text-center mb-4 bg-slate-50 p-4 rounded border">
                                    <img :src="'data:image/jpeg;base64,' + item.image_b64" class="max-h-96 mx-auto rounded shadow-sm bg-black">
                                </div>

                                <div class="text-sm prose" v-html="md(item.summary)"></div>
                            </div>
                        </section>
                    </div>
                </div>
            </div>
        </main>
    </div>
    <script>
        const { createApp, ref, nextTick } = Vue;
        createApp({
            setup() {
                const files = ref([]);
                const isAnalyzing = ref(false);
                const result = ref(null);
                const preprocessMode = ref("Auto");
                const goal = ref("");
                const equipment = ref("STEM");

                const md = (t) => marked.parse(t||'');
                const hF = (e) => files.value.push(...e.target.files);

                const analyze = async () => {
                    isAnalyzing.value = true;
                    const fd = new FormData();
                    files.value.forEach(f => fd.append('files', f));
                    fd.append('preprocess_mode', preprocessMode.value);
                    fd.append('goal', goal.value);
                    fd.append('equipment', equipment.value);

                    try {
                        const res = await fetch('/api/analyze', { method: 'POST', body: fd });
                        const data = await res.json();
                        result.value = { ...data, mode: preprocessMode.value };
                        await nextTick();
                        
                        // Draw Charts
                        data.results.forEach((item, idx) => {
                            if (item.chart_data) {
                                const trace = {
                                    x: item.chart_data.map(d=>d.x), y: item.chart_data.map(d=>d.y),
                                    mode: 'lines', name: 'Data', line: {color: '#db2777'}
                                };
                                Plotly.newPlot('chart-'+idx, [trace], {margin:{t:10,b:30,l:40,r:10}}, {displayModeBar:false});
                            }
                        });
                        lucide.createIcons();
                    } catch(e) { alert(e); } 
                    finally { isAnalyzing.value = false; }
                };
                setTimeout(() => lucide.createIcons(), 100);
                return { files, isAnalyzing, result, preprocessMode, goal, equipment, hF, analyze, md };
            }
        }).mount('#app');
    </script>
</body>
</html>







ì‹ ê·œ ì½”ë“œ

import os
import io
import asyncio
import base64
import json
import re
from typing import List, Dict, Any, Optional
from contextlib import asynccontextmanager

# --- [í•„ìˆ˜] í”„ë¡ì‹œ ê°•ì œ í•´ì œ ---
os.environ['NO_PROXY'] = '127.0.0.1,localhost,0.0.0.0,::1'
if 'HTTP_PROXY' in os.environ: del os.environ['HTTP_PROXY']
if 'HTTPS_PROXY' in os.environ: del os.environ['HTTPS_PROXY']
# ------------------------------

from fastapi import FastAPI, UploadFile, File, Form
from fastapi.responses import HTMLResponse, FileResponse
from fastapi.middleware.cors import CORSMiddleware

import pandas as pd
import numpy as np
from scipy.signal import find_peaks
from scipy.ndimage import label, find_objects # â˜… í•µì‹¬ ì¶”ê°€: ì•„ì¼ëœë“œ ê°ì§€ìš©
from ollama import Client 
from PIL import Image
from pdf2image import convert_from_bytes
from pptx import Presentation
from pptx.enum.shapes import MSO_SHAPE_TYPE

# --- ì„¤ì • ---
OLLAMA_HOST = "http://127.0.0.1:11434" 
ollama_client = Client(host=OLLAMA_HOST)

VISION_MODEL = "qwen3-vl:30b-a3b-instruct" 
TEXT_MODEL = "llama3:70b"

# --- [í•µì‹¬] ì—‘ì…€ ì„¬(Island) ê°ì§€ ì•Œê³ ë¦¬ì¦˜ ---

def detect_excel_blocks(df: pd.DataFrame) -> List[pd.DataFrame]:
    """
    ì»´í“¨í„° ë¹„ì „ì˜ 'Connected Components Labeling'ì„ ì‚¬ìš©í•˜ì—¬
    ìƒí•˜ì¢Œìš°ë¡œ ë–¨ì–´ì§„ ëª¨ë“  ë°ì´í„° ë¸”ë¡ì„ ë…ë¦½ì ìœ¼ë¡œ ê°ì§€í•©ë‹ˆë‹¤.
    """
    blocks = []
    if df.empty: return blocks

    # 1. ë°ì´í„°ê°€ ìˆëŠ” ê³³ì„ True, ì—†ëŠ” ê³³ì„ Falseë¡œ ë§ˆìŠ¤í‚¹
    #    (í—¤ë”ë‚˜ ì¸ë±ìŠ¤ ì²˜ë¦¬ë¥¼ ìœ„í•´ ì¼ë‹¨ ì „ì²´ë¥¼ ë´„)
    mask = ~df.isnull().to_numpy() # numpy ë°°ì—´ë¡œ ë³€í™˜
    
    # 2. ì—°ê²°ëœ ìš”ì†Œ ë¼ë²¨ë§ (ëŒ€ê°ì„  ì—°ê²°ë„ í¬í•¨í•˜ë ¤ë©´ structure=[[1,1,1],[1,1,1],[1,1,1]])
    #    ì—¬ê¸°ì„œëŠ” ìƒí•˜ì¢Œìš° ì—°ê²°ë§Œ ì¸ì • (ì‹­ìê°€ êµ¬ì¡°)
    structure = [[0,1,0], [1,1,1], [0,1,0]]
    labeled_array, num_features = label(mask, structure=structure)
    
    if num_features == 0:
        return blocks

    # 3. ê° ë¼ë²¨(ì„¬)ë³„ë¡œ bounding box ì¶”ì¶œ
    slices = find_objects(labeled_array)
    
    for i, slice_obj in enumerate(slices):
        # slice_objëŠ” (row_slice, col_slice) íŠœí”Œì„
        # ì›ë³¸ ë°ì´í„°í”„ë ˆì„ì—ì„œ í•´ë‹¹ ì˜ì—­ë§Œ ì˜ë¼ëƒ„
        block = df.iloc[slice_obj]
        
        # 4. ë„ˆë¬´ ì‘ì€ ë¸”ë¡(ë…¸ì´ì¦ˆ) ì œê±° (ì˜ˆ: 2x2 ë¯¸ë§Œ)
        if block.shape[0] >= 2 and block.shape[1] >= 2:
            # ì¸ë±ìŠ¤ ì´ˆê¸°í™” (ê¹”ë”í•˜ê²Œ ì²˜ë¦¬í•˜ê¸° ìœ„í•¨)
            blocks.append(block.reset_index(drop=True))
            
    return blocks

async def analyze_vision_ollama(image_bytes: bytes, prompt: str) -> str:
    """Vision ëª¨ë¸ í˜¸ì¶œ"""
    try:
        img_b64 = base64.b64encode(image_bytes).decode('utf-8')
        response = await asyncio.to_thread(
            ollama_client.chat,
            model=VISION_MODEL,
            messages=[{'role': 'user', 'content': prompt, 'images': [img_b64]}]
        )
        content = response['message']['content']
        if "<script" in content or "mwg-internal" in content:
            return "[Error: Blocked by Firewall]"
        return content
    except Exception as e:
        return f"[Vision Error: {e}]"

# --- íŒŒì„œ (Parsers) ---

async def parse_excel_v5(content: bytes, filename: str, lang: str):
    """ì—‘ì…€ íŒŒì‹± (ê°œì„ ëœ ë¸”ë¡ ê°ì§€ ì ìš©)"""
    results = []
    try:
        xls = pd.ExcelFile(io.BytesIO(content))
        
        for sheet_name in xls.sheet_names:
            # í—¤ë” ì—†ì´ ì›ë³¸ ê·¸ëŒ€ë¡œ ì½ìŒ
            df = pd.read_excel(xls, sheet_name=sheet_name, header=None)
            
            # â˜… ê°œì„ ëœ ê°ì§€ ë¡œì§ í˜¸ì¶œ
            blocks = detect_excel_blocks(df)
            
            print(f"ğŸ“Š Sheet '{sheet_name}': Detected {len(blocks)} blocks.") # ë””ë²„ê¹…ìš© ë¡œê·¸

            for i, block in enumerate(blocks):
                # ì°¨íŠ¸ ë°ì´í„° ì¶”ì¶œ ì‹œë„
                chart_data = []
                try:
                    # ë¸”ë¡ ë‚´ì—ì„œ ìˆ«ìë¡œë§Œ êµ¬ì„±ëœ ê°€ì¥ í° ì˜ì—­ ì°¾ê¸°
                    # (í—¤ë”ë‚˜ í…ìŠ¤íŠ¸ ì»¬ëŸ¼ì„ ì œì™¸í•˜ê¸° ìœ„í•¨)
                    num_block = block.apply(pd.to_numeric, errors='coerce')
                    num_block = num_block.dropna(how='all', axis=0).dropna(how='all', axis=1)
                    
                    if not num_block.empty and num_block.shape[0] > 2:
                        # ì²«ë²ˆì§¸ ìˆ«ì ì»¬ëŸ¼ = X, ë‘ë²ˆì§¸ ìˆ«ì ì»¬ëŸ¼ = Y
                        x = num_block.iloc[:, 0].fillna(0).values
                        y = num_block.iloc[:, 1].fillna(0).values if num_block.shape[1] > 1 else x
                        
                        # ë°ì´í„° í¬ì¸íŠ¸ê°€ ë„ˆë¬´ ë§ìœ¼ë©´ ìƒ˜í”Œë§
                        step = max(1, len(x) // 100)
                        chart_data = [{"x": float(v1), "y": float(v2)} for v1, v2 in zip(x[::step], y[::step])]
                except:
                    pass

                # LLMì—ê²Œ ì¤„ í…ìŠ¤íŠ¸ (ì•ë¶€ë¶„ 2000ì)
                raw_csv = block.to_csv(index=False, header=False)[:2000]
                
                results.append({
                    "type": "excel_block",
                    "filename": f"{filename} ({sheet_name} - Block {i+1})",
                    "summary": f"Data Block {i+1} found in sheet '{sheet_name}'. Contains {block.shape[0]} rows x {block.shape[1]} columns.",
                    "raw_context": f"Excel Data Block {i+1} ({sheet_name}):\n{raw_csv}",
                    "chart_data": chart_data,
                    "image_b64": None
                })
                
    except Exception as e:
        return [{"type": "error", "filename": filename, "msg": str(e)}]
    return results

async def parse_ppt_v4(content: bytes, filename: str, lang: str):
    """PPTX íŒŒì‹±"""
    try:
        prs = Presentation(io.BytesIO(content))
        slides_data = []
        full_context = ""
        lang_inst = "Korean" if lang == 'korean' else "English"

        for i, slide in enumerate(prs.slides):
            slide_text = []
            extracted_images = []
            
            for shape in slide.shapes:
                if hasattr(shape, "text") and shape.text.strip():
                    slide_text.append(shape.text)
                if shape.shape_type == MSO_SHAPE_TYPE.PICTURE:
                    try:
                        image_blob = shape.image.blob
                        img_b64 = base64.b64encode(image_blob).decode('utf-8')
                        prompt = f"Analyze this image in {lang_inst}. Explain scientific details."
                        desc = await analyze_vision_ollama(image_blob, prompt)
                        extracted_images.append({"b64": img_b64, "desc": desc})
                    except: pass

            combined_txt = "\n".join(slide_text)
            img_desc_combined = "\n".join([f"[Image]: {img['desc']}" for img in extracted_images])
            full_context += f"### Slide {i+1}\nText: {combined_txt}\nVisuals: {img_desc_combined}\n"
            
            slides_data.append({
                "slide_num": i+1,
                "text": combined_txt,
                "images": extracted_images
            })
            
        return {
            "type": "ppt",
            "filename": filename,
            "summary": f"**PPT Analysis ({len(prs.slides)} slides)**",
            "raw_context": full_context,
            "slides": slides_data
        }
    except Exception as e:
        return {"type": "error", "filename": filename, "msg": str(e)}

async def parse_pdf_v4(content: bytes, filename: str, lang: str):
    """PDF íŒŒì‹±"""
    try:
        images = convert_from_bytes(content, dpi=150, fmt='jpeg')
        pages_data = []
        full_context = ""
        lang_inst = "Korean" if lang == 'korean' else "English"
        
        print(f"ğŸ“„ PDF: {filename} ({len(images)} pages)")

        for i, img in enumerate(images):
            buffered = io.BytesIO()
            img.save(buffered, format="JPEG")
            img_bytes = buffered.getvalue()
            img_b64 = base64.b64encode(img_bytes).decode('utf-8')
            
            prompt = f"""
            Analyze page {i+1} in {lang_inst}.
            1. Summarize content.
            2. Describe figures/tables clearly.
            3. No LaTeX. Plain text only.
            """
            print(f"   - Page {i+1} Vision Analysis...")
            page_desc = await analyze_vision_ollama(img_bytes, prompt)
            
            full_context += f"\n--- Page {i+1} ---\n{page_desc}\n"
            pages_data.append({"page_num": i+1, "image_b64": img_b64, "desc": page_desc})

        return {
            "type": "pdf",
            "filename": filename,
            "summary": f"**PDF Analysis ({len(images)} pages)**",
            "raw_context": full_context,
            "pages": pages_data
        }
    except Exception as e:
        return {"type": "error", "filename": filename, "msg": f"PDF Error: {str(e)}"}

async def parse_image_v4(content: bytes, filename: str, lang: str):
    try:
        img = Image.open(io.BytesIO(content)).convert("RGB")
        buffered = io.BytesIO()
        img.save(buffered, format="JPEG")
        img_bytes = buffered.getvalue()
        img_b64 = base64.b64encode(img_bytes).decode('utf-8')
        
        lang_inst = "Korean" if lang == 'korean' else "English"
        desc = await analyze_vision_ollama(img_bytes, f"Analyze in {lang_inst}. Focus on scientific details.")
        
        return {
            "type": "image",
            "filename": filename,
            "summary": desc,
            "image_b64": img_b64,
            "raw_context": desc
        }
    except Exception as e:
        return {"type": "error", "filename": filename, "msg": str(e)}

async def parse_spectrum(content: bytes, filename: str):
    try:
        try: df = pd.read_csv(io.BytesIO(content))
        except: df = pd.read_csv(io.BytesIO(content), delimiter='\t')
        x = df.iloc[:, 0].values
        y = df.iloc[:, 1].values if df.shape[1] > 1 else df.iloc[:, 0].values
        peaks, _ = find_peaks(y, height=np.mean(y), distance=20)
        step = max(1, len(x) // 500)
        chart_data = [{"x": float(xi), "y": float(yi)} for xi, yi in zip(x[::step], y[::step])]
        return {
            "type": "spectrum",
            "filename": filename,
            "summary": f"Spectrum: {len(peaks)} peaks.",
            "chart_data": chart_data,
            "raw_context": f"Spectrum Peaks: {peaks}, Max: {np.max(y)}"
        }
    except Exception as e:
        return {"type": "error", "filename": filename, "msg": str(e)}

# --- ë©”ì¸ ë¼ìš°í„° ---
app = FastAPI(title="Analyst V5")
app.add_middleware(CORSMiddleware, allow_origins=["*"], allow_methods=["*"], allow_headers=["*"])

@app.post("/api/analyze")
async def analyze_orchestrator(
    files: List[UploadFile] = File(...),
    custom_prompt: Optional[str] = Form(""),
    language: str = Form("korean")
):
    print(f"ğŸš€ Processing Request. Lang: {language}")
    
    file_data_list = []
    for file in files:
        file_data_list.append({"name": file.filename, "content": await file.read()})
    
    tasks = []
    final_results = []
    
    for f in file_data_list:
        fname = f["name"].lower()
        content = f["content"]
        
        if fname.endswith(('.xlsx', '.xls')):
            # ì—‘ì…€ì€ ì¦‰ì‹œ ì²˜ë¦¬
            blocks = await parse_excel_v5(content, f["name"], language)
            final_results.extend(blocks)
        elif fname.endswith(('.ppt', '.pptx')):
            tasks.append(parse_ppt_v4(content, f["name"], language))
        elif fname.endswith('.pdf'):
            tasks.append(parse_pdf_v4(content, f["name"], language))
        elif fname.endswith(('.csv', '.txt')):
            tasks.append(parse_spectrum(content, f["name"]))
        elif fname.endswith(('.png', '.jpg', '.jpeg', '.tif', '.tiff', '.bmp')):
            tasks.append(parse_image_v4(content, f["name"], language))

    other_results = await asyncio.gather(*tasks)
    final_results.extend(list(other_results))
    
    # ì¢…í•© ë¦¬í¬íŠ¸
    valid_data = [r for r in final_results if r.get("type") != "error"]
    context_text = ""
    for item in valid_data:
        raw = item.get("raw_context", "")
        if "mwg-internal" not in raw:
            context_text += f"\n=== Source: {item['filename']} ===\n{raw[:5000]}\n"

    lang_inst = "Korean" if language == 'korean' else "English"
    system_prompt = f"You are a Senior Scientist. Write the report in **{lang_inst}**. User Request: {custom_prompt}"
    
    final_report = "ë¶„ì„ ì‹¤íŒ¨"
    if context_text:
        try:
            print("ğŸ“ Synthesizing Report...")
            res = await asyncio.to_thread(
                ollama_client.chat,
                model=TEXT_MODEL,
                messages=[{'role': 'system', 'content': system_prompt}, {'role': 'user', 'content': context_text}]
            )
            final_report = res['message']['content']
        except Exception as e:
            final_report = f"Error: {e}"

    return {"results": final_results, "final_report": final_report}

@app.get("/", response_class=HTMLResponse)
async def serve_index():
    if os.path.exists("index.html"): return FileResponse("index.html")
    return "<h1>index.html not found</h1>"

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)







import os
import io
import asyncio
import base64
import json
import re
from typing import List, Dict, Any, Optional
from contextlib import asynccontextmanager

# --- [í•„ìˆ˜] í”„ë¡ì‹œ ê°•ì œ í•´ì œ (ë³´ì•ˆë§ ìš°íšŒ) ---
os.environ['NO_PROXY'] = '127.0.0.1,localhost,0.0.0.0,::1'
if 'HTTP_PROXY' in os.environ: del os.environ['HTTP_PROXY']
if 'HTTPS_PROXY' in os.environ: del os.environ['HTTPS_PROXY']
# ---------------------------------------------

from fastapi import FastAPI, UploadFile, File, Form
from fastapi.responses import HTMLResponse, FileResponse
from fastapi.middleware.cors import CORSMiddleware

import pandas as pd
import numpy as np
from scipy.signal import find_peaks
from ollama import Client 
from PIL import Image
from pdf2image import convert_from_bytes
from pptx import Presentation
from pptx.enum.shapes import MSO_SHAPE_TYPE

# --- ì„¤ì • ---
OLLAMA_HOST = "http://127.0.0.1:11434" 
ollama_client = Client(host=OLLAMA_HOST)

# ëª¨ë¸ (ë³´ìœ í•˜ì‹  ëª¨ë¸ëª… í™•ì¸)
VISION_MODEL = "qwen3-vl:30b-a3b-instruct" 
TEXT_MODEL = "llama3:70b"

# --- ìœ í‹¸ë¦¬í‹° ---

def clean_latex(text: str) -> str:
    """ê°„ë‹¨í•œ LaTeX íƒœê·¸ ì œê±° (ê°€ë…ì„± í–¥ìƒ)"""
    # $...$ ì œê±°í•˜ê³  ë‚´ë¶€ í…ìŠ¤íŠ¸ë§Œ ë‚¨ê¸°ê±°ë‚˜, íŠ¹ìˆ˜ë¬¸ì ì™„í™”
    # ì—¬ê¸°ì„œëŠ” ë³µì¡í•œ ë³€í™˜ë³´ë‹¤, LLMì—ê²Œ Plain Textë¥¼ ìš”ì²­í•˜ëŠ” ê²ƒì´ ìš°ì„ ì„.
    return text

def detect_excel_blocks(df: pd.DataFrame) -> List[pd.DataFrame]:
    """ì—‘ì…€ ë¸”ë¡ ê°ì§€ ë¡œì§ ê°œì„ """
    blocks = []
    if df.empty: return blocks
    
    # ë°ì´í„°ê°€ ìˆëŠ” í–‰/ì—´ë§Œ ë‚¨ê¸°ê¸° (ì „ì²´ ê³µë°± ì œê±°)
    df = df.dropna(how='all', axis=0).dropna(how='all', axis=1)
    
    if df.empty: return blocks

    # ì¸ë±ìŠ¤ ë¦¬ì…‹í•˜ì—¬ ì—°ì†ì„± í™•ì¸
    df = df.reset_index(drop=True)
    
    # ê°„ë‹¨í•˜ê²Œ: 3í–‰ ì´ìƒ ì—°ì†ëœ ë¹ˆ í–‰ì´ ìˆìœ¼ë©´ ë¶„ë¦¬
    # (ë³µì¡í•œ ë¡œì§ ëŒ€ì‹ , í™•ì‹¤í•œ ë°ì´í„° ë©ì–´ë¦¬ë¥¼ ì°¾ìŒ)
    
    # 1. ì¼ë‹¨ ì „ì²´ë¥¼ í•˜ë‚˜ì˜ ë¸”ë¡ìœ¼ë¡œ ë³´ë˜, 
    # ë°ì´í„° íƒ€ì…ì´ ì„ì—¬ìˆê±°ë‚˜ í—¤ë”ê°€ ì¤‘ê°„ì— ë‚˜ì˜¤ë©´ ìª¼ê°œëŠ” ë°©ì‹ì€ ë³µì¡í•˜ë¯€ë¡œ
    # í˜„ì¬ëŠ” ì „ì²´ ì‹œíŠ¸ë¥¼ í•˜ë‚˜ì˜ ìœ íš¨ ë¸”ë¡ìœ¼ë¡œ ì²˜ë¦¬í•˜ë˜, 
    # ë„ˆë¬´ ë“¬ì„±ë“¬ì„±í•œ ê²½ìš°(Island)ë¥¼ ìœ„í•´ í´ëŸ¬ìŠ¤í„°ë§ì„ í•´ì•¼ í•¨.
    
    # ì—¬ê¸°ì„œëŠ” 'ë¹ˆ í–‰'ì„ ê¸°ì¤€ìœ¼ë¡œ ë‹¨ìˆœ ë¶„ë¦¬ (ê°€ì¥ íš¨ê³¼ì )
    is_empty = df.isnull().all(axis=1)
    groups = (is_empty != is_empty.shift()).cumsum()
    
    for _, group in df.groupby(groups):
        if not group.isnull().all().all(): # ë°ì´í„°ê°€ ìˆëŠ” ê·¸ë£¹ë§Œ
            # 2x2 ì´ìƒì¸ ê²½ìš°ë§Œ ë¸”ë¡ìœ¼ë¡œ ì¸ì •
            if group.shape[0] >= 2 and group.shape[1] >= 2:
                blocks.append(group)
                
    return blocks

async def analyze_vision_ollama(image_bytes: bytes, prompt: str) -> str:
    """Vision ëª¨ë¸ í˜¸ì¶œ"""
    try:
        img_b64 = base64.b64encode(image_bytes).decode('utf-8')
        response = await asyncio.to_thread(
            ollama_client.chat,
            model=VISION_MODEL,
            messages=[{'role': 'user', 'content': prompt, 'images': [img_b64]}]
        )
        content = response['message']['content']
        
        # ë³´ì•ˆë§ ì—ëŸ¬ í•„í„°ë§
        if "<script" in content or "mwg-internal" in content:
            return "[Error: Image Blocked by Firewall]"
        return content
    except Exception as e:
        return f"[Vision Error: {e}]"

# --- íŒŒì„œ (Parsers) ---

async def parse_excel_v4(content: bytes, filename: str, lang: str):
    """ì—‘ì…€ íŒŒì‹± (ëª¨ë“  ë¸”ë¡ ë°˜í™˜)"""
    results = []
    try:
        xls = pd.ExcelFile(io.BytesIO(content))
        for sheet_name in xls.sheet_names:
            df = pd.read_excel(xls, sheet_name=sheet_name, header=None)
            blocks = detect_excel_blocks(df)
            
            if not blocks: # ë¸”ë¡ ê°ì§€ ì‹¤íŒ¨ ì‹œ ì „ì²´ë¥¼ í•˜ë‚˜ë¡œ ì‹œë„
                blocks = [df]

            for i, block in enumerate(blocks):
                # ì°¨íŠ¸ ë°ì´í„° ì¶”ì¶œ ì‹œë„
                chart_data = []
                try:
                    # ìˆ«ì ì»¬ëŸ¼ë§Œ ì¶”ì¶œ
                    num_df = block.apply(pd.to_numeric, errors='coerce').dropna(axis=1, how='all')
                    if not num_df.empty and num_df.shape[0] > 1:
                        x = num_df.iloc[:, 0].fillna(0).values
                        y = num_df.iloc[:, 1].fillna(0).values if num_df.shape[1] > 1 else x
                        # ìƒ˜í”Œë§
                        step = max(1, len(x) // 100)
                        chart_data = [{"x": float(v1), "y": float(v2)} for v1, v2 in zip(x[::step], y[::step])]
                except:
                    pass

                # ë°ì´í„° ë¯¸ë¦¬ë³´ê¸° (í…ìŠ¤íŠ¸)
                raw_csv = block.to_csv(index=False, header=False)[:1000]
                
                results.append({
                    "type": "excel_block",
                    "filename": f"{filename} ({sheet_name} - Block {i+1})",
                    "summary": f"Data extracted from {sheet_name}, Block {i+1}.",
                    "raw_context": f"Excel Data ({sheet_name}):\n{raw_csv}",
                    "chart_data": chart_data,
                    "image_b64": None # ì—‘ì…€ì€ ì´ë¯¸ì§€ê°€ ì—†ìŒ
                })
    except Exception as e:
        return [{"type": "error", "filename": filename, "msg": str(e)}]
    return results

async def parse_ppt_v4(content: bytes, filename: str, lang: str):
    """PPTX íŒŒì‹± (ì´ë¯¸ì§€ ì¶”ì¶œ í¬í•¨)"""
    try:
        prs = Presentation(io.BytesIO(content))
        slides_data = []
        full_context = ""
        
        lang_instruction = "Korean" if lang == 'korean' else "English"

        for i, slide in enumerate(prs.slides):
            slide_text = []
            extracted_images = []
            
            # 1. í…ìŠ¤íŠ¸ ì¶”ì¶œ
            for shape in slide.shapes:
                if hasattr(shape, "text") and shape.text.strip():
                    slide_text.append(shape.text)
                
                # 2. ì´ë¯¸ì§€ ì¶”ì¶œ ë° ë¶„ì„
                if shape.shape_type == MSO_SHAPE_TYPE.PICTURE:
                    try:
                        image_blob = shape.image.blob
                        # Base64ë¡œ ë³€í™˜í•˜ì—¬ í”„ë¡ íŠ¸ì—”ë“œ ì „ì†¡ ì¤€ë¹„
                        img_b64 = base64.b64encode(image_blob).decode('utf-8')
                        
                        # Vision ë¶„ì„
                        prompt = f"Analyze this image in {lang_instruction}. If it's a chart, explain the trend."
                        desc = await analyze_vision_ollama(image_blob, prompt)
                        
                        extracted_images.append({
                            "b64": img_b64,
                            "desc": desc
                        })
                    except:
                        pass

            slide_txt_combined = "\n".join(slide_text)
            img_desc_combined = "\n".join([f"[Image]: {img['desc']}" for img in extracted_images])
            
            # ì»¨í…ìŠ¤íŠ¸ ëˆ„ì 
            full_context += f"### Slide {i+1}\nText: {slide_txt_combined}\nVisual Analysis: {img_desc_combined}\n"
            
            # ê²°ê³¼ êµ¬ì¡°í™”
            slides_data.append({
                "slide_num": i+1,
                "text": slide_txt_combined,
                "images": extracted_images # ì´ë¯¸ì§€ ë¦¬ìŠ¤íŠ¸
            })
            
        return {
            "type": "ppt",
            "filename": filename,
            "summary": f"**PPT Analysis ({len(prs.slides)} slides)**",
            "raw_context": full_context,
            "slides": slides_data # ìƒì„¸ ë°ì´í„° í¬í•¨
        }

    except Exception as e:
        return {"type": "error", "filename": filename, "msg": str(e)}

async def parse_pdf_v4(content: bytes, filename: str, lang: str):
    """PDF íŒŒì‹± (í˜ì´ì§€ë³„ ì´ë¯¸ì§€ ë° ìš”ì•½)"""
    try:
        images = convert_from_bytes(content, dpi=150, fmt='jpeg') # DPI ì¡°ì • (ì†ë„/ìš©ëŸ‰)
        pages_data = []
        full_context = ""
        
        lang_instruction = "Korean" if lang == 'korean' else "English"
        
        print(f"ğŸ“„ PDF Processing: {filename} ({len(images)} pages)")

        for i, img in enumerate(images):
            # ì´ë¯¸ì§€ Base64 ë³€í™˜ (í”„ë¡ íŠ¸ì—”ë“œ í‘œì‹œìš©)
            buffered = io.BytesIO()
            img.save(buffered, format="JPEG")
            img_bytes = buffered.getvalue()
            img_b64 = base64.b64encode(img_bytes).decode('utf-8')
            
            # Vision ë¶„ì„
            # LaTeX ì¶œë ¥ì„ ë§‰ê¸° ìœ„í•´ Plain Text ìš”ì²­
            prompt = f"""
            Analyze this page in {lang_instruction}.
            1. Summarize the main content.
            2. If there are tables/charts, describe them in detail.
            3. Do NOT use LaTeX formatting (no $ symbols). Use plain text.
            4. Do NOT hallucinate image tags like <!-- image -->.
            """
            print(f"   - Analyzing Page {i+1}...")
            page_desc = await analyze_vision_ollama(img_bytes, prompt)
            
            full_context += f"\n--- Page {i+1} ---\n{page_desc}\n"
            
            pages_data.append({
                "page_num": i+1,
                "image_b64": img_b64,
                "desc": page_desc
            })

        return {
            "type": "pdf",
            "filename": filename,
            "summary": f"**PDF Analysis ({len(images)} pages)**",
            "raw_context": full_context,
            "pages": pages_data
        }
    except Exception as e:
        return {"type": "error", "filename": filename, "msg": f"PDF Error: {str(e)}"}

async def parse_image_v4(content: bytes, filename: str, lang: str):
    """ë‹¨ì¼ ì´ë¯¸ì§€ íŒŒì‹±"""
    try:
        img = Image.open(io.BytesIO(content)).convert("RGB")
        buffered = io.BytesIO()
        img.save(buffered, format="JPEG")
        img_bytes = buffered.getvalue()
        img_b64 = base64.b64encode(img_bytes).decode('utf-8')
        
        lang_instruction = "Korean" if lang == 'korean' else "English"
        desc = await analyze_vision_ollama(img_bytes, f"Analyze this image in {lang_instruction}. Focus on scientific details.")
        
        return {
            "type": "image",
            "filename": filename,
            "summary": desc,
            "image_b64": img_b64,
            "raw_context": desc
        }
    except Exception as e:
        return {"type": "error", "filename": filename, "msg": str(e)}

async def parse_spectrum(content: bytes, filename: str):
    """ìŠ¤í™íŠ¸ëŸ¼ (ì–¸ì–´ ë¬´ê´€, ìˆ˜ì¹˜ ë¶„ì„)"""
    try:
        try: df = pd.read_csv(io.BytesIO(content))
        except: df = pd.read_csv(io.BytesIO(content), delimiter='\t')
        x = df.iloc[:, 0].values
        y = df.iloc[:, 1].values if df.shape[1] > 1 else df.iloc[:, 0].values
        peaks, _ = find_peaks(y, height=np.mean(y), distance=20)
        step = max(1, len(x) // 500)
        chart_data = [{"x": float(xi), "y": float(yi)} for xi, yi in zip(x[::step], y[::step])]
        return {
            "type": "spectrum",
            "filename": filename,
            "summary": f"Spectrum: {len(peaks)} peaks detected.",
            "chart_data": chart_data,
            "raw_context": f"Spectrum Peaks at: {peaks}, Max: {np.max(y)}"
        }
    except Exception as e:
        return {"type": "error", "filename": filename, "msg": str(e)}


# --- ë©”ì¸ ë¼ìš°í„° ---
app = FastAPI(title="Analyst V4")
app.add_middleware(CORSMiddleware, allow_origins=["*"], allow_methods=["*"], allow_headers=["*"])

@app.post("/api/analyze")
async def analyze_orchestrator(
    files: List[UploadFile] = File(...),
    custom_prompt: Optional[str] = Form(""),
    language: str = Form("korean")
):
    print(f"ğŸš€ Processing V4 Request. Lang: {language}")
    
    file_data_list = []
    for file in files:
        file_data_list.append({"name": file.filename, "content": await file.read()})
    
    tasks = []
    final_results = []
    
    # 1. íƒœìŠ¤í¬ ìƒì„±
    for f in file_data_list:
        fname = f["name"].lower()
        content = f["content"]
        
        if fname.endswith(('.xlsx', '.xls')):
            # ì—‘ì…€ì€ ì¦‰ì‹œ ì²˜ë¦¬ (ê²°ê³¼ê°€ ë¦¬ìŠ¤íŠ¸ë¼ êµ¬ì¡°ìƒ ë¶„ë¦¬)
            blocks = await parse_excel_v4(content, f["name"], language)
            final_results.extend(blocks)
        elif fname.endswith(('.ppt', '.pptx')):
            tasks.append(parse_ppt_v4(content, f["name"], language))
        elif fname.endswith('.pdf'):
            tasks.append(parse_pdf_v4(content, f["name"], language))
        elif fname.endswith(('.csv', '.txt')):
            tasks.append(parse_spectrum(content, f["name"]))
        elif fname.endswith(('.png', '.jpg', '.jpeg', '.tif', '.tiff', '.bmp')):
            tasks.append(parse_image_v4(content, f["name"], language))

    # 2. ë¹„ë™ê¸° ì‹¤í–‰
    other_results = await asyncio.gather(*tasks)
    final_results.extend(list(other_results))
    
    # 3. ì¢…í•© ë¦¬í¬íŠ¸ ìƒì„±
    valid_data = [r for r in final_results if r.get("type") != "error"]
    context_text = ""
    for item in valid_data:
        # mwg-internal í•„í„°ë§
        raw = item.get("raw_context", "")
        if "mwg-internal" not in raw:
            context_text += f"\n=== Source: {item['filename']} ===\n{raw[:5000]}\n"

    lang_inst = "Korean (Hangul)" if language == 'korean' else "English"
    
    system_prompt = f"""
    You are a Senior Scientist. Synthesize the results.
    1. Write the report in **{lang_inst}**.
    2. Do NOT use LaTeX syntax for text. Use plain text.
    3. Interpret the data deeply.
    4. User Request: "{custom_prompt}"
    """
    
    final_report = "ë¶„ì„ ì‹¤íŒ¨"
    if context_text:
        try:
            print("ğŸ“ Synthesizing Report...")
            res = await asyncio.to_thread(
                ollama_client.chat,
                model=TEXT_MODEL,
                messages=[
                    {'role': 'system', 'content': system_prompt},
                    {'role': 'user', 'content': f"Analysis Data:\n{context_text}"}
                ]
            )
            final_report = res['message']['content']
        except Exception as e:
            final_report = f"Synthesis Error: {e}"

    return {"results": final_results, "final_report": final_report}

@app.get("/", response_class=HTMLResponse)
async def serve_index():
    if os.path.exists("index.html"): return FileResponse("index.html")
    return "<h1>index.html not found</h1>"

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)






<!DOCTYPE html>
<html lang="ko">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI Analyst V4 (Pro)</title>
    
    <script src="https://cdn.tailwindcss.com"></script>
    <script src="https://unpkg.com/vue@3/dist/vue.global.js"></script>
    <script src="https://cdn.plot.ly/plotly-2.27.0.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/marked/marked.min.js"></script>
    <script src="https://unpkg.com/lucide@latest"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/html2pdf.js/0.10.1/html2pdf.bundle.min.js"></script>

    <style>
        @import url('https://fonts.googleapis.com/css2?family=Inter:wght@400;600;700&display=swap');
        body { font-family: 'Inter', sans-serif; }
        
        /* í…ìŠ¤íŠ¸ ì¤„ë°”ê¿ˆ ê°•ì œ ì ìš© (ë ˆì´ì•„ì›ƒ ê¹¨ì§ ë°©ì§€) */
        .prose { 
            max-width: none; 
            overflow-wrap: break-word; 
            word-wrap: break-word; 
            word-break: break-word;
        }
        .prose pre { white-space: pre-wrap; word-break: break-all; }
        
        .loader { border: 3px solid #f3f3f3; border-top: 3px solid #4f46e5; border-radius: 50%; width: 20px; height: 20px; animation: spin 1s linear infinite; }
        @keyframes spin { 0% { transform: rotate(0deg); } 100% { transform: rotate(360deg); } }
    </style>
</head>
<body class="bg-slate-50 text-slate-800">
    <div id="app" class="min-h-screen p-6">
        
        <!-- Header -->
        <header class="max-w-7xl mx-auto mb-8 flex items-center justify-between">
            <div class="flex items-center gap-3">
                <div class="bg-indigo-600 p-2.5 rounded-xl shadow-lg">
                    <i data-lucide="microscope" class="w-8 h-8 text-white"></i>
                </div>
                <div>
                    <h1 class="text-2xl font-bold text-slate-900">AI Analyst V4</h1>
                    <p class="text-sm text-slate-500">Dual GPU / Multi-Block Excel / PDF Full Parse</p>
                </div>
            </div>
        </header>

        <main class="max-w-7xl mx-auto grid grid-cols-1 lg:grid-cols-12 gap-8">
            
            <!-- Left Panel -->
            <div class="lg:col-span-4 space-y-6">
                <!-- Upload -->
                <div class="bg-white p-6 rounded-2xl shadow-sm border border-slate-100">
                    <h2 class="font-bold mb-4 flex items-center gap-2">Files</h2>
                    <div 
                        class="border-2 border-dashed border-slate-200 rounded-xl p-8 text-center cursor-pointer hover:border-indigo-400 hover:bg-indigo-50"
                        @click="$refs.fileInput.click()"
                        @drop.prevent="handleDrop" @dragover.prevent
                    >
                        <input type="file" ref="fileInput" multiple class="hidden" @change="handleFileSelect">
                        <p class="font-semibold text-slate-600">Click to Upload</p>
                    </div>
                    <div class="mt-4 space-y-2">
                        <div v-for="(f, i) in files" :key="i" class="flex justify-between p-2 bg-slate-50 rounded text-sm">
                            <span class="truncate">{{ f.name }}</span>
                            <button @click="removeFile(i)" class="text-red-400">x</button>
                        </div>
                    </div>
                </div>

                <!-- Options -->
                <div class="bg-white p-6 rounded-2xl shadow-sm border border-slate-100">
                    <h2 class="font-bold mb-4">Options</h2>
                    <div class="mb-4">
                        <label class="block text-xs font-bold text-slate-400 uppercase mb-1">Language</label>
                        <select v-model="language" class="w-full p-2 bg-slate-50 border rounded">
                            <option value="korean">í•œê¸€ (Korean)</option>
                            <option value="english">English</option>
                        </select>
                    </div>
                    <div class="mb-4">
                        <label class="block text-xs font-bold text-slate-400 uppercase mb-1">Custom Prompt</label>
                        <textarea v-model="customPrompt" class="w-full p-2 bg-slate-50 border rounded text-sm" rows="3"></textarea>
                    </div>
                    <button @click="analyze" :disabled="isAnalyzing || files.length===0" class="w-full py-3 bg-indigo-600 text-white rounded-xl font-bold flex justify-center gap-2">
                        <div v-if="isAnalyzing" class="loader"></div>
                        <span>{{ isAnalyzing ? 'Analyzing...' : 'Start Analysis' }}</span>
                    </button>
                </div>
            </div>

            <!-- Right Panel -->
            <div class="lg:col-span-8">
                <div v-if="result" class="space-y-6 animate-fade-in-up">
                    <div class="flex justify-end">
                        <button @click="downloadPDF" class="px-4 py-2 bg-slate-800 text-white rounded hover:bg-slate-900 text-sm">Download PDF</button>
                    </div>

                    <div id="report-content" class="bg-white p-10 rounded-2xl shadow-sm border border-slate-100">
                        <h1 class="text-3xl font-bold mb-2">Final Report</h1>
                        <p class="text-slate-400 text-sm mb-8">{{ new Date().toLocaleString() }}</p>

                        <!-- Executive Synthesis -->
                        <section class="mb-12">
                            <h2 class="text-xl font-bold text-indigo-700 mb-4 border-b pb-2">1. Executive Synthesis</h2>
                            <div class="prose text-slate-700" v-html="renderMarkdown(result.final_report)"></div>
                        </section>

                        <!-- Visual Evidence -->
                        <section>
                            <h2 class="text-xl font-bold text-slate-800 mb-6 border-b pb-2">2. Visual Evidence & Data</h2>

                            <div v-for="(item, idx) in result.results" :key="idx" class="mb-10 break-inside-avoid">
                                <div class="flex items-center gap-2 mb-4">
                                    <span class="px-2 py-1 bg-slate-100 rounded text-xs font-bold uppercase">{{ item.type }}</span>
                                    <h3 class="font-bold text-lg">{{ item.filename }}</h3>
                                </div>

                                <!-- 1. PDF Viewer (All Pages) -->
                                <div v-if="item.type === 'pdf' && item.pages" class="space-y-6">
                                    <div v-for="page in item.pages" :key="page.page_num" class="bg-slate-50 p-4 rounded border border-slate-200 flex gap-4 break-inside-avoid">
                                        <!-- Page Image -->
                                        <div class="w-1/3 flex-shrink-0">
                                            <img :src="'data:image/jpeg;base64,' + page.image_b64" class="w-full rounded border shadow-sm">
                                            <p class="text-center text-xs text-slate-400 mt-1">Page {{ page.page_num }}</p>
                                        </div>
                                        <!-- Page Analysis -->
                                        <div class="w-2/3 text-sm prose" v-html="renderMarkdown(page.desc)"></div>
                                    </div>
                                </div>

                                <!-- 2. PPT Viewer (Slides & Extracted Images) -->
                                <div v-if="item.type === 'ppt' && item.slides" class="space-y-6">
                                    <div v-for="slide in item.slides" :key="slide.slide_num" class="bg-slate-50 p-4 rounded border border-slate-200 break-inside-avoid">
                                        <h4 class="font-bold text-sm mb-2 text-indigo-600">Slide {{ slide.slide_num }}</h4>
                                        <!-- Slide Text -->
                                        <div class="text-sm mb-4 whitespace-pre-wrap">{{ slide.text }}</div>
                                        <!-- Extracted Images -->
                                        <div v-if="slide.images && slide.images.length > 0" class="grid grid-cols-2 gap-4">
                                            <div v-for="(img, imgIdx) in slide.images" :key="imgIdx" class="bg-white p-2 rounded border">
                                                <img :src="'data:image/jpeg;base64,' + img.b64" class="max-h-40 mx-auto object-contain">
                                                <p class="text-xs text-slate-500 mt-2 p-2 bg-slate-50 rounded">{{ img.desc }}</p>
                                            </div>
                                        </div>
                                    </div>
                                </div>

                                <!-- 3. Excel Charts -->
                                <div v-if="item.type === 'excel_block' || item.type === 'spectrum'" class="bg-slate-50 p-4 rounded border">
                                    <div v-if="item.chart_data && item.chart_data.length > 0">
                                        <div :id="'chart-' + idx" class="w-full h-64 bg-white"></div>
                                    </div>
                                    <div class="mt-4 text-sm prose" v-html="renderMarkdown(item.summary)"></div>
                                </div>

                                <!-- 4. Single Image -->
                                <div v-if="item.type === 'image'" class="bg-slate-50 p-4 rounded border text-center">
                                    <img :src="'data:image/jpeg;base64,' + item.image_b64" class="max-h-96 mx-auto rounded shadow-sm">
                                    <div class="mt-4 text-sm text-left prose" v-html="renderMarkdown(item.summary)"></div>
                                </div>

                            </div>
                        </section>
                    </div>
                </div>
            </div>
        </main>
    </div>

    <script>
        const { createApp, ref, nextTick } = Vue;

        createApp({
            setup() {
                const files = ref([]);
                const isAnalyzing = ref(false);
                const result = ref(null);
                const customPrompt = ref("");
                const language = ref("korean");

                // Remove Image tags from Markdown to prevent 403 errors
                const renderer = new marked.Renderer();
                renderer.image = function() { return ''; };
                marked.use({ renderer });

                const handleFileSelect = (e) => files.value.push(...Array.from(e.target.files));
                const handleDrop = (e) => files.value.push(...Array.from(e.dataTransfer.files));
                const removeFile = (i) => files.value.splice(i, 1);
                const renderMarkdown = (text) => marked.parse(text || '');

                const analyze = async () => {
                    isAnalyzing.value = true;
                    result.value = null;
                    const formData = new FormData();
                    files.value.forEach(f => formData.append('files', f));
                    formData.append('custom_prompt', customPrompt.value);
                    formData.append('language', language.value);

                    try {
                        const res = await fetch('/api/analyze', { method: 'POST', body: formData });
                        const data = await res.json();
                        result.value = data;
                        await nextTick();
                        drawCharts(data.results);
                        lucide.createIcons();
                    } catch (e) {
                        alert(e);
                    } finally {
                        isAnalyzing.value = false;
                    }
                };

                const drawCharts = (results) => {
                    results.forEach((item, idx) => {
                        if (item.chart_data && item.chart_data.length > 0) {
                            const trace = {
                                x: item.chart_data.map(d => d.x),
                                y: item.chart_data.map(d => d.y),
                                mode: 'lines',
                                line: { color: '#4f46e5' }
                            };
                            Plotly.newPlot('chart-' + idx, [trace], {
                                margin: { t: 10, b: 30, l: 40, r: 10 },
                                xaxis: { title: 'X' }, yaxis: { title: 'Y' }
                            }, {displayModeBar: false});
                        }
                    });
                };

                const downloadPDF = () => {
                    const el = document.getElementById('report-content');
                    html2pdf().set({
                        margin: 10, filename: 'Report.pdf',
                        image: { type: 'jpeg', quality: 0.98 },
                        html2canvas: { scale: 2 },
                        jsPDF: { unit: 'mm', format: 'a4', orientation: 'portrait' }
                    }).from(el).save();
                };

                setTimeout(() => lucide.createIcons(), 100);

                return {
                    files, isAnalyzing, result, customPrompt, language,
                    handleFileSelect, handleDrop, removeFile, analyze, renderMarkdown, downloadPDF
                };
            }
        }).mount('#app');
    </script>
</body>
</html>


