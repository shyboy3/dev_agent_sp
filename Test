model_clients.py

# model_clients.py

from __future__ import annotations
from typing import Optional, Dict, Any, List
import base64
import json
import os
import httpx

FigureAnalysis = Dict[str, Any]


class TextModelClient:
    """
    텍스트 전용 LLM 클라이언트.
    - 기본은 OpenAI 호환 /v1/chat/completions 스타일 가정.
    - gpt-oss:20b, qwen3-vl:30b-a3b-instruct(텍스트만) 등에 사용.
    """

    def __init__(
        self,
        api_url: str,
        model: str,
        api_key: Optional[str] = None,
        backend: str = "openai_compat",
        timeout: float = 120.0,
    ):
        self.api_url = api_url.rstrip("/")
        self.model = model
        self.api_key = api_key
        self.backend = backend
        self.timeout = timeout

    async def chat_completion(
        self,
        system_prompt: str,
        user_content: str | List[Dict[str, Any]],
        temperature: float = 0.2,
        max_tokens: Optional[int] = None,
    ) -> str:
        """
        system + user 메시지를 던지고, 텍스트 응답만 문자열로 받는 헬퍼.
        user_content가 string이면 그대로, list면 content로 사용.
        """

        # OpenAI 호환 backend 기준
        if self.backend == "openai_compat":
            messages: List[Dict[str, Any]] = []

            if system_prompt:
                messages.append({"role": "system", "content": system_prompt})

            if isinstance(user_content, str):
                messages.append({"role": "user", "content": user_content})
            else:
                # 이미 content 배열이 준비된 경우 (예: 다중 파트)
                messages.append({"role": "user", "content": user_content})

            payload: Dict[str, Any] = {
                "model": self.model,
                "messages": messages,
                "temperature": temperature,
            }
            if max_tokens is not None:
                payload["max_tokens"] = max_tokens

            headers: Dict[str, str] = {}
            if self.api_key:
                headers["Authorization"] = f"Bearer {self.api_key}"

            async with httpx.AsyncClient(timeout=self.timeout) as client:
                r = await client.post(f"{self.api_url}/v1/chat/completions", json=payload, headers=headers)
            r.raise_for_status()
            data = r.json()
            return data["choices"][0]["message"]["content"]

        # 다른 backend 타입을 쓰고 싶으면 여기서 분기 추가
        raise NotImplementedError(f"Unsupported text backend: {self.backend}")
    

class VisionModelClient:
    """
    비전(LMV) 클라이언트.
    - 이미지 + 텍스트를 넣고, FigureAnalysis JSON을 받도록 설계.
    - backend / model 설정만 바꿔주면 Qwen2.5-VL, Qwen3-VL 등 교체 가능.
    """

    def __init__(
        self,
        api_url: str,
        model: str,
        api_key: Optional[str] = None,
        backend: str = "openai_compat",
        timeout: float = 120.0,
    ):
        self.api_url = api_url.rstrip("/")
        self.model = model
        self.api_key = api_key
        self.backend = backend
        self.timeout = timeout

    def _figure_prompt(self) -> str:
        # 여기 프롬프트는 공통. 모델이 바뀌어도 그대로 사용.
        return """You are an expert scientific figure interpreter.

You will receive ONE scientific image (plot, spectrum, table screenshot, or microscopy image).
Carefully analyze it and respond ONLY with a single JSON object, no extra text.

The JSON schema MUST be:

{
  "kind": "plot" | "spectrum" | "table" | "microscopy" | "logo_or_decorative" | "other",
  "short_caption": "one-sentence description in Korean",
  "key_findings": [
    "bullet 1 in Korean",
    "bullet 2 in Korean"
  ],
  "axes": {
    "x_label": "string or null",
    "y_label": "string or null",
    "x_unit": "string or null",
    "y_unit": "string or null"
  },
  "table": {
    "header": [ "col1", "col2", ... ],
    "rows": [
      ["r1c1","r1c2",...],
      ["r2c1","r2c2",...]
    ]
  } | null
}

Rules:
- If the image is NOT a table, set "table" to null.
- If the image is a table screenshot, try to reconstruct header/rows in "table".
- If it is clearly just a company logo, icon, or UI decoration, set "kind": "logo_or_decorative".
- Do not add any fields beyond this schema.
- Respond with VALID JSON only (no comments, no markdown, no explanation).
"""

    async def analyze_figure(self, image_path: str) -> Optional[FigureAnalysis]:
        """
        이미지 1장을 분석해서 FigureAnalysis dict로 반환.
        실패 시 None.
        """
        if self.backend == "openai_compat":
            return await self._analyze_openai_compat(image_path)

        # 다른 backend (예: ollama, custom) 쓰고 싶으면 여기 분기 추가
        raise NotImplementedError(f"Unsupported vision backend: {self.backend}")

    async def _analyze_openai_compat(self, image_path: str) -> Optional[FigureAnalysis]:
        try:
            with open(image_path, "rb") as f:
                img_bytes = f.read()
            img_b64 = base64.b64encode(img_bytes).decode("utf-8")

            prompt = self._figure_prompt()

            # OpenAI 호환 멀티모달 payload
            payload = {
                "model": self.model,
                "messages": [
                    {
                        "role": "user",
                        "content": [
                            {"type": "text", "text": prompt},
                            {
                                "type": "image_url",
                                "image_url": {
                                    "url": f"data:image/png;base64,{img_b64}"
                                },
                            },
                        ],
                    }
                ],
                "temperature": 0.1,
            }

            headers: Dict[str, str] = {}
            if self.api_key:
                headers["Authorization"] = f"Bearer {self.api_key}"

            async with httpx.AsyncClient(timeout=self.timeout) as client:
                r = await client.post(f"{self.api_url}/v1/chat/completions", json=payload, headers=headers)
            r.raise_for_status()
            data = r.json()
            content = data["choices"][0]["message"]["content"]

            # 모델이 JSON string으로 응답하도록 프롬프트를 강제했기 때문에, 여기서 파싱
            return json.loads(content)

        except Exception as e:
            print(f"[WARN] Vision model call failed ({image_path}): {e}")
            return None




research_report_server_all_in_one.py (혹은 메인 서버 파일) 상단에서:


import os
from model_clients import TextModelClient, VisionModelClient

# 예시: 환경변수에서 읽기 (없으면 기본값)
TEXT_API_URL = os.getenv("TEXT_API_URL", "http://localhost:8000")
TEXT_MODEL   = os.getenv("TEXT_MODEL_NAME", "gpt-oss:20b")
TEXT_API_KEY = os.getenv("TEXT_API_KEY", "")

VISION_API_URL = os.getenv("VISION_API_URL", "http://localhost:8000")
VISION_MODEL   = os.getenv("VISION_MODEL_NAME", "qwen2.5-vl:7b")
VISION_API_KEY = os.getenv("VISION_API_KEY", "")

# 텍스트 모델 클라이언트 (기본: gpt-oss:20b)
text_client = TextModelClient(
    api_url=TEXT_API_URL,
    model=TEXT_MODEL,
    api_key=TEXT_API_KEY or None,
    backend="openai_compat",
)

# 비전 모델 클라이언트 (기본: qwen2.5-vl:7b)
vision_client = VisionModelClient(
    api_url=VISION_API_URL,
    model=VISION_MODEL,
    api_key=VISION_API_KEY or None,
    backend="openai_compat",
)



TEXT_MODEL_NAME=qwen3-vl:30b-a3b-instruct 이런 식으로 환경변수만 갈아끼우면 됨.
	•	비전 모델을 Qwen3-VL로 바꾸고 싶으면:
VISION_MODEL_NAME=qwen3-vl:30b-a3b-instruct

즉, 코드는 그대로 두고 설정만 바꾸는 구조.






3. 텍스트 리포트 생성 부분 바꾸기 (generate_report_text 예시)

지금 generate_report_text(meta, parsed_files) 안에서
직접 httpx로 LLM을 부르고 있을 텐데, 그 부분을 text_client로 바꾸면 돼.


예를들어:
# 예시: 기존에 있던 것 (대략)
# async def generate_report_text(meta, parsed_files):
#     ctx = build_context(meta, parsed_files)
#     system_prompt = SYSTEM_PROMPT
#     payload = {...}
#     async with httpx.AsyncClient() as client:
#         ...
#     return content


이렇게 바꾸는 느낌:
import json

SYSTEM_PROMPT = """
당신은 재료/소자 분석 연구 리포트를 작성하는 전문 리포트 작성 어시스턴트입니다.
(중략: 이미 쓰고 있던 SYSTEM 프롬프트 내용)
"""

async def generate_report_text(meta, parsed_files):
    ctx = build_context(meta, parsed_files)  # meta + files + headings + tables + mm_summaries 등
    user_content = json.dumps(ctx, ensure_ascii=False)

    # text_client는 전역으로 만들어둔 TextModelClient 인스턴스
    report_text = await text_client.chat_completion(
        system_prompt=SYSTEM_PROMPT,
        user_content=user_content,
        temperature=0.3,
        max_tokens=None,  # 필요하면 제한
    )
    return report_text


이제 텍스트 LLM을 변경하고 싶으면 text_client 생성 시 model만 바꾸면 됨.


4. 비전 해석 붙이는 부분 바꾸기 (enrich_items_with_mm)

이전에는 Qwen 전용 함수를 썼다면,
이제는 vision_client.analyze_figure(image_path)만 부르면 됨.

from PIL import Image as PILImage
import os
from typing import List, Dict, Any

async def enrich_items_with_mm(parsed_files: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """
    - parsed_files 안의 각 item에서 extracted_images를 찾아
      적당한 이미지들만 골라 VisionModelClient로 해석.
    - 결과는 item["mm_summaries"]에 누적.
    """
    if vision_client is None:
        return parsed_files

    tasks = []

    for item in parsed_files:
        imgs = item.get("extracted_images") or []
        if not imgs:
            continue

        selected: List[str] = []

        for path in imgs:
            if not path or not os.path.exists(path):
                continue

            try:
                with PILImage.open(path) as im:
                    w, h = im.size
                area = w * h
                ratio = max(w, h) / max(1, min(w, h))
            except Exception:
                area = 0
                ratio = 1

            # 너무 작은 거나 길쭉한 배너는 로고/장식으로 보고 스킵
            if area < 50_000 or ratio > 8:
                continue

            selected.append(path)
            if len(selected) >= 6:  # 파일당 최대 6장 정도만
                break

        for path in selected:
            tasks.append((item, path))

    async def worker(itm: Dict[str, Any], img_path: str):
        analysis = await vision_client.analyze_figure(img_path)
        if not analysis:
            return
        mm_list = itm.setdefault("mm_summaries", [])
        mm_list.append({
            "image_path": img_path,
            "analysis": analysis,
        })

    # 비동기로 병렬 처리
    import asyncio
    await asyncio.gather(*(worker(itm, p) for itm, p in tasks))
    return parsed_files


여기서도 Qwen이라는 단어가 한 번도 안 나와:
	•	Vision 모델을 Qwen2.5-vl:7b → Qwen3-vl:30b 로 바꾸고 싶으면
VISION_MODEL_NAME만 갈아끼우면 끝.

⸻

5. 전체 흐름 요약
	1.	모델 클라이언트 모듈 (model_clients.py)에
	•	TextModelClient
	•	VisionModelClient
구현
	2.	서버 시작할 때:
	•	text_client = TextModelClient(...)
	•	vision_client = VisionModelClient(...)
를 전역/DI로 만들어둠
	3.	리포트 생성 플로우:
	•	파일 파싱 → parsed_files
	•	parsed_files = await enrich_items_with_mm(parsed_files)
	•	report_text = await generate_report_text(meta, parsed_files)
	•	render_pdf(report_text, parsed_files, ...)
	4.	모델 교체가 필요하면:
	•	.env나 설정에서
	•	TEXT_MODEL_NAME
	•	VISION_MODEL_NAME
	•	(필요 시 URL/API_KEY)
만 수정하면 됨.

이 정도 구조면:
	•	지금 말한
	•	텍스트: gpt-oss:20b (또는 qwen3-vl:30b-a3b-instruct)
	•	비전: qwen2.5-vl:7b 또는 qwen3-vl:30b-a3b-instruct
	•	이후에 다른 멀티모달 모델 추가할 때도
adapter 함수 한 군데만 손보면 되니까 꽤 오래 버틸 거야.

이제 네 쪽 엔드포인트(URL, 경로)가 만약 /v1/chat/completions랑 조금 다르면
그 부분만 같이 맞춰보면 돼.
필요하면, 너가 실제로 쓰는 curl 예시 한 줄만 알려줘도
그 형식에 딱 맞게 TextModelClient / VisionModelClient 내부를 구체적으로 맞춰 줄게.





pip install python-docx
# (선택) .doc까지 처리하고 싶으면
pip install textract





import os
import io
from typing import Dict, Any, List

def parse_doc_like(raw: bytes, filename: str, tmp_dir: str) -> Dict[str, Any]:
    """
    .doc / .docx 문서를 텍스트 기반으로 파싱해서
    리포트 컨텍스트에 넣기 좋은 형태로 반환.

    반환 형식 예:
    {
      "type": "docx_doc",
      "filename": "aaa.docx",
      "text_chunks": ["문단1...", "문단2...", ...],
    }
    """
    ext = os.path.splitext(filename.lower())[1]
    base_type = "docx_doc"

    text_chunks: List[str] = []

    if ext == ".docx":
        # python-docx 사용
        try:
            from docx import Document  # type: ignore
        except ImportError:
            print("[WARN] python-docx 미설치: docx 파싱 불가")
            return {
                "type": base_type,
                "filename": filename,
                "text_chunks": [],
            }

        doc = Document(io.BytesIO(raw))

        # 1) 일반 문단
        for p in doc.paragraphs:
            txt = (p.text or "").strip()
            if txt:
                text_chunks.append(txt)

        # 2) 표 안의 텍스트도 추가 (있으면)
        for tbl in doc.tables:
            for row in tbl.rows:
                row_text = [cell.text.strip() for cell in row.cells if cell.text.strip()]
                if row_text:
                    text_chunks.append(" | ".join(row_text))

    elif ext == ".doc":
        base_type = "doc_doc"
        # .doc는 python-docx 지원이 안되므로, textract 같은 걸로 텍스트만 추출 (옵션)
        try:
            import tempfile
            import textract  # type: ignore

            tmp_path = os.path.join(tmp_dir, filename)
            with open(tmp_path, "wb") as f:
                f.write(raw)

            txt = textract.process(tmp_path).decode("utf-8", errors="ignore")
            lines = [line.strip() for line in txt.splitlines() if line.strip()]
            text_chunks.extend(lines)
        except Exception as e:
            print(f"[WARN] .doc 파싱 실패 (textract 필요): {e}")

    else:
        # 확장자 안 맞는 경우
        return {
            "type": "unknown_doc",
            "filename": filename,
            "text_chunks": [],
        }

    # 너무 잘게 쪼개지지 않게 일정 길이로 chunking
    merged_chunks: List[str] = []
    cur: List[str] = []
    cur_len = 0
    MAX_CHARS = 2000

    for t in text_chunks:
        if cur_len + len(t) > MAX_CHARS and cur:
            merged_chunks.append("\n".join(cur))
            cur = [t]
            cur_len = len(t)
        else:
            cur.append(t)
            cur_len += len(t)
    if cur:
        merged_chunks.append("\n".join(cur))

    return {
        "type": base_type,
        "filename": filename,
        "text_chunks": merged_chunks,
    }






import pandas as pd
from typing import List, Tuple

def split_dataframe_into_blocks(df: pd.DataFrame) -> List[Tuple[int, int, int, int]]:
    """
    DataFrame에서 '비어있지 않은 셀'들의 연결된 덩어리(connected component)를 찾아
    각 덩어리의 bounding box (row_start, row_end, col_start, col_end)를 반환.

    - 인접 정의: 상하좌우(4방향)
    - 1~2줄 간격으로 떨어진 여러 테이블도 각기 다른 블록으로 인식 가능.
    """
    # 비어있는 셀은 False, 내용 있는 셀은 True
    mask = df.notna() & (df.astype(str).apply(lambda s: s.str.strip()) != "")
    mask = mask.fillna(False)

    visited = pd.DataFrame(False, index=df.index, columns=df.columns)
    blocks: List[Tuple[int, int, int, int]] = []

    max_row, max_col = df.shape

    # index, columns가 꼭 0,1,2,...가 아닐 수 있어서,
    # 위치 기반으로 순회
    for r in range(max_row):
        for c in range(max_col):
            if not mask.iat[r, c] or visited.iat[r, c]:
                continue

            # 새로운 블록 시작: BFS/DFS로 연결된 셀 탐색
            stack = [(r, c)]
            visited.iat[r, c] = True
            r_min = r_max = r
            c_min = c_max = c

            while stack:
                rr, cc = stack.pop()
                r_min = min(r_min, rr)
                r_max = max(r_max, rr)
                c_min = min(c_min, cc)
                c_max = max(c_max, cc)

                for dr, dc in [(-1, 0), (1, 0), (0, -1), (0, 1)]:
                    nr, nc = rr + dr, cc + dc
                    if 0 <= nr < max_row and 0 <= nc < max_col:
                        if mask.iat[nr, nc] and not visited.iat[nr, nc]:
                            visited.iat[nr, nc] = True
                            stack.append((nr, nc))

            # 너무 작은 (1셀짜리 등) 블록은 노이즈로 버리고 싶으면 여기서 필터링
            if (r_max - r_min + 1) >= 2 or (c_max - c_min + 1) >= 2:
                blocks.append((r_min, r_max, c_min, c_max))

    return blocks







def infer_table_from_block(df_block: pd.DataFrame) -> pd.DataFrame:
    """
    블록에서:
    - 바깥쪽 완전 빈 행/열 제거
    - 데이터 방향(세로/가로) 대략 추론
    - 헤더 행(첫 행)을 컬럼명으로 사용하는 간단한 로직

    필요하면 이 함수만 더 튜닝해서 헤더/유닛/샘플 이름 등 고급 추론 가능.
    """

    # 1) 바깥쪽 빈 행/열 제거
    df_block = df_block.copy()

    # 모두 NaN/공백인 행/열 제거
    def _is_empty_series(s: pd.Series) -> bool:
        return (~s.notna()).all() or (s.astype(str).str.strip() == "").all()

    # 위쪽/아래쪽 빈 행
    while df_block.shape[0] > 0 and _is_empty_series(df_block.iloc[0]):
        df_block = df_block.iloc[1:]
    while df_block.shape[0] > 0 and _is_empty_series(df_block.iloc[-1]):
        df_block = df_block.iloc[:-1]

    # 왼쪽/오른쪽 빈 열
    while df_block.shape[1] > 0 and _is_empty_series(df_block.iloc[:, 0]):
        df_block = df_block.iloc[:, 1:]
    while df_block.shape[1] > 0 and _is_empty_series(df_block.iloc[:, -1]):
        df_block = df_block.iloc[:, :-1]

    if df_block.empty:
        return df_block

    # 2) 숫자 비율을 이용해 "행이 레코드"인지, "열이 레코드"인지 대충 판단
    num_df = df_block.apply(pd.to_numeric, errors="coerce")
    row_numeric_ratio = num_df.notna().sum(axis=1) / max(1, df_block.shape[1])
    col_numeric_ratio = num_df.notna().sum(axis=0) / max(1, df_block.shape[0])

    avg_row_ratio = row_numeric_ratio.mean()
    avg_col_ratio = col_numeric_ratio.mean()

    oriented = df_block

    if avg_col_ratio > avg_row_ratio:
        # 열 쪽에 숫자가 많이 몰려 있다 → 아마 가로로 누워있는 테이블일 확률 ↑
        oriented = df_block.T

    # 3) 헤더 행: 첫 번째 행을 헤더로 가정 (많은 실험 데이터가 이 패턴)
    oriented = oriented.reset_index(drop=True)
    header_row = oriented.iloc[0].fillna("").astype(str).str.strip()
    data = oriented.iloc[1:].reset_index(drop=True)

    # 컬럼명 비어있는 건 "col_0" 이런 식으로 보완
    cols = []
    for i, h in enumerate(header_row):
        if h:
            cols.append(h)
        else:
            cols.append(f"col_{i}")
    data.columns = cols

    return data





import io
from typing import Dict, Any, List

def parse_csv_multi_blocks(raw: bytes, filename: str, encoding: str = "utf-8") -> Dict[str, Any]:
    """
    CSV를 읽되, 하나의 시트에 여러 개의 데이터 블록이 있을 수 있다고 가정하고
    블록별로 테이블을 추출한다.

    반환 예:
    {
      "type": "csv_result",
      "filename": "...csv",
      "blocks": [
        {
          "block_id": 1,
          "table": <DataFrame>,
        },
        ...
      ]
    }
    """
    text = raw.decode(encoding, errors="ignore")
    df = pd.read_csv(io.StringIO(text), header=None)  # header=None으로 날것 전체 읽기

    blocks_info = split_dataframe_into_blocks(df)
    blocks: List[Dict[str, Any]] = []

    for idx, (r0, r1, c0, c1) in enumerate(blocks_info, start=1):
        sub = df.iloc[r0:r1+1, c0:c1+1]
        table = infer_table_from_block(sub)
        if not table.empty:
            blocks.append({
                "block_id": idx,
                "row_range": (r0, r1),
                "col_range": (c0, c1),
                "table": table,
            })

    return {
        "type": "csv_result",
        "filename": filename,
        "blocks": blocks,
    }







import tempfile

def parse_xlsx_multi_blocks(raw: bytes, filename: str, tmp_dir: str) -> Dict[str, Any]:
    """
    XLSX 파일을 파싱해서,
    - 각 sheet에서 여러 개의 데이터 블록(NxM)을 찾아내고
    - 각 블록을 infer_table_from_block으로 정리.

    반환 예:
    {
      "type": "xlsx_result",
      "filename": "aaa.xlsx",
      "sheets": [
        {
          "sheet_name": "Sheet1",
          "blocks": [
            {
              "block_id": 1,
              "row_range": (r0, r1),
              "col_range": (c0, c1),
              "table": <DataFrame>,
            },
            ...
          ]
        },
        ...
      ]
    }
    """
    # raw를 임시 파일로 저장
    xlsx_path = os.path.join(tmp_dir, filename)
    with open(xlsx_path, "wb") as f:
        f.write(raw)

    # 모든 시트를 다 읽음: dict(sheet_name -> DataFrame)
    sheets_dict = pd.read_excel(xlsx_path, sheet_name=None, header=None, engine=None)

    sheet_results: List[Dict[str, Any]] = []

    for sheet_name, df in sheets_dict.items():
        blocks_info = split_dataframe_into_blocks(df)
        blocks: List[Dict[str, Any]] = []

        for idx, (r0, r1, c0, c1) in enumerate(blocks_info, start=1):
            sub = df.iloc[r0:r1+1, c0:c1+1]
            table = infer_table_from_block(sub)
            if not table.empty:
                blocks.append({
                    "block_id": idx,
                    "row_range": (r0, r1),
                    "col_range": (c0, c1),
                    "table": table,
                })

        sheet_results.append({
            "sheet_name": sheet_name,
            "blocks": blocks,
        })

    return {
        "type": "xlsx_result",
        "filename": filename,
        "sheets": sheet_results,
    }






if name.lower().endswith(".pdf"):
    parsed = parse_pdf_local(raw, name, tmp_dir)
elif name.lower().endswith(".pptx"):
    parsed = parse_pptx_local(raw, name, tmp_dir)
elif name.lower().endswith(".csv"):
    parsed = parse_csv_multi_blocks(raw, name)
elif name.lower().endswith(".xlsx"):
    parsed = parse_xlsx_multi_blocks(raw, name, tmp_dir)
elif name.lower().endswith(".docx") or name.lower().endswith(".doc"):
    parsed = parse_doc_like(raw, name, tmp_dir)
...
parsed_files.append(parsed)






