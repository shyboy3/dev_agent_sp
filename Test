import os
import io
import asyncio
import base64
import json
import re
from typing import List, Dict, Any, Optional

# ==========================================
# [1] ÎÑ§Ìä∏ÏõåÌÅ¨/Î≥¥Ïïà ÏÑ§Ï†ï (ÌïÑÏàò)
# ==========================================
# ÌöåÏÇ¨ Î≥¥ÏïàÎßù(McAfee Îì±)Ïù¥ Î°úÏª¨ ÌÜµÏã†(127.0.0.1)ÏùÑ Í∞ÄÎ°úÏ±ÑÏßÄ Î™ªÌïòÍ≤å Í∞ïÏ†ú ÏÑ§Ï†ï
os.environ['NO_PROXY'] = '127.0.0.1,localhost,0.0.0.0,::1'
if 'HTTP_PROXY' in os.environ: del os.environ['HTTP_PROXY']
if 'HTTPS_PROXY' in os.environ: del os.environ['HTTPS_PROXY']

from fastapi import FastAPI, UploadFile, File, Form
from fastapi.middleware.cors import CORSMiddleware

import pandas as pd
import numpy as np

# Í≥ºÌïô Ïó∞ÏÇ∞ Î∞è Ïù¥ÎØ∏ÏßÄ Ï≤òÎ¶¨ ÎùºÏù¥Î∏åÎü¨Î¶¨
from scipy.signal import find_peaks, savgol_filter, peak_widths
from scipy.optimize import curve_fit
from scipy.ndimage import label, find_objects, gaussian_filter
from skimage.feature import blob_log
import cv2

from ollama import Client 
from PIL import Image
from pdf2image import convert_from_bytes
from pptx import Presentation
from pptx.enum.shapes import MSO_SHAPE_TYPE

# ==========================================
# [2] ÏÑ§Ï†ï
# ==========================================
OLLAMA_HOST = "http://127.0.0.1:11434"
ollama_client = Client(host=OLLAMA_HOST)

VISION_MODEL = "qwen3-vl:30b-a3b-instruct" 
TEXT_MODEL = "llama3:70b"

# ==========================================
# [3] V8 Í≥ºÌïô Ï≤òÎ¶¨ ÏóîÏßÑ (Science Core)
# ==========================================

class ScienceProcessorV8:
    
    @staticmethod
    def detect_footer_boundary(img_array):
        """
        [V8 ÌïµÏã¨] Ïä§ÎßàÌä∏ Ï†ïÎ≥¥Î∞î Í∞êÏßÄ ÏïåÍ≥†Î¶¨Ï¶ò
        Ïù¥ÎØ∏ÏßÄ ÌïòÎã®Ïùò 'Í≤ΩÍ≥ÑÏÑ†'ÏùÑ Ï∞æÏïÑÎÇ¥Ïñ¥ Îç∞Ïù¥ÌÑ∞ ÏòÅÏó≠Í≥º Ï†ïÎ≥¥ ÏòÅÏó≠ÏùÑ Î∂ÑÎ¶¨Ìï©ÎãàÎã§.
        """
        h, w = img_array.shape[:2]
        
        # ÌïòÎã® 25% ÏòÅÏó≠Îßå ÌÉêÏÉâ (ÏÜçÎèÑ/Ï†ïÌôïÎèÑ Ìñ•ÏÉÅ)
        search_ratio = 0.25
        search_h = int(h * search_ratio)
        roi = img_array[h-search_h:, :]
        
        # Ïó£ÏßÄ Í≤ÄÏ∂ú
        if len(roi.shape) == 3:
            gray = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)
        else:
            gray = roi
            
        # ÏàòÌèâ Ïó£ÏßÄ Í∞ïÏ°∞ (Sobel Y)
        sobel_y = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=3)
        abs_sobel = np.absolute(sobel_y)
        edges = np.uint8(abs_sobel)
        
        # ÏàòÌèâ Ìà¨ÏòÅ (Horizontal Projection)
        row_sums = np.sum(edges, axis=1)
        
        # Í∞ÄÏû• Í∞ïÎ†•Ìïú ÏàòÌèâÏÑ† Ï∞æÍ∏∞
        max_val = np.max(row_sums)
        if max_val < (w * 30): # Ïó£ÏßÄÍ∞Ä ÎÑàÎ¨¥ ÏïΩÌïòÎ©¥ Ï†ïÎ≥¥Î∞î ÏóÜÏùå
            return None
            
        split_idx_roi = np.argmax(row_sums)
        
        # ÎÑàÎ¨¥ Î∞îÎã•Ïù¥Î©¥ Î¨¥Ïãú
        if split_idx_roi > search_h * 0.95:
            return None
            
        real_split_y = (h - search_h) + split_idx_roi
        return real_split_y

    @staticmethod
    def simple_baseline(y):
        # Moving Minimum + SmoothingÏúºÎ°ú Î≤†Ïù¥Ïä§ÎùºÏù∏ Ï∂îÏ†ï
        window = max(5, len(y)//10)
        baseline = pd.Series(y).rolling(window=window, center=True).min().fillna(method='bfill').fillna(method='ffill').values
        if len(baseline) > 51:
            baseline = savgol_filter(baseline, window_length=51, polyorder=3)
        return baseline

    @staticmethod
    def process_spectrum(x, y, mode, goal):
        log = []
        y_raw = y.copy()
        
        # 1. Mode: None
        if mode == "None":
            peaks, _ = find_peaks(y, height=np.max(y)*0.05)
            return {
                "x": x, "y_raw": y, "y_proc": y, "y_base": np.zeros_like(y),
                "peaks": [{"x": float(x[p]), "y": float(y[p])} for p in peaks],
                "log": ["Preprocessing Skipped"]
            }

        # 2. Params
        window = 15
        if mode == "AI-Adaptive" and "noise" in goal.lower(): window = 31
        
        # 3. Processing
        base = ScienceProcessorV8.simple_baseline(y)
        y_corr = np.maximum(y - base, 0)
        y_proc = savgol_filter(y_corr, window, 3) if len(y_corr) > window else y_corr
        peaks, _ = find_peaks(y_proc, height=np.max(y_proc)*0.05, distance=10)

        return {
            "x": x, "y_raw": y_raw, "y_proc": y_proc, "y_base": base,
            "peaks": [{"x": float(x[p]), "y": float(y_proc[p])} for p in peaks],
            "log": log
        }

    @staticmethod
    def process_image(img_bytes, equipment, mode, goal):
        nparr = np.frombuffer(img_bytes, np.uint8)
        img_raw = cv2.imdecode(nparr, cv2.IMREAD_COLOR)
        if img_raw is None: return None, {}
        
        # --- [V8 Smart Split] ---
        footer_img = None
        body_img = img_raw
        has_footer = False
        
        # ÌòÑÎØ∏Í≤Ω Ïû•ÎπÑÏùº Í≤ΩÏö∞ Ïä§ÎßàÌä∏ Í∞êÏßÄ ÏãúÎèÑ
        if equipment in ["SEM", "TEM", "STEM", "FIB", "AFM"]:
            split_y = ScienceProcessorV8.detect_footer_boundary(img_raw)
            if split_y:
                body_img = img_raw[:split_y, :]   # ÏÉÅÎã® Îç∞Ïù¥ÌÑ∞
                footer_img = img_raw[split_y:, :] # ÌïòÎã® Ï†ïÎ≥¥Î∞î
                has_footer = True
        
        # Ïù∏ÏΩîÎî© (Body)
        _, buf_body = cv2.imencode('.jpg', body_img)
        b64_body = base64.b64encode(buf_body).decode('utf-8')
        
        # Ïù∏ÏΩîÎî© (Footer)
        b64_footer = None
        if has_footer:
            _, buf_footer = cv2.imencode('.jpg', footer_img)
            b64_footer = base64.b64encode(buf_footer).decode('utf-8')

        # Mode: None -> Ï≤òÎ¶¨ ÏóÜÏù¥ Î∞òÌôò (CropÏùÄ Îêú ÏÉÅÌÉú)
        if mode == "None":
            return {
                "raw_b64": b64_body, 
                "proc_b64": b64_body,
                "footer_b64": b64_footer,
                "stats": {"info": "Smart Cropped (Raw)"}
            }

        # CV Analysis (on Body Image)
        gray = cv2.cvtColor(body_img, cv2.COLOR_BGR2GRAY)
        gray = cv2.GaussianBlur(gray, (5, 5), 0)
        overlay = body_img.copy()
        stats = {"info": "Smart Cropped & Processed"}

        # Adaptive Logic
        detect_atoms = False
        detect_particles = False
        
        if mode == "AI-Adaptive":
            if "atom" in goal.lower() or "ÏõêÏûê" in goal: detect_atoms = True
            if "particle" in goal.lower() or "ÏûÖÏûê" in goal: detect_particles = True
        elif mode == "Auto":
            if equipment in ["STEM", "TEM"]: detect_atoms = True
            if equipment in ["SEM", "Optical"]: detect_particles = True

        if detect_atoms:
            blobs = blob_log(gray, min_sigma=3, max_sigma=15, threshold=0.1)
            for blob in blobs:
                y, x, r = blob
                cv2.circle(overlay, (int(x), int(y)), int(r*1.414), (0, 0, 255), 2)
            stats["count"] = len(blobs)
            stats["type"] = "Atoms"
        elif detect_particles:
            _, thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)
            cnts, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
            cv2.drawContours(overlay, cnts, -1, (0, 255, 0), 2)
            stats["count"] = len(cnts)
            stats["type"] = "Particles"

        _, buf_proc = cv2.imencode('.jpg', overlay)
        
        return {
            "raw_b64": b64_body,
            "proc_b64": base64.b64encode(buf_proc).decode('utf-8'),
            "footer_b64": b64_footer,
            "stats": stats
        }

# ==========================================
# [4] Ìó¨Ìçº Ìï®Ïàò (Excel, Vision API)
# ==========================================

def detect_excel_blocks(df: pd.DataFrame) -> List[pd.DataFrame]:
    """ÏóëÏÖÄ ÏïÑÏùºÎûúÎìú Í∞êÏßÄ (ÏÉÅÌïòÏ¢åÏö∞ Î∂ÑÎ¶¨)"""
    blocks = []
    if df.empty: return blocks
    mask = ~df.isnull().to_numpy()
    structure = [[0,1,0], [1,1,1], [0,1,0]]
    labeled_array, num_features = label(mask, structure=structure)
    slices = find_objects(labeled_array)
    for slice_obj in slices:
        block = df.iloc[slice_obj]
        if block.shape[0] >= 3 and block.shape[1] >= 2:
            blocks.append(block.reset_index(drop=True))
    return blocks

async def analyze_vision_ollama(image_bytes: bytes, prompt: str) -> str:
    """Vision API Ìò∏Ï∂ú Î∞è Î≥¥ÏïàÎßù ÏóêÎü¨ ÌïÑÌÑ∞ÎßÅ"""
    try:
        img_b64 = base64.b64encode(image_bytes).decode('utf-8')
        response = await asyncio.to_thread(
            ollama_client.chat,
            model=VISION_MODEL,
            messages=[{'role': 'user', 'content': prompt, 'images': [img_b64]}]
        )
        content = response['message']['content']
        if "<script" in content or "mwg-internal" in content: 
            return "[Error: Firewall blocked connection]"
        return content
    except Exception as e: 
        return f"[Vision Error: {e}]"

# ==========================================
# [5] Î©îÏù∏ ÎùºÏö∞ÌÑ∞ (Orchestrator)
# ==========================================
app = FastAPI(title="Analyst V8 Full")
app.add_middleware(CORSMiddleware, allow_origins=["*"], allow_methods=["*"], allow_headers=["*"])

@app.post("/api/analyze")
async def analyze_orchestrator(
    files: List[UploadFile] = File(...),
    file_configs: str = Form(...) # JSON String
):
    configs = json.loads(file_configs)
    print(f"üöÄ V8 Processing {len(configs)} files")
    
    tasks = []
    final_results = []
    file_map = {f.filename: await f.read() for f in files}
    
    for filename, config in configs.items():
        if filename not in file_map: continue
        content = file_map[filename]
        
        eq = config.get("equipment", "General")
        goal = config.get("goal", "")
        mode = config.get("mode", "Auto")
        fname_lower = filename.lower()
        
        # [A] Document (Literature) Path - Ï†ÑÏ≤òÎ¶¨ Í±¥ÎÑàÎúÄ
        if eq == "Document" or fname_lower.endswith(('.pdf', '.ppt', '.pptx')):
            async def process_doc(c, n, e):
                # PDF
                if n.lower().endswith('.pdf'):
                    images = convert_from_bytes(c, dpi=150, fmt='jpeg')[:10]
                    full_txt = ""
                    pages = []
                    for idx, img in enumerate(images):
                        buf = io.BytesIO()
                        img.save(buf, format="JPEG")
                        # OCR/Summary
                        desc = await analyze_vision_ollama(buf.getvalue(), "Summarize this page content.")
                        full_txt += f"\nPage {idx+1}: {desc}"
                        # B64 for display
                        b64 = base64.b64encode(buf.getvalue()).decode('utf-8')
                        pages.append({"page_num": idx+1, "image_b64": b64, "desc": desc})
                    return {"type": "pdf", "filename": n, "summary": "PDF Document", "raw_context": full_txt, "pages": pages, "equipment": "Literature"}
                
                # PPT
                elif n.lower().endswith(('.ppt', '.pptx')):
                    prs = Presentation(io.BytesIO(c))
                    slides = []
                    full_txt = ""
                    for i, slide in enumerate(prs.slides):
                        txt = ""
                        for s in slide.shapes:
                            if hasattr(s, "text"): txt += s.text + "\n"
                        
                        # PPT Ïù¥ÎØ∏ÏßÄ Ï∂îÏ∂ú
                        imgs = []
                        for s in slide.shapes:
                            if s.shape_type == MSO_SHAPE_TYPE.PICTURE:
                                try:
                                    iblob = s.image.blob
                                    ib64 = base64.b64encode(iblob).decode('utf-8')
                                    idesc = await analyze_vision_ollama(iblob, "Describe image.")
                                    imgs.append({"b64": ib64, "desc": idesc})
                                except: pass
                        
                        full_txt += f"Slide {i+1}: {txt}\n"
                        slides.append({"slide_num": i+1, "text": txt, "images": imgs})
                    return {"type": "ppt", "filename": n, "summary": "PPT Document", "raw_context": full_txt, "slides": slides, "equipment": "Literature"}
                else:
                    return {"type": "doc", "filename": n, "summary": "Doc", "equipment": "Literature"}

            tasks.append(process_doc(content, filename, eq))
            continue

        # [B] Raw Data Path - Ï†ÑÏ≤òÎ¶¨ ÏàòÌñâ
        
        # 1. Excel Spectrum
        if fname_lower.endswith(('.xlsx', '.xls')):
            try:
                xls = pd.ExcelFile(io.BytesIO(content))
                for sheet_name in xls.sheet_names:
                    df = pd.read_excel(xls, sheet_name=sheet_name, header=None)
                    blocks = detect_excel_blocks(df)
                    for i, block in enumerate(blocks):
                        try:
                            num_block = block.apply(pd.to_numeric, errors='coerce').dropna(how='all').dropna(how='all', axis=1)
                            if not num_block.empty and num_block.shape[0] > 5:
                                x = num_block.iloc[:, 0].fillna(0).values
                                y = num_block.iloc[:, 1].fillna(0).values
                                
                                # V8 Spectrum Process
                                res = ScienceProcessorV8.process_spectrum(x, y, mode, goal)
                                
                                # Chart Data
                                step = max(1, len(x)//100)
                                chart_data = []
                                for idx in range(0, len(x), step):
                                    chart_data.append({
                                        "x": float(res["x"][idx]),
                                        "y_raw": float(res["y_raw"][idx]),
                                        "y_proc": float(res["y_proc"][idx]),
                                        "y_base": float(res["y_base"][idx])
                                    })
                                
                                raw_txt = block.to_csv(index=False, header=False)[:500]
                                final_results.append({
                                    "type": "spectrum",
                                    "filename": f"{filename} ({sheet_name}-{i+1})",
                                    "equipment": eq,
                                    "summary": f"Peaks: {len(res['peaks'])}",
                                    "raw_context": f"Eq: {eq}. Peaks: {res['peaks']}\nRaw:\n{raw_txt}",
                                    "chart_data": chart_data,
                                    "log": res["log"]
                                })
                        except: pass
            except Exception as e:
                final_results.append({"type": "error", "filename": filename, "msg": str(e)})

        # 2. Image Analysis (Dual-Pass)
        elif fname_lower.endswith(('.png', '.jpg', '.jpeg', '.tif', '.bmp')):
            async def process_img_v8(c, n, e, g, m):
                try:
                    # Step 1: Smart Split & CV Process
                    vis_res = ScienceProcessorV8.process_image(c, e, m, g)
                    
                    # Step 2: Metadata Extraction (Footer OCR)
                    meta_text = ""
                    if vis_res.get("footer_b64"):
                        ft_bytes = base64.b64decode(vis_res["footer_b64"])
                        meta_prompt = "Read text (Scale bar, HV, Mag) from this image info bar."
                        meta_text = await analyze_vision_ollama(ft_bytes, meta_prompt)
                    
                    # Step 3: Main Analysis (Body + Metadata)
                    body_bytes = base64.b64decode(vis_res["proc_b64"])
                    lang_inst = "Korean"
                    
                    prompt = f"""
                    Analyze this {e} image in {lang_inst}.
                    User Goal: {g}.
                    CV Stats: {vis_res['stats']}.
                    [Metadata from Info Bar]: {meta_text}
                    Use the metadata to interpret size/scale if available.
                    """
                    desc = await analyze_vision_ollama(body_bytes, prompt)
                    
                    return {
                        "type": "image",
                        "filename": n,
                        "equipment": e,
                        "summary": desc,
                        "raw_context": f"Image: {desc}\nMetadata: {meta_text}\nStats: {vis_res['stats']}",
                        "raw_b64": vis_res["raw_b64"],
                        "proc_b64": vis_res["proc_b64"], # Crop & Overlay
                        "footer_b64": vis_res.get("footer_b64"),
                        "stats": vis_res["stats"]
                    }
                except Exception as ex:
                    return {"type": "error", "filename": n, "msg": str(ex)}
            
            tasks.append(process_img_v8(content, filename, eq, goal, mode))

    # Execute Async Tasks
    if tasks:
        results = await asyncio.gather(*tasks)
        final_results.extend(list(results))
        
    # Synthesis Report
    data_context = ""
    lit_context = ""
    
    for r in final_results:
        if r.get("type") != "error":
            block = f"\n=== File: {r['filename']} ===\n{r.get('raw_context')[:3000]}\n"
            if r.get("equipment") in ["Literature", "Document"]:
                lit_context += block
            else:
                data_context += block
            
    system_prompt = """
    You are a Senior Scientist. 
    Integrate 'Experimental Data' with 'Reference Literature'.
    Write in Korean. Use Markdown tables.
    """
    final_input = f"--- Data ---\n{data_context}\n\n--- Literature ---\n{lit_context}"
    
    final_report = "Synthesis Failed"
    if data_context or lit_context:
        try:
            res = await asyncio.to_thread(
                ollama_client.chat,
                model=TEXT_MODEL,
                messages=[{'role': 'system', 'content': system_prompt}, {'role': 'user', 'content': final_input}]
            )
            final_report = res['message']['content']
        except Exception as e:
            final_report = str(e)
            
    return {"results": final_results, "final_report": final_report}

@app.get("/", response_class=HTMLResponse)
async def serve_index():
    if os.path.exists("index.html"): return FileResponse("index.html")
    return "<h1>index.html not found</h1>"

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)






<!DOCTYPE html>
<html lang="ko">
<head>
    <meta charset="UTF-8">
    <title>AI Research Center V8</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <script src="https://unpkg.com/vue@3/dist/vue.global.js"></script>
    <script src="https://cdn.plot.ly/plotly-2.27.0.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/marked/marked.min.js"></script>
    <script src="https://unpkg.com/lucide@latest"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/html2pdf.js/0.10.1/html2pdf.bundle.min.js"></script>
    <style>
        @import url('https://fonts.googleapis.com/css2?family=Inter:wght@400;600;700&display=swap');
        body { font-family: 'Inter', sans-serif; }
        .prose table { border-collapse: collapse; width: 100%; margin: 1em 0; }
        .prose th, .prose td { border: 1px solid #cbd5e1; padding: 8px; font-size: 0.9rem; }
        .prose th { background-color: #f1f5f9; }
        .loader { border: 3px solid #f3f3f3; border-top: 3px solid #0ea5e9; border-radius: 50%; width: 20px; height: 20px; animation: spin 1s linear infinite; }
        @keyframes spin { 0% { transform: rotate(0deg); } 100% { transform: rotate(360deg); } }
    </style>
</head>
<body class="bg-slate-100 text-slate-800">
    <div id="app" class="min-h-screen p-6">
        <header class="max-w-7xl mx-auto mb-6 flex items-center gap-3">
            <div class="bg-sky-600 p-2 rounded-lg shadow"><i data-lucide="microscope" class="w-6 h-6 text-white"></i></div>
            <div><h1 class="text-2xl font-bold">AI Research Center V8</h1><p class="text-xs text-slate-500">Smart Cropping ‚Ä¢ Metadata OCR ‚Ä¢ Dual-Pass Analysis</p></div>
        </header>

        <main class="max-w-7xl mx-auto grid grid-cols-1 lg:grid-cols-12 gap-6">
            
            <!-- Staging Area -->
            <div class="lg:col-span-4 space-y-4">
                <div class="bg-white p-5 rounded-xl shadow-sm border border-slate-200">
                    <h2 class="font-bold mb-3 flex items-center gap-2"><i data-lucide="files" class="w-4 h-4"></i> Data Staging</h2>
                    
                    <div class="border-2 border-dashed border-slate-300 rounded-lg p-6 text-center hover:bg-sky-50 cursor-pointer" @click="$refs.f.click()">
                        <input type="file" ref="f" multiple class="hidden" @change="addFiles">
                        <p class="text-sm font-semibold text-slate-600">Add Files</p>
                    </div>

                    <div v-if="stagedFiles.length" class="mt-4 space-y-3 max-h-[60vh] overflow-y-auto pr-1">
                        <div v-for="(item, i) in stagedFiles" :key="i" class="p-3 bg-slate-50 border rounded-lg text-sm">
                            <div class="flex justify-between items-center mb-2">
                                <span class="font-bold truncate w-2/3" :title="item.file.name">{{ item.file.name }}</span>
                                <button @click="removeFile(i)" class="text-red-400 hover:text-red-600"><i data-lucide="trash-2" class="w-4 h-4"></i></button>
                            </div>
                            
                            <!-- Configs -->
                            <div class="grid grid-cols-1 gap-2">
                                <select v-model="item.equipment" class="w-full p-1.5 border rounded text-xs bg-white font-semibold text-sky-700">
                                    <optgroup label="Literature">
                                        <option value="Document">Document (PDF/PPT)</option>
                                    </optgroup>
                                    <optgroup label="Spectroscopy">
                                        <option value="XRD">XRD</option>
                                        <option value="EDS">EDS</option>
                                        <option value="Raman">Raman</option>
                                    </optgroup>
                                    <optgroup label="Microscopy">
                                        <option value="SEM">SEM (Smart Crop)</option>
                                        <option value="TEM">TEM (Smart Crop)</option>
                                        <option value="STEM">STEM</option>
                                        <option value="AFM">AFM</option>
                                    </optgroup>
                                </select>

                                <!-- Mode Select (Disabled for Documents) -->
                                <select v-if="item.equipment !== 'Document'" v-model="item.mode" class="w-full p-1.5 border rounded text-xs bg-white text-indigo-700">
                                    <option value="Auto">‚ú® Auto Process</option>
                                    <option value="None">üö´ None (Raw)</option>
                                    <option value="AI-Adaptive">üß† AI-Adaptive</option>
                                </select>
                                <div v-else class="text-[10px] text-slate-400 text-center italic">Processing skipped</div>

                                <input v-if="item.equipment !== 'Document'" v-model="item.goal" type="text" placeholder="Goal (e.g. Count atoms)" class="w-full p-1.5 border rounded text-xs">
                            </div>
                        </div>
                    </div>

                    <button @click="analyze" :disabled="isAnalyzing || stagedFiles.length===0" class="w-full mt-4 py-3 bg-sky-600 text-white rounded-lg font-bold flex justify-center gap-2 shadow hover:bg-sky-700 disabled:opacity-50">
                        <div v-if="isAnalyzing" class="loader"></div>
                        <span>Generate Synthesis Report</span>
                    </button>
                </div>
            </div>

            <!-- Report Area -->
            <div class="lg:col-span-8">
                <div v-if="result" class="space-y-6 animate-fade-in">
                    <div id="report-view" class="bg-white p-8 rounded-xl shadow-sm border border-slate-200">
                        <div class="flex justify-between items-end border-b pb-4 mb-6">
                            <div>
                                <h1 class="text-2xl font-bold text-slate-900">Comprehensive Report</h1>
                                <p class="text-xs text-slate-500">{{ new Date().toLocaleString() }}</p>
                            </div>
                            <button @click="dlPDF" class="px-3 py-1.5 bg-slate-800 text-white text-xs rounded">PDF Export</button>
                        </div>

                        <!-- Synthesis -->
                        <section class="mb-10">
                            <h2 class="text-lg font-bold text-sky-700 mb-3 border-b pb-1">1. Executive Synthesis</h2>
                            <div class="prose prose-sm text-slate-700 max-w-none" v-html="md(result.final_report)"></div>
                        </section>

                        <!-- Details -->
                        <section>
                            <h2 class="text-lg font-bold text-slate-800 mb-5 border-b pb-1">2. Data Evidence</h2>
                            
                            <div v-for="(item, idx) in result.results" :key="idx" class="mb-8 p-4 bg-slate-50 rounded-lg border break-inside-avoid">
                                <div class="flex items-center gap-2 mb-3">
                                    <span class="px-2 py-0.5 bg-white border rounded text-[10px] font-bold uppercase text-slate-500">{{ item.equipment }}</span>
                                    <h3 class="font-bold text-sm">{{ item.filename }}</h3>
                                </div>

                                <!-- Spectrum Chart -->
                                <div v-if="item.chart_data" class="bg-white p-2 rounded border mb-3">
                                    <div :id="'chart-'+idx" class="w-full h-60"></div>
                                    <div class="text-[10px] text-slate-400 mt-1 flex gap-2 justify-center">
                                        <span class="flex items-center gap-1"><span class="w-2 h-2 bg-gray-300 rounded-full"></span>Raw</span>
                                        <span class="flex items-center gap-1"><span class="w-2 h-2 bg-sky-500 rounded-full"></span>Proc</span>
                                        <span class="flex items-center gap-1"><span class="w-2 h-2 bg-orange-400 rounded-full"></span>Base</span>
                                    </div>
                                </div>

                                <!-- Image Viewer (Raw / Proc / Info Bar) -->
                                <div v-if="item.raw_b64" class="bg-white p-2 rounded border mb-3">
                                    <div class="flex gap-2 mb-2 text-xs justify-center">
                                        <button @click="item.view='proc'" :class="item.view!=='raw'?'font-bold text-sky-600':''">Processed Body</button>
                                        <span class="text-slate-300">|</span>
                                        <button @click="item.view='raw'" :class="item.view==='raw'?'font-bold text-sky-600':''">Original</button>
                                        <span class="text-slate-300" v-if="item.footer_b64">|</span>
                                        <button v-if="item.footer_b64" @click="item.view='footer'" :class="item.view==='footer'?'font-bold text-sky-600':''">Info Bar (OCR)</button>
                                    </div>
                                    
                                    <!-- Main Image -->
                                    <img v-if="item.view!=='footer'" :src="'data:image/jpeg;base64,' + (item.view==='raw' ? item.raw_b64 : item.proc_b64)" class="max-h-80 mx-auto object-contain bg-black">
                                    
                                    <!-- Footer Image -->
                                    <img v-if="item.view==='footer'" :src="'data:image/jpeg;base64,' + item.footer_b64" class="max-h-24 mx-auto object-contain border mt-2">

                                    <p class="text-[10px] text-slate-400 mt-2 text-center" v-if="item.stats">Stats: {{ item.stats }}</p>
                                </div>

                                <!-- Docs -->
                                <div v-if="item.pages" class="grid grid-cols-1 gap-4">
                                    <div v-for="p in item.pages" :key="p.page_num" class="flex gap-3 bg-white p-3 rounded border">
                                        <img :src="'data:image/jpeg;base64,'+p.image_b64" class="w-24 object-contain border">
                                        <div class="text-xs prose" v-html="md(p.desc)"></div>
                                    </div>
                                </div>
                                <div v-if="item.slides" class="space-y-2">
                                    <div v-for="s in item.slides" :key="s.slide_num" class="bg-white p-3 rounded border">
                                        <div class="font-bold text-xs mb-1 text-sky-600">Slide {{s.slide_num}}</div>
                                        <div class="text-xs prose">{{ s.text }}</div>
                                        <div v-if="s.images.length" class="grid grid-cols-3 gap-2 mt-2">
                                            <img v-for="(im,imx) in s.images" :key="imx" :src="'data:image/jpeg;base64,'+im.b64" class="border">
                                        </div>
                                    </div>
                                </div>

                                <div class="text-xs text-slate-600 mt-2 prose" v-html="md(item.summary)"></div>
                            </div>
                        </section>
                    </div>
                </div>
            </div>
        </main>
    </div>
    <script>
        const { createApp, ref, nextTick } = Vue;
        createApp({
            setup() {
                const stagedFiles = ref([]);
                const isAnalyzing = ref(false);
                const result = ref(null);
                const renderer = new marked.Renderer();
                renderer.image = () => '';
                marked.use({ renderer });
                const md = (t) => marked.parse(t||'');

                const addFiles = (e) => {
                    Array.from(e.target.files).forEach(f => {
                        let eq = 'General';
                        if(f.name.match(/\.(pdf|ppt|pptx)$/i)) eq = 'Document';
                        stagedFiles.value.push({ file: f, equipment: eq, goal: '', mode: 'Auto', view: 'proc' });
                    });
                    e.target.value = ''; 
                };
                const removeFile = (i) => stagedFiles.value.splice(i, 1);

                const analyze = async () => {
                    isAnalyzing.value = true;
                    const fd = new FormData();
                    const configs = {};
                    stagedFiles.value.forEach(item => {
                        fd.append('files', item.file);
                        configs[item.file.name] = { equipment: item.equipment, goal: item.goal, mode: item.mode };
                    });
                    fd.append('file_configs', JSON.stringify(configs));

                    try {
                        const res = await fetch('/api/analyze', { method: 'POST', body: fd });
                        const data = await res.json();
                        result.value = data;
                        await nextTick();
                        
                        data.results.forEach((item, i) => {
                            if(item.chart_data) {
                                const raw = { x: item.chart_data.map(d=>d.x), y: item.chart_data.map(d=>d.y_raw), mode:'lines', name:'Raw', line:{color:'#cbd5e1', width:1} };
                                const proc = { x: item.chart_data.map(d=>d.x), y: item.chart_data.map(d=>d.y_proc), mode:'lines', name:'Proc', line:{color:'#0ea5e9', width:2} };
                                const base = { x: item.chart_data.map(d=>d.x), y: item.chart_data.map(d=>d.y_base), mode:'lines', name:'Base', line:{color:'#fb923c', width:1, dash:'dot'} };
                                Plotly.newPlot('chart-'+i, [raw, base, proc], {margin:{t:10,b:30,l:40,r:10}, showlegend:false}, {displayModeBar:false});
                            }
                        });
                        lucide.createIcons();
                    } catch(e) { alert(e); } 
                    finally { isAnalyzing.value = false; }
                };

                const dlPDF = () => {
                    html2pdf().set({ margin:10, filename:'V8_Report.pdf', image:{type:'jpeg',quality:0.98}, html2canvas:{scale:2}, jsPDF:{unit:'mm',format:'a4'} }).from(document.getElementById('report-view')).save();
                };

                setTimeout(()=>lucide.createIcons(), 100);
                return { stagedFiles, isAnalyzing, result, addFiles, removeFile, analyze, md, dlPDF };
            }
        }).mount('#app');
    </script>
</body>
</html>


