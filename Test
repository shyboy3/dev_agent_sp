NV32

import os
import io
# [0] OMP Error Fix
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"
import torch

import asyncio
import base64
import json
import re
import math
import uuid
import time
from typing import List, Dict, Any, Optional
from pydantic import BaseModel
from contextlib import asynccontextmanager
from collections import Counter

# [1] Security
os.environ['NO_PROXY'] = '127.0.0.1,localhost,0.0.0.0,::1'
if 'HTTP_PROXY' in os.environ: del os.environ['HTTP_PROXY']
if 'HTTPS_PROXY' in os.environ: del os.environ['HTTPS_PROXY']

from fastapi import FastAPI, UploadFile, File, Form, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import HTMLResponse, FileResponse
import pandas as pd
import numpy as np

# SciPy & CV & Sklearn
from sklearn.decomposition import NMF
from scipy.signal import find_peaks, savgol_filter
from scipy.optimize import curve_fit
from scipy.ndimage import center_of_mass
from skimage.feature import peak_local_max
import cv2
import matplotlib
import matplotlib.pyplot as plt 

# Models
import pypdfium2 as pdfium 
from segment_anything import sam_model_registry, SamAutomaticMaskGenerator
from ollama import Client 
from pptx import Presentation
from pptx.enum.shapes import MSO_SHAPE_TYPE

# [2] Config
OLLAMA_HOST = "http://127.0.0.1:11434"
ollama_client = Client(host=OLLAMA_HOST)
VISION_MODEL = "qwen3-vl:30b-a3b-instruct" 
TEXT_MODEL = "llama3:70b"

SAM_CHECKPOINT = "sam_vit_b_01ec64.pth"
SAM_MODEL_TYPE = "vit_b"
device = "cuda" if torch.cuda.is_available() else "cpu"

mask_generator = None 
JOBS = {} 

@asynccontextmanager
async def lifespan(app: FastAPI):
    global mask_generator
    if os.path.exists(SAM_CHECKPOINT):
        try:
            sam = sam_model_registry[SAM_MODEL_TYPE](checkpoint=SAM_CHECKPOINT)
            sam.to(device=device)
            mask_generator = SamAutomaticMaskGenerator(
                model=sam, points_per_side=32, pred_iou_thresh=0.86,
                stability_score_thresh=0.90, min_mask_region_area=50
            )
        except: pass
    yield
    JOBS.clear()

def sanitize_json(obj):
    if isinstance(obj, float):
        if np.isnan(obj) or np.isinf(obj): return 0.0
        return obj
    elif isinstance(obj, dict): return {k: sanitize_json(v) for k, v in obj.items()}
    elif isinstance(obj, list): return [sanitize_json(v) for v in obj]
    elif isinstance(obj, (np.int64, np.int32)): return int(obj)
    elif isinstance(obj, (np.float64, np.float32)):
        if np.isnan(obj) or np.isinf(obj): return 0.0
        return float(obj)
    return obj

# ==========================================
# [3] NV34 User Logic Integrated
# ==========================================
class ScienceProcessorNV34:
    
    # --- [User Code] Universal Loader ---
    @staticmethod
    def read_universal_dataframe(content: bytes, filename: str) -> List[pd.DataFrame]:
        dfs = []
        fname = filename.lower()
        if fname.endswith(('.xlsx', '.xls')):
            try:
                xls = pd.ExcelFile(io.BytesIO(content))
                for s in xls.sheet_names:
                    df = pd.read_excel(xls, sheet_name=s, header=None)
                    num = df.apply(pd.to_numeric, errors='coerce').notna().sum().sum()
                    if num < 5: continue
                    if not df.empty: dfs.append(df)
                if dfs: return dfs
            except: pass
        text_content = ""
        for enc in ['utf-8', 'cp949', 'latin1']:
            try: text_content = content.decode(enc); break
            except: continue
        if text_content:
            lines = text_content.splitlines()
            data_start = -1; delimiter = None
            separators = [',', '\t', ';', '\s+']
            for i, line in enumerate(lines[:100]):
                for sep in separators:
                    parts = line.split() if sep=='\s+' else line.split(sep)
                    nums = sum(1 for p in parts if re.match(r'^-?\d+(\.\d+)?(?:[eE][+-]?\d+)?$', p.strip()))
                    if nums >= 2: data_start = i; delimiter = sep; break
                if data_start != -1: break
            if data_start != -1:
                try:
                    data_str = "\n".join(lines[data_start:])
                    sep_arg = '\s+' if delimiter=='\s+' else delimiter
                    df = pd.read_csv(io.StringIO(data_str), sep=sep_arg, header=None, engine='python')
                    dfs.append(df)
                except: pass
        return dfs

    # --- [User Code] Block Parser ---
    @staticmethod
    def detect_structured_blocks(df: pd.DataFrame) -> List[pd.DataFrame]:
        blocks = []
        if df.empty: return []
        df_num = df.apply(pd.to_numeric, errors='coerce')
        if df.shape[1] > 0: is_data_row = df_num.iloc[:, 0].notna()
        else: is_data_row = df_num.notna().any(axis=1)
        groups = (is_data_row != is_data_row.shift()).cumsum()
        for _, group in df.groupby(groups):
            if is_data_row.loc[group.index[0]]:
                block = df_num.loc[group.index].dropna(how='all', axis=1)
                if block.shape[0] >= 10: blocks.append(block.reset_index(drop=True))
        return blocks

    # --- [User Code] Split Horizontal ---
    @staticmethod
    def split_horizontal_blocks(df: pd.DataFrame) -> List[pd.DataFrame]:
        sub_blocks = []
        if df.empty: return []
        col_counts = df.apply(pd.to_numeric, errors='coerce').notna().sum()
        valid = np.where(col_counts >= 2)[0]
        if len(valid) == 0: return []
        splits = np.where(np.diff(valid) > 1)[0] + 1
        groups = np.split(valid, splits)
        for g in groups:
            if len(g) > 0:
                sub = df.iloc[:, g].reset_index(drop=True)
                if sub.shape[1] >= 1 and sub.shape[0] >= 2: sub_blocks.append(sub)
        return sub_blocks

    # --- [User Code] Extract Series (Smart Axis Logic) ---
    @staticmethod
    def extract_series_from_df(df: pd.DataFrame) -> List[Dict]:
        series_list = []
        try:
            vals = df.values; rows, cols = vals.shape
            if rows < 5: return []
            if cols == 1:
                y = vals[:, 0]; mask = ~np.isnan(y)
                if np.sum(mask) > 5: series_list.append({"x": np.arange(len(y))[mask], "y": y[mask], "name": "Single"})
                return series_list
            
            def is_axis(arr):
                arr = arr[~np.isnan(arr)]
                if len(arr) < 5: return False
                diff = np.diff(arr)
                if np.all(diff >= 0) or np.all(diff <= 0): return True
                return (np.sum(diff>0)/len(diff)>0.9) or (np.sum(diff<0)/len(diff)>0.9)
            
            current_x = vals[:, 0]
            if not is_axis(current_x): current_x = np.arange(rows)
            
            for i in range(1, cols):
                col_data = vals[:, i]; mask_valid = ~np.isnan(col_data)
                if np.sum(mask_valid) < 5: continue
                
                # User's brilliant logic: If a column looks like an axis (monotonic or constant),
                # update current_x and SKIP adding it as data.
                # This naturally filters out "straight lines" (constant 0s) because they satisfy is_axis!
                if is_axis(col_data): 
                    current_x = col_data
                    continue 
                
                mask = ~np.isnan(current_x) & ~np.isnan(col_data)
                if np.sum(mask) > 5:
                    series_list.append({"x": current_x[mask], "y": col_data[mask], "name": f"Col-{i}"})
        except: pass
        return series_list

    # --- Spectrum Logic (Conservative) ---
    @staticmethod
    def simple_baseline(y):
        try:
            w = max(5, len(y)//10)
            s = pd.Series(y).rolling(window=w, center=True).min()
            b = s.bfill().ffill().values 
            if len(b) > 51: b = savgol_filter(b, 51, 3)
            return b, w
        except: return np.zeros_like(y), 0

    @staticmethod
    def process_spectrum(x, y, mode, goal):
        try:
            y_raw = y.copy(); y_base = np.zeros_like(y); y_proc = y.copy()
            if mode == "None":
                peaks, _ = find_peaks(y, height=np.max(y)*0.05)
                return {"x": x, "y_raw": y_raw, "y_proc": y_raw, "y_base": y_base, "peaks": [{"x": float(x[p]), "y": float(y[p])} for p in peaks], "log": ["Mode: Raw"], "stats": f"Raw Peaks: {len(peaks)}"}
            elif mode == "Auto" or mode == "AI-Adaptive":
                win = 5
                if "base" in goal.lower():
                    base, bw = ScienceProcessorNV34.simple_baseline(y)
                    y_proc = np.maximum(y - base, 0); y_base = base
                if len(y_proc) > win: y_proc = savgol_filter(y_proc, win, 3)
                peaks, _ = find_peaks(y_proc, height=np.max(y_proc)*0.05, distance=10)
                return {"x": x, "y_raw": y_raw, "y_proc": y_proc, "y_base": y_base, "peaks": [{"x": float(x[p]), "y": float(y_proc[p])} for p in peaks], "log": [f"Mode: {mode}"], "stats": f"Proc Peaks: {len(peaks)}"}
            else: return {"x": x, "y_raw": y, "y_proc": y, "y_base": y_base, "peaks": [], "log": ["Unknown"], "stats": ""}
        except Exception as e: return {"x": x, "y_raw": y, "y_proc": y, "y_base": np.zeros_like(y), "peaks": [], "log": [str(e)], "stats": "Error"}

    # --- Footer ---
    @staticmethod
    def separate_footer(img_bgr):
        h, w = img_bgr.shape[:2]; roi_h = int(h * 0.82); roi = img_bgr[roi_h:, :]
        gray = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)
        sobel_y = np.abs(cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=3))
        row_sums = np.sum(sobel_y, axis=1)
        split_idx = np.argmax(row_sums)
        if row_sums[split_idx] > w * 30: return img_bgr[:roi_h + split_idx, :], img_bgr[roi_h + split_idx:, :]
        return img_bgr, None

    # --- [E] FFT-NMF (NV26) ---
    @staticmethod
    def analyze_crystal_fft_nmf(img_bgr, patch_size=128, step=16, n_components=10):
        log = []; gray = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2GRAY); h, w = gray.shape
        log.append(f"FFT-NMF: Patch={patch_size}, Step={step}, K={n_components}")
        custom_colors = [(0,0,255),(0,255,0),(255,0,0),(0,255,255),(255,255,0),(255,0,255),(0,165,255),(238,130,238),(128,128,0),(19,69,139)]
        while len(custom_colors) < n_components: custom_colors.append((255,255,255))
        cols = []; y_steps = list(range(0, h-patch_size+1, step)); x_steps = list(range(0, w-patch_size+1, step))
        grid_h = len(y_steps); grid_w = len(x_steps)
        cy, cx = patch_size//2, patch_size//2
        Y, X = np.ogrid[:patch_size, :patch_size]; dc_mask = ((X-cx)**2 + (Y-cy)**2) <= 4**2
        for y in y_steps:
            for x in x_steps:
                roi = gray[y:y+patch_size, x:x+patch_size]
                fshift = np.fft.fftshift(np.fft.fft2(roi))
                mag = np.abs(fshift); mag[dc_mask] = 0; cols.append(mag.flatten())
        if not cols: return {"type":"error", "msg":"Img too small"}
        try:
            V = np.array(cols).T.astype(np.float32); V = V - V.min()
            if V.max() > 0: V = V/V.max()
            nmf = NMF(n_components=n_components, init='nndsvda', random_state=42, max_iter=2000)
            W = nmf.fit_transform(V); H = nmf.components_
            if H.shape[1] != grid_h*grid_w: raise ValueError("Shape Mismatch")
            idx_H = np.argmax(H, axis=0)
            map_high = cv2.resize(idx_H.reshape(grid_h, grid_w).astype(np.uint8), (w, h), interpolation=cv2.INTER_NEAREST)
            overlay = img_bgr.copy(); mask_layer = np.zeros_like(img_bgr)
            counts = Counter(idx_H); total = len(idx_H); stats = {}
            for i in range(n_components):
                mask = (map_high == i); mask_layer[mask] = custom_colors[i]
                cnt = counts.get(i, 0); stats[f"Pattern {i}"] = f"{cnt} patches ({cnt/total*100:.1f}%)"
            mask_idx = np.any(mask_layer > 0, axis=-1)
            overlay[mask_idx] = (overlay[mask_idx]*(1-0.4) + mask_layer[mask_idx]*0.4).astype(np.uint8)
            basis_combo_list = []
            for k in range(n_components):
                pat = W[:, k].reshape(patch_size, patch_size)
                pat_vis = cv2.applyColorMap(cv2.resize(cv2.normalize(np.log1p(pat*1000),None,0,255,cv2.NORM_MINMAX).astype(np.uint8),(128,128),interpolation=cv2.INTER_NEAREST), cv2.COLORMAP_MAGMA)
                cv2.putText(pat_vis, "W (Basis)", (5, 15), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255,255,255), 1)
                h_vec = H[k, :]; h_vis = cv2.applyColorMap(cv2.resize(cv2.normalize(h_vec.reshape(grid_h,grid_w),None,0,255,cv2.NORM_MINMAX).astype(np.uint8),(128,128),interpolation=cv2.INTER_NEAREST), cv2.COLORMAP_VIRIDIS)
                cv2.putText(h_vis, "H (Dist)", (5, 15), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255,255,255), 1)
                cbar_vis = cv2.applyColorMap(np.tile(np.linspace(0,255,128).astype(np.uint8),(25,1)), cv2.COLORMAP_VIRIDIS)
                cv2.putText(cbar_vis, f"{h_vec.min():.1f}", (2,17), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255,255,255), 1)
                cv2.putText(cbar_vis, f"{h_vec.max():.1f}", (128-40,17), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (0,0,0), 1)
                legend = np.zeros((30, 128, 3), dtype=np.uint8); legend[:] = custom_colors[k]
                cv2.putText(legend, f"P{k}", (50, 22), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255,255,255), 2)
                combo = np.vstack([pat_vis, h_vis, cbar_vis, legend])
                cv2.rectangle(combo, (0,0), (127, combo.shape[0]-1), custom_colors[k], 2)
                basis_combo_list.append(combo)
            full_strip_large = np.hstack(basis_combo_list)
            def to_b64(im): return base64.b64encode(cv2.imencode('.jpg', im)[1]).decode('utf-8')
            return {"type":"image", "raw_b64":to_b64(img_bgr), "proc_b64":to_b64(overlay), "footer_b64":to_b64(full_strip_large), "stats":stats, "log":log}
        except Exception as e: return {"type":"image", "raw_b64":base64.b64encode(cv2.imencode('.jpg', img_bgr)[1]).decode('utf-8'), "proc_b64":base64.b64encode(cv2.imencode('.jpg', img_bgr)[1]).decode('utf-8'), "stats":{"Error":str(e)}, "log":log}

    # --- [F] Diffraction ---
    @staticmethod
    def analyze_diffraction_nv7(img_bgr, user_center=None):
        log = []; stats = {}
        gray = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2GRAY)
        if user_center and len(user_center)==2 and user_center[0]>0:
            cx, cy = user_center
            log.append(f"Center: Manual [{cx}, {cy}]")
        else:
            _, th = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY+cv2.THRESH_OTSU)
            cyf, cxf = center_of_mass(th)
            if np.isnan(cyf): cyf, cxf = gray.shape[0]//2, gray.shape[1]//2
            cx, cy = int(cxf), int(cyf)
            log.append(f"Center: Auto [{cx}, {cy}]")

        g1 = cv2.GaussianBlur(gray, (3,3), 0)
        g2 = cv2.GaussianBlur(gray, (51,51), 0)
        diff = cv2.subtract(g1, g2)
        coords = peak_local_max(diff, min_distance=10, threshold_abs=20)
        enhanced = np.log1p(diff.astype(np.float32))
        enhanced = cv2.normalize(enhanced, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)
        vis = cv2.cvtColor(enhanced, cv2.COLOR_GRAY2BGR)
        cv2.drawMarker(vis, (cx, cy), (0,0,255), cv2.MARKER_CROSS, 20, 2)
        peak_list = []
        for p in coords:
            py, px = p
            if abs(py-cy)<5 and abs(px-cx)<5: continue
            cv2.circle(vis, (px, py), 3, (0,255,0), 1)
            peak_list.append(f"[{px},{py}]")
        log.append(f"Spots: {len(peak_list)}")
        def to_b64(im): return base64.b64encode(cv2.imencode('.jpg', im)[1]).decode('utf-8')
        return {"type": "image", "raw_b64": to_b64(img_bgr), "proc_b64": to_b64(vis), "stats": {"center": [cx, cy], "spots": len(peak_list)}, "log": log}

    # --- [G] SAM ---
    @staticmethod
    def analyze_sam_results_nv7(img_rgb, masks, data_type):
        overlay = img_rgb.copy()
        mask_overlay = np.zeros_like(img_rgb)
        h_img, w_img = img_rgb.shape[:2]
        stats = {"count": 0}
        log = []
        valid_masks = []
        for ann in masks:
            area = ann['area']; bbox = ann['bbox']; x, y, w, h = [int(v) for v in bbox]
            if area > h_img * w_img * 0.95 or area < 20: continue 
            if data_type == "Particle":
                if x<=1 or y<=1 or (x+w)>=w_img-1 or (y+h)>=h_img-1: continue
                mask_uint8 = (ann['segmentation'] * 255).astype(np.uint8)
                cnts, _ = cv2.findContours(mask_uint8, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
                if cnts:
                    perim = cv2.arcLength(cnts[0], True)
                    circ = 4*np.pi*area/(perim**2) if perim>0 else 0
                    if circ < 0.25: continue
            valid_masks.append(ann)

        if data_type == "Thin Film":
            sorted_masks = sorted(valid_masks, key=lambda x: x['bbox'][1])
        else:
            sorted_masks = sorted(valid_masks, key=lambda x: x['area'], reverse=True)

        thickness_vals = []; diameters = []
        for i, ann in enumerate(sorted_masks):
            m = ann['segmentation']
            bbox = ann['bbox']; x, y, w, h = [int(v) for v in bbox]
            cx, cy = int(x + w/2), int(y + h/2)
            color = (0, 255, 0) 
            if data_type == "Thin Film":
                is_bound = (i == 0 or i == len(sorted_masks)-1)
                cv2.putText(overlay, f"#{i+1}", (cx, cy), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (255, 255, 255), 3)
                if not is_bound:
                    thk = ann['area'] / w
                    thickness_vals.append(thk)
                    color = (255, 0, 0) 
                else: color = (100, 100, 100)
                if i < len(sorted_masks) - 1:
                    ys, xs = np.where(m)
                    if len(xs) > 10:
                        roughness = np.std(ys)
                        cv2.line(overlay, (x, y+h), (x+w, y+h), (0, 255, 255), 2)
                        cv2.putText(overlay, f"Int#{i+1} R:{roughness:.2f}", (x+10, y+h-5), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)
            elif data_type == "Particle":
                equivalent_diameter = 2 * np.sqrt(ann['area'] / np.pi)
                diameters.append(equivalent_diameter)
            mask_overlay[m] = color
            mask_uint8 = (m * 255).astype(np.uint8)
            cnts, _ = cv2.findContours(mask_uint8, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
            if cnts: cv2.drawContours(overlay, [cnts[0]], -1, color, 1)

        alpha = 0.35
        mask_idx = np.any(mask_overlay > 0, axis=-1)
        overlay[mask_idx] = (overlay[mask_idx]*(1-alpha) + mask_overlay[mask_idx]*alpha).astype(np.uint8)
        stats["count"] = len(sorted_masks)
        if data_type == "Thin Film":
            avg = round(np.mean(thickness_vals), 2) if thickness_vals else 0
            stats["thickness_avg"] = avg
            log.append(f"Layers: {len(sorted_masks)}, Avg Thk: {avg}")
        elif data_type == "Particle":
            avg = round(np.mean(diameters), 2) if diameters else 0
            stats["diameter_avg"] = avg
            log.append(f"Particles: {len(sorted_masks)}")
        return overlay, stats, log

    # --- Router ---
    @staticmethod
    def process_image_nv34(img_bytes, equipment, data_type, mode, goal, user_center=None, patch_size=128, step=16, n_components=10):
        nparr = np.frombuffer(img_bytes, np.uint8); img_raw = cv2.imdecode(nparr, cv2.IMREAD_COLOR)
        if img_raw is None: return {}
        body, footer = ScienceProcessorNV34.separate_footer(img_raw)
        footer_b64 = base64.b64encode(cv2.imencode('.jpg', footer)[1]).decode('utf-8') if footer is not None else None
        
        if data_type == "Crystal Structure (FFT)":
            return ScienceProcessorNV34.analyze_crystal_fft_nmf(body, patch_size, step, n_components)
        if data_type == "2D Diffraction":
            res = ScienceProcessorNV34.analyze_diffraction_nv7(body, user_center)
            return {**res, "footer_b64": footer_b64}
        
        if mode == "None":
             def to_b64(im): return base64.b64encode(cv2.imencode('.jpg', im)[1]).decode('utf-8')
             return {"type":"image", "raw_b64":to_b64(body), "proc_b64":to_b64(body), "stats":{}, "log":["Mode: Raw"], "footer_b64": footer_b64}

        img_proc = body.copy(); log = []
        if mode == "Auto" or mode == "AI-Adaptive": img_proc = cv2.bilateralFilter(img_proc, 9, 75, 75)
        img_rgb = cv2.cvtColor(img_proc, cv2.COLOR_BGR2RGB)
        if mask_generator:
            try:
                masks = mask_generator.generate(img_rgb)
                overlay, stats, sam_log = ScienceProcessorNV34.analyze_sam_results_nv7(img_rgb, masks, data_type)
                log.extend(sam_log); overlay = cv2.cvtColor(overlay, cv2.COLOR_RGB2BGR)
            except Exception as e: log.append(f"SAM Error: {str(e)}"); overlay = body; stats = {}
        else: log.append("SAM Not Loaded"); overlay = body; stats = {}
        def to_b64(im): return base64.b64encode(cv2.imencode('.jpg', im)[1]).decode('utf-8')
        return {"type": "image", "raw_b64": to_b64(body), "proc_b64": to_b64(overlay), "stats": stats, "log": log, "footer_b64": footer_b64}

# [4] Helper (Same)
async def analyze_vision_ollama(image_bytes: bytes, prompt: str) -> str:
    try:
        b64 = base64.b64encode(image_bytes).decode('utf-8')
        res = await asyncio.to_thread(ollama_client.chat, model=VISION_MODEL, messages=[{'role':'user','content':prompt,'images':[b64]}])
        return res['message']['content']
    except: return "Vision Error"

async def interpret_spectrum_data(raw_context: str, equipment: str) -> str:
    try:
        prompt = f"Analyze spectrum ({equipment}). Brief interpretation in Korean. Data: {raw_context}"
        res = await asyncio.to_thread(ollama_client.chat, model=TEXT_MODEL, messages=[{'role':'user','content':prompt}])
        return res['message']['content']
    except: return ""

# [5] App
app = FastAPI(title="Analyst NV34 User Integrated", lifespan=lifespan)
app.add_middleware(CORSMiddleware, allow_origins=["*"], allow_methods=["*"], allow_headers=["*"])

class TranslateReq(BaseModel): text: str
@app.post("/api/translate")
async def translate_text(req: TranslateReq):
    try:
        res = await asyncio.to_thread(ollama_client.chat, model=TEXT_MODEL, messages=[{'role':'system','content':'Translate to English.'}, {'role':'user','content':req.text}])
        return {"translated": res['message']['content']}
    except Exception as e: return {"translated": str(e)}

async def process_analysis_job(job_id: str, files_map: Dict, configs: Dict):
    try:
        JOBS[job_id]["status"] = "Processing"; JOBS[job_id]["progress"] = 10; final_results = []; total = len(configs); cnt = 0
        for filename, config in configs.items():
            if filename not in files_map: continue
            content = files_map[filename]; eq = config.get("equipment", "General"); data_type = config.get("data_type", "General")
            goal = config.get("goal", ""); mode = config.get("mode", "Auto")
            p_size = int(config.get("patch_size", 128)); p_step = int(config.get("step", 16)); n_comp = int(config.get("n_components", 10))
            fname_lower = filename.lower(); JOBS[job_id]["step"] = f"Analyzing {filename}..."
            
            if eq == "Document" or fname_lower.endswith(('.pdf', '.ppt', '.pptx')):
                try:
                    if fname_lower.endswith('.pdf'):
                        pdf = pdfium.PdfDocument(io.BytesIO(content)); n_pages = len(pdf); full_txt = ""; pages = []
                        for i in range(min(n_pages, 10)):
                            page = pdf[i]; bitmap = page.render(scale=2); pil_image = bitmap.to_pil()
                            buf = io.BytesIO(); pil_image.save(buf, format="JPEG"); b64 = base64.b64encode(buf.getvalue()).decode()
                            desc = await asyncio.to_thread(ollama_client.chat, model=VISION_MODEL, messages=[{'role':'user','content':'한글 요약.','images':[b64]}])
                            d_txt = desc['message']['content']; full_txt += f"\nPage {i+1}: {d_txt}"
                            pages.append({"id": str(uuid.uuid4()), "page_num": i+1, "image_b64": b64, "desc": d_txt})
                        final_results.append({"type": "pdf", "filename": filename, "equipment": "Lit", "raw_context": full_txt, "pages": pages})
                    elif fname_lower.endswith(('.ppt', '.pptx')):
                        try:
                            prs = Presentation(io.BytesIO(content)); slides = []; full_txt = ""
                            for i, slide in enumerate(prs.slides):
                                txt = ""; imgs = []
                                for s in slide.shapes:
                                    if hasattr(s, "text"): txt += s.text + "\n"
                                    if s.shape_type == MSO_SHAPE_TYPE.PICTURE:
                                        try:
                                            ib64 = base64.b64encode(s.image.blob).decode('utf-8')
                                            imgs.append({"b64":ib64, "desc":"Img"})
                                        except: pass
                                if not txt.strip(): txt = "(No text)"
                                full_txt += f"Slide {i+1}: {txt}\n"; slides.append({"id": str(uuid.uuid4()), "slide_num": i+1, "text": txt, "images": imgs})
                            final_results.append({"type": "ppt", "filename": filename, "equipment": "Lit", "raw_context": full_txt, "slides": slides})
                        except Exception as ppt_err: final_results.append({"type":"error", "filename":filename, "msg":f"PPT Error: {ppt_err}"})
                except Exception as e: final_results.append({"type": "error", "filename": filename, "msg": str(e)})

            elif fname_lower.endswith(('.xlsx', '.xls', '.csv', '.txt')):
                try:
                    dfs = ScienceProcessorNV34.read_universal_dataframe(content, filename)
                    for i, df in enumerate(dfs):
                        v_blocks = ScienceProcessorNV34.detect_structured_blocks(df)
                        for v_block in v_blocks:
                            h_blocks = ScienceProcessorNV34.split_horizontal_blocks(v_block)
                            for h_block in h_blocks:
                                series_list = ScienceProcessorNV34.extract_series_from_df(h_block)
                                for s in series_list:
                                    res = ScienceProcessorNV34.process_spectrum(s['x'], s['y'], mode, goal)
                                    step = max(1, len(s['x'])//100)
                                    chart = [{"x": float(res["x"][k]), "y_proc": float(res["y_proc"][k]), "y_raw": float(res["y_raw"][k]), "y_base": float(res["y_base"][k])} for k in range(0, len(s['x']), step)]
                                    ctx = f"Data: {s['name']}\nStats: {res['stats']}"
                                    interp = await interpret_spectrum_data(ctx, eq)
                                    final_results.append({
                                        "type": "spectrum", "filename": f"{filename} ({s['name']})", "equipment": eq,
                                        "raw_context": f"{ctx}\nInterp: {interp}", 
                                        "chart_data": chart, "log": res["log"], "interpretation": interp
                                    })
                except Exception as ex: final_results.append({"type": "error", "filename": filename, "msg": str(ex)})

            elif fname_lower.endswith(('.png', '.jpg', '.jpeg', '.tif', '.bmp')):
                try:
                    vis_res = await asyncio.to_thread(ScienceProcessorNV34.process_image_nv34, content, eq, data_type, mode, goal, None, p_size, p_step, n_comp)
                    if vis_res:
                        scale_info = "Unknown"
                        final_footer = vis_res.get("footer_b64")
                        if final_footer and data_type != "Crystal Structure (FFT)":
                            scale_res = await asyncio.to_thread(ollama_client.chat, model=VISION_MODEL, messages=[{'role':'user','content':'Read scale bar text only.','images':[final_footer]}])
                            scale_info = scale_res['message']['content']; vis_res["log"].append(f"Scale: {scale_info}")
                        
                        stats_clean = "\n".join([f"- {k}: {v}" for k,v in vis_res.get('stats',{}).items()])
                        prompt = f"Analyze {eq} image. Stats: {stats_clean}. Goal: {goal}. Summary in Korean."
                        desc_res = await asyncio.to_thread(ollama_client.chat, model=VISION_MODEL, messages=[{'role':'user','content':prompt,'images':[vis_res["proc_b64"]]}])
                        desc = desc_res['message']['content']
                        final_results.append({"type":"image", "filename":filename, "equipment":eq, "summary":desc, "raw_context":f"Img: {desc}\nStats:{stats_clean}", "raw_b64":vis_res["raw_b64"], "proc_b64":vis_res["proc_b64"], "footer_b64":final_footer, "stats":vis_res.get("stats",{}), "log":vis_res.get("log",[])})
                except Exception as ex: final_results.append({"type":"error", "filename":filename, "msg":str(ex)})
            
            cnt += 1; JOBS[job_id]["progress"] = 10 + int((cnt/total)*80)

        JOBS[job_id]["step"] = "Writing Report..."; data_ctx = ""
        for r in final_results:
            if r.get("type")!="error": data_ctx += f"\n=== {r['filename']} ===\n{r.get('raw_context','')[:2000]}\n"
        
        rep_res = await asyncio.to_thread(ollama_client.chat, model=TEXT_MODEL, messages=[{'role':'system','content':'Write scientific report in Korean.'}, {'role':'user','content':f"Data:\n{data_ctx}"}])
        JOBS[job_id]["results"] = {"results": final_results, "final_report": rep_res['message']['content']}
        JOBS[job_id]["status"] = "Completed"; JOBS[job_id]["progress"] = 100
    except Exception as e: JOBS[job_id]["status"] = "Failed"; JOBS[job_id]["error"] = str(e)

@app.post("/api/analyze")
async def start_analysis(background_tasks: BackgroundTasks, files: List[UploadFile] = File(...), file_configs: str = Form(...)):
    job_id = str(uuid.uuid4()); configs = json.loads(file_configs)
    files_map = {f.filename: await f.read() for f in files}
    JOBS[job_id] = {"status": "Pending", "progress": 0, "step": "Init", "results": None}
    background_tasks.add_task(process_analysis_job, job_id, files_map, configs)
    return {"job_id": job_id}

@app.get("/api/status/{job_id}")
async def get_status(job_id: str): return JOBS.get(job_id, {"status": "Not Found"})
@app.get("/", response_class=HTMLResponse)
async def serve_index(): return FileResponse("index.html") if os.path.exists("index.html") else "<h1>NV34 Backend</h1>"

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)






NV33

import os
import io
# [0] OMP Error Fix
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"
import torch

import asyncio
import base64
import json
import re
import math
import uuid
import time
from typing import List, Dict, Any, Optional
from pydantic import BaseModel
from contextlib import asynccontextmanager
from collections import Counter

# [1] Security
os.environ['NO_PROXY'] = '127.0.0.1,localhost,0.0.0.0,::1'
if 'HTTP_PROXY' in os.environ: del os.environ['HTTP_PROXY']
if 'HTTPS_PROXY' in os.environ: del os.environ['HTTPS_PROXY']

from fastapi import FastAPI, UploadFile, File, Form, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import HTMLResponse, FileResponse
import pandas as pd
import numpy as np

# SciPy & CV & Sklearn
from sklearn.decomposition import NMF
from scipy.signal import find_peaks, savgol_filter
from scipy.optimize import curve_fit
from scipy.ndimage import center_of_mass
from skimage.feature import peak_local_max
import cv2
import matplotlib
import matplotlib.pyplot as plt 

# Models
import pypdfium2 as pdfium 
from segment_anything import sam_model_registry, SamAutomaticMaskGenerator
from ollama import Client 
from pptx import Presentation
from pptx.enum.shapes import MSO_SHAPE_TYPE

# [2] Config
OLLAMA_HOST = "http://127.0.0.1:11434"
ollama_client = Client(host=OLLAMA_HOST)
VISION_MODEL = "qwen3-vl:30b-a3b-instruct" 
TEXT_MODEL = "llama3:70b"

SAM_CHECKPOINT = "sam_vit_b_01ec64.pth"
SAM_MODEL_TYPE = "vit_b"
device = "cuda" if torch.cuda.is_available() else "cpu"

mask_generator = None 
JOBS = {} 

@asynccontextmanager
async def lifespan(app: FastAPI):
    global mask_generator
    if os.path.exists(SAM_CHECKPOINT):
        print(f"Loading SAM ({SAM_MODEL_TYPE}) on {device}...")
        try:
            sam = sam_model_registry[SAM_MODEL_TYPE](checkpoint=SAM_CHECKPOINT)
            sam.to(device=device)
            mask_generator = SamAutomaticMaskGenerator(
                model=sam, points_per_side=32, pred_iou_thresh=0.86,
                stability_score_thresh=0.90, min_mask_region_area=50
            )
            print("SAM Loaded.")
        except: print("SAM Load Failed.")
    yield
    JOBS.clear()

def sanitize_json(obj):
    if isinstance(obj, float):
        if np.isnan(obj) or np.isinf(obj): return 0.0
        return obj
    elif isinstance(obj, dict): return {k: sanitize_json(v) for k, v in obj.items()}
    elif isinstance(obj, list): return [sanitize_json(v) for v in obj]
    elif isinstance(obj, (np.int64, np.int32)): return int(obj)
    elif isinstance(obj, (np.float64, np.float32)):
        if np.isnan(obj) or np.isinf(obj): return 0.0
        return float(obj)
    return obj

# ==========================================
# [3] NV33: NV20 Logic Restored
# ==========================================
class ScienceProcessorNV33:
    
    # --- Universal Loader ---
    @staticmethod
    def read_universal_dataframe(content: bytes, filename: str) -> List[pd.DataFrame]:
        dfs = []
        fname = filename.lower()
        if fname.endswith(('.xlsx', '.xls')):
            try:
                xls = pd.ExcelFile(io.BytesIO(content))
                for s in xls.sheet_names:
                    # NV20 Style: Just read it, no fancy checks yet
                    df = pd.read_excel(xls, sheet_name=s, header=None)
                    dfs.append(df)
                if dfs: return dfs
            except: pass
        
        text_content = ""
        for enc in ['utf-8', 'cp949', 'latin1']:
            try: text_content = content.decode(enc); break
            except: continue
        
        if text_content:
            try:
                # NV20 Style: Try reading directly first (Smart Engine of Pandas)
                # If separator is tricky, try auto-detection
                if filename.endswith('.csv'):
                    df = pd.read_csv(io.StringIO(text_content), header=None)
                else:
                    df = pd.read_csv(io.StringIO(text_content), sep=None, engine='python', header=None)
                dfs.append(df)
            except: pass
        return dfs

    # --- [NV33 Core] NV20-Style Simple Block Detection ---
    @staticmethod
    def detect_structured_blocks(df: pd.DataFrame) -> List[pd.DataFrame]:
        # NV20 Logic: Don't try to split blocks horizontally or detect complex islands.
        # Just find where the numeric data starts and crop the header.
        
        if df.empty: return []
        
        # 1. Convert to numeric (coercing errors to NaN)
        df_num = df.apply(pd.to_numeric, errors='coerce')
        
        # 2. Find the first row that looks like DATA (e.g., has > 2 numbers)
        # This skips the 5-row header naturally.
        row_counts = df_num.notna().sum(axis=1)
        data_start_idx = -1
        
        for idx, count in row_counts.items():
            if count >= 2: # At least 2 columns have numbers
                data_start_idx = idx
                break
        
        if data_start_idx == -1: return [] # No data found
        
        # 3. Crop everything above
        df_body = df_num.iloc[data_start_idx:].reset_index(drop=True)
        
        # 4. Clean up columns (The only "modern" addition to NV20 logic)
        # Drop columns that are entirely NaN
        df_body = df_body.dropna(how='all', axis=1)
        
        # Drop columns that are effectively empty (all zeros) IF they are artifacts
        # But be careful not to drop real 0 data. 
        # Safest bet: Just drop NaNs. The user said "No empty rows/cols in data".
        
        if df_body.shape[0] > 5:
            return [df_body] # Return as a single block
        
        return []

    @staticmethod
    def split_horizontal_blocks(df: pd.DataFrame) -> List[pd.DataFrame]:
        # NV20 didn't split horizontally. It treated the sheet as one table.
        return [df]

    # --- Extract Series (NV20 Style + X Sort) ---
    @staticmethod
    def extract_series_from_df(df: pd.DataFrame) -> List[Dict]:
        series_list = []
        try:
            vals = df.values
            rows, cols = vals.shape
            if rows < 5: return []
            
            # NV20 Logic: First column is X, rest are Y. Simple.
            current_x = vals[:, 0]
            
            # Modern addition: Sort X if it's descending (User request)
            # This doesn't break NV20 logic, just improves display.
            # But we must check if X is valid numbers
            if np.isnan(current_x).all():
                current_x = np.arange(rows)
            
            start_col = 1
            valid_idx = 1
            
            for i in range(start_col, cols):
                col_data = vals[:, i]
                mask = ~np.isnan(current_x) & ~np.isnan(col_data)
                
                if np.sum(mask) < 5: continue
                
                # Check for "Straight Line" artifacts (Standard NV20 didn't do this, 
                # but it's necessary for the "9 graphs" issue)
                # If a column is ALL Zeros, and it wasn't in the original 5 columns, it's an artifact.
                # However, user said "I have no empty cols". 
                # So if we see a column of zeros, it's likely a parsing artifact of ",,"
                if np.nanstd(col_data) == 0: continue 

                x_valid = current_x[mask]
                y_valid = col_data[mask]
                
                # Sort for plotting
                sort_idx = np.argsort(x_valid)
                
                series_list.append({
                    "x": x_valid[sort_idx], 
                    "y": y_valid[sort_idx], 
                    "name": f"Col-{valid_idx}" 
                })
                valid_idx += 1
        except: pass
        return series_list

    # --- Spectrum Logic (NV20 + NV32 Conservative) ---
    @staticmethod
    def simple_baseline(y):
        try:
            w = max(5, len(y)//10)
            s = pd.Series(y).rolling(window=w, center=True).min()
            b = s.bfill().ffill().values 
            if len(b) > 51: b = savgol_filter(b, 51, 3)
            return b, w
        except: return np.zeros_like(y), 0

    @staticmethod
    def process_spectrum(x, y, mode, goal):
        try:
            y_raw = y.copy(); y_base = np.zeros_like(y); y_proc = y.copy()
            
            if mode == "None":
                peaks, _ = find_peaks(y, height=np.max(y)*0.05)
                return {"x": x, "y_raw": y_raw, "y_proc": y_raw, "y_base": y_base, "peaks": [{"x": float(x[p]), "y": float(y[p])} for p in peaks], "log": ["Mode: Raw"], "stats": f"Raw Peaks: {len(peaks)}"}
            
            # NV20 mostly just plotted. NV32 added "Safe Auto".
            # We keep Safe Auto.
            elif mode == "Auto" or mode == "AI-Adaptive":
                win = 5
                if "base" in goal.lower():
                    base, bw = ScienceProcessorNV33.simple_baseline(y)
                    y_proc = np.maximum(y - base, 0); y_base = base
                
                if len(y_proc) > win: 
                    y_proc = savgol_filter(y_proc, win, 3)
                
                peaks, _ = find_peaks(y_proc, height=np.max(y_proc)*0.05, distance=10)
                return {"x": x, "y_raw": y_raw, "y_proc": y_proc, "y_base": y_base, "peaks": [{"x": float(x[p]), "y": float(y_proc[p])} for p in peaks], "log": [f"Mode: {mode}"], "stats": f"Proc Peaks: {len(peaks)}"}
            else:
                return {"x": x, "y_raw": y, "y_proc": y, "y_base": y_base, "peaks": [], "log": ["Unknown"], "stats": ""}
        except Exception as e: return {"x": x, "y_raw": y, "y_proc": y, "y_base": np.zeros_like(y), "peaks": [], "log": [str(e)], "stats": "Error"}

    # --- Footer ---
    @staticmethod
    def separate_footer(img_bgr):
        h, w = img_bgr.shape[:2]; roi_h = int(h * 0.82); roi = img_bgr[roi_h:, :]
        gray = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)
        sobel_y = np.abs(cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=3))
        row_sums = np.sum(sobel_y, axis=1)
        split_idx = np.argmax(row_sums)
        if row_sums[split_idx] > w * 30: return img_bgr[:roi_h + split_idx, :], img_bgr[roi_h + split_idx:, :]
        return img_bgr, None

    # --- [E] FFT-NMF (NV26) ---
    @staticmethod
    def analyze_crystal_fft_nmf(img_bgr, patch_size=128, step=16, n_components=10):
        log = []; gray = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2GRAY); h, w = gray.shape
        log.append(f"FFT-NMF: Patch={patch_size}, Step={step}, K={n_components}")
        custom_colors = [(0,0,255),(0,255,0),(255,0,0),(0,255,255),(255,255,0),(255,0,255),(0,165,255),(238,130,238),(128,128,0),(19,69,139)]
        while len(custom_colors) < n_components: custom_colors.append((255,255,255))
        cols = []; y_steps = list(range(0, h-patch_size+1, step)); x_steps = list(range(0, w-patch_size+1, step))
        grid_h = len(y_steps); grid_w = len(x_steps)
        cy, cx = patch_size//2, patch_size//2
        Y, X = np.ogrid[:patch_size, :patch_size]; dc_mask = ((X-cx)**2 + (Y-cy)**2) <= 4**2
        for y in y_steps:
            for x in x_steps:
                roi = gray[y:y+patch_size, x:x+patch_size]
                fshift = np.fft.fftshift(np.fft.fft2(roi))
                mag = np.abs(fshift); mag[dc_mask] = 0; cols.append(mag.flatten())
        if not cols: return {"type":"error", "msg":"Img too small"}
        try:
            V = np.array(cols).T.astype(np.float32); V = V - V.min()
            if V.max() > 0: V = V/V.max()
            nmf = NMF(n_components=n_components, init='nndsvda', random_state=42, max_iter=2000)
            W = nmf.fit_transform(V); H = nmf.components_
            if H.shape[1] != grid_h*grid_w: raise ValueError("Shape Mismatch")
            idx_H = np.argmax(H, axis=0)
            map_high = cv2.resize(idx_H.reshape(grid_h, grid_w).astype(np.uint8), (w, h), interpolation=cv2.INTER_NEAREST)
            overlay = img_bgr.copy(); mask_layer = np.zeros_like(img_bgr)
            counts = Counter(idx_H); total = len(idx_H); stats = {}
            for i in range(n_components):
                mask = (map_high == i); mask_layer[mask] = custom_colors[i]
                cnt = counts.get(i, 0); stats[f"Pattern {i}"] = f"{cnt} patches ({cnt/total*100:.1f}%)"
            mask_idx = np.any(mask_layer > 0, axis=-1)
            overlay[mask_idx] = (overlay[mask_idx]*(1-0.4) + mask_layer[mask_idx]*0.4).astype(np.uint8)
            basis_combo_list = []
            for k in range(n_components):
                pat = W[:, k].reshape(patch_size, patch_size)
                pat_vis = cv2.applyColorMap(cv2.resize(cv2.normalize(np.log1p(pat*1000),None,0,255,cv2.NORM_MINMAX).astype(np.uint8),(128,128),interpolation=cv2.INTER_NEAREST), cv2.COLORMAP_MAGMA)
                cv2.putText(pat_vis, "W (Basis)", (5, 15), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255,255,255), 1)
                h_vec = H[k, :]; h_vis = cv2.applyColorMap(cv2.resize(cv2.normalize(h_vec.reshape(grid_h,grid_w),None,0,255,cv2.NORM_MINMAX).astype(np.uint8),(128,128),interpolation=cv2.INTER_NEAREST), cv2.COLORMAP_VIRIDIS)
                cv2.putText(h_vis, "H (Dist)", (5, 15), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255,255,255), 1)
                cbar_vis = cv2.applyColorMap(np.tile(np.linspace(0,255,128).astype(np.uint8),(25,1)), cv2.COLORMAP_VIRIDIS)
                cv2.putText(cbar_vis, f"{h_vec.min():.1f}", (2,17), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255,255,255), 1)
                cv2.putText(cbar_vis, f"{h_vec.max():.1f}", (128-40,17), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (0,0,0), 1)
                legend = np.zeros((30, 128, 3), dtype=np.uint8); legend[:] = custom_colors[k]
                cv2.putText(legend, f"P{k}", (50, 22), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255,255,255), 2)
                combo = np.vstack([pat_vis, h_vis, cbar_vis, legend])
                cv2.rectangle(combo, (0,0), (127, combo.shape[0]-1), custom_colors[k], 2)
                basis_combo_list.append(combo)
            full_strip_large = np.hstack(basis_combo_list)
            def to_b64(im): return base64.b64encode(cv2.imencode('.jpg', im)[1]).decode('utf-8')
            return {"type":"image", "raw_b64":to_b64(img_bgr), "proc_b64":to_b64(overlay), "footer_b64":to_b64(full_strip_large), "stats":stats, "log":log}
        except Exception as e: return {"type":"image", "raw_b64":base64.b64encode(cv2.imencode('.jpg', img_bgr)[1]).decode('utf-8'), "proc_b64":base64.b64encode(cv2.imencode('.jpg', img_bgr)[1]).decode('utf-8'), "stats":{"Error":str(e)}, "log":log}

    # --- [F] Diffraction ---
    @staticmethod
    def analyze_diffraction_nv7(img_bgr, user_center=None):
        log = []; stats = {}
        gray = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2GRAY)
        if user_center and len(user_center)==2 and user_center[0]>0:
            cx, cy = user_center
            log.append(f"Center: Manual [{cx}, {cy}]")
        else:
            _, th = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY+cv2.THRESH_OTSU)
            cyf, cxf = center_of_mass(th)
            if np.isnan(cyf): cyf, cxf = gray.shape[0]//2, gray.shape[1]//2
            cx, cy = int(cxf), int(cyf)
            log.append(f"Center: Auto [{cx}, {cy}]")

        g1 = cv2.GaussianBlur(gray, (3,3), 0)
        g2 = cv2.GaussianBlur(gray, (51,51), 0)
        diff = cv2.subtract(g1, g2)
        coords = peak_local_max(diff, min_distance=10, threshold_abs=20)
        enhanced = np.log1p(diff.astype(np.float32))
        enhanced = cv2.normalize(enhanced, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)
        vis = cv2.cvtColor(enhanced, cv2.COLOR_GRAY2BGR)
        cv2.drawMarker(vis, (cx, cy), (0,0,255), cv2.MARKER_CROSS, 20, 2)
        peak_list = []
        for p in coords:
            py, px = p
            if abs(py-cy)<5 and abs(px-cx)<5: continue
            cv2.circle(vis, (px, py), 3, (0,255,0), 1)
            peak_list.append(f"[{px},{py}]")
        log.append(f"Spots: {len(peak_list)}")
        def to_b64(im): return base64.b64encode(cv2.imencode('.jpg', im)[1]).decode('utf-8')
        return {"type": "image", "raw_b64": to_b64(img_bgr), "proc_b64": to_b64(vis), "stats": {"center": [cx, cy], "spots": len(peak_list)}, "log": log}

    # --- [G] SAM ---
    @staticmethod
    def analyze_sam_results_nv7(img_rgb, masks, data_type):
        overlay = img_rgb.copy()
        mask_overlay = np.zeros_like(img_rgb)
        h_img, w_img = img_rgb.shape[:2]
        stats = {"count": 0}
        log = []
        valid_masks = []
        for ann in masks:
            area = ann['area']; bbox = ann['bbox']; x, y, w, h = [int(v) for v in bbox]
            if area > h_img * w_img * 0.95 or area < 20: continue 
            if data_type == "Particle":
                if x<=1 or y<=1 or (x+w)>=w_img-1 or (y+h)>=h_img-1: continue
                mask_uint8 = (ann['segmentation'] * 255).astype(np.uint8)
                cnts, _ = cv2.findContours(mask_uint8, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
                if cnts:
                    perim = cv2.arcLength(cnts[0], True)
                    circ = 4*np.pi*area/(perim**2) if perim>0 else 0
                    if circ < 0.25: continue
            valid_masks.append(ann)

        if data_type == "Thin Film":
            sorted_masks = sorted(valid_masks, key=lambda x: x['bbox'][1])
        else:
            sorted_masks = sorted(valid_masks, key=lambda x: x['area'], reverse=True)

        thickness_vals = []; diameters = []
        for i, ann in enumerate(sorted_masks):
            m = ann['segmentation']
            bbox = ann['bbox']; x, y, w, h = [int(v) for v in bbox]
            cx, cy = int(x + w/2), int(y + h/2)
            color = (0, 255, 0) 
            if data_type == "Thin Film":
                is_bound = (i == 0 or i == len(sorted_masks)-1)
                cv2.putText(overlay, f"#{i+1}", (cx, cy), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (255, 255, 255), 3)
                if not is_bound:
                    thk = ann['area'] / w
                    thickness_vals.append(thk)
                    color = (255, 0, 0) 
                else: color = (100, 100, 100)
                if i < len(sorted_masks) - 1:
                    ys, xs = np.where(m)
                    if len(xs) > 10:
                        roughness = np.std(ys)
                        cv2.line(overlay, (x, y+h), (x+w, y+h), (0, 255, 255), 2)
                        cv2.putText(overlay, f"Int#{i+1} R:{roughness:.2f}", (x+10, y+h-5), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)
            elif data_type == "Particle":
                equivalent_diameter = 2 * np.sqrt(ann['area'] / np.pi)
                diameters.append(equivalent_diameter)
            mask_overlay[m] = color
            mask_uint8 = (m * 255).astype(np.uint8)
            cnts, _ = cv2.findContours(mask_uint8, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
            if cnts: cv2.drawContours(overlay, [cnts[0]], -1, color, 1)

        alpha = 0.35
        mask_idx = np.any(mask_overlay > 0, axis=-1)
        overlay[mask_idx] = (overlay[mask_idx]*(1-alpha) + mask_overlay[mask_idx]*alpha).astype(np.uint8)
        stats["count"] = len(sorted_masks)
        if data_type == "Thin Film":
            avg = round(np.mean(thickness_vals), 2) if thickness_vals else 0
            stats["thickness_avg"] = avg
            log.append(f"Layers: {len(sorted_masks)}, Avg Thk: {avg}")
        elif data_type == "Particle":
            avg = round(np.mean(diameters), 2) if diameters else 0
            stats["diameter_avg"] = avg
            log.append(f"Particles: {len(sorted_masks)}")
        return overlay, stats, log

    # --- Router ---
    @staticmethod
    def process_image_nv33(img_bytes, equipment, data_type, mode, goal, user_center=None, patch_size=128, step=16, n_components=10):
        nparr = np.frombuffer(img_bytes, np.uint8); img_raw = cv2.imdecode(nparr, cv2.IMREAD_COLOR)
        if img_raw is None: return {}
        body, footer = ScienceProcessorNV33.separate_footer(img_raw)
        footer_b64 = base64.b64encode(cv2.imencode('.jpg', footer)[1]).decode('utf-8') if footer is not None else None
        
        if data_type == "Crystal Structure (FFT)":
            return ScienceProcessorNV33.analyze_crystal_fft_nmf(body, patch_size, step, n_components)
        if data_type == "2D Diffraction":
            res = ScienceProcessorNV33.analyze_diffraction_nv7(body, user_center)
            return {**res, "footer_b64": footer_b64}
        
        if mode == "None":
             def to_b64(im): return base64.b64encode(cv2.imencode('.jpg', im)[1]).decode('utf-8')
             return {"type":"image", "raw_b64":to_b64(body), "proc_b64":to_b64(body), "stats":{}, "log":["Mode: Raw"], "footer_b64": footer_b64}

        img_proc = body.copy(); log = []
        if mode == "Auto" or mode == "AI-Adaptive": img_proc = cv2.bilateralFilter(img_proc, 9, 75, 75)
        img_rgb = cv2.cvtColor(img_proc, cv2.COLOR_BGR2RGB)
        if mask_generator:
            try:
                masks = mask_generator.generate(img_rgb)
                overlay, stats, sam_log = ScienceProcessorNV33.analyze_sam_results_nv7(img_rgb, masks, data_type)
                log.extend(sam_log); overlay = cv2.cvtColor(overlay, cv2.COLOR_RGB2BGR)
            except Exception as e: log.append(f"SAM Error: {str(e)}"); overlay = body; stats = {}
        else: log.append("SAM Not Loaded"); overlay = body; stats = {}
        def to_b64(im): return base64.b64encode(cv2.imencode('.jpg', im)[1]).decode('utf-8')
        return {"type": "image", "raw_b64": to_b64(body), "proc_b64": to_b64(overlay), "stats": stats, "log": log, "footer_b64": footer_b64}

# [4] Helper (Same)
async def analyze_vision_ollama(image_bytes: bytes, prompt: str) -> str:
    try:
        b64 = base64.b64encode(image_bytes).decode('utf-8')
        res = await asyncio.to_thread(ollama_client.chat, model=VISION_MODEL, messages=[{'role':'user','content':prompt,'images':[b64]}])
        return res['message']['content']
    except: return "Vision Error"

async def interpret_spectrum_data(raw_context: str, equipment: str) -> str:
    try:
        prompt = f"Analyze spectrum ({equipment}). Brief interpretation in Korean. Data: {raw_context}"
        res = await asyncio.to_thread(ollama_client.chat, model=TEXT_MODEL, messages=[{'role':'user','content':prompt}])
        return res['message']['content']
    except: return ""

# [5] App
app = FastAPI(title="Analyst NV33 Restored", lifespan=lifespan)
app.add_middleware(CORSMiddleware, allow_origins=["*"], allow_methods=["*"], allow_headers=["*"])

class TranslateReq(BaseModel): text: str
@app.post("/api/translate")
async def translate_text(req: TranslateReq):
    try:
        res = await asyncio.to_thread(ollama_client.chat, model=TEXT_MODEL, messages=[{'role':'system','content':'Translate to English.'}, {'role':'user','content':req.text}])
        return {"translated": res['message']['content']}
    except Exception as e: return {"translated": str(e)}

async def process_analysis_job(job_id: str, files_map: Dict, configs: Dict):
    try:
        JOBS[job_id]["status"] = "Processing"; JOBS[job_id]["progress"] = 10; final_results = []; total = len(configs); cnt = 0
        for filename, config in configs.items():
            if filename not in files_map: continue
            content = files_map[filename]; eq = config.get("equipment", "General"); data_type = config.get("data_type", "General")
            goal = config.get("goal", ""); mode = config.get("mode", "Auto")
            p_size = int(config.get("patch_size", 128)); p_step = int(config.get("step", 16)); n_comp = int(config.get("n_components", 10))
            fname_lower = filename.lower(); JOBS[job_id]["step"] = f"Analyzing {filename}..."
            
            if eq == "Document" or fname_lower.endswith(('.pdf', '.ppt', '.pptx')):
                try:
                    if fname_lower.endswith('.pdf'):
                        pdf = pdfium.PdfDocument(io.BytesIO(content)); n_pages = len(pdf); full_txt = ""; pages = []
                        for i in range(min(n_pages, 10)):
                            page = pdf[i]; bitmap = page.render(scale=2); pil_image = bitmap.to_pil()
                            buf = io.BytesIO(); pil_image.save(buf, format="JPEG"); b64 = base64.b64encode(buf.getvalue()).decode()
                            desc = await asyncio.to_thread(ollama_client.chat, model=VISION_MODEL, messages=[{'role':'user','content':'한글 요약.','images':[b64]}])
                            d_txt = desc['message']['content']; full_txt += f"\nPage {i+1}: {d_txt}"
                            pages.append({"id": str(uuid.uuid4()), "page_num": i+1, "image_b64": b64, "desc": d_txt})
                        final_results.append({"type": "pdf", "filename": filename, "equipment": "Lit", "raw_context": full_txt, "pages": pages})
                    elif fname_lower.endswith(('.ppt', '.pptx')):
                        if fname_lower.endswith('.ppt'): final_results.append({"type":"error", "filename":filename, "msg":".ppt not supported"})
                        else:
                            prs = Presentation(io.BytesIO(content)); slides = []; full_txt = ""
                            for i, slide in enumerate(prs.slides):
                                txt = ""; imgs = []
                                for s in slide.shapes:
                                    if hasattr(s, "text"): txt += s.text + "\n"
                                    if s.shape_type == MSO_SHAPE_TYPE.PICTURE:
                                        try:
                                            ib64 = base64.b64encode(s.image.blob).decode('utf-8')
                                            imgs.append({"b64":ib64, "desc":"Img"})
                                        except: pass
                                if not txt.strip(): txt = "(No text)"
                                full_txt += f"Slide {i+1}: {txt}\n"; slides.append({"id": str(uuid.uuid4()), "slide_num": i+1, "text": txt, "images": imgs})
                            final_results.append({"type": "ppt", "filename": filename, "equipment": "Lit", "raw_context": full_txt, "slides": slides})
                except Exception as e: final_results.append({"type": "error", "filename": filename, "msg": str(e)})

            elif fname_lower.endswith(('.xlsx', '.xls', '.csv', '.txt')):
                try:
                    dfs = ScienceProcessorNV33.read_universal_dataframe(content, filename)
                    for i, df in enumerate(dfs):
                        v_blocks = ScienceProcessorNV33.detect_structured_blocks(df)
                        for v_block in v_blocks:
                            h_blocks = ScienceProcessorNV33.split_horizontal_blocks(v_block)
                            for h_block in h_blocks:
                                series_list = ScienceProcessorNV33.extract_series_from_df(h_block)
                                for s in series_list:
                                    res = ScienceProcessorNV33.process_spectrum(s['x'], s['y'], mode, goal)
                                    step = max(1, len(s['x'])//100)
                                    chart = [{"x": float(res["x"][k]), "y_proc": float(res["y_proc"][k]), "y_raw": float(res["y_raw"][k]), "y_base": float(res["y_base"][k])} for k in range(0, len(s['x']), step)]
                                    ctx = f"Data: {s['name']}\nStats: {res['stats']}"
                                    interp = await interpret_spectrum_data(ctx, eq)
                                    final_results.append({
                                        "type": "spectrum", "filename": f"{filename} ({s['name']})", "equipment": eq,
                                        "raw_context": f"{ctx}\nInterp: {interp}", 
                                        "chart_data": chart, "log": res["log"], "interpretation": interp
                                    })
                except Exception as ex: final_results.append({"type": "error", "filename": filename, "msg": str(ex)})

            elif fname_lower.endswith(('.png', '.jpg', '.jpeg', '.tif', '.bmp')):
                try:
                    vis_res = await asyncio.to_thread(ScienceProcessorNV33.process_image_nv33, content, eq, data_type, mode, goal, None, p_size, p_step, n_comp)
                    if vis_res:
                        scale_info = "Unknown"
                        final_footer = vis_res.get("footer_b64")
                        if final_footer and data_type != "Crystal Structure (FFT)":
                            scale_res = await asyncio.to_thread(ollama_client.chat, model=VISION_MODEL, messages=[{'role':'user','content':'Read scale bar text only.','images':[final_footer]}])
                            scale_info = scale_res['message']['content']; vis_res["log"].append(f"Scale: {scale_info}")
                        
                        stats_clean = "\n".join([f"- {k}: {v}" for k,v in vis_res.get('stats',{}).items()])
                        prompt = f"Analyze {eq} image. Stats: {stats_clean}. Goal: {goal}. Summary in Korean."
                        desc_res = await asyncio.to_thread(ollama_client.chat, model=VISION_MODEL, messages=[{'role':'user','content':prompt,'images':[vis_res["proc_b64"]]}])
                        desc = desc_res['message']['content']
                        final_results.append({"type":"image", "filename":filename, "equipment":eq, "summary":desc, "raw_context":f"Img: {desc}\nStats:{stats_clean}", "raw_b64":vis_res["raw_b64"], "proc_b64":vis_res["proc_b64"], "footer_b64":final_footer, "stats":vis_res.get("stats",{}), "log":vis_res.get("log",[])})
                except Exception as ex: final_results.append({"type":"error", "filename":filename, "msg":str(ex)})
            
            cnt += 1; JOBS[job_id]["progress"] = 10 + int((cnt/total)*80)

        JOBS[job_id]["step"] = "Writing Report..."; data_ctx = ""
        for r in final_results:
            if r.get("type")!="error": data_ctx += f"\n=== {r['filename']} ===\n{r.get('raw_context','')[:2000]}\n"
        
        rep_res = await asyncio.to_thread(ollama_client.chat, model=TEXT_MODEL, messages=[{'role':'system','content':'Write scientific report in Korean.'}, {'role':'user','content':f"Data:\n{data_ctx}"}])
        JOBS[job_id]["results"] = {"results": final_results, "final_report": rep_res['message']['content']}
        JOBS[job_id]["status"] = "Completed"; JOBS[job_id]["progress"] = 100
    except Exception as e: JOBS[job_id]["status"] = "Failed"; JOBS[job_id]["error"] = str(e)

@app.post("/api/analyze")
async def start_analysis(background_tasks: BackgroundTasks, files: List[UploadFile] = File(...), file_configs: str = Form(...)):
    job_id = str(uuid.uuid4()); configs = json.loads(file_configs)
    files_map = {f.filename: await f.read() for f in files}
    JOBS[job_id] = {"status": "Pending", "progress": 0, "step": "Init", "results": None}
    background_tasks.add_task(process_analysis_job, job_id, files_map, configs)
    return {"job_id": job_id}

@app.get("/api/status/{job_id}")
async def get_status(job_id: str): return JOBS.get(job_id, {"status": "Not Found"})
@app.get("/", response_class=HTMLResponse)
async def serve_index(): return FileResponse("index.html") if os.path.exists("index.html") else "<h1>NV33 Backend</h1>"

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)





NV32 Python COde

import os
import io
# [0] OMP Error Fix
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"
import torch

import asyncio
import base64
import json
import re
import math
import uuid
import time
from typing import List, Dict, Any, Optional
from pydantic import BaseModel
from contextlib import asynccontextmanager
from collections import Counter

# [1] Security
os.environ['NO_PROXY'] = '127.0.0.1,localhost,0.0.0.0,::1'
if 'HTTP_PROXY' in os.environ: del os.environ['HTTP_PROXY']
if 'HTTPS_PROXY' in os.environ: del os.environ['HTTPS_PROXY']

from fastapi import FastAPI, UploadFile, File, Form, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import HTMLResponse, FileResponse
import pandas as pd
import numpy as np

# SciPy & CV & Sklearn
from sklearn.decomposition import NMF
from scipy.signal import find_peaks, savgol_filter
from scipy.optimize import curve_fit
from scipy.ndimage import center_of_mass
from skimage.feature import peak_local_max
import cv2
import matplotlib
import matplotlib.pyplot as plt 

# Models
import pypdfium2 as pdfium 
from segment_anything import sam_model_registry, SamAutomaticMaskGenerator
from ollama import Client 
from pptx import Presentation
from pptx.enum.shapes import MSO_SHAPE_TYPE

# [2] Config
OLLAMA_HOST = "http://127.0.0.1:11434"
ollama_client = Client(host=OLLAMA_HOST)
VISION_MODEL = "qwen3-vl:30b-a3b-instruct" 
TEXT_MODEL = "llama3:70b"

SAM_CHECKPOINT = "sam_vit_b_01ec64.pth"
SAM_MODEL_TYPE = "vit_b"
device = "cuda" if torch.cuda.is_available() else "cpu"

mask_generator = None 
JOBS = {} 

@asynccontextmanager
async def lifespan(app: FastAPI):
    global mask_generator
    if os.path.exists(SAM_CHECKPOINT):
        print(f"Loading SAM ({SAM_MODEL_TYPE}) on {device}...")
        try:
            sam = sam_model_registry[SAM_MODEL_TYPE](checkpoint=SAM_CHECKPOINT)
            sam.to(device=device)
            mask_generator = SamAutomaticMaskGenerator(
                model=sam, points_per_side=32, pred_iou_thresh=0.86,
                stability_score_thresh=0.90, min_mask_region_area=50
            )
            print("SAM Loaded.")
        except: print("SAM Load Failed.")
    yield
    JOBS.clear()

def sanitize_json(obj):
    if isinstance(obj, float):
        if np.isnan(obj) or np.isinf(obj): return 0.0
        return obj
    elif isinstance(obj, dict): return {k: sanitize_json(v) for k, v in obj.items()}
    elif isinstance(obj, list): return [sanitize_json(v) for v in obj]
    elif isinstance(obj, (np.int64, np.int32)): return int(obj)
    elif isinstance(obj, (np.float64, np.float32)):
        if np.isnan(obj) or np.isinf(obj): return 0.0
        return float(obj)
    return obj

# ==========================================
# [3] NV32 Intelligent Cleaner Engine
# ==========================================
class ScienceProcessorNV32:
    
    # --- Universal Loader ---
    @staticmethod
    def read_universal_dataframe(content: bytes, filename: str) -> List[pd.DataFrame]:
        dfs = []
        fname = filename.lower()
        if fname.endswith(('.xlsx', '.xls')):
            try:
                xls = pd.ExcelFile(io.BytesIO(content))
                for s in xls.sheet_names:
                    df = pd.read_excel(xls, sheet_name=s, header=None)
                    if df.apply(pd.to_numeric, errors='coerce').notna().sum().sum() > 5: dfs.append(df)
                if dfs: return dfs
            except: pass
        text_content = ""
        for enc in ['utf-8', 'cp949', 'latin1']:
            try: text_content = content.decode(enc); break
            except: continue
        if text_content:
            lines = text_content.splitlines()
            data_start = -1; delimiter = None
            separators = [',', '\t', ';', '\s+']
            for i, line in enumerate(lines[:100]):
                for sep in separators:
                    parts = line.split() if sep=='\s+' else line.split(sep)
                    nums = sum(1 for p in parts if re.match(r'^-?\d+(\.\d+)?(?:[eE][+-]?\d+)?$', p.strip()))
                    if nums >= 2: data_start = i; delimiter = sep; break
                if data_start != -1: break
            if data_start != -1:
                try:
                    data_str = "\n".join(lines[data_start:])
                    sep_arg = '\s+' if delimiter=='\s+' else delimiter
                    df = pd.read_csv(io.StringIO(data_str), sep=sep_arg, header=None, engine='python')
                    dfs.append(df)
                except: pass
        return dfs

    # --- Block Parser ---
    @staticmethod
    def detect_structured_blocks(df: pd.DataFrame) -> List[pd.DataFrame]:
        blocks = []
        if df.empty: return []
        df_num = df.apply(pd.to_numeric, errors='coerce')
        if df.shape[1] > 0: is_data_row = df_num.iloc[:, 0].notna()
        else: is_data_row = df_num.notna().any(axis=1)
        groups = (is_data_row != is_data_row.shift()).cumsum()
        for _, group in df.groupby(groups):
            if is_data_row.loc[group.index[0]]:
                block = df_num.loc[group.index].dropna(how='all', axis=1)
                if block.shape[0] >= 5: blocks.append(block.reset_index(drop=True))
        return blocks

    @staticmethod
    def split_horizontal_blocks(df: pd.DataFrame) -> List[pd.DataFrame]:
        sub_blocks = []
        if df.empty: return []
        col_counts = df.apply(pd.to_numeric, errors='coerce').notna().sum()
        valid = np.where(col_counts >= 2)[0]
        if len(valid) == 0: return []
        splits = np.where(np.diff(valid) > 1)[0] + 1
        groups = np.split(valid, splits)
        for g in groups:
            if len(g) > 0:
                sub = df.iloc[:, g].reset_index(drop=True)
                if sub.shape[1] >= 1 and sub.shape[0] >= 2: sub_blocks.append(sub)
        return sub_blocks

    # [NV32 Core Fix] Signal-Aware Extraction
    @staticmethod
    def extract_series_from_df(df: pd.DataFrame) -> List[Dict]:
        series_list = []
        try:
            vals = df.values; rows, cols = vals.shape
            if rows < 5: return []
            
            # Helper: Is this column monotonic? (Index Candidate)
            def is_axis(arr):
                arr = arr[~np.isnan(arr)]
                if len(arr) < 5: return False
                diff = np.diff(arr)
                if np.all(diff >= 0) or np.all(diff <= 0): return True
                return (np.sum(diff>0)/len(diff)>0.9) or (np.sum(diff<0)/len(diff)>0.9)

            # 1. Determine X-axis
            col0 = vals[:, 0]
            if is_axis(col0):
                current_x = col0; start_col = 1
            else:
                current_x = np.arange(rows); start_col = 0

            valid_idx = 1
            for i in range(start_col, cols):
                col_data = vals[:, i]
                
                # Check 1: Enough valid data?
                mask = ~np.isnan(current_x) & ~np.isnan(col_data)
                if np.sum(mask) < 5: continue
                
                y_data = col_data[mask]
                
                # [NV32 Fix] "Straight Line" Removal
                # If standard deviation is 0, it means value is constant (straight line).
                # This filters out empty/dummy columns that were parsed as 0.
                if np.std(y_data) == 0: continue 

                # [NV32 Fix] Sort by X (Ascending)
                # Fixes the issue where data comes in reverse order (1000->6)
                x_data = current_x[mask]
                sort_idx = np.argsort(x_data)
                x_sorted = x_data[sort_idx]
                y_sorted = y_data[sort_idx]

                series_list.append({
                    "x": x_sorted, 
                    "y": y_sorted, 
                    "name": f"Col-{valid_idx}" 
                })
                valid_idx += 1
        except: pass
        return series_list

    # --- Spectrum Logic ---
    @staticmethod
    def simple_baseline(y):
        try:
            w = max(5, len(y)//10)
            s = pd.Series(y).rolling(window=w, center=True).min()
            b = s.bfill().ffill().values 
            if len(b) > 51: b = savgol_filter(b, 51, 3)
            return b, w
        except: return np.zeros_like(y), 0

    # [NV32 Fix] Conservative Filter for CSV
    @staticmethod
    def process_spectrum(x, y, mode, goal):
        try:
            y_raw = y.copy(); y_base = np.zeros_like(y); y_proc = y.copy()
            
            # Raw Mode
            if mode == "None":
                peaks, _ = find_peaks(y, height=np.max(y)*0.05)
                return {"x": x, "y_raw": y_raw, "y_proc": y_raw, "y_base": y_base, "peaks": [{"x": float(x[p]), "y": float(y[p])} for p in peaks], "log": ["Mode: Raw"], "stats": f"Raw Peaks: {len(peaks)}"}
            
            # Auto Mode
            elif mode == "Auto" or mode == "AI-Adaptive":
                # [NV32] Minimal smoothing, NO baseline removal by default
                win = 5
                
                # Only remove baseline if explicitly requested (not default for CSV)
                if "base" in goal.lower():
                    base, bw = ScienceProcessorNV32.simple_baseline(y)
                    y_proc = np.maximum(y - base, 0)
                    y_base = base
                
                if len(y_proc) > win: 
                    y_proc = savgol_filter(y_proc, win, 3)
                
                peaks, _ = find_peaks(y_proc, height=np.max(y_proc)*0.05, distance=10)
                return {"x": x, "y_raw": y_raw, "y_proc": y_proc, "y_base": y_base, "peaks": [{"x": float(x[p]), "y": float(y_proc[p])} for p in peaks], "log": [f"Mode: {mode}", "Minimal Filter"], "stats": f"Proc Peaks: {len(peaks)}"}
            else:
                return {"x": x, "y_raw": y, "y_proc": y, "y_base": y_base, "peaks": [], "log": ["Unknown"], "stats": ""}
        except Exception as e: return {"x": x, "y_raw": y, "y_proc": y, "y_base": np.zeros_like(y), "peaks": [], "log": [str(e)], "stats": "Error"}

    # --- Footer ---
    @staticmethod
    def separate_footer(img_bgr):
        h, w = img_bgr.shape[:2]; roi_h = int(h * 0.82); roi = img_bgr[roi_h:, :]
        gray = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)
        sobel_y = np.abs(cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=3))
        row_sums = np.sum(sobel_y, axis=1)
        split_idx = np.argmax(row_sums)
        if row_sums[split_idx] > w * 30: return img_bgr[:roi_h + split_idx, :], img_bgr[roi_h + split_idx:, :]
        return img_bgr, None

    # --- [E] FFT-NMF (NV26 4-Stack) ---
    @staticmethod
    def analyze_crystal_fft_nmf(img_bgr, patch_size=128, step=16, n_components=10):
        log = []; gray = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2GRAY); h, w = gray.shape
        log.append(f"FFT-NMF: Patch={patch_size}, Step={step}, K={n_components}")
        custom_colors = [(0,0,255),(0,255,0),(255,0,0),(0,255,255),(255,255,0),(255,0,255),(0,165,255),(238,130,238),(128,128,0),(19,69,139)]
        while len(custom_colors) < n_components: custom_colors.append((255,255,255))
        cols = []; y_steps = list(range(0, h-patch_size+1, step)); x_steps = list(range(0, w-patch_size+1, step))
        grid_h = len(y_steps); grid_w = len(x_steps)
        cy, cx = patch_size//2, patch_size//2
        Y, X = np.ogrid[:patch_size, :patch_size]; dc_mask = ((X-cx)**2 + (Y-cy)**2) <= 4**2
        for y in y_steps:
            for x in x_steps:
                roi = gray[y:y+patch_size, x:x+patch_size]
                fshift = np.fft.fftshift(np.fft.fft2(roi))
                mag = np.abs(fshift); mag[dc_mask] = 0; cols.append(mag.flatten())
        if not cols: return {"type":"error", "msg":"Img too small"}
        try:
            V = np.array(cols).T.astype(np.float32); V = V - V.min()
            if V.max() > 0: V = V/V.max()
            nmf = NMF(n_components=n_components, init='nndsvda', random_state=42, max_iter=2000)
            W = nmf.fit_transform(V); H = nmf.components_
            if H.shape[1] != grid_h*grid_w: raise ValueError("Shape Mismatch")
            idx_H = np.argmax(H, axis=0)
            map_high = cv2.resize(idx_H.reshape(grid_h, grid_w).astype(np.uint8), (w, h), interpolation=cv2.INTER_NEAREST)
            overlay = img_bgr.copy(); mask_layer = np.zeros_like(img_bgr)
            counts = Counter(idx_H); total = len(idx_H); stats = {}
            for i in range(n_components):
                mask = (map_high == i); mask_layer[mask] = custom_colors[i]
                cnt = counts.get(i, 0); stats[f"Pattern {i}"] = f"{cnt} patches ({cnt/total*100:.1f}%)"
            mask_idx = np.any(mask_layer > 0, axis=-1)
            overlay[mask_idx] = (overlay[mask_idx]*(1-0.4) + mask_layer[mask_idx]*0.4).astype(np.uint8)
            basis_combo_list = []
            for k in range(n_components):
                pat = W[:, k].reshape(patch_size, patch_size)
                pat_vis = cv2.applyColorMap(cv2.resize(cv2.normalize(np.log1p(pat*1000),None,0,255,cv2.NORM_MINMAX).astype(np.uint8),(128,128),interpolation=cv2.INTER_NEAREST), cv2.COLORMAP_MAGMA)
                cv2.putText(pat_vis, "W (Basis)", (5, 15), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255,255,255), 1)
                h_vec = H[k, :]; h_vis = cv2.applyColorMap(cv2.resize(cv2.normalize(h_vec.reshape(grid_h,grid_w),None,0,255,cv2.NORM_MINMAX).astype(np.uint8),(128,128),interpolation=cv2.INTER_NEAREST), cv2.COLORMAP_VIRIDIS)
                cv2.putText(h_vis, "H (Dist)", (5, 15), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255,255,255), 1)
                cbar_vis = cv2.applyColorMap(np.tile(np.linspace(0,255,128).astype(np.uint8),(25,1)), cv2.COLORMAP_VIRIDIS)
                cv2.putText(cbar_vis, f"{h_vec.min():.1f}", (2,17), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255,255,255), 1)
                cv2.putText(cbar_vis, f"{h_vec.max():.1f}", (128-40,17), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (0,0,0), 1)
                legend = np.zeros((30, 128, 3), dtype=np.uint8); legend[:] = custom_colors[k]
                cv2.putText(legend, f"P{k}", (50, 22), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255,255,255), 2)
                combo = np.vstack([pat_vis, h_vis, cbar_vis, legend])
                cv2.rectangle(combo, (0,0), (127, combo.shape[0]-1), custom_colors[k], 2)
                basis_combo_list.append(combo)
            full_strip_large = np.hstack(basis_combo_list)
            def to_b64(im): return base64.b64encode(cv2.imencode('.jpg', im)[1]).decode('utf-8')
            return {"type":"image", "raw_b64":to_b64(img_bgr), "proc_b64":to_b64(overlay), "footer_b64":to_b64(full_strip_large), "stats":stats, "log":log}
        except Exception as e: return {"type":"image", "raw_b64":base64.b64encode(cv2.imencode('.jpg', img_bgr)[1]).decode('utf-8'), "proc_b64":base64.b64encode(cv2.imencode('.jpg', img_bgr)[1]).decode('utf-8'), "stats":{"Error":str(e)}, "log":log}

    # --- [F] Diffraction ---
    @staticmethod
    def analyze_diffraction_nv7(img_bgr, user_center=None):
        log = []; stats = {}
        gray = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2GRAY)
        if user_center and len(user_center)==2 and user_center[0]>0:
            cx, cy = user_center
            log.append(f"Center: Manual [{cx}, {cy}]")
        else:
            _, th = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY+cv2.THRESH_OTSU)
            cyf, cxf = center_of_mass(th)
            if np.isnan(cyf): cyf, cxf = gray.shape[0]//2, gray.shape[1]//2
            cx, cy = int(cxf), int(cyf)
            log.append(f"Center: Auto [{cx}, {cy}]")

        g1 = cv2.GaussianBlur(gray, (3,3), 0)
        g2 = cv2.GaussianBlur(gray, (51,51), 0)
        diff = cv2.subtract(g1, g2)
        coords = peak_local_max(diff, min_distance=10, threshold_abs=20)
        enhanced = np.log1p(diff.astype(np.float32))
        enhanced = cv2.normalize(enhanced, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)
        vis = cv2.cvtColor(enhanced, cv2.COLOR_GRAY2BGR)
        cv2.drawMarker(vis, (cx, cy), (0,0,255), cv2.MARKER_CROSS, 20, 2)
        peak_list = []
        for p in coords:
            py, px = p
            if abs(py-cy)<5 and abs(px-cx)<5: continue
            cv2.circle(vis, (px, py), 3, (0,255,0), 1)
            peak_list.append(f"[{px},{py}]")
        log.append(f"Spots: {len(peak_list)}")
        def to_b64(im): return base64.b64encode(cv2.imencode('.jpg', im)[1]).decode('utf-8')
        return {"type": "image", "raw_b64": to_b64(img_bgr), "proc_b64": to_b64(vis), "stats": {"center": [cx, cy], "spots": len(peak_list)}, "log": log}

    # --- [G] SAM ---
    @staticmethod
    def analyze_sam_results_nv7(img_rgb, masks, data_type):
        overlay = img_rgb.copy()
        mask_overlay = np.zeros_like(img_rgb)
        h_img, w_img = img_rgb.shape[:2]
        stats = {"count": 0}
        log = []
        valid_masks = []
        for ann in masks:
            area = ann['area']; bbox = ann['bbox']; x, y, w, h = [int(v) for v in bbox]
            if area > h_img * w_img * 0.95 or area < 20: continue 
            if data_type == "Particle":
                if x<=1 or y<=1 or (x+w)>=w_img-1 or (y+h)>=h_img-1: continue
                mask_uint8 = (ann['segmentation'] * 255).astype(np.uint8)
                cnts, _ = cv2.findContours(mask_uint8, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
                if cnts:
                    perim = cv2.arcLength(cnts[0], True)
                    circ = 4*np.pi*area/(perim**2) if perim>0 else 0
                    if circ < 0.25: continue
            valid_masks.append(ann)

        if data_type == "Thin Film":
            sorted_masks = sorted(valid_masks, key=lambda x: x['bbox'][1])
        else:
            sorted_masks = sorted(valid_masks, key=lambda x: x['area'], reverse=True)

        thickness_vals = []; diameters = []
        for i, ann in enumerate(sorted_masks):
            m = ann['segmentation']
            bbox = ann['bbox']; x, y, w, h = [int(v) for v in bbox]
            cx, cy = int(x + w/2), int(y + h/2)
            color = (0, 255, 0) 
            if data_type == "Thin Film":
                is_bound = (i == 0 or i == len(sorted_masks)-1)
                cv2.putText(overlay, f"#{i+1}", (cx, cy), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (255, 255, 255), 3)
                if not is_bound:
                    thk = ann['area'] / w
                    thickness_vals.append(thk)
                    color = (255, 0, 0) 
                else: color = (100, 100, 100)
                if i < len(sorted_masks) - 1:
                    ys, xs = np.where(m)
                    if len(xs) > 10:
                        roughness = np.std(ys)
                        cv2.line(overlay, (x, y+h), (x+w, y+h), (0, 255, 255), 2)
                        cv2.putText(overlay, f"Int#{i+1} R:{roughness:.2f}", (x+10, y+h-5), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)
            elif data_type == "Particle":
                equivalent_diameter = 2 * np.sqrt(ann['area'] / np.pi)
                diameters.append(equivalent_diameter)
            mask_overlay[m] = color
            mask_uint8 = (m * 255).astype(np.uint8)
            cnts, _ = cv2.findContours(mask_uint8, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
            if cnts: cv2.drawContours(overlay, [cnts[0]], -1, color, 1)

        alpha = 0.35
        mask_idx = np.any(mask_overlay > 0, axis=-1)
        overlay[mask_idx] = (overlay[mask_idx]*(1-alpha) + mask_overlay[mask_idx]*alpha).astype(np.uint8)
        stats["count"] = len(sorted_masks)
        if data_type == "Thin Film":
            avg = round(np.mean(thickness_vals), 2) if thickness_vals else 0
            stats["thickness_avg"] = avg
            log.append(f"Layers: {len(sorted_masks)}, Avg Thk: {avg}")
        elif data_type == "Particle":
            avg = round(np.mean(diameters), 2) if diameters else 0
            stats["diameter_avg"] = avg
            log.append(f"Particles: {len(sorted_masks)}")
        return overlay, stats, log

    # --- Router ---
    @staticmethod
    def process_image_nv32(img_bytes, equipment, data_type, mode, goal, user_center=None, patch_size=128, step=16, n_components=10):
        nparr = np.frombuffer(img_bytes, np.uint8); img_raw = cv2.imdecode(nparr, cv2.IMREAD_COLOR)
        if img_raw is None: return {}
        body, footer = ScienceProcessorNV32.separate_footer(img_raw)
        footer_b64 = base64.b64encode(cv2.imencode('.jpg', footer)[1]).decode('utf-8') if footer is not None else None
        
        if data_type == "Crystal Structure (FFT)":
            return ScienceProcessorNV32.analyze_crystal_fft_nmf(body, patch_size, step, n_components)
        if data_type == "2D Diffraction":
            res = ScienceProcessorNV32.analyze_diffraction_nv7(body, user_center)
            return {**res, "footer_b64": footer_b64}
        
        if mode == "None":
             def to_b64(im): return base64.b64encode(cv2.imencode('.jpg', im)[1]).decode('utf-8')
             return {"type":"image", "raw_b64":to_b64(body), "proc_b64":to_b64(body), "stats":{}, "log":["Mode: Raw"], "footer_b64": footer_b64}

        img_proc = body.copy(); log = []
        if mode == "Auto" or mode == "AI-Adaptive": img_proc = cv2.bilateralFilter(img_proc, 9, 75, 75)
        img_rgb = cv2.cvtColor(img_proc, cv2.COLOR_BGR2RGB)
        if mask_generator:
            try:
                masks = mask_generator.generate(img_rgb)
                overlay, stats, sam_log = ScienceProcessorNV32.analyze_sam_results_nv7(img_rgb, masks, data_type)
                log.extend(sam_log); overlay = cv2.cvtColor(overlay, cv2.COLOR_RGB2BGR)
            except Exception as e: log.append(f"SAM Error: {str(e)}"); overlay = body; stats = {}
        else: log.append("SAM Not Loaded"); overlay = body; stats = {}
        def to_b64(im): return base64.b64encode(cv2.imencode('.jpg', im)[1]).decode('utf-8')
        return {"type": "image", "raw_b64": to_b64(body), "proc_b64": to_b64(overlay), "stats": stats, "log": log, "footer_b64": footer_b64}

# [4] Helper (Same)
async def analyze_vision_ollama(image_bytes: bytes, prompt: str) -> str:
    try:
        b64 = base64.b64encode(image_bytes).decode('utf-8')
        res = await asyncio.to_thread(ollama_client.chat, model=VISION_MODEL, messages=[{'role':'user','content':prompt,'images':[b64]}])
        return res['message']['content']
    except: return "Vision Error"

async def interpret_spectrum_data(raw_context: str, equipment: str) -> str:
    try:
        prompt = f"Analyze spectrum ({equipment}). Brief interpretation in Korean. Data: {raw_context}"
        res = await asyncio.to_thread(ollama_client.chat, model=TEXT_MODEL, messages=[{'role':'user','content':prompt}])
        return res['message']['content']
    except: return ""

# [5] App
app = FastAPI(title="Analyst NV32 Intelligent Cleaner", lifespan=lifespan)
app.add_middleware(CORSMiddleware, allow_origins=["*"], allow_methods=["*"], allow_headers=["*"])

class TranslateReq(BaseModel): text: str
@app.post("/api/translate")
async def translate_text(req: TranslateReq):
    try:
        res = await asyncio.to_thread(ollama_client.chat, model=TEXT_MODEL, messages=[{'role':'system','content':'Translate to English.'}, {'role':'user','content':req.text}])
        return {"translated": res['message']['content']}
    except Exception as e: return {"translated": str(e)}

async def process_analysis_job(job_id: str, files_map: Dict, configs: Dict):
    try:
        JOBS[job_id]["status"] = "Processing"; JOBS[job_id]["progress"] = 10; final_results = []; total = len(configs); cnt = 0
        for filename, config in configs.items():
            if filename not in files_map: continue
            content = files_map[filename]; eq = config.get("equipment", "General"); data_type = config.get("data_type", "General")
            goal = config.get("goal", ""); mode = config.get("mode", "Auto")
            p_size = int(config.get("patch_size", 128)); p_step = int(config.get("step", 16)); n_comp = int(config.get("n_components", 10))
            fname_lower = filename.lower(); JOBS[job_id]["step"] = f"Analyzing {filename}..."
            
            if eq == "Document" or fname_lower.endswith(('.pdf', '.ppt', '.pptx')):
                try:
                    if fname_lower.endswith('.pdf'):
                        pdf = pdfium.PdfDocument(io.BytesIO(content)); n_pages = len(pdf); full_txt = ""; pages = []
                        for i in range(min(n_pages, 10)):
                            page = pdf[i]; bitmap = page.render(scale=2); pil_image = bitmap.to_pil()
                            buf = io.BytesIO(); pil_image.save(buf, format="JPEG"); b64 = base64.b64encode(buf.getvalue()).decode()
                            desc = await asyncio.to_thread(ollama_client.chat, model=VISION_MODEL, messages=[{'role':'user','content':'한글 요약.','images':[b64]}])
                            d_txt = desc['message']['content']; full_txt += f"\nPage {i+1}: {d_txt}"
                            pages.append({"id": str(uuid.uuid4()), "page_num": i+1, "image_b64": b64, "desc": d_txt})
                        final_results.append({"type": "pdf", "filename": filename, "equipment": "Lit", "raw_context": full_txt, "pages": pages})
                    elif fname_lower.endswith(('.ppt', '.pptx')):
                        try:
                            prs = Presentation(io.BytesIO(content)); slides = []; full_txt = ""
                            for i, slide in enumerate(prs.slides):
                                txt = ""; imgs = []
                                for s in slide.shapes:
                                    if hasattr(s, "text"): txt += s.text + "\n"
                                    if s.shape_type == MSO_SHAPE_TYPE.PICTURE:
                                        try:
                                            ib64 = base64.b64encode(s.image.blob).decode('utf-8')
                                            imgs.append({"b64":ib64, "desc":"Img"})
                                        except: pass
                                if not txt.strip(): txt = "(No text)"
                                full_txt += f"Slide {i+1}: {txt}\n"; slides.append({"id": str(uuid.uuid4()), "slide_num": i+1, "text": txt, "images": imgs})
                            final_results.append({"type": "ppt", "filename": filename, "equipment": "Lit", "raw_context": full_txt, "slides": slides})
                        except Exception as ppt_err: final_results.append({"type":"error", "filename":filename, "msg":f"PPT Error: {ppt_err}"})
                except Exception as e: final_results.append({"type": "error", "filename": filename, "msg": str(e)})

            elif fname_lower.endswith(('.xlsx', '.xls', '.csv', '.txt')):
                try:
                    dfs = ScienceProcessorNV32.read_universal_dataframe(content, filename)
                    for i, df in enumerate(dfs):
                        v_blocks = ScienceProcessorNV32.detect_structured_blocks(df)
                        for v_block in v_blocks:
                            h_blocks = ScienceProcessorNV32.split_horizontal_blocks(v_block)
                            for h_block in h_blocks:
                                series_list = ScienceProcessorNV32.extract_series_from_df(h_block)
                                for s in series_list:
                                    res = ScienceProcessorNV32.process_spectrum(s['x'], s['y'], mode, goal)
                                    step = max(1, len(s['x'])//100)
                                    chart = [{"x": float(res["x"][k]), "y_proc": float(res["y_proc"][k]), "y_raw": float(res["y_raw"][k]), "y_base": float(res["y_base"][k])} for k in range(0, len(s['x']), step)]
                                    ctx = f"Data: {s['name']}\nStats: {res['stats']}"
                                    interp = await interpret_spectrum_data(ctx, eq)
                                    final_results.append({
                                        "type": "spectrum", "filename": f"{filename} ({s['name']})", "equipment": eq,
                                        "raw_context": f"{ctx}\nInterp: {interp}", 
                                        "chart_data": chart, "log": res["log"], "interpretation": interp
                                    })
                except Exception as ex: final_results.append({"type": "error", "filename": filename, "msg": str(ex)})

            elif fname_lower.endswith(('.png', '.jpg', '.jpeg', '.tif', '.bmp')):
                try:
                    vis_res = await asyncio.to_thread(ScienceProcessorNV32.process_image_nv32, content, eq, data_type, mode, goal, None, p_size, p_step, n_comp)
                    if vis_res:
                        scale_info = "Unknown"
                        final_footer = vis_res.get("footer_b64")
                        if final_footer and data_type != "Crystal Structure (FFT)":
                            scale_res = await asyncio.to_thread(ollama_client.chat, model=VISION_MODEL, messages=[{'role':'user','content':'Read scale bar text only.','images':[final_footer]}])
                            scale_info = scale_res['message']['content']; vis_res["log"].append(f"Scale: {scale_info}")
                        
                        stats_clean = "\n".join([f"- {k}: {v}" for k,v in vis_res.get('stats',{}).items()])
                        prompt = f"Analyze {eq} image. Stats: {stats_clean}. Goal: {goal}. Summary in Korean."
                        desc_res = await asyncio.to_thread(ollama_client.chat, model=VISION_MODEL, messages=[{'role':'user','content':prompt,'images':[vis_res["proc_b64"]]}])
                        desc = desc_res['message']['content']
                        final_results.append({"type":"image", "filename":filename, "equipment":eq, "summary":desc, "raw_context":f"Img: {desc}\nStats:{stats_clean}", "raw_b64":vis_res["raw_b64"], "proc_b64":vis_res["proc_b64"], "footer_b64":final_footer, "stats":vis_res.get("stats",{}), "log":vis_res.get("log",[])})
                except Exception as ex: final_results.append({"type":"error", "filename":filename, "msg":str(ex)})
            
            cnt += 1; JOBS[job_id]["progress"] = 10 + int((cnt/total)*80)

        JOBS[job_id]["step"] = "Writing Report..."; data_ctx = ""
        for r in final_results:
            if r.get("type")!="error": data_ctx += f"\n=== {r['filename']} ===\n{r.get('raw_context','')[:2000]}\n"
        
        rep_res = await asyncio.to_thread(ollama_client.chat, model=TEXT_MODEL, messages=[{'role':'system','content':'Write scientific report in Korean.'}, {'role':'user','content':f"Data:\n{data_ctx}"}])
        JOBS[job_id]["results"] = {"results": final_results, "final_report": rep_res['message']['content']}
        JOBS[job_id]["status"] = "Completed"; JOBS[job_id]["progress"] = 100
    except Exception as e: JOBS[job_id]["status"] = "Failed"; JOBS[job_id]["error"] = str(e)

@app.post("/api/analyze")
async def start_analysis(background_tasks: BackgroundTasks, files: List[UploadFile] = File(...), file_configs: str = Form(...)):
    job_id = str(uuid.uuid4()); configs = json.loads(file_configs)
    files_map = {f.filename: await f.read() for f in files}
    JOBS[job_id] = {"status": "Pending", "progress": 0, "step": "Init", "results": None}
    background_tasks.add_task(process_analysis_job, job_id, files_map, configs)
    return {"job_id": job_id}

@app.get("/api/status/{job_id}")
async def get_status(job_id: str): return JOBS.get(job_id, {"status": "Not Found"})
@app.get("/", response_class=HTMLResponse)
async def serve_index(): return FileResponse("index.html") if os.path.exists("index.html") else "<h1>NV32 Backend</h1>"

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)



NV31 Expanded

import os
import io
# [0] OMP Error Fix
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"
import torch

import asyncio
import base64
import json
import re
import math
import uuid
import time
from typing import List, Dict, Any, Optional
from pydantic import BaseModel
from contextlib import asynccontextmanager
from collections import Counter

# [1] Security
os.environ['NO_PROXY'] = '127.0.0.1,localhost,0.0.0.0,::1'
if 'HTTP_PROXY' in os.environ: del os.environ['HTTP_PROXY']
if 'HTTPS_PROXY' in os.environ: del os.environ['HTTPS_PROXY']

from fastapi import FastAPI, UploadFile, File, Form, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import HTMLResponse, FileResponse
import pandas as pd
import numpy as np

# SciPy & CV & Sklearn
from sklearn.decomposition import NMF
from scipy.signal import find_peaks, savgol_filter
from scipy.optimize import curve_fit
from scipy.ndimage import center_of_mass
from skimage.feature import peak_local_max
import cv2
import matplotlib
import matplotlib.pyplot as plt 

# Models
import pypdfium2 as pdfium 
from segment_anything import sam_model_registry, SamAutomaticMaskGenerator
from ollama import Client 
from pptx import Presentation
from pptx.enum.shapes import MSO_SHAPE_TYPE

# [2] Config
OLLAMA_HOST = "http://127.0.0.1:11434"
ollama_client = Client(host=OLLAMA_HOST)
VISION_MODEL = "qwen3-vl:30b-a3b-instruct" 
TEXT_MODEL = "llama3:70b"

SAM_CHECKPOINT = "sam_vit_b_01ec64.pth"
SAM_MODEL_TYPE = "vit_b"
device = "cuda" if torch.cuda.is_available() else "cpu"

mask_generator = None 
JOBS = {} 

@asynccontextmanager
async def lifespan(app: FastAPI):
    global mask_generator
    if os.path.exists(SAM_CHECKPOINT):
        print(f"Loading SAM ({SAM_MODEL_TYPE}) on {device}...")
        try:
            sam = sam_model_registry[SAM_MODEL_TYPE](checkpoint=SAM_CHECKPOINT)
            sam.to(device=device)
            mask_generator = SamAutomaticMaskGenerator(
                model=sam, points_per_side=32, pred_iou_thresh=0.86,
                stability_score_thresh=0.90, min_mask_region_area=50
            )
            print("SAM Loaded.")
        except: print("SAM Load Failed.")
    yield
    JOBS.clear()

def sanitize_json(obj):
    if isinstance(obj, float):
        if np.isnan(obj) or np.isinf(obj): return 0.0
        return obj
    elif isinstance(obj, dict): return {k: sanitize_json(v) for k, v in obj.items()}
    elif isinstance(obj, list): return [sanitize_json(v) for v in obj]
    elif isinstance(obj, (np.int64, np.int32)): return int(obj)
    elif isinstance(obj, (np.float64, np.float32)):
        if np.isnan(obj) or np.isinf(obj): return 0.0
        return float(obj)
    return obj

# ==========================================
# [3] NV31 Final Logic Engine (Expanded)
# ==========================================
class ScienceProcessorNV31:
    
    # --- Universal Loader ---
    @staticmethod
    def read_universal_dataframe(content: bytes, filename: str) -> List[pd.DataFrame]:
        dfs = []
        fname = filename.lower()
        if fname.endswith(('.xlsx', '.xls')):
            try:
                xls = pd.ExcelFile(io.BytesIO(content))
                for s in xls.sheet_names:
                    df = pd.read_excel(xls, sheet_name=s, header=None)
                    if df.apply(pd.to_numeric, errors='coerce').notna().sum().sum() > 5: 
                        dfs.append(df)
                if dfs: return dfs
            except: pass
        
        text_content = ""
        for enc in ['utf-8', 'cp949', 'latin1']:
            try: text_content = content.decode(enc); break
            except: continue
        
        if text_content:
            lines = text_content.splitlines()
            data_start = -1; delimiter = None
            separators = [',', '\t', ';', '\s+']
            
            for i, line in enumerate(lines[:100]):
                for sep in separators:
                    parts = line.split() if sep=='\s+' else line.split(sep)
                    nums = sum(1 for p in parts if re.match(r'^-?\d+(\.\d+)?(?:[eE][+-]?\d+)?$', p.strip()))
                    if nums >= 2: 
                        data_start = i; delimiter = sep; break
                if data_start != -1: break
            
            if data_start != -1:
                try:
                    data_str = "\n".join(lines[data_start:])
                    sep_arg = '\s+' if delimiter=='\s+' else delimiter
                    df = pd.read_csv(io.StringIO(data_str), sep=sep_arg, header=None, engine='python')
                    dfs.append(df)
                except: pass
        return dfs

    # --- Block Parser ---
    @staticmethod
    def detect_structured_blocks(df: pd.DataFrame) -> List[pd.DataFrame]:
        blocks = []
        if df.empty: return []
        
        df_num = df.apply(pd.to_numeric, errors='coerce')
        is_data_row = df_num.notna().any(axis=1)
        
        groups = (is_data_row != is_data_row.shift()).cumsum()
        
        for _, group in df.groupby(groups):
            if is_data_row.loc[group.index[0]]:
                block = df_num.loc[group.index]
                
                # 1. Drop completely empty columns
                block = block.dropna(how='all', axis=1)
                
                # 2. [NV31] Drop Constant/Empty Columns (Ghost Columns)
                valid_cols = []
                for c in block.columns:
                    col_data = block[c]
                    # If std is 0 (constant line) or all 0, skip it
                    if col_data.std() == 0 or (col_data == 0).all(): 
                        continue
                    valid_cols.append(c)
                
                if len(valid_cols) >= 2:
                    block = block[valid_cols]
                
                if block.shape[0] >= 5 and block.shape[1] >= 2: 
                    blocks.append(block.reset_index(drop=True))
        return blocks

    @staticmethod
    def split_horizontal_blocks(df: pd.DataFrame) -> List[pd.DataFrame]:
        sub_blocks = []
        if df.empty: return []
        col_counts = df.apply(pd.to_numeric, errors='coerce').notna().sum()
        valid = np.where(col_counts >= 2)[0]
        if len(valid) == 0: return []
        splits = np.where(np.diff(valid) > 1)[0] + 1
        groups = np.split(valid, splits)
        for g in groups:
            if len(g) > 0:
                sub = df.iloc[:, g].reset_index(drop=True)
                if sub.shape[1] >= 1 and sub.shape[0] >= 2: sub_blocks.append(sub)
        return sub_blocks

    # --- Extract Series (NV31 Enhanced) ---
    @staticmethod
    def extract_series_from_df(df: pd.DataFrame) -> List[Dict]:
        series_list = []
        try:
            vals = df.values
            rows, cols = vals.shape
            if rows < 5: return []
            
            # Identify Axis
            def is_axis(arr):
                arr = arr[~np.isnan(arr)]
                if len(arr) < 5: return False
                diff = np.diff(arr)
                if np.all(diff >= 0) or np.all(diff <= 0): return True
                return (np.sum(diff>0)/len(diff)>0.9) or (np.sum(diff<0)/len(diff)>0.9)

            col0 = vals[:, 0]
            if is_axis(col0):
                current_x = col0
                start_col = 1
            else:
                current_x = np.arange(rows)
                start_col = 0

            valid_idx = 1
            for i in range(start_col, cols):
                col_data = vals[:, i]
                mask = ~np.isnan(current_x) & ~np.isnan(col_data)
                
                if np.sum(mask) < 5: continue
                
                # Double Check: Skip constant data
                y_data = col_data[mask]
                if np.std(y_data) == 0: continue 

                # Sort by X (To handle descending index properly)
                x_data = current_x[mask]
                sort_idx = np.argsort(x_data)
                
                series_list.append({
                    "x": x_data[sort_idx], 
                    "y": y_data[sort_idx], 
                    "name": f"Col-{valid_idx}" 
                })
                valid_idx += 1
        except: pass
        return series_list

    # --- Spectrum Logic ---
    @staticmethod
    def simple_baseline(y):
        try:
            w = max(5, len(y)//5)
            s = pd.Series(y).rolling(window=w, center=True).min()
            b = s.bfill().ffill().values 
            if len(b) > 51: b = savgol_filter(b, 51, 3)
            return b, w
        except: return np.zeros_like(y), 0

    @staticmethod
    def process_spectrum(x, y, mode, goal):
        try:
            y_raw = y.copy()
            y_base = np.zeros_like(y)
            y_proc = y.copy()
            
            if mode == "None":
                peaks, _ = find_peaks(y, height=np.max(y)*0.05)
                return {
                    "x": x, "y_raw": y_raw, "y_proc": y_raw, "y_base": y_base, 
                    "peaks": [{"x": float(x[p]), "y": float(y[p])} for p in peaks], 
                    "log": ["Mode: Raw"], "stats": f"Raw Peaks: {len(peaks)}"
                }
            
            elif mode == "Auto" or mode == "AI-Adaptive":
                win = 5 # Very mild smoothing
                
                # Only subtract baseline if requested
                if "base" in goal.lower():
                    base, bw = ScienceProcessorNV31.simple_baseline(y)
                    y_proc = np.maximum(y - base, 0)
                    y_base = base
                
                if len(y_proc) > win:
                    y_proc = savgol_filter(y_proc, win, 3)
                
                peaks, _ = find_peaks(y_proc, height=np.max(y_proc)*0.05, distance=10)
                return {
                    "x": x, "y_raw": y_raw, "y_proc": y_proc, "y_base": y_base, 
                    "peaks": [{"x": float(x[p]), "y": float(y_proc[p])} for p in peaks], 
                    "log": [f"Mode: {mode}", "Minimal Filter"], "stats": f"Proc Peaks: {len(peaks)}"
                }
            else:
                return {"x": x, "y_raw": y, "y_proc": y, "y_base": y_base, "peaks": [], "log": ["Unknown"], "stats": ""}
        except Exception as e: 
            return {"x": x, "y_raw": y, "y_proc": y, "y_base": np.zeros_like(y), "peaks": [], "log": [str(e)], "stats": "Error"}

    # --- Footer ---
    @staticmethod
    def separate_footer(img_bgr):
        h, w = img_bgr.shape[:2]
        roi_h = int(h * 0.82)
        roi = img_bgr[roi_h:, :]
        gray = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)
        sobel_y = np.abs(cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=3))
        row_sums = np.sum(sobel_y, axis=1)
        split_idx = np.argmax(row_sums)
        if row_sums[split_idx] > w * 30: 
            return img_bgr[:roi_h + split_idx, :], img_bgr[roi_h + split_idx:, :]
        return img_bgr, None

    # --- [E] FFT-NMF (NV26 4-Stack) ---
    @staticmethod
    def analyze_crystal_fft_nmf(img_bgr, patch_size=128, step=16, n_components=10):
        log = []
        gray = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2GRAY)
        h, w = gray.shape
        log.append(f"FFT-NMF: Patch={patch_size}, Step={step}, K={n_components}")
        
        custom_colors = [
            (0, 0, 255), (0, 255, 0), (255, 0, 0), (0, 255, 255), (255, 255, 0),
            (255, 0, 255), (0, 165, 255), (238, 130, 238), (128, 128, 0), (19, 69, 139)
        ]
        while len(custom_colors) < n_components: custom_colors.append((255, 255, 255))
        
        cols = [] 
        y_steps = list(range(0, h - patch_size + 1, step))
        x_steps = list(range(0, w - patch_size + 1, step))
        grid_h = len(y_steps); grid_w = len(x_steps)
        
        cy, cx = patch_size // 2, patch_size // 2
        Y, X = np.ogrid[:patch_size, :patch_size]
        dist_sq = (X - cx)**2 + (Y - cy)**2
        dc_mask = dist_sq <= 4**2

        for y in y_steps:
            for x in x_steps:
                roi = gray[y:y+patch_size, x:x+patch_size]
                f = np.fft.fft2(roi)
                fshift = np.fft.fftshift(f)
                mag = np.abs(fshift) 
                mag[dc_mask] = 0
                cols.append(mag.flatten())
        
        if not cols: return {"type":"error", "msg":"Img too small"}
        
        try:
            V = np.array(cols).T.astype(np.float32)
            V = V - V.min()
            if V.max() > 0: V = V / V.max()
            
            nmf = NMF(n_components=n_components, init='nndsvda', random_state=42, max_iter=2000)
            W = nmf.fit_transform(V)
            H = nmf.components_
            
            if H.shape[1] != grid_h * grid_w: raise ValueError("Shape Mismatch")
            
            idx_H = np.argmax(H, axis=0) 
            map_low = idx_H.reshape(grid_h, grid_w).astype(np.uint8)
            map_high = cv2.resize(map_low, (w, h), interpolation=cv2.INTER_NEAREST)
            
            overlay = img_bgr.copy()
            mask_layer = np.zeros_like(img_bgr)
            
            counts = Counter(idx_H)
            total = len(idx_H)
            stats = {}
            
            for i in range(n_components):
                mask = (map_high == i)
                mask_layer[mask] = custom_colors[i]
                cnt = counts.get(i, 0)
                stats[f"Pattern {i}"] = f"{cnt} patches ({cnt/total*100:.1f}%)"
                
            alpha = 0.4
            mask_idx = np.any(mask_layer > 0, axis=-1)
            overlay[mask_idx] = (overlay[mask_idx]*(1-alpha) + mask_layer[mask_idx]*alpha).astype(np.uint8)
            
            basis_combo_list = []
            for k in range(n_components):
                # W
                pat = W[:, k].reshape(patch_size, patch_size)
                pat_vis = cv2.applyColorMap(cv2.resize(cv2.normalize(np.log1p(pat*1000),None,0,255,cv2.NORM_MINMAX).astype(np.uint8),(128,128),interpolation=cv2.INTER_NEAREST), cv2.COLORMAP_MAGMA)
                cv2.putText(pat_vis, "W (Basis)", (5, 15), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255,255,255), 1)
                # H
                h_vec = H[k, :] 
                h_min, h_max = h_vec.min(), h_vec.max()
                h_vis = cv2.applyColorMap(cv2.resize(cv2.normalize(h_vec.reshape(grid_h,grid_w),None,0,255,cv2.NORM_MINMAX).astype(np.uint8),(128,128),interpolation=cv2.INTER_NEAREST), cv2.COLORMAP_VIRIDIS)
                cv2.putText(h_vis, "H (Dist)", (5, 15), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255,255,255), 1)
                # Colorbar
                cbar_vis = cv2.applyColorMap(np.tile(np.linspace(0,255,128).astype(np.uint8),(25,1)), cv2.COLORMAP_VIRIDIS)
                cv2.putText(cbar_vis, f"{h_min:.1f}", (2, 17), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255,255,255), 1)
                cv2.putText(cbar_vis, f"{h_max:.1f}", (128-40, 17), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (0,0,0), 1)
                # Legend
                legend = np.zeros((30, 128, 3), dtype=np.uint8); legend[:] = custom_colors[k]
                cv2.putText(legend, f"P{k}", (50, 22), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255,255,255), 2)
                
                combo = np.vstack([pat_vis, h_vis, cbar_vis, legend])
                cv2.rectangle(combo, (0,0), (127, combo.shape[0]-1), custom_colors[k], 2)
                basis_combo_list.append(combo)
            
            full_strip_large = np.hstack(basis_combo_list)
            
            def to_b64(im): return base64.b64encode(cv2.imencode('.jpg', im)[1]).decode('utf-8')
            return {"type":"image", "raw_b64":to_b64(img_bgr), "proc_b64":to_b64(overlay), "footer_b64":to_b64(full_strip_large), "stats":stats, "log":log}
        except Exception as e: return {"type":"image", "raw_b64":base64.b64encode(cv2.imencode('.jpg', img_bgr)[1]).decode('utf-8'), "proc_b64":base64.b64encode(cv2.imencode('.jpg', img_bgr)[1]).decode('utf-8'), "stats":{"Error":str(e)}, "log":log}

    # --- [F] Diffraction ---
    @staticmethod
    def analyze_diffraction_nv7(img_bgr, user_center=None):
        log = []; stats = {}
        gray = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2GRAY)
        if user_center and len(user_center)==2 and user_center[0]>0:
            cx, cy = user_center
            log.append(f"Center: Manual [{cx}, {cy}]")
        else:
            _, th = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY+cv2.THRESH_OTSU)
            cyf, cxf = center_of_mass(th)
            if np.isnan(cyf): cyf, cxf = gray.shape[0]//2, gray.shape[1]//2
            cx, cy = int(cxf), int(cyf)
            log.append(f"Center: Auto [{cx}, {cy}]")

        g1 = cv2.GaussianBlur(gray, (3,3), 0)
        g2 = cv2.GaussianBlur(gray, (51,51), 0)
        diff = cv2.subtract(g1, g2)
        coords = peak_local_max(diff, min_distance=10, threshold_abs=20)
        enhanced = np.log1p(diff.astype(np.float32))
        enhanced = cv2.normalize(enhanced, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)
        vis = cv2.cvtColor(enhanced, cv2.COLOR_GRAY2BGR)
        cv2.drawMarker(vis, (cx, cy), (0,0,255), cv2.MARKER_CROSS, 20, 2)
        peak_list = []
        for p in coords:
            py, px = p
            if abs(py-cy)<5 and abs(px-cx)<5: continue
            cv2.circle(vis, (px, py), 3, (0,255,0), 1)
            peak_list.append(f"[{px},{py}]")
        log.append(f"Spots: {len(peak_list)}")
        def to_b64(im): return base64.b64encode(cv2.imencode('.jpg', im)[1]).decode('utf-8')
        return {"type": "image", "raw_b64": to_b64(img_bgr), "proc_b64": to_b64(vis), "stats": {"center": [cx, cy], "spots": len(peak_list)}, "log": log}

    # --- [G] SAM ---
    @staticmethod
    def analyze_sam_results_nv7(img_rgb, masks, data_type):
        overlay = img_rgb.copy()
        mask_overlay = np.zeros_like(img_rgb)
        h_img, w_img = img_rgb.shape[:2]
        stats = {"count": 0}
        log = []
        valid_masks = []
        for ann in masks:
            area = ann['area']; bbox = ann['bbox']; x, y, w, h = [int(v) for v in bbox]
            if area > h_img * w_img * 0.95 or area < 20: continue 
            if data_type == "Particle":
                if x<=1 or y<=1 or (x+w)>=w_img-1 or (y+h)>=h_img-1: continue
                mask_uint8 = (ann['segmentation'] * 255).astype(np.uint8)
                cnts, _ = cv2.findContours(mask_uint8, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
                if cnts:
                    perim = cv2.arcLength(cnts[0], True)
                    circ = 4*np.pi*area/(perim**2) if perim>0 else 0
                    if circ < 0.25: continue
            valid_masks.append(ann)

        if data_type == "Thin Film":
            sorted_masks = sorted(valid_masks, key=lambda x: x['bbox'][1])
        else:
            sorted_masks = sorted(valid_masks, key=lambda x: x['area'], reverse=True)

        thickness_vals = []; diameters = []
        for i, ann in enumerate(sorted_masks):
            m = ann['segmentation']
            bbox = ann['bbox']; x, y, w, h = [int(v) for v in bbox]
            cx, cy = int(x + w/2), int(y + h/2)
            color = (0, 255, 0) 
            if data_type == "Thin Film":
                is_bound = (i == 0 or i == len(sorted_masks)-1)
                cv2.putText(overlay, f"#{i+1}", (cx, cy), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (255, 255, 255), 3)
                if not is_bound:
                    thk = ann['area'] / w
                    thickness_vals.append(thk)
                    color = (255, 0, 0) 
                else: color = (100, 100, 100)
                if i < len(sorted_masks) - 1:
                    ys, xs = np.where(m)
                    if len(xs) > 10:
                        roughness = np.std(ys)
                        cv2.line(overlay, (x, y+h), (x+w, y+h), (0, 255, 255), 2)
                        cv2.putText(overlay, f"Int#{i+1} R:{roughness:.2f}", (x+10, y+h-5), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)
            elif data_type == "Particle":
                equivalent_diameter = 2 * np.sqrt(ann['area'] / np.pi)
                diameters.append(equivalent_diameter)
            mask_overlay[m] = color
            mask_uint8 = (m * 255).astype(np.uint8)
            cnts, _ = cv2.findContours(mask_uint8, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
            if cnts: cv2.drawContours(overlay, [cnts[0]], -1, color, 1)

        alpha = 0.35
        mask_idx = np.any(mask_overlay > 0, axis=-1)
        overlay[mask_idx] = (overlay[mask_idx]*(1-alpha) + mask_overlay[mask_idx]*alpha).astype(np.uint8)
        stats["count"] = len(sorted_masks)
        if data_type == "Thin Film":
            avg = round(np.mean(thickness_vals), 2) if thickness_vals else 0
            stats["thickness_avg"] = avg
            log.append(f"Layers: {len(sorted_masks)}, Avg Thk: {avg}")
        elif data_type == "Particle":
            avg = round(np.mean(diameters), 2) if diameters else 0
            stats["diameter_avg"] = avg
            log.append(f"Particles: {len(sorted_masks)}")
        return overlay, stats, log

    # --- Router ---
    @staticmethod
    def process_image_nv31(img_bytes, equipment, data_type, mode, goal, user_center=None, patch_size=128, step=16, n_components=10):
        nparr = np.frombuffer(img_bytes, np.uint8); img_raw = cv2.imdecode(nparr, cv2.IMREAD_COLOR)
        if img_raw is None: return {}
        
        body, footer = ScienceProcessorNV31.separate_footer(img_raw)
        footer_b64 = None
        if footer is not None:
            footer_b64 = base64.b64encode(cv2.imencode('.jpg', footer)[1]).decode('utf-8')

        if data_type == "Crystal Structure (FFT)":
            return ScienceProcessorNV31.analyze_crystal_fft_nmf(body, patch_size, step, n_components)
            
        if data_type == "2D Diffraction":
            return {**ScienceProcessorNV31.analyze_diffraction_nv7(body, user_center), "footer_b64": footer_b64}
        
        if mode == "None":
             def to_b64(im): return base64.b64encode(cv2.imencode('.jpg', im)[1]).decode('utf-8')
             return {"type":"image", "raw_b64":to_b64(body), "proc_b64":to_b64(body), "stats":{}, "log":["Mode: Raw"], "footer_b64": footer_b64}

        img_proc = body.copy(); log = []
        if mode == "Auto" or mode == "AI-Adaptive":
            img_proc = cv2.bilateralFilter(img_proc, 9, 75, 75)
            log.append("Preproc: Denoise")

        img_rgb = cv2.cvtColor(img_proc, cv2.COLOR_BGR2RGB)
        
        if mask_generator:
            try:
                masks = mask_generator.generate(img_rgb)
                log.append(f"SAM: {len(masks)} Segments")
                overlay, stats, sam_log = ScienceProcessorNV31.analyze_sam_results_nv7(img_rgb, masks, data_type)
                log.extend(sam_log)
                overlay = cv2.cvtColor(overlay, cv2.COLOR_RGB2BGR)
            except Exception as e:
                log.append(f"SAM Error: {str(e)}")
                overlay = body; stats = {}
        else:
            log.append("SAM Not Loaded")
            overlay = body; stats = {}

        def to_b64(im): return base64.b64encode(cv2.imencode('.jpg', im)[1]).decode('utf-8')
        return {"type": "image", "raw_b64": to_b64(body), "proc_b64": to_b64(overlay), "stats": stats, "log": log, "footer_b64": footer_b64}

# [4] Helper (Same)
async def analyze_vision_ollama(image_bytes: bytes, prompt: str) -> str:
    try:
        b64 = base64.b64encode(image_bytes).decode('utf-8')
        res = await asyncio.to_thread(ollama_client.chat, model=VISION_MODEL, messages=[{'role':'user','content':prompt,'images':[b64]}])
        return res['message']['content']
    except: return "Vision Error"

async def interpret_spectrum_data(raw_context: str, equipment: str) -> str:
    try:
        prompt = f"Analyze spectrum ({equipment}). Brief interpretation in Korean. Data: {raw_context}"
        res = await asyncio.to_thread(ollama_client.chat, model=TEXT_MODEL, messages=[{'role':'user','content':prompt}])
        return res['message']['content']
    except: return ""

# [5] App
app = FastAPI(title="Analyst NV31 Final Expanded", lifespan=lifespan)
app.add_middleware(CORSMiddleware, allow_origins=["*"], allow_methods=["*"], allow_headers=["*"])

class TranslateReq(BaseModel): text: str

@app.post("/api/translate")
async def translate_text(req: TranslateReq):
    try:
        res = await asyncio.to_thread(ollama_client.chat, model=TEXT_MODEL, messages=[{'role':'system','content':'Translate to English.'}, {'role':'user','content':req.text}])
        return {"translated": res['message']['content']}
    except Exception as e: return {"translated": str(e)}

async def process_analysis_job(job_id: str, files_map: Dict, configs: Dict):
    try:
        JOBS[job_id]["status"] = "Processing"
        JOBS[job_id]["progress"] = 10
        final_results = []
        total_files = len(configs)
        processed_count = 0

        for filename, config in configs.items():
            if filename not in files_map: continue
            content = files_map[filename]
            eq = config.get("equipment", "General")
            data_type = config.get("data_type", "General")
            goal = config.get("goal", "")
            mode = config.get("mode", "Auto")
            
            user_center = None
            if config.get("center_coords"):
                try: user_center = [int(v) for v in config["center_coords"].split(",")]
                except: pass
            
            p_size = int(config.get("patch_size", 128))
            p_step = int(config.get("step", 16))
            n_comp = int(config.get("n_components", 10))

            fname_lower = filename.lower()
            JOBS[job_id]["step"] = f"Analyzing {filename}..."
            
            if eq == "Document" or fname_lower.endswith(('.pdf', '.ppt', '.pptx')):
                try:
                    if fname_lower.endswith('.pdf'):
                        pdf = pdfium.PdfDocument(io.BytesIO(content))
                        n_pages = len(pdf)
                        full_txt = ""; pages = []
                        for i in range(min(n_pages, 10)):
                            page = pdf[i]
                            bitmap = page.render(scale=2)
                            pil_image = bitmap.to_pil()
                            buf = io.BytesIO()
                            pil_image.save(buf, format="JPEG")
                            b64 = base64.b64encode(buf.getvalue()).decode()
                            desc = await analyze_vision_ollama(buf.getvalue(), "한글 요약.")
                            full_txt += f"\nPage {i+1}: {desc}"
                            pages.append({"id": str(uuid.uuid4()), "page_num": i+1, "image_b64": b64, "desc": desc})
                        final_results.append({"type": "pdf", "filename": filename, "equipment": "Lit", "raw_context": full_txt, "pages": pages})
                    elif fname_lower.endswith(('.ppt', '.pptx')):
                        try:
                            prs = Presentation(io.BytesIO(content))
                            slides = []; full_txt = ""
                            for i, slide in enumerate(prs.slides):
                                txt = ""; imgs = []
                                for s in slide.shapes:
                                    if hasattr(s, "text"): txt += s.text + "\n"
                                    if s.shape_type == MSO_SHAPE_TYPE.PICTURE:
                                        try:
                                            ib64 = base64.b64encode(s.image.blob).decode('utf-8')
                                            desc = "Image"
                                            imgs.append({"b64": ib64, "desc": desc})
                                        except: pass
                                if not txt.strip(): txt = "(No text)"
                                full_txt += f"Slide {i+1}: {txt}\n"
                                slides.append({"id": str(uuid.uuid4()), "slide_num": i+1, "text": txt, "images": imgs})
                            final_results.append({"type": "ppt", "filename": filename, "equipment": "Lit", "raw_context": full_txt, "slides": slides})
                        except Exception as ppt_err:
                            final_results.append({"type": "error", "filename": filename, "msg": f"PPTX Error: {ppt_err}"})
                except Exception as e: 
                    final_results.append({"type": "error", "filename": filename, "msg": str(e)})

            elif fname_lower.endswith(('.xlsx', '.xls', '.csv', '.txt')):
                try:
                    dfs = ScienceProcessorNV31.read_universal_dataframe(content, filename)
                    for i, df in enumerate(dfs):
                        v_blocks = ScienceProcessorNV31.detect_structured_blocks(df)
                        for v_block in v_blocks:
                            h_blocks = ScienceProcessorNV31.split_horizontal_blocks(v_block)
                            for h_block in h_blocks:
                                series_list = ScienceProcessorNV31.extract_series_from_df(h_block)
                                for s in series_list:
                                    res = ScienceProcessorNV31.process_spectrum(s['x'], s['y'], mode, goal)
                                    step = max(1, len(s['x'])//100)
                                    chart = [{"x": float(res["x"][k]), "y_proc": float(res["y_proc"][k]), "y_raw": float(res["y_raw"][k]), "y_base": float(res["y_base"][k])} for k in range(0, len(s['x']), step)]
                                    ctx = f"Data: {s['name']}\nStats: {res['stats']}"
                                    interp = await interpret_spectrum_data(ctx, eq)
                                    final_results.append({
                                        "type": "spectrum", "filename": f"{filename} ({s['name']})", "equipment": eq,
                                        "raw_context": f"{ctx}\nInterp: {interp}", 
                                        "chart_data": chart, "log": res["log"], "interpretation": interp
                                    })
                except Exception as ex: final_results.append({"type": "error", "filename": filename, "msg": str(ex)})

            elif fname_lower.endswith(('.png', '.jpg', '.jpeg', '.tif', '.bmp')):
                try:
                    vis_res = await asyncio.to_thread(ScienceProcessorNV31.process_image_nv31, content, eq, data_type, mode, goal, user_center, p_size, p_step, n_comp)
                    if vis_res:
                        scale_info = "Unknown"
                        final_footer = vis_res.get("footer_b64")
                        if final_footer and data_type != "Crystal Structure (FFT)":
                            scale_text = await analyze_vision_ollama(base64.b64decode(final_footer), "Read scale bar text only.")
                            scale_info = scale_text
                            vis_res["log"].append(f"Scale: {scale_text}")

                        stats_clean = ""
                        for k, v in vis_res.get('stats',{}).items():
                            stats_clean += f"- {k}: {v}\n"
                        
                        desc_prompt = f"Analyze {eq} image ({data_type}). Stats: \n{stats_clean}\n. Goal: {goal}. Summary in Korean."
                        desc = await analyze_vision_ollama(base64.b64decode(vis_res["proc_b64"]), desc_prompt)
                        
                        final_results.append({
                            "type": "image", "filename": filename, "equipment": eq, "summary": desc,
                            "raw_context": f"Img {eq}: {desc}\nStats:\n{stats_clean}", 
                            "raw_b64": vis_res["raw_b64"], "proc_b64": vis_res["proc_b64"], "footer_b64": final_footer,
                            "stats": vis_res.get("stats",{}), "log": vis_res.get("log", [])
                        })
                except Exception as ex: final_results.append({"type": "error", "filename": filename, "msg": str(ex)})
            
            processed_count += 1
            JOBS[job_id]["progress"] = 10 + int((processed_count / total_files) * 80)

        JOBS[job_id]["step"] = "Writing Report..."
        final_results = sanitize_json(final_results)
        
        data_ctx = ""
        for r in final_results:
            if r.get("type") != "error":
                raw = r.get("raw_context") or ""
                data_ctx += f"\n=== File: {r['filename']} ({r.get('equipment')}) ===\n{raw[:2000]}\n"

        final_report = "Fail"
        if data_ctx:
            try:
                system_prompt = "You are a research assistant. Write a comprehensive report in Korean."
                res = await asyncio.to_thread(ollama_client.chat, model=TEXT_MODEL, messages=[{'role':'system','content':system_prompt}, {'role':'user','content':f"Data Summaries:\n{data_ctx}"}])
                final_report = res['message']['content']
            except Exception as e: final_report = str(e)

        JOBS[job_id]["results"] = {"results": final_results, "final_report": final_report}
        JOBS[job_id]["status"] = "Completed"
        JOBS[job_id]["progress"] = 100
    except Exception as e:
        JOBS[job_id]["status"] = "Failed"; JOBS[job_id]["error"] = str(e)

@app.post("/api/analyze")
async def start_analysis(background_tasks: BackgroundTasks, files: List[UploadFile] = File(...), file_configs: str = Form(...)):
    job_id = str(uuid.uuid4()); configs = json.loads(file_configs)
    files_map = {f.filename: await f.read() for f in files}
    JOBS[job_id] = {"status": "Pending", "progress": 0, "step": "Init", "results": None}
    background_tasks.add_task(process_analysis_job, job_id, files_map, configs)
    return {"job_id": job_id}

@app.get("/api/status/{job_id}")
async def get_status(job_id: str): return JOBS.get(job_id, {"status": "Not Found"})

@app.get("/", response_class=HTMLResponse)
async def serve_index():
    if os.path.exists("index.html"): return FileResponse("index.html")
    return "<h1>NV31 Final Backend</h1>"

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)






NV31

import os
import io
# [0] OMP Error Fix
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"
import torch

import asyncio
import base64
import json
import re
import math
import uuid
import time
from typing import List, Dict, Any, Optional
from pydantic import BaseModel
from contextlib import asynccontextmanager
from collections import Counter

# [1] Security
os.environ['NO_PROXY'] = '127.0.0.1,localhost,0.0.0.0,::1'
if 'HTTP_PROXY' in os.environ: del os.environ['HTTP_PROXY']
if 'HTTPS_PROXY' in os.environ: del os.environ['HTTPS_PROXY']

from fastapi import FastAPI, UploadFile, File, Form, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import HTMLResponse, FileResponse
import pandas as pd
import numpy as np

# SciPy & CV & Sklearn
from sklearn.decomposition import NMF
from scipy.signal import find_peaks, savgol_filter
from scipy.optimize import curve_fit
from scipy.ndimage import center_of_mass
from skimage.feature import peak_local_max
import cv2
import matplotlib
import matplotlib.pyplot as plt 

# Models
import pypdfium2 as pdfium 
from segment_anything import sam_model_registry, SamAutomaticMaskGenerator
from ollama import Client 
from pptx import Presentation
from pptx.enum.shapes import MSO_SHAPE_TYPE

# [2] Config
OLLAMA_HOST = "http://127.0.0.1:11434"
ollama_client = Client(host=OLLAMA_HOST)
VISION_MODEL = "qwen3-vl:30b-a3b-instruct" 
TEXT_MODEL = "llama3:70b"

SAM_CHECKPOINT = "sam_vit_b_01ec64.pth"
SAM_MODEL_TYPE = "vit_b"
device = "cuda" if torch.cuda.is_available() else "cpu"

mask_generator = None 
JOBS = {} 

@asynccontextmanager
async def lifespan(app: FastAPI):
    global mask_generator
    if os.path.exists(SAM_CHECKPOINT):
        try:
            sam = sam_model_registry[SAM_MODEL_TYPE](checkpoint=SAM_CHECKPOINT)
            sam.to(device=device)
            mask_generator = SamAutomaticMaskGenerator(
                model=sam, points_per_side=32, pred_iou_thresh=0.86,
                stability_score_thresh=0.90, min_mask_region_area=50
            )
        except: pass
    yield
    JOBS.clear()

def sanitize_json(obj):
    if isinstance(obj, float):
        if np.isnan(obj) or np.isinf(obj): return 0.0
        return obj
    elif isinstance(obj, dict): return {k: sanitize_json(v) for k, v in obj.items()}
    elif isinstance(obj, list): return [sanitize_json(v) for v in obj]
    elif isinstance(obj, (np.int64, np.int32)): return int(obj)
    elif isinstance(obj, (np.float64, np.float32)):
        if np.isnan(obj) or np.isinf(obj): return 0.0
        return float(obj)
    return obj

# ==========================================
# [3] NV31 Final Logic Fix Engine
# ==========================================
class ScienceProcessorNV31:
    
    # --- Universal Loader ---
    @staticmethod
    def read_universal_dataframe(content: bytes, filename: str) -> List[pd.DataFrame]:
        dfs = []
        fname = filename.lower()
        if fname.endswith(('.xlsx', '.xls')):
            try:
                xls = pd.ExcelFile(io.BytesIO(content))
                for s in xls.sheet_names:
                    df = pd.read_excel(xls, sheet_name=s, header=None)
                    if df.apply(pd.to_numeric, errors='coerce').notna().sum().sum() > 5: dfs.append(df)
                if dfs: return dfs
            except: pass
        text_content = ""
        for enc in ['utf-8', 'cp949', 'latin1']:
            try: text_content = content.decode(enc); break
            except: continue
        if text_content:
            lines = text_content.splitlines()
            data_start = -1; delimiter = None
            separators = [',', '\t', ';', '\s+']
            for i, line in enumerate(lines[:100]):
                for sep in separators:
                    parts = line.split() if sep=='\s+' else line.split(sep)
                    nums = sum(1 for p in parts if re.match(r'^-?\d+(\.\d+)?(?:[eE][+-]?\d+)?$', p.strip()))
                    if nums >= 2: data_start = i; delimiter = sep; break
                if data_start != -1: break
            if data_start != -1:
                try:
                    data_str = "\n".join(lines[data_start:])
                    sep_arg = '\s+' if delimiter=='\s+' else delimiter
                    df = pd.read_csv(io.StringIO(data_str), sep=sep_arg, header=None, engine='python')
                    dfs.append(df)
                except: pass
        return dfs

    # --- Block Parser ---
    @staticmethod
    def detect_structured_blocks(df: pd.DataFrame) -> List[pd.DataFrame]:
        blocks = []
        if df.empty: return []
        df_num = df.apply(pd.to_numeric, errors='coerce')
        if df.shape[1] > 0: is_data_row = df_num.iloc[:, 0].notna()
        else: is_data_row = df_num.notna().any(axis=1)
        groups = (is_data_row != is_data_row.shift()).cumsum()
        for _, group in df.groupby(groups):
            if is_data_row.loc[group.index[0]]:
                block = df_num.loc[group.index].dropna(how='all', axis=1)
                if block.shape[0] >= 5: blocks.append(block.reset_index(drop=True))
        return blocks

    @staticmethod
    def split_horizontal_blocks(df: pd.DataFrame) -> List[pd.DataFrame]:
        sub_blocks = []
        if df.empty: return []
        col_counts = df.apply(pd.to_numeric, errors='coerce').notna().sum()
        valid = np.where(col_counts >= 2)[0]
        if len(valid) == 0: return []
        splits = np.where(np.diff(valid) > 1)[0] + 1
        groups = np.split(valid, splits)
        for g in groups:
            if len(g) > 0:
                sub = df.iloc[:, g].reset_index(drop=True)
                if sub.shape[1] >= 1 and sub.shape[0] >= 2: sub_blocks.append(sub)
        return sub_blocks

    # [NV31 Fix] Sort by X + Remove Parsing Artifacts
    @staticmethod
    def extract_series_from_df(df: pd.DataFrame) -> List[Dict]:
        series_list = []
        try:
            vals = df.values; rows, cols = vals.shape
            if rows < 5: return []
            
            # Helper: Check monotonicity
            def is_axis(arr):
                arr = arr[~np.isnan(arr)]
                if len(arr) < 5: return False
                diff = np.diff(arr)
                if np.all(diff >= 0) or np.all(diff <= 0): return True
                return (np.sum(diff>0)/len(diff)>0.9) or (np.sum(diff<0)/len(diff)>0.9)

            col0 = vals[:, 0]
            if is_axis(col0):
                current_x = col0; start_col = 1
            else:
                current_x = np.arange(rows); start_col = 0

            valid_idx = 1
            for i in range(start_col, cols):
                col_data = vals[:, i]
                mask = ~np.isnan(current_x) & ~np.isnan(col_data)
                
                if np.sum(mask) < 5: continue
                
                # [NV31] Filter Artifacts
                # Even if user data is clean, the parser might have created columns 
                # from trailing commas or spaces. These usually have 0 variance.
                y_data = col_data[mask]
                if np.std(y_data) == 0: continue 

                # [NV31] Sort by X (Fix Reverse Order)
                # Plotly needs sorted X to draw correct lines
                x_data = current_x[mask]
                sort_indices = np.argsort(x_data)
                x_sorted = x_data[sort_indices]
                y_sorted = y_data[sort_indices]

                series_list.append({
                    "x": x_sorted, 
                    "y": y_sorted, 
                    "name": f"Col-{valid_idx}" 
                })
                valid_idx += 1
        except: pass
        return series_list

    # --- Spectrum Logic ---
    @staticmethod
    def simple_baseline(y):
        try:
            w = max(5, len(y)//5) 
            s = pd.Series(y).rolling(window=w, center=True).min()
            b = s.bfill().ffill().values 
            if len(b) > 51: b = savgol_filter(b, 51, 3)
            return b, w
        except: return np.zeros_like(y), 0

    # [NV31 Fix] Minimal Processing for CSV
    @staticmethod
    def process_spectrum(x, y, mode, goal):
        try:
            y_raw = y.copy()
            y_base = np.zeros_like(y)
            y_proc = y.copy()
            
            # Raw Mode
            if mode == "None":
                peaks, _ = find_peaks(y, height=np.max(y)*0.05)
                return {
                    "x": x, "y_raw": y_raw, "y_proc": y_raw, "y_base": y_base, 
                    "peaks": [{"x": float(x[p]), "y": float(y[p])} for p in peaks], 
                    "log": ["Mode: Raw"], "stats": f"Raw Peaks: {len(peaks)}"
                }
            
            # Auto / AI-Adaptive
            # [NV31] For CSVs, default Auto should NOT crush signals.
            # We skip baseline removal unless explicitly asked.
            # We apply VERY mild smoothing just to help peak detection.
            elif mode == "Auto" or mode == "AI-Adaptive":
                win = 5 # Minimal smoothing window
                
                # Only remove baseline if requested or if data looks like image spectrum
                # Assuming simple CSVs don't need it.
                if "base" in goal.lower():
                    base, bw = ScienceProcessorNV31.simple_baseline(y)
                    y_proc = np.maximum(y - base, 0)
                    y_base = base
                
                if len(y_proc) > win:
                    y_proc = savgol_filter(y_proc, win, 3)
                
                # If peak is lost (e.g. 50000 -> 15000), use raw for peak finding?
                # No, smoothing shouldn't reduce peak that much unless it's a spike.
                # If smoothing reduces it, it was noise. 
                # But here we trust the user. Let's use y_proc closer to y_raw.
                
                peaks, _ = find_peaks(y_proc, height=np.max(y_proc)*0.05, distance=10)
                return {
                    "x": x, "y_raw": y_raw, "y_proc": y_proc, "y_base": y_base, 
                    "peaks": [{"x": float(x[p]), "y": float(y_proc[p])} for p in peaks], 
                    "log": [f"Mode: {mode}", "Minimal Filter"], "stats": f"Proc Peaks: {len(peaks)}"
                }
            else:
                return {"x": x, "y_raw": y, "y_proc": y, "y_base": y_base, "peaks": [], "log": ["Unknown"], "stats": ""}
        except Exception as e: 
            return {"x": x, "y_raw": y, "y_proc": y, "y_base": np.zeros_like(y), "peaks": [], "log": [str(e)], "stats": "Error"}

    # --- Footer ---
    @staticmethod
    def separate_footer(img_bgr):
        h, w = img_bgr.shape[:2]; roi_h = int(h * 0.82); roi = img_bgr[roi_h:, :]
        gray = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)
        sobel_y = np.abs(cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=3))
        row_sums = np.sum(sobel_y, axis=1)
        split_idx = np.argmax(row_sums)
        if row_sums[split_idx] > w * 30: return img_bgr[:roi_h + split_idx, :], img_bgr[roi_h + split_idx:, :]
        return img_bgr, None

    # --- [E] FFT-NMF (NV26) ---
    @staticmethod
    def analyze_crystal_fft_nmf(img_bgr, patch_size=128, step=16, n_components=10):
        log = []; gray = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2GRAY); h, w = gray.shape
        log.append(f"FFT-NMF: Patch={patch_size}, Step={step}, K={n_components}")
        custom_colors = [(0,0,255),(0,255,0),(255,0,0),(0,255,255),(255,255,0),(255,0,255),(0,165,255),(238,130,238),(128,128,0),(19,69,139)]
        while len(custom_colors) < n_components: custom_colors.append((255,255,255))
        cols = []; y_steps = list(range(0, h-patch_size+1, step)); x_steps = list(range(0, w-patch_size+1, step))
        grid_h = len(y_steps); grid_w = len(x_steps)
        cy, cx = patch_size//2, patch_size//2
        Y, X = np.ogrid[:patch_size, :patch_size]; dc_mask = ((X-cx)**2 + (Y-cy)**2) <= 4**2
        for y in y_steps:
            for x in x_steps:
                roi = gray[y:y+patch_size, x:x+patch_size]
                fshift = np.fft.fftshift(np.fft.fft2(roi))
                mag = np.abs(fshift); mag[dc_mask] = 0; cols.append(mag.flatten())
        if not cols: return {"type":"error", "msg":"Img too small"}
        try:
            V = np.array(cols).T.astype(np.float32); V = V - V.min()
            if V.max() > 0: V = V/V.max()
            nmf = NMF(n_components=n_components, init='nndsvda', random_state=42, max_iter=2000)
            W = nmf.fit_transform(V); H = nmf.components_
            if H.shape[1] != grid_h*grid_w: raise ValueError("Shape Mismatch")
            idx_H = np.argmax(H, axis=0)
            map_high = cv2.resize(idx_H.reshape(grid_h, grid_w).astype(np.uint8), (w, h), interpolation=cv2.INTER_NEAREST)
            overlay = img_bgr.copy(); mask_layer = np.zeros_like(img_bgr)
            counts = Counter(idx_H); total = len(idx_H); stats = {}
            for i in range(n_components):
                mask = (map_high == i); mask_layer[mask] = custom_colors[i]
                cnt = counts.get(i, 0); stats[f"Pattern {i}"] = f"{cnt} patches ({cnt/total*100:.1f}%)"
            mask_idx = np.any(mask_layer > 0, axis=-1)
            overlay[mask_idx] = (overlay[mask_idx]*(1-0.4) + mask_layer[mask_idx]*0.4).astype(np.uint8)
            basis_combo_list = []
            for k in range(n_components):
                pat = W[:, k].reshape(patch_size, patch_size)
                pat_vis = cv2.applyColorMap(cv2.resize(cv2.normalize(np.log1p(pat*1000),None,0,255,cv2.NORM_MINMAX).astype(np.uint8),(128,128),interpolation=cv2.INTER_NEAREST), cv2.COLORMAP_MAGMA)
                cv2.putText(pat_vis, "W (Basis)", (5, 15), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255,255,255), 1)
                h_vec = H[k, :]; h_vis = cv2.applyColorMap(cv2.resize(cv2.normalize(h_vec.reshape(grid_h,grid_w),None,0,255,cv2.NORM_MINMAX).astype(np.uint8),(128,128),interpolation=cv2.INTER_NEAREST), cv2.COLORMAP_VIRIDIS)
                cv2.putText(h_vis, "H (Dist)", (5, 15), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255,255,255), 1)
                cbar_vis = cv2.applyColorMap(np.tile(np.linspace(0,255,128).astype(np.uint8),(25,1)), cv2.COLORMAP_VIRIDIS)
                cv2.putText(cbar_vis, f"{h_vec.min():.1f}", (2,17), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255,255,255), 1)
                cv2.putText(cbar_vis, f"{h_vec.max():.1f}", (128-40,17), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (0,0,0), 1)
                legend = np.zeros((30, 128, 3), dtype=np.uint8); legend[:] = custom_colors[k]
                cv2.putText(legend, f"P{k}", (50, 22), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255,255,255), 2)
                combo = np.vstack([pat_vis, h_vis, cbar_vis, legend])
                cv2.rectangle(combo, (0,0), (127, combo.shape[0]-1), custom_colors[k], 2)
                basis_combo_list.append(combo)
            full_strip_large = np.hstack(basis_combo_list)
            def to_b64(im): return base64.b64encode(cv2.imencode('.jpg', im)[1]).decode('utf-8')
            return {"type":"image", "raw_b64":to_b64(img_bgr), "proc_b64":to_b64(overlay), "footer_b64":to_b64(full_strip_large), "stats":stats, "log":log}
        except Exception as e: return {"type":"image", "raw_b64":base64.b64encode(cv2.imencode('.jpg', img_bgr)[1]).decode('utf-8'), "proc_b64":base64.b64encode(cv2.imencode('.jpg', img_bgr)[1]).decode('utf-8'), "stats":{"Error":str(e)}, "log":log}

    # --- [F] Diffraction ---
    @staticmethod
    def analyze_diffraction_nv7(img_bgr, user_center=None):
        log = []; stats = {}
        gray = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2GRAY)
        if user_center and len(user_center)==2 and user_center[0]>0:
            cx, cy = user_center
            log.append(f"Center: Manual [{cx}, {cy}]")
        else:
            _, th = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY+cv2.THRESH_OTSU)
            cyf, cxf = center_of_mass(th)
            if np.isnan(cyf): cyf, cxf = gray.shape[0]//2, gray.shape[1]//2
            cx, cy = int(cxf), int(cyf)
            log.append(f"Center: Auto [{cx}, {cy}]")

        g1 = cv2.GaussianBlur(gray, (3,3), 0)
        g2 = cv2.GaussianBlur(gray, (51,51), 0)
        diff = cv2.subtract(g1, g2)
        coords = peak_local_max(diff, min_distance=10, threshold_abs=20)
        enhanced = np.log1p(diff.astype(np.float32))
        enhanced = cv2.normalize(enhanced, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)
        vis = cv2.cvtColor(enhanced, cv2.COLOR_GRAY2BGR)
        cv2.drawMarker(vis, (cx, cy), (0,0,255), cv2.MARKER_CROSS, 20, 2)
        peak_list = []
        for p in coords:
            py, px = p
            if abs(py-cy)<5 and abs(px-cx)<5: continue
            cv2.circle(vis, (px, py), 3, (0,255,0), 1)
            peak_list.append(f"[{px},{py}]")
        log.append(f"Spots: {len(peak_list)}")
        def to_b64(im): return base64.b64encode(cv2.imencode('.jpg', im)[1]).decode('utf-8')
        return {"type": "image", "raw_b64": to_b64(img_bgr), "proc_b64": to_b64(vis), "stats": {"center": [cx, cy], "spots": len(peak_list)}, "log": log}

    # --- [G] SAM ---
    @staticmethod
    def analyze_sam_results_nv7(img_rgb, masks, data_type):
        overlay = img_rgb.copy()
        mask_overlay = np.zeros_like(img_rgb)
        h_img, w_img = img_rgb.shape[:2]
        stats = {"count": 0}
        log = []
        valid_masks = []
        for ann in masks:
            area = ann['area']; bbox = ann['bbox']; x, y, w, h = [int(v) for v in bbox]
            if area > h_img * w_img * 0.95 or area < 20: continue 
            if data_type == "Particle":
                if x<=1 or y<=1 or (x+w)>=w_img-1 or (y+h)>=h_img-1: continue
                mask_uint8 = (ann['segmentation'] * 255).astype(np.uint8)
                cnts, _ = cv2.findContours(mask_uint8, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
                if cnts:
                    perim = cv2.arcLength(cnts[0], True)
                    circ = 4*np.pi*area/(perim**2) if perim>0 else 0
                    if circ < 0.25: continue
            valid_masks.append(ann)

        if data_type == "Thin Film":
            sorted_masks = sorted(valid_masks, key=lambda x: x['bbox'][1])
        else:
            sorted_masks = sorted(valid_masks, key=lambda x: x['area'], reverse=True)

        thickness_vals = []; diameters = []
        for i, ann in enumerate(sorted_masks):
            m = ann['segmentation']
            bbox = ann['bbox']; x, y, w, h = [int(v) for v in bbox]
            cx, cy = int(x + w/2), int(y + h/2)
            color = (0, 255, 0) 
            if data_type == "Thin Film":
                is_bound = (i == 0 or i == len(sorted_masks)-1)
                cv2.putText(overlay, f"#{i+1}", (cx, cy), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (255, 255, 255), 3)
                if not is_bound:
                    thk = ann['area'] / w
                    thickness_vals.append(thk)
                    color = (255, 0, 0) 
                else: color = (100, 100, 100)
                if i < len(sorted_masks) - 1:
                    ys, xs = np.where(m)
                    if len(xs) > 10:
                        roughness = np.std(ys)
                        cv2.line(overlay, (x, y+h), (x+w, y+h), (0, 255, 255), 2)
                        cv2.putText(overlay, f"Int#{i+1} R:{roughness:.2f}", (x+10, y+h-5), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)
            elif data_type == "Particle":
                equivalent_diameter = 2 * np.sqrt(ann['area'] / np.pi)
                diameters.append(equivalent_diameter)
            mask_overlay[m] = color
            mask_uint8 = (m * 255).astype(np.uint8)
            cnts, _ = cv2.findContours(mask_uint8, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
            if cnts: cv2.drawContours(overlay, [cnts[0]], -1, color, 1)

        alpha = 0.35
        mask_idx = np.any(mask_overlay > 0, axis=-1)
        overlay[mask_idx] = (overlay[mask_idx]*(1-alpha) + mask_overlay[mask_idx]*alpha).astype(np.uint8)
        stats["count"] = len(sorted_masks)
        if data_type == "Thin Film":
            avg = round(np.mean(thickness_vals), 2) if thickness_vals else 0
            stats["thickness_avg"] = avg
            log.append(f"Layers: {len(sorted_masks)}, Avg Thk: {avg}")
        elif data_type == "Particle":
            avg = round(np.mean(diameters), 2) if diameters else 0
            stats["diameter_avg"] = avg
            log.append(f"Particles: {len(sorted_masks)}")
        return overlay, stats, log

    # --- Router ---
    @staticmethod
    def process_image_nv31(img_bytes, equipment, data_type, mode, goal, user_center=None, patch_size=128, step=16, n_components=10):
        nparr = np.frombuffer(img_bytes, np.uint8); img_raw = cv2.imdecode(nparr, cv2.IMREAD_COLOR)
        if img_raw is None: return {}
        body, footer = ScienceProcessorNV31.separate_footer(img_raw)
        footer_b64 = base64.b64encode(cv2.imencode('.jpg', footer)[1]).decode('utf-8') if footer is not None else None
        
        if data_type == "Crystal Structure (FFT)":
            return ScienceProcessorNV31.analyze_crystal_fft_nmf(body, patch_size, step, n_components)
        if data_type == "2D Diffraction":
            res = ScienceProcessorNV31.analyze_diffraction_nv7(body, user_center)
            return {**res, "footer_b64": footer_b64}
        
        if mode == "None":
             def to_b64(im): return base64.b64encode(cv2.imencode('.jpg', im)[1]).decode('utf-8')
             return {"type":"image", "raw_b64":to_b64(body), "proc_b64":to_b64(body), "stats":{}, "log":["Mode: Raw"], "footer_b64": footer_b64}

        img_proc = body.copy(); log = []
        if mode == "Auto" or mode == "AI-Adaptive": img_proc = cv2.bilateralFilter(img_proc, 9, 75, 75)
        img_rgb = cv2.cvtColor(img_proc, cv2.COLOR_BGR2RGB)
        if mask_generator:
            try:
                masks = mask_generator.generate(img_rgb)
                overlay, stats, sam_log = ScienceProcessorNV31.analyze_sam_results_nv7(img_rgb, masks, data_type)
                log.extend(sam_log); overlay = cv2.cvtColor(overlay, cv2.COLOR_RGB2BGR)
            except Exception as e: log.append(f"SAM Error: {str(e)}"); overlay = body; stats = {}
        else: log.append("SAM Not Loaded"); overlay = body; stats = {}
        def to_b64(im): return base64.b64encode(cv2.imencode('.jpg', im)[1]).decode('utf-8')
        return {"type": "image", "raw_b64": to_b64(body), "proc_b64": to_b64(overlay), "stats": stats, "log": log, "footer_b64": footer_b64}

# [4] Helper (Same)
async def analyze_vision_ollama(image_bytes: bytes, prompt: str) -> str:
    try:
        b64 = base64.b64encode(image_bytes).decode('utf-8')
        res = await asyncio.to_thread(ollama_client.chat, model=VISION_MODEL, messages=[{'role':'user','content':prompt,'images':[b64]}])
        return res['message']['content']
    except: return "Vision Error"

async def interpret_spectrum_data(raw_context: str, equipment: str) -> str:
    try:
        prompt = f"Analyze spectrum ({equipment}). Brief interpretation in Korean. Data: {raw_context}"
        res = await asyncio.to_thread(ollama_client.chat, model=TEXT_MODEL, messages=[{'role':'user','content':prompt}])
        return res['message']['content']
    except: return ""

# [5] App
app = FastAPI(title="Analyst NV31 Final Logic Fix", lifespan=lifespan)
app.add_middleware(CORSMiddleware, allow_origins=["*"], allow_methods=["*"], allow_headers=["*"])

class TranslateReq(BaseModel): text: str
@app.post("/api/translate")
async def translate_text(req: TranslateReq):
    try:
        res = await asyncio.to_thread(ollama_client.chat, model=TEXT_MODEL, messages=[{'role':'system','content':'Translate to English.'}, {'role':'user','content':req.text}])
        return {"translated": res['message']['content']}
    except Exception as e: return {"translated": str(e)}

async def process_analysis_job(job_id: str, files_map: Dict, configs: Dict):
    try:
        JOBS[job_id]["status"] = "Processing"; JOBS[job_id]["progress"] = 10; final_results = []; total = len(configs); cnt = 0
        for filename, config in configs.items():
            if filename not in files_map: continue
            content = files_map[filename]; eq = config.get("equipment", "General"); data_type = config.get("data_type", "General")
            goal = config.get("goal", ""); mode = config.get("mode", "Auto")
            p_size = int(config.get("patch_size", 128)); p_step = int(config.get("step", 16)); n_comp = int(config.get("n_components", 10))
            fname_lower = filename.lower(); JOBS[job_id]["step"] = f"Analyzing {filename}..."
            
            if eq == "Document" or fname_lower.endswith(('.pdf', '.ppt', '.pptx')):
                try:
                    if fname_lower.endswith('.pdf'):
                        pdf = pdfium.PdfDocument(io.BytesIO(content)); n_pages = len(pdf); full_txt = ""; pages = []
                        for i in range(min(n_pages, 10)):
                            page = pdf[i]; bitmap = page.render(scale=2); pil_image = bitmap.to_pil()
                            buf = io.BytesIO(); pil_image.save(buf, format="JPEG"); b64 = base64.b64encode(buf.getvalue()).decode()
                            desc = await asyncio.to_thread(ollama_client.chat, model=VISION_MODEL, messages=[{'role':'user','content':'한글 요약.','images':[b64]}])
                            d_txt = desc['message']['content']; full_txt += f"\nPage {i+1}: {d_txt}"
                            pages.append({"id": str(uuid.uuid4()), "page_num": i+1, "image_b64": b64, "desc": d_txt})
                        final_results.append({"type": "pdf", "filename": filename, "equipment": "Lit", "raw_context": full_txt, "pages": pages})
                    elif fname_lower.endswith(('.ppt', '.pptx')):
                        if fname_lower.endswith('.ppt'): final_results.append({"type":"error", "filename":filename, "msg":".ppt not supported"})
                        else:
                            prs = Presentation(io.BytesIO(content)); slides = []; full_txt = ""
                            for i, slide in enumerate(prs.slides):
                                txt = ""; imgs = []
                                for s in slide.shapes:
                                    if hasattr(s, "text"): txt += s.text + "\n"
                                    if s.shape_type == MSO_SHAPE_TYPE.PICTURE:
                                        try:
                                            ib64 = base64.b64encode(s.image.blob).decode('utf-8')
                                            imgs.append({"b64":ib64, "desc":"Img"})
                                        except: pass
                                if not txt.strip(): txt = "(No text)"
                                full_txt += f"Slide {i+1}: {txt}\n"; slides.append({"id": str(uuid.uuid4()), "slide_num": i+1, "text": txt, "images": imgs})
                            final_results.append({"type": "ppt", "filename": filename, "equipment": "Lit", "raw_context": full_txt, "slides": slides})
                except Exception as e: final_results.append({"type": "error", "filename": filename, "msg": str(e)})

            elif fname_lower.endswith(('.xlsx', '.xls', '.csv', '.txt')):
                try:
                    dfs = ScienceProcessorNV31.read_universal_dataframe(content, filename)
                    for i, df in enumerate(dfs):
                        v_blocks = ScienceProcessorNV31.detect_structured_blocks(df)
                        for v_block in v_blocks:
                            h_blocks = ScienceProcessorNV31.split_horizontal_blocks(v_block)
                            for h_block in h_blocks:
                                series_list = ScienceProcessorNV31.extract_series_from_df(h_block)
                                for s in series_list:
                                    res = ScienceProcessorNV31.process_spectrum(s['x'], s['y'], mode, goal)
                                    step = max(1, len(s['x'])//100)
                                    chart = [{"x": float(res["x"][k]), "y_proc": float(res["y_proc"][k]), "y_raw": float(res["y_raw"][k]), "y_base": float(res["y_base"][k])} for k in range(0, len(s['x']), step)]
                                    ctx = f"Data: {s['name']}\nStats: {res['stats']}"
                                    interp = await interpret_spectrum_data(ctx, eq)
                                    final_results.append({
                                        "type": "spectrum", "filename": f"{filename} ({s['name']})", "equipment": eq,
                                        "raw_context": f"{ctx}\nInterp: {interp}", 
                                        "chart_data": chart, "log": res["log"], "interpretation": interp
                                    })
                except Exception as ex: final_results.append({"type": "error", "filename": filename, "msg": str(ex)})

            elif fname_lower.endswith(('.png', '.jpg', '.jpeg', '.tif', '.bmp')):
                try:
                    vis_res = await asyncio.to_thread(ScienceProcessorNV31.process_image_nv31, content, eq, data_type, mode, goal, None, p_size, p_step, n_comp)
                    if vis_res:
                        scale_info = "Unknown"
                        final_footer = vis_res.get("footer_b64")
                        if final_footer and data_type != "Crystal Structure (FFT)":
                            scale_res = await asyncio.to_thread(ollama_client.chat, model=VISION_MODEL, messages=[{'role':'user','content':'Read scale bar text only.','images':[final_footer]}])
                            scale_info = scale_res['message']['content']; vis_res["log"].append(f"Scale: {scale_info}")
                        
                        stats_clean = "\n".join([f"- {k}: {v}" for k,v in vis_res.get('stats',{}).items()])
                        prompt = f"Analyze {eq} image. Stats: {stats_clean}. Goal: {goal}. Summary in Korean."
                        desc_res = await asyncio.to_thread(ollama_client.chat, model=VISION_MODEL, messages=[{'role':'user','content':prompt,'images':[vis_res["proc_b64"]]}])
                        desc = desc_res['message']['content']
                        final_results.append({"type":"image", "filename":filename, "equipment":eq, "summary":desc, "raw_context":f"Img: {desc}\nStats:{stats_clean}", "raw_b64":vis_res["raw_b64"], "proc_b64":vis_res["proc_b64"], "footer_b64":final_footer, "stats":vis_res.get("stats",{}), "log":vis_res.get("log",[])})
                except Exception as ex: final_results.append({"type":"error", "filename":filename, "msg":str(ex)})
            
            cnt += 1; JOBS[job_id]["progress"] = 10 + int((cnt/total)*80)

        JOBS[job_id]["step"] = "Writing Report..."; data_ctx = ""
        for r in final_results:
            if r.get("type")!="error": data_ctx += f"\n=== {r['filename']} ===\n{r.get('raw_context','')[:2000]}\n"
        
        rep_res = await asyncio.to_thread(ollama_client.chat, model=TEXT_MODEL, messages=[{'role':'system','content':'Write scientific report in Korean.'}, {'role':'user','content':f"Data:\n{data_ctx}"}])
        JOBS[job_id]["results"] = {"results": final_results, "final_report": rep_res['message']['content']}
        JOBS[job_id]["status"] = "Completed"; JOBS[job_id]["progress"] = 100
    except Exception as e: JOBS[job_id]["status"] = "Failed"; JOBS[job_id]["error"] = str(e)

@app.post("/api/analyze")
async def start_analysis(background_tasks: BackgroundTasks, files: List[UploadFile] = File(...), file_configs: str = Form(...)):
    job_id = str(uuid.uuid4()); configs = json.loads(file_configs)
    files_map = {f.filename: await f.read() for f in files}
    JOBS[job_id] = {"status": "Pending", "progress": 0, "step": "Init", "results": None}
    background_tasks.add_task(process_analysis_job, job_id, files_map, configs)
    return {"job_id": job_id}

@app.get("/api/status/{job_id}")
async def get_status(job_id: str): return JOBS.get(job_id, {"status": "Not Found"})
@app.get("/", response_class=HTMLResponse)
async def serve_index(): return FileResponse("index.html") if os.path.exists("index.html") else "<h1>NV31 Backend</h1>"

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)



NV29 Complete

import os
import io
# [0] OMP Error Fix
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"
import torch

import asyncio
import base64
import json
import re
import math
import uuid
import time
from typing import List, Dict, Any, Optional
from pydantic import BaseModel
from contextlib import asynccontextmanager
from collections import Counter

# [1] Security
os.environ['NO_PROXY'] = '127.0.0.1,localhost,0.0.0.0,::1'
if 'HTTP_PROXY' in os.environ: del os.environ['HTTP_PROXY']
if 'HTTPS_PROXY' in os.environ: del os.environ['HTTPS_PROXY']

from fastapi import FastAPI, UploadFile, File, Form, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import HTMLResponse, FileResponse
import pandas as pd
import numpy as np

# SciPy & CV & Sklearn
from sklearn.decomposition import NMF
from scipy.signal import find_peaks, savgol_filter
from scipy.optimize import curve_fit
from scipy.ndimage import center_of_mass
from skimage.feature import peak_local_max
import cv2
import matplotlib
import matplotlib.pyplot as plt 

# Models
import pypdfium2 as pdfium 
from segment_anything import sam_model_registry, SamAutomaticMaskGenerator
from ollama import Client 
from pptx import Presentation
from pptx.enum.shapes import MSO_SHAPE_TYPE

# [2] Config
OLLAMA_HOST = "http://127.0.0.1:11434"
ollama_client = Client(host=OLLAMA_HOST)
VISION_MODEL = "qwen3-vl:30b-a3b-instruct" 
TEXT_MODEL = "llama3:70b"

SAM_CHECKPOINT = "sam_vit_b_01ec64.pth"
SAM_MODEL_TYPE = "vit_b"
device = "cuda" if torch.cuda.is_available() else "cpu"

mask_generator = None 
JOBS = {} 

@asynccontextmanager
async def lifespan(app: FastAPI):
    global mask_generator
    if os.path.exists(SAM_CHECKPOINT):
        print(f"Loading SAM ({SAM_MODEL_TYPE}) on {device}...")
        try:
            sam = sam_model_registry[SAM_MODEL_TYPE](checkpoint=SAM_CHECKPOINT)
            sam.to(device=device)
            mask_generator = SamAutomaticMaskGenerator(
                model=sam, points_per_side=32, pred_iou_thresh=0.86,
                stability_score_thresh=0.90, min_mask_region_area=50
            )
            print("SAM Loaded.")
        except: print("SAM Load Failed.")
    yield
    JOBS.clear()

def sanitize_json(obj):
    if isinstance(obj, float):
        if np.isnan(obj) or np.isinf(obj): return 0.0
        return obj
    elif isinstance(obj, dict): return {k: sanitize_json(v) for k, v in obj.items()}
    elif isinstance(obj, list): return [sanitize_json(v) for v in obj]
    elif isinstance(obj, (np.int64, np.int32)): return int(obj)
    elif isinstance(obj, (np.float64, np.float32)):
        if np.isnan(obj) or np.isinf(obj): return 0.0
        return float(obj)
    return obj

# ==========================================
# [3] NV29 Matrix Structure Parser
# ==========================================
class ScienceProcessorNV29:
    
    # --- Universal Loader ---
    @staticmethod
    def read_universal_dataframe(content: bytes, filename: str) -> List[pd.DataFrame]:
        dfs = []
        fname = filename.lower()
        if fname.endswith(('.xlsx', '.xls')):
            try:
                xls = pd.ExcelFile(io.BytesIO(content))
                for s in xls.sheet_names:
                    df = pd.read_excel(xls, sheet_name=s, header=None)
                    # Simple validity check
                    if df.apply(pd.to_numeric, errors='coerce').notna().sum().sum() > 5:
                        dfs.append(df)
                if dfs: return dfs
            except: pass
        
        # Text/CSV Logic
        text_content = ""
        for enc in ['utf-8', 'cp949', 'latin1']:
            try: text_content = content.decode(enc); break
            except: continue
            
        if text_content:
            lines = text_content.splitlines()
            # Find start of data (skip headers)
            data_start = -1; delimiter = None
            separators = [',', '\t', ';', '\s+']
            
            for i, line in enumerate(lines[:100]):
                for sep in separators:
                    parts = line.split() if sep=='\s+' else line.split(sep)
                    # Check if row has at least 2 numbers
                    nums = sum(1 for p in parts if re.match(r'^-?\d+(\.\d+)?(?:[eE][+-]?\d+)?$', p.strip()))
                    if nums >= 2: 
                        data_start = i; delimiter = sep; break
                if data_start != -1: break
                
            if data_start != -1:
                try:
                    data_str = "\n".join(lines[data_start:])
                    sep_arg = '\s+' if delimiter=='\s+' else delimiter
                    df = pd.read_csv(io.StringIO(data_str), sep=sep_arg, header=None, engine='python')
                    dfs.append(df)
                except: pass
        return dfs

    # --- Block Parser ---
    @staticmethod
    def detect_structured_blocks(df: pd.DataFrame) -> List[pd.DataFrame]:
        blocks = []
        if df.empty: return []
        
        # Convert everything to numeric (headers become NaN)
        df_num = df.apply(pd.to_numeric, errors='coerce')
        
        # Row is valid if it has at least one valid number
        is_data_row = df_num.notna().any(axis=1)
        
        # Group consecutive data rows
        groups = (is_data_row != is_data_row.shift()).cumsum()
        
        for _, group in df.groupby(groups):
            if is_data_row.loc[group.index[0]]:
                # Extract block
                block = df_num.loc[group.index]
                # Drop columns that are completely NaN in this block (Clean up spacing)
                block = block.dropna(how='all', axis=1)
                
                if block.shape[0] >= 5: 
                    blocks.append(block.reset_index(drop=True))
        return blocks

    @staticmethod
    def split_horizontal_blocks(df: pd.DataFrame) -> List[pd.DataFrame]:
        # For simple matrix structure, splitting usually isn't needed unless
        # there's a huge gap of NaNs in the middle. 
        # Keep simple for now.
        return [df]

    # [NV29 Core Logic] Intelligent Matrix Parsing
    @staticmethod
    def extract_series_from_df(df: pd.DataFrame) -> List[Dict]:
        series_list = []
        try:
            # df contains numeric values (or NaNs)
            vals = df.values
            rows, cols = vals.shape
            if rows < 5: return []
            
            # 1. Identify Index Column (X-axis)
            # Find the first column that is monotonic (increasing or decreasing)
            # and has enough valid data points.
            index_col_idx = -1
            current_x = None
            
            for c in range(cols):
                col_data = vals[:, c]
                valid_mask = ~np.isnan(col_data)
                valid_data = col_data[valid_mask]
                
                if len(valid_data) < 5: continue
                
                # Check monotonicity
                diff = np.diff(valid_data)
                is_increasing = np.all(diff >= 0)
                is_decreasing = np.all(diff <= 0)
                
                # If monotonic, this is our Index.
                if is_increasing or is_decreasing:
                    index_col_idx = c
                    current_x = col_data
                    break
            
            # If no monotonic column found, generate 0..N index
            if index_col_idx == -1:
                current_x = np.arange(rows)
            
            # 2. Extract Data Columns (Y-axis)
            # All OTHER columns that contain numbers are treated as Data.
            data_count = 1
            for c in range(cols):
                if c == index_col_idx: continue # Skip the index itself
                
                col_data = vals[:, c]
                mask = ~np.isnan(current_x) & ~np.isnan(col_data)
                
                if np.sum(mask) < 5: continue # Skip empty/garbage columns
                
                # Extract valid X, Y pairs
                x_valid = current_x[mask]
                y_valid = col_data[mask]
                
                # [Crucial Fix] Sort by X to handle Descending Index (A6~A1000)
                # Plotly needs sorted X to draw lines correctly (otherwise connects start to end)
                sort_indices = np.argsort(x_valid)
                x_sorted = x_valid[sort_indices]
                y_sorted = y_valid[sort_indices]
                
                series_list.append({
                    "x": x_sorted,
                    "y": y_sorted,
                    "name": f"Col-{data_count}"
                })
                data_count += 1
                
        except Exception as e: print(f"Extract Error: {e}")
        return series_list

    # --- Spectrum Logic (Same as NV28 - Respect Mode) ---
    @staticmethod
    def simple_baseline(y):
        try:
            w = max(5, len(y)//10)
            s = pd.Series(y).rolling(window=w, center=True).min()
            b = s.bfill().ffill().values 
            if len(b) > 51: b = savgol_filter(b, 51, 3)
            return b, w
        except: return np.zeros_like(y), 0

    @staticmethod
    def process_spectrum(x, y, mode, goal):
        try:
            y_raw = y.copy()
            y_base = np.zeros_like(y)
            y_proc = y.copy()
            
            # "None" -> Raw Data
            if mode == "None":
                peaks, _ = find_peaks(y, height=np.max(y)*0.05)
                return {
                    "x": x, "y_raw": y_raw, "y_proc": y_raw, "y_base": y_base, 
                    "peaks": [{"x": float(x[p]), "y": float(y[p])} for p in peaks], 
                    "log": ["Mode: Raw"], "stats": f"Raw Peaks: {len(peaks)}"
                }
            
            # "Auto" -> Process
            elif mode == "Auto" or mode == "AI-Adaptive":
                win = 15
                if mode == "AI-Adaptive" and "noise" in goal.lower(): win = 31
                
                # Baseline
                base, bw = ScienceProcessorNV29.simple_baseline(y)
                y_proc = np.maximum(y - base, 0)
                y_base = base
                
                # Smooth
                if len(y_proc) > win: y_proc = savgol_filter(y_proc, win, 3)
                
                peaks, _ = find_peaks(y_proc, height=np.max(y_proc)*0.05, distance=10)
                return {
                    "x": x, "y_raw": y_raw, "y_proc": y_proc, "y_base": y_base, 
                    "peaks": [{"x": float(x[p]), "y": float(y_proc[p])} for p in peaks], 
                    "log": [f"Mode: {mode}"], "stats": f"Proc Peaks: {len(peaks)}"
                }
            else:
                return {"x": x, "y_raw": y, "y_proc": y, "y_base": y_base, "peaks": [], "log": ["Unknown"], "stats": ""}
        except Exception as e: 
            return {"x": x, "y_raw": y, "y_proc": y, "y_base": np.zeros_like(y), "peaks": [], "log": [str(e)], "stats": "Error"}

    # --- Footer ---
    @staticmethod
    def separate_footer(img_bgr):
        h, w = img_bgr.shape[:2]; roi_h = int(h * 0.82); roi = img_bgr[roi_h:, :]
        gray = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)
        sobel_y = np.abs(cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=3))
        row_sums = np.sum(sobel_y, axis=1)
        split_idx = np.argmax(row_sums)
        if row_sums[split_idx] > w * 30: return img_bgr[:roi_h + split_idx, :], img_bgr[roi_h + split_idx:, :]
        return img_bgr, None

    # --- [E] FFT-NMF (NV26) ---
    @staticmethod
    def analyze_crystal_fft_nmf(img_bgr, patch_size=128, step=16, n_components=10):
        log = []; gray = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2GRAY); h, w = gray.shape
        log.append(f"FFT-NMF: Patch={patch_size}, Step={step}, K={n_components}")
        custom_colors = [(0,0,255),(0,255,0),(255,0,0),(0,255,255),(255,255,0),(255,0,255),(0,165,255),(238,130,238),(128,128,0),(19,69,139)]
        while len(custom_colors) < n_components: custom_colors.append((255,255,255))
        cols = []; y_steps = list(range(0, h-patch_size+1, step)); x_steps = list(range(0, w-patch_size+1, step))
        grid_h = len(y_steps); grid_w = len(x_steps)
        cy, cx = patch_size//2, patch_size//2
        Y, X = np.ogrid[:patch_size, :patch_size]; dc_mask = ((X-cx)**2 + (Y-cy)**2) <= 4**2
        for y in y_steps:
            for x in x_steps:
                roi = gray[y:y+patch_size, x:x+patch_size]
                fshift = np.fft.fftshift(np.fft.fft2(roi))
                mag = np.abs(fshift); mag[dc_mask] = 0; cols.append(mag.flatten())
        if not cols: return {"type":"error", "msg":"Img too small"}
        try:
            V = np.array(cols).T.astype(np.float32); V = V - V.min()
            if V.max() > 0: V = V/V.max()
            nmf = NMF(n_components=n_components, init='nndsvda', random_state=42, max_iter=2000)
            W = nmf.fit_transform(V); H = nmf.components_
            if H.shape[1] != grid_h*grid_w: raise ValueError("Shape Mismatch")
            idx_H = np.argmax(H, axis=0)
            map_high = cv2.resize(idx_H.reshape(grid_h, grid_w).astype(np.uint8), (w, h), interpolation=cv2.INTER_NEAREST)
            overlay = img_bgr.copy(); mask_layer = np.zeros_like(img_bgr)
            counts = Counter(idx_H); total = len(idx_H); stats = {}
            for i in range(n_components):
                mask = (map_high == i); mask_layer[mask] = custom_colors[i]
                cnt = counts.get(i, 0); stats[f"Pattern {i}"] = f"{cnt} patches ({cnt/total*100:.1f}%)"
            mask_idx = np.any(mask_layer > 0, axis=-1)
            overlay[mask_idx] = (overlay[mask_idx]*(1-0.4) + mask_layer[mask_idx]*0.4).astype(np.uint8)
            basis_combo_list = []
            for k in range(n_components):
                pat = W[:, k].reshape(patch_size, patch_size)
                pat_vis = cv2.applyColorMap(cv2.resize(cv2.normalize(np.log1p(pat*1000),None,0,255,cv2.NORM_MINMAX).astype(np.uint8),(128,128),interpolation=cv2.INTER_NEAREST), cv2.COLORMAP_MAGMA)
                cv2.putText(pat_vis, "W (Basis)", (5, 15), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255,255,255), 1)
                h_vec = H[k, :]; h_vis = cv2.applyColorMap(cv2.resize(cv2.normalize(h_vec.reshape(grid_h,grid_w),None,0,255,cv2.NORM_MINMAX).astype(np.uint8),(128,128),interpolation=cv2.INTER_NEAREST), cv2.COLORMAP_VIRIDIS)
                cv2.putText(h_vis, "H (Dist)", (5, 15), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255,255,255), 1)
                cbar_vis = cv2.applyColorMap(np.tile(np.linspace(0,255,128).astype(np.uint8),(25,1)), cv2.COLORMAP_VIRIDIS)
                cv2.putText(cbar_vis, f"{h_vec.min():.1f}", (2,17), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255,255,255), 1)
                cv2.putText(cbar_vis, f"{h_vec.max():.1f}", (128-40,17), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (0,0,0), 1)
                legend = np.zeros((30, 128, 3), dtype=np.uint8); legend[:] = custom_colors[k]
                cv2.putText(legend, f"P{k}", (50, 22), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255,255,255), 2)
                combo = np.vstack([pat_vis, h_vis, cbar_vis, legend])
                cv2.rectangle(combo, (0,0), (127, combo.shape[0]-1), custom_colors[k], 2)
                basis_combo_list.append(combo)
            full_strip_large = np.hstack(basis_combo_list)
            def to_b64(im): return base64.b64encode(cv2.imencode('.jpg', im)[1]).decode('utf-8')
            return {"type":"image", "raw_b64":to_b64(img_bgr), "proc_b64":to_b64(overlay), "footer_b64":to_b64(full_strip_large), "stats":stats, "log":log}
        except Exception as e: return {"type":"image", "raw_b64":base64.b64encode(cv2.imencode('.jpg', img_bgr)[1]).decode('utf-8'), "proc_b64":base64.b64encode(cv2.imencode('.jpg', img_bgr)[1]).decode('utf-8'), "stats":{"Error":str(e)}, "log":log}

    # --- [F] Diffraction ---
    @staticmethod
    def analyze_diffraction_nv7(img_bgr, user_center=None):
        log = []; stats = {}
        gray = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2GRAY)
        if user_center and len(user_center)==2 and user_center[0]>0:
            cx, cy = user_center
            log.append(f"Center: Manual [{cx}, {cy}]")
        else:
            _, th = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY+cv2.THRESH_OTSU)
            cyf, cxf = center_of_mass(th)
            if np.isnan(cyf): cyf, cxf = gray.shape[0]//2, gray.shape[1]//2
            cx, cy = int(cxf), int(cyf)
            log.append(f"Center: Auto [{cx}, {cy}]")

        g1 = cv2.GaussianBlur(gray, (3,3), 0)
        g2 = cv2.GaussianBlur(gray, (51,51), 0)
        diff = cv2.subtract(g1, g2)
        coords = peak_local_max(diff, min_distance=10, threshold_abs=20)
        enhanced = np.log1p(diff.astype(np.float32))
        enhanced = cv2.normalize(enhanced, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)
        vis = cv2.cvtColor(enhanced, cv2.COLOR_GRAY2BGR)
        cv2.drawMarker(vis, (cx, cy), (0,0,255), cv2.MARKER_CROSS, 20, 2)
        peak_list = []
        for p in coords:
            py, px = p
            if abs(py-cy)<5 and abs(px-cx)<5: continue
            cv2.circle(vis, (px, py), 3, (0,255,0), 1)
            peak_list.append(f"[{px},{py}]")
        log.append(f"Spots: {len(peak_list)}")
        def to_b64(im): return base64.b64encode(cv2.imencode('.jpg', im)[1]).decode('utf-8')
        return {"type": "image", "raw_b64": to_b64(img_bgr), "proc_b64": to_b64(vis), "stats": {"center": [cx, cy], "spots": len(peak_list)}, "log": log}

    # --- [G] SAM ---
    @staticmethod
    def analyze_sam_results_nv7(img_rgb, masks, data_type):
        overlay = img_rgb.copy()
        mask_overlay = np.zeros_like(img_rgb)
        h_img, w_img = img_rgb.shape[:2]
        stats = {"count": 0}
        log = []
        valid_masks = []
        for ann in masks:
            area = ann['area']; bbox = ann['bbox']; x, y, w, h = [int(v) for v in bbox]
            if area > h_img * w_img * 0.95 or area < 20: continue 
            if data_type == "Particle":
                if x<=1 or y<=1 or (x+w)>=w_img-1 or (y+h)>=h_img-1: continue
                mask_uint8 = (ann['segmentation'] * 255).astype(np.uint8)
                cnts, _ = cv2.findContours(mask_uint8, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
                if cnts:
                    perim = cv2.arcLength(cnts[0], True)
                    circ = 4*np.pi*area/(perim**2) if perim>0 else 0
                    if circ < 0.25: continue
            valid_masks.append(ann)

        if data_type == "Thin Film":
            sorted_masks = sorted(valid_masks, key=lambda x: x['bbox'][1])
        else:
            sorted_masks = sorted(valid_masks, key=lambda x: x['area'], reverse=True)

        thickness_vals = []; diameters = []
        for i, ann in enumerate(sorted_masks):
            m = ann['segmentation']
            bbox = ann['bbox']; x, y, w, h = [int(v) for v in bbox]
            cx, cy = int(x + w/2), int(y + h/2)
            color = (0, 255, 0) 
            if data_type == "Thin Film":
                is_bound = (i == 0 or i == len(sorted_masks)-1)
                cv2.putText(overlay, f"#{i+1}", (cx, cy), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (255, 255, 255), 3)
                if not is_bound:
                    thk = ann['area'] / w
                    thickness_vals.append(thk)
                    color = (255, 0, 0) 
                else: color = (100, 100, 100)
                if i < len(sorted_masks) - 1:
                    ys, xs = np.where(m)
                    if len(xs) > 10:
                        roughness = np.std(ys)
                        cv2.line(overlay, (x, y+h), (x+w, y+h), (0, 255, 255), 2)
                        cv2.putText(overlay, f"Int#{i+1} R:{roughness:.2f}", (x+10, y+h-5), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)
            elif data_type == "Particle":
                equivalent_diameter = 2 * np.sqrt(ann['area'] / np.pi)
                diameters.append(equivalent_diameter)
            mask_overlay[m] = color
            mask_uint8 = (m * 255).astype(np.uint8)
            cnts, _ = cv2.findContours(mask_uint8, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
            if cnts: cv2.drawContours(overlay, [cnts[0]], -1, color, 1)

        alpha = 0.35
        mask_idx = np.any(mask_overlay > 0, axis=-1)
        overlay[mask_idx] = (overlay[mask_idx]*(1-alpha) + mask_overlay[mask_idx]*alpha).astype(np.uint8)
        stats["count"] = len(sorted_masks)
        if data_type == "Thin Film":
            avg = round(np.mean(thickness_vals), 2) if thickness_vals else 0
            stats["thickness_avg"] = avg
            log.append(f"Layers: {len(sorted_masks)}, Avg Thk: {avg}")
        elif data_type == "Particle":
            avg = round(np.mean(diameters), 2) if diameters else 0
            stats["diameter_avg"] = avg
            log.append(f"Particles: {len(sorted_masks)}")
        return overlay, stats, log

    # --- Router ---
    @staticmethod
    def process_image_nv29(img_bytes, equipment, data_type, mode, goal, user_center=None, 
                           patch_size=128, step=16, n_components=10):
        nparr = np.frombuffer(img_bytes, np.uint8)
        img_raw = cv2.imdecode(nparr, cv2.IMREAD_COLOR)
        if img_raw is None: return {}
        
        body, footer = ScienceProcessorNV29.separate_footer(img_raw)
        footer_b64 = None
        if footer is not None:
            footer_b64 = base64.b64encode(cv2.imencode('.jpg', footer)[1]).decode('utf-8')

        if data_type == "Crystal Structure (FFT)":
            return ScienceProcessorNV29.analyze_crystal_fft_nmf(body, patch_size, step, n_components)
            
        if data_type == "2D Diffraction":
            return {**ScienceProcessorNV29.analyze_diffraction_nv7(body, user_center), "footer_b64": footer_b64}
        
        if mode == "None":
             def to_b64(im): return base64.b64encode(cv2.imencode('.jpg', im)[1]).decode('utf-8')
             return {"type":"image", "raw_b64":to_b64(body), "proc_b64":to_b64(body), "stats":{}, "log":["Mode: Raw"], "footer_b64": footer_b64}

        img_proc = body.copy(); log = []
        if mode == "Auto" or mode == "AI-Adaptive":
            img_proc = cv2.bilateralFilter(img_proc, 9, 75, 75)
            log.append("Preproc: Denoise")

        img_rgb = cv2.cvtColor(img_proc, cv2.COLOR_BGR2RGB)
        
        if mask_generator:
            try:
                masks = mask_generator.generate(img_rgb)
                log.append(f"SAM: {len(masks)} Segments")
                overlay, stats, sam_log = ScienceProcessorNV29.analyze_sam_results_nv7(img_rgb, masks, data_type)
                log.extend(sam_log)
                overlay = cv2.cvtColor(overlay, cv2.COLOR_RGB2BGR)
            except Exception as e:
                log.append(f"SAM Error: {str(e)}")
                overlay = body; stats = {}
        else:
            log.append("SAM Not Loaded")
            overlay = body; stats = {}

        def to_b64(im): return base64.b64encode(cv2.imencode('.jpg', im)[1]).decode('utf-8')
        return {"type": "image", "raw_b64": to_b64(body), "proc_b64": to_b64(overlay), "stats": stats, "log": log, "footer_b64": footer_b64}

# [4] Helper (Same)
async def analyze_vision_ollama(image_bytes: bytes, prompt: str) -> str:
    try:
        b64 = base64.b64encode(image_bytes).decode('utf-8')
        res = await asyncio.to_thread(ollama_client.chat, model=VISION_MODEL, messages=[{'role':'user','content':prompt,'images':[b64]}])
        return res['message']['content']
    except: return "Vision Error"

async def interpret_spectrum_data(raw_context: str, equipment: str) -> str:
    try:
        prompt = f"Analyze spectrum ({equipment}). Brief interpretation in Korean. Data: {raw_context}"
        res = await asyncio.to_thread(ollama_client.chat, model=TEXT_MODEL, messages=[{'role':'user','content':prompt}])
        return res['message']['content']
    except: return ""

# [5] App
app = FastAPI(title="Analyst NV29 Matrix Parser", lifespan=lifespan)
app.add_middleware(CORSMiddleware, allow_origins=["*"], allow_methods=["*"], allow_headers=["*"])

class TranslateReq(BaseModel): text: str

@app.post("/api/translate")
async def translate_text(req: TranslateReq):
    try:
        res = await asyncio.to_thread(ollama_client.chat, model=TEXT_MODEL, messages=[{'role':'system','content':'Translate to English.'}, {'role':'user','content':req.text}])
        return {"translated": res['message']['content']}
    except Exception as e: return {"translated": str(e)}

async def process_analysis_job(job_id: str, files_map: Dict, configs: Dict):
    try:
        JOBS[job_id]["status"] = "Processing"
        JOBS[job_id]["progress"] = 10
        final_results = []
        total_files = len(configs)
        processed_count = 0

        for filename, config in configs.items():
            if filename not in files_map: continue
            content = files_map[filename]
            eq = config.get("equipment", "General")
            data_type = config.get("data_type", "General")
            goal = config.get("goal", "")
            mode = config.get("mode", "Auto")
            
            user_center = None
            if config.get("center_coords"):
                try: user_center = [int(v) for v in config["center_coords"].split(",")]
                except: pass
            
            p_size = int(config.get("patch_size", 128))
            p_step = int(config.get("step", 16))
            n_comp = int(config.get("n_components", 10))

            fname_lower = filename.lower()
            JOBS[job_id]["step"] = f"Analyzing {filename}..."
            
            if eq == "Document" or fname_lower.endswith(('.pdf', '.ppt', '.pptx')):
                try:
                    if fname_lower.endswith('.pdf'):
                        pdf = pdfium.PdfDocument(io.BytesIO(content))
                        n_pages = len(pdf)
                        full_txt = ""; pages = []
                        for i in range(min(n_pages, 10)):
                            page = pdf[i]
                            bitmap = page.render(scale=2)
                            pil_image = bitmap.to_pil()
                            buf = io.BytesIO()
                            pil_image.save(buf, format="JPEG")
                            b64 = base64.b64encode(buf.getvalue()).decode()
                            desc = await analyze_vision_ollama(buf.getvalue(), "한글 요약.")
                            full_txt += f"\nPage {i+1}: {desc}"
                            pages.append({"id": str(uuid.uuid4()), "page_num": i+1, "image_b64": b64, "desc": desc})
                        final_results.append({"type": "pdf", "filename": filename, "equipment": "Lit", "raw_context": full_txt, "pages": pages})
                    elif fname_lower.endswith(('.ppt', '.pptx')):
                        try:
                            prs = Presentation(io.BytesIO(content))
                            slides = []; full_txt = ""
                            for i, slide in enumerate(prs.slides):
                                txt = ""; imgs = []
                                for s in slide.shapes:
                                    if hasattr(s, "text"): txt += s.text + "\n"
                                    if s.shape_type == MSO_SHAPE_TYPE.PICTURE:
                                        try:
                                            ib64 = base64.b64encode(s.image.blob).decode('utf-8')
                                            desc = "Image"
                                            imgs.append({"b64": ib64, "desc": desc})
                                        except: pass
                                if not txt.strip(): txt = "(No text)"
                                full_txt += f"Slide {i+1}: {txt}\n"
                                slides.append({"id": str(uuid.uuid4()), "slide_num": i+1, "text": txt, "images": imgs})
                            final_results.append({"type": "ppt", "filename": filename, "equipment": "Lit", "raw_context": full_txt, "slides": slides})
                        except Exception as ppt_err:
                            final_results.append({"type": "error", "filename": filename, "msg": f"PPTX Error: {ppt_err}"})
                except Exception as e: 
                    final_results.append({"type": "error", "filename": filename, "msg": str(e)})

            elif fname_lower.endswith(('.xlsx', '.xls', '.csv', '.txt')):
                try:
                    dfs = ScienceProcessorNV29.read_universal_dataframe(content, filename)
                    for i, df in enumerate(dfs):
                        v_blocks = ScienceProcessorNV29.detect_structured_blocks(df)
                        for v_block in v_blocks:
                            h_blocks = ScienceProcessorNV29.split_horizontal_blocks(v_block)
                            for h_block in h_blocks:
                                series_list = ScienceProcessorNV29.extract_series_from_df(h_block)
                                for s in series_list:
                                    res = ScienceProcessorNV29.process_spectrum(s['x'], s['y'], mode, goal)
                                    step = max(1, len(s['x'])//100)
                                    chart = [{"x": float(res["x"][k]), "y_proc": float(res["y_proc"][k]), "y_raw": float(res["y_raw"][k]), "y_base": float(res["y_base"][k])} for k in range(0, len(s['x']), step)]
                                    ctx = f"Data: {s['name']}\nStats: {res['stats']}"
                                    interp = await interpret_spectrum_data(ctx, eq)
                                    final_results.append({
                                        "type": "spectrum", "filename": f"{filename} ({s['name']})", "equipment": eq,
                                        "raw_context": f"{ctx}\nInterp: {interp}", 
                                        "chart_data": chart, "log": res["log"], "interpretation": interp
                                    })
                except Exception as ex: final_results.append({"type": "error", "filename": filename, "msg": str(ex)})

            elif fname_lower.endswith(('.png', '.jpg', '.jpeg', '.tif', '.bmp')):
                try:
                    vis_res = await asyncio.to_thread(ScienceProcessorNV29.process_image_nv29, content, eq, data_type, mode, goal, user_center, p_size, p_step, n_comp)
                    if vis_res:
                        scale_info = "Unknown"
                        final_footer = vis_res.get("footer_b64")
                        if final_footer and data_type != "Crystal Structure (FFT)":
                            scale_text = await analyze_vision_ollama(base64.b64decode(final_footer), "Read scale bar text only.")
                            scale_info = scale_text
                            vis_res["log"].append(f"Scale: {scale_text}")

                        stats_clean = ""
                        for k, v in vis_res.get('stats',{}).items():
                            stats_clean += f"- {k}: {v}\n"
                        
                        desc_prompt = f"Analyze {eq} image ({data_type}). Stats: \n{stats_clean}\n. Goal: {goal}. Summary in Korean."
                        desc = await analyze_vision_ollama(base64.b64decode(vis_res["proc_b64"]), desc_prompt)
                        
                        final_results.append({
                            "type": "image", "filename": filename, "equipment": eq, "summary": desc,
                            "raw_context": f"Img {eq}: {desc}\nStats:\n{stats_clean}", 
                            "raw_b64": vis_res["raw_b64"], "proc_b64": vis_res["proc_b64"], "footer_b64": final_footer,
                            "stats": vis_res.get("stats",{}), "log": vis_res.get("log", [])
                        })
                except Exception as ex: final_results.append({"type": "error", "filename": filename, "msg": str(ex)})
            
            processed_count += 1
            JOBS[job_id]["progress"] = 10 + int((processed_count / total_files) * 80)

        JOBS[job_id]["step"] = "Writing Report..."
        final_results = sanitize_json(final_results)
        
        data_ctx = ""
        for r in final_results:
            if r.get("type") != "error":
                raw = r.get("raw_context") or ""
                data_ctx += f"\n=== File: {r['filename']} ({r.get('equipment')}) ===\n{raw[:2000]}\n"

        final_report = "Fail"
        if data_ctx:
            try:
                system_prompt = "You are a research assistant. Write a comprehensive report in Korean."
                res = await asyncio.to_thread(ollama_client.chat, model=TEXT_MODEL, messages=[{'role':'system','content':system_prompt}, {'role':'user','content':f"Data Summaries:\n{data_ctx}"}])
                final_report = res['message']['content']
            except Exception as e: final_report = str(e)

        JOBS[job_id]["results"] = {"results": final_results, "final_report": final_report}
        JOBS[job_id]["status"] = "Completed"
        JOBS[job_id]["progress"] = 100
    except Exception as e:
        JOBS[job_id]["status"] = "Failed"; JOBS[job_id]["error"] = str(e)

@app.post("/api/analyze")
async def start_analysis(background_tasks: BackgroundTasks, files: List[UploadFile] = File(...), file_configs: str = Form(...)):
    job_id = str(uuid.uuid4()); configs = json.loads(file_configs)
    files_map = {f.filename: await f.read() for f in files}
    JOBS[job_id] = {"status": "Pending", "progress": 0, "step": "Init", "results": None}
    background_tasks.add_task(process_analysis_job, job_id, files_map, configs)
    return {"job_id": job_id}

@app.get("/api/status/{job_id}")
async def get_status(job_id: str): return JOBS.get(job_id, {"status": "Not Found"})

@app.get("/", response_class=HTMLResponse)
async def serve_index():
    if os.path.exists("index.html"): return FileResponse("index.html")
    return "<h1>NV29 Matrix Parser Backend</h1>"

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)






NV27 Fix

import os
import io
# [0] OMP Error Fix
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"
import torch

import asyncio
import base64
import json
import re
import math
import uuid
import time
from typing import List, Dict, Any, Optional
from pydantic import BaseModel
from contextlib import asynccontextmanager
from collections import Counter

# [1] Security
os.environ['NO_PROXY'] = '127.0.0.1,localhost,0.0.0.0,::1'
if 'HTTP_PROXY' in os.environ: del os.environ['HTTP_PROXY']
if 'HTTPS_PROXY' in os.environ: del os.environ['HTTPS_PROXY']

from fastapi import FastAPI, UploadFile, File, Form, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import HTMLResponse, FileResponse
import pandas as pd
import numpy as np

# SciPy & CV & Sklearn
from sklearn.decomposition import NMF
from scipy.signal import find_peaks, savgol_filter
from scipy.optimize import curve_fit
from scipy.ndimage import center_of_mass
from skimage.feature import peak_local_max
import cv2
import matplotlib
import matplotlib.pyplot as plt 

# Models
import pypdfium2 as pdfium 
from segment_anything import sam_model_registry, SamAutomaticMaskGenerator
from ollama import Client 
from pptx import Presentation
from pptx.enum.shapes import MSO_SHAPE_TYPE

# [2] Config
OLLAMA_HOST = "http://127.0.0.1:11434"
ollama_client = Client(host=OLLAMA_HOST)
VISION_MODEL = "qwen3-vl:30b-a3b-instruct" 
TEXT_MODEL = "llama3:70b"

SAM_CHECKPOINT = "sam_vit_b_01ec64.pth"
SAM_MODEL_TYPE = "vit_b"
device = "cuda" if torch.cuda.is_available() else "cpu"

mask_generator = None 
JOBS = {} 

@asynccontextmanager
async def lifespan(app: FastAPI):
    global mask_generator
    if os.path.exists(SAM_CHECKPOINT):
        print(f"Loading SAM ({SAM_MODEL_TYPE}) on {device}...")
        try:
            sam = sam_model_registry[SAM_MODEL_TYPE](checkpoint=SAM_CHECKPOINT)
            sam.to(device=device)
            mask_generator = SamAutomaticMaskGenerator(
                model=sam, points_per_side=32, pred_iou_thresh=0.86,
                stability_score_thresh=0.90, min_mask_region_area=50
            )
            print("SAM Loaded.")
        except: print("SAM Load Failed.")
    yield
    JOBS.clear()

def sanitize_json(obj):
    if isinstance(obj, float):
        if np.isnan(obj) or np.isinf(obj): return 0.0
        return obj
    elif isinstance(obj, dict): return {k: sanitize_json(v) for k, v in obj.items()}
    elif isinstance(obj, list): return [sanitize_json(v) for v in obj]
    elif isinstance(obj, (np.int64, np.int32)): return int(obj)
    elif isinstance(obj, (np.float64, np.float32)):
        if np.isnan(obj) or np.isinf(obj): return 0.0
        return float(obj)
    return obj

# ==========================================
# [3] NV27 Complete Engine
# ==========================================
class ScienceProcessorNV27:
    
    # --- Universal Loader ---
    @staticmethod
    def read_universal_dataframe(content: bytes, filename: str) -> List[pd.DataFrame]:
        dfs = []
        fname = filename.lower()
        if fname.endswith(('.xlsx', '.xls')):
            try:
                xls = pd.ExcelFile(io.BytesIO(content))
                for s in xls.sheet_names:
                    df = pd.read_excel(xls, sheet_name=s, header=None)
                    if df.apply(pd.to_numeric, errors='coerce').notna().sum().sum() > 5: dfs.append(df)
                if dfs: return dfs
            except: pass
        text_content = ""
        for enc in ['utf-8', 'cp949', 'latin1']:
            try: text_content = content.decode(enc); break
            except: continue
        if text_content:
            lines = text_content.splitlines()
            data_start = -1; delimiter = None
            separators = [',', '\t', ';', '\s+']
            for i, line in enumerate(lines[:100]):
                for sep in separators:
                    parts = line.split() if sep=='\s+' else line.split(sep)
                    nums = sum(1 for p in parts if re.match(r'^-?\d+(\.\d+)?(?:[eE][+-]?\d+)?$', p.strip()))
                    if nums >= 2: data_start = i; delimiter = sep; break
                if data_start != -1: break
            if data_start != -1:
                try:
                    data_str = "\n".join(lines[data_start:])
                    sep_arg = '\s+' if delimiter=='\s+' else delimiter
                    df = pd.read_csv(io.StringIO(data_str), sep=sep_arg, header=None, engine='python')
                    dfs.append(df)
                except: pass
        return dfs

    # --- Block Parser ---
    @staticmethod
    def detect_structured_blocks(df: pd.DataFrame) -> List[pd.DataFrame]:
        blocks = []
        if df.empty: return []
        df_num = df.apply(pd.to_numeric, errors='coerce')
        if df.shape[1] > 0: is_data_row = df_num.iloc[:, 0].notna()
        else: is_data_row = df_num.notna().any(axis=1)
        groups = (is_data_row != is_data_row.shift()).cumsum()
        for _, group in df.groupby(groups):
            if is_data_row.loc[group.index[0]]:
                block = df_num.loc[group.index].dropna(how='all', axis=1)
                if block.shape[0] >= 5: blocks.append(block.reset_index(drop=True))
        return blocks

    @staticmethod
    def split_horizontal_blocks(df: pd.DataFrame) -> List[pd.DataFrame]:
        sub_blocks = []
        if df.empty: return []
        col_counts = df.apply(pd.to_numeric, errors='coerce').notna().sum()
        valid = np.where(col_counts >= 2)[0]
        if len(valid) == 0: return []
        splits = np.where(np.diff(valid) > 1)[0] + 1
        groups = np.split(valid, splits)
        for g in groups:
            if len(g) > 0:
                sub = df.iloc[:, g].reset_index(drop=True)
                if sub.shape[1] >= 1 and sub.shape[0] >= 2: sub_blocks.append(sub)
        return sub_blocks

    # [NV27 Fix] Smart Series Extraction (Skip Constant Columns)
    @staticmethod
    def extract_series_from_df(df: pd.DataFrame) -> List[Dict]:
        series_list = []
        try:
            vals = df.values; rows, cols = vals.shape
            if rows < 5: return []
            
            def is_axis(arr):
                arr = arr[~np.isnan(arr)]
                if len(arr) < 5: return False
                diff = np.diff(arr)
                if np.all(diff >= 0) or np.all(diff <= 0): return True
                return (np.sum(diff>0)/len(diff)>0.9) or (np.sum(diff<0)/len(diff)>0.9)

            col0 = vals[:, 0]
            if is_axis(col0):
                current_x = col0; start_col = 1
            else:
                current_x = np.arange(rows); start_col = 0

            valid_idx = 1
            for i in range(start_col, cols):
                col_data = vals[:, i]
                mask = ~np.isnan(current_x) & ~np.isnan(col_data)
                
                # Skip empty or very short columns
                if np.sum(mask) < 5: continue
                
                # [Fix] Skip constant columns (Straight lines)
                y_valid = col_data[mask]
                if np.std(y_valid) == 0: continue 

                series_list.append({
                    "x": current_x[mask], 
                    "y": col_data[mask], 
                    "name": f"Col-{valid_idx}" 
                })
                valid_idx += 1
        except: pass
        return series_list

    # --- Spectrum Logic ---
    @staticmethod
    def simple_baseline(y):
        try:
            w = max(5, len(y)//5) # [Fix] More conservative window
            s = pd.Series(y).rolling(window=w, center=True).min()
            b = s.bfill().ffill().values 
            if len(b) > 51: b = savgol_filter(b, 51, 3)
            return b, w
        except: return np.zeros_like(y), 0

    # [NV27 Fix] Conservative Processing for CSV
    @staticmethod
    def process_spectrum(x, y, mode, goal):
        try:
            # Default: Minimal processing for CSV to preserve Peak Intensity
            y_raw = y.copy()
            y_base = np.zeros_like(y)
            y_proc = y.copy()
            
            if mode == "None":
                peaks, _ = find_peaks(y, height=np.max(y)*0.05)
                return {"x": x, "y_raw": y, "y_proc": y, "y_base": y_base, "peaks": [{"x": float(x[p]), "y": float(y[p])} for p in peaks], "log": ["Raw Data"], "stats": f"Peaks: {len(peaks)}"}
            
            # Auto / AI-Adaptive
            # [Fix] Do NOT subtract baseline by default for CSVs to avoid cutting peaks
            # Only smooth very slightly
            win = 5 # Very small window
            if len(y) > win:
                y_proc = savgol_filter(y, win, 3)
            
            # Only if specifically requested (e.g. "remove background"), do baseline
            if "base" in goal.lower() or "back" in goal.lower():
                base, bw = ScienceProcessorNV27.simple_baseline(y)
                y_proc = np.maximum(y_proc - base, 0)
                y_base = base

            peaks, _ = find_peaks(y_proc, height=np.max(y_proc)*0.05, distance=10)
            return {"x": x, "y_raw": y_raw, "y_proc": y_proc, "y_base": y_base, "peaks": [{"x": float(x[p]), "y": float(y_proc[p])} for p in peaks], "log": ["Minimal Smooth"], "stats": f"Peaks: {len(peaks)}"}
        except Exception as e: return {"x": x, "y_raw": y, "y_proc": y, "y_base": np.zeros_like(y), "peaks": [], "log": [str(e)], "stats": "Error"}

    # --- Footer ---
    @staticmethod
    def separate_footer(img_bgr):
        h, w = img_bgr.shape[:2]; roi_h = int(h * 0.82); roi = img_bgr[roi_h:, :]
        gray = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)
        sobel_y = np.abs(cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=3))
        row_sums = np.sum(sobel_y, axis=1)
        split_idx = np.argmax(row_sums)
        if row_sums[split_idx] > w * 30: return img_bgr[:roi_h + split_idx, :], img_bgr[roi_h + split_idx:, :]
        return img_bgr, None

    # --- [E] FFT-NMF (NV26 4-Stack Visual) ---
    @staticmethod
    def analyze_crystal_fft_nmf(img_bgr, patch_size=128, step=16, n_components=10):
        log = []
        gray = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2GRAY)
        h, w = gray.shape
        
        log.append(f"FFT-NMF: Patch={patch_size}, Step={step}, K={n_components}")
        
        custom_colors = [
            (0, 0, 255), (0, 255, 0), (255, 0, 0), (0, 255, 255), (255, 255, 0),
            (255, 0, 255), (0, 165, 255), (238, 130, 238), (128, 128, 0), (19, 69, 139)
        ]
        while len(custom_colors) < n_components: custom_colors.append((255, 255, 255))
        
        cols = [] 
        y_steps = list(range(0, h - patch_size + 1, step))
        x_steps = list(range(0, w - patch_size + 1, step))
        grid_h = len(y_steps)
        grid_w = len(x_steps)
        
        # DC Mask (Radius 4)
        cy, cx = patch_size // 2, patch_size // 2
        Y, X = np.ogrid[:patch_size, :patch_size]
        dist_sq = (X - cx)**2 + (Y - cy)**2
        dc_mask = dist_sq <= 4**2

        for y in y_steps:
            for x in x_steps:
                roi = gray[y:y+patch_size, x:x+patch_size]
                f = np.fft.fft2(roi)
                fshift = np.fft.fftshift(f)
                mag = np.abs(fshift) 
                mag[dc_mask] = 0
                cols.append(mag.flatten())
        
        if not cols: return {"type":"error", "msg":"Img too small"}
        
        try:
            V = np.array(cols).T.astype(np.float32)
            V = V - V.min()
            if V.max() > 0: V = V / V.max()
            
            nmf = NMF(n_components=n_components, init='nndsvda', random_state=42, max_iter=2000)
            W = nmf.fit_transform(V) # (Pixels, K)
            H = nmf.components_      # (K, Patches)
            
            if H.shape[1] != grid_h * grid_w: raise ValueError("Shape Mismatch")
            
            idx_H = np.argmax(H, axis=0) 
            map_low = idx_H.reshape(grid_h, grid_w).astype(np.uint8)
            map_high = cv2.resize(map_low, (w, h), interpolation=cv2.INTER_NEAREST)
            
            overlay = img_bgr.copy()
            mask_layer = np.zeros_like(img_bgr)
            
            counts = Counter(idx_H)
            total = len(idx_H)
            stats = {}
            
            for i in range(n_components):
                mask = (map_high == i)
                mask_layer[mask] = custom_colors[i]
                cnt = counts.get(i, 0)
                stats[f"Pattern {i}"] = f"{cnt} patches ({cnt/total*100:.1f}%)"
                
            alpha = 0.4
            mask_idx = np.any(mask_layer > 0, axis=-1)
            overlay[mask_idx] = (overlay[mask_idx]*(1-alpha) + mask_layer[mask_idx]*alpha).astype(np.uint8)
            
            # [NV26] 4-Stack: W -> H -> Colorbar -> Legend
            basis_combo_list = []
            
            for k in range(n_components):
                # 1. W Matrix (Basis Pattern)
                pat = W[:, k].reshape(patch_size, patch_size)
                pat_log = np.log1p(pat * 1000) 
                pat_norm = cv2.normalize(pat_log, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)
                pat_large = cv2.resize(pat_norm, (128, 128), interpolation=cv2.INTER_NEAREST)
                pat_vis = cv2.applyColorMap(pat_large, cv2.COLORMAP_MAGMA)
                cv2.putText(pat_vis, "W (Basis)", (5, 15), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255,255,255), 1)
                
                # 2. H Matrix (Distribution)
                h_vec = H[k, :] 
                h_min, h_max = h_vec.min(), h_vec.max()
                
                h_grid = h_vec.reshape(grid_h, grid_w)
                h_large = cv2.resize(h_grid, (128, 128), interpolation=cv2.INTER_NEAREST)
                h_norm = cv2.normalize(h_large, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)
                h_vis = cv2.applyColorMap(h_norm, cv2.COLORMAP_VIRIDIS)
                cv2.putText(h_vis, "H (Dist)", (5, 15), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255,255,255), 1)
                
                # 3. [NV26] H Colorbar (Gradient + Min/Max)
                cbar_h = 25; cbar_w = 128
                gradient = np.linspace(0, 255, cbar_w).astype(np.uint8)
                gradient = np.tile(gradient, (cbar_h, 1))
                cbar_vis = cv2.applyColorMap(gradient, cv2.COLORMAP_VIRIDIS)
                cv2.putText(cbar_vis, f"{h_min:.1f}", (2, 17), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255,255,255), 1)
                (tw, th), _ = cv2.getTextSize(f"{h_max:.1f}", cv2.FONT_HERSHEY_SIMPLEX, 0.4, 1)
                cv2.putText(cbar_vis, f"{h_max:.1f}", (cbar_w - tw - 2, 17), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (0,0,0), 1)
                
                # 4. Legend
                legend_h = 30
                legend = np.zeros((legend_h, 128, 3), dtype=np.uint8)
                legend[:] = custom_colors[k]
                cv2.putText(legend, f"P{k}", (50, 22), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255,255,255), 2)
                
                # Stack
                combo = np.vstack([pat_vis, h_vis, cbar_vis, legend])
                cv2.rectangle(combo, (0,0), (combo.shape[1]-1, combo.shape[0]-1), custom_colors[k], 2)
                basis_combo_list.append(combo)
            
            full_strip = np.hstack(basis_combo_list)
            
            def to_b64(im): return base64.b64encode(cv2.imencode('.jpg', im)[1]).decode('utf-8')
            
            return {
                "type": "image", 
                "raw_b64": to_b64(img_bgr), 
                "proc_b64": to_b64(overlay),
                "footer_b64": to_b64(full_strip), 
                "stats": stats, 
                "log": log
            }

        except Exception as e:
            print(f"NMF Error: {e}")
            def to_b64(im): return base64.b64encode(cv2.imencode('.jpg', im)[1]).decode('utf-8')
            return {"type": "image", "raw_b64": to_b64(img_bgr), "proc_b64": to_b64(img_bgr), "stats": {"Error": str(e)}, "log": log}

    # --- [F] Diffraction ---
    @staticmethod
    def analyze_diffraction_nv7(img_bgr, user_center=None):
        log = []; stats = {}
        gray = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2GRAY)
        if user_center and len(user_center)==2 and user_center[0]>0:
            cx, cy = user_center
            log.append(f"Center: Manual [{cx}, {cy}]")
        else:
            _, th = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY+cv2.THRESH_OTSU)
            cyf, cxf = center_of_mass(th)
            if np.isnan(cyf): cyf, cxf = gray.shape[0]//2, gray.shape[1]//2
            cx, cy = int(cxf), int(cyf)
            log.append(f"Center: Auto [{cx}, {cy}]")

        g1 = cv2.GaussianBlur(gray, (3,3), 0)
        g2 = cv2.GaussianBlur(gray, (51,51), 0)
        diff = cv2.subtract(g1, g2)
        coords = peak_local_max(diff, min_distance=10, threshold_abs=20)
        enhanced = np.log1p(diff.astype(np.float32))
        enhanced = cv2.normalize(enhanced, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)
        vis = cv2.cvtColor(enhanced, cv2.COLOR_GRAY2BGR)
        cv2.drawMarker(vis, (cx, cy), (0,0,255), cv2.MARKER_CROSS, 20, 2)
        peak_list = []
        for p in coords:
            py, px = p
            if abs(py-cy)<5 and abs(px-cx)<5: continue
            cv2.circle(vis, (px, py), 3, (0,255,0), 1)
            peak_list.append(f"[{px},{py}]")
        log.append(f"Spots: {len(peak_list)}")
        def to_b64(im): return base64.b64encode(cv2.imencode('.jpg', im)[1]).decode('utf-8')
        return {"type": "image", "raw_b64": to_b64(img_bgr), "proc_b64": to_b64(vis), "stats": {"center": [cx, cy], "spots": len(peak_list)}, "log": log}

    # --- [G] SAM ---
    @staticmethod
    def analyze_sam_results_nv7(img_rgb, masks, data_type):
        overlay = img_rgb.copy()
        mask_overlay = np.zeros_like(img_rgb)
        h_img, w_img = img_rgb.shape[:2]
        stats = {"count": 0}
        log = []
        valid_masks = []
        for ann in masks:
            area = ann['area']; bbox = ann['bbox']; x, y, w, h = [int(v) for v in bbox]
            if area > h_img * w_img * 0.95 or area < 20: continue 
            if data_type == "Particle":
                if x<=1 or y<=1 or (x+w)>=w_img-1 or (y+h)>=h_img-1: continue
                mask_uint8 = (ann['segmentation'] * 255).astype(np.uint8)
                cnts, _ = cv2.findContours(mask_uint8, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
                if cnts:
                    perim = cv2.arcLength(cnts[0], True)
                    circ = 4*np.pi*area/(perim**2) if perim>0 else 0
                    if circ < 0.25: continue
            valid_masks.append(ann)

        if data_type == "Thin Film":
            sorted_masks = sorted(valid_masks, key=lambda x: x['bbox'][1])
        else:
            sorted_masks = sorted(valid_masks, key=lambda x: x['area'], reverse=True)

        thickness_vals = []; diameters = []
        for i, ann in enumerate(sorted_masks):
            m = ann['segmentation']
            bbox = ann['bbox']; x, y, w, h = [int(v) for v in bbox]
            cx, cy = int(x + w/2), int(y + h/2)
            color = (0, 255, 0) 
            if data_type == "Thin Film":
                is_bound = (i == 0 or i == len(sorted_masks)-1)
                cv2.putText(overlay, f"#{i+1}", (cx, cy), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (255, 255, 255), 3)
                if not is_bound:
                    thk = ann['area'] / w
                    thickness_vals.append(thk)
                    color = (255, 0, 0) 
                else: color = (100, 100, 100)
                if i < len(sorted_masks) - 1:
                    ys, xs = np.where(m)
                    if len(xs) > 10:
                        roughness = np.std(ys)
                        cv2.line(overlay, (x, y+h), (x+w, y+h), (0, 255, 255), 2)
                        cv2.putText(overlay, f"Int#{i+1} R:{roughness:.2f}", (x+10, y+h-5), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)
            elif data_type == "Particle":
                equivalent_diameter = 2 * np.sqrt(ann['area'] / np.pi)
                diameters.append(equivalent_diameter)
            mask_overlay[m] = color
            mask_uint8 = (m * 255).astype(np.uint8)
            cnts, _ = cv2.findContours(mask_uint8, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
            if cnts: cv2.drawContours(overlay, [cnts[0]], -1, color, 1)

        alpha = 0.35
        mask_idx = np.any(mask_overlay > 0, axis=-1)
        overlay[mask_idx] = (overlay[mask_idx]*(1-alpha) + mask_overlay[mask_idx]*alpha).astype(np.uint8)
        stats["count"] = len(sorted_masks)
        if data_type == "Thin Film":
            avg = round(np.mean(thickness_vals), 2) if thickness_vals else 0
            stats["thickness_avg"] = avg
            log.append(f"Layers: {len(sorted_masks)}, Avg Thk: {avg}")
        elif data_type == "Particle":
            avg = round(np.mean(diameters), 2) if diameters else 0
            stats["diameter_avg"] = avg
            log.append(f"Particles: {len(sorted_masks)}")
        return overlay, stats, log

    # --- Router ---
    @staticmethod
    def process_image_nv27(img_bytes, equipment, data_type, mode, goal, user_center=None, 
                           patch_size=128, step=16, n_components=10):
        nparr = np.frombuffer(img_bytes, np.uint8)
        img_raw = cv2.imdecode(nparr, cv2.IMREAD_COLOR)
        if img_raw is None: return {}
        
        body, footer = ScienceProcessorNV27.separate_footer(img_raw)
        footer_b64 = None
        if footer is not None:
            footer_b64 = base64.b64encode(cv2.imencode('.jpg', footer)[1]).decode('utf-8')

        if data_type == "Crystal Structure (FFT)":
            return ScienceProcessorNV27.analyze_crystal_fft_nmf(body, patch_size, step, n_components)
            
        if data_type == "2D Diffraction":
            return {**ScienceProcessorNV27.analyze_diffraction_nv7(body, user_center), "footer_b64": footer_b64}
        
        if mode == "None":
             def to_b64(im): return base64.b64encode(cv2.imencode('.jpg', im)[1]).decode('utf-8')
             return {"type":"image", "raw_b64":to_b64(body), "proc_b64":to_b64(body), "stats":{}, "log":["Mode: Raw"], "footer_b64": footer_b64}

        img_proc = body.copy(); log = []
        if mode == "Auto" or mode == "AI-Adaptive":
            img_proc = cv2.bilateralFilter(img_proc, 9, 75, 75)
            log.append("Preproc: Denoise")

        img_rgb = cv2.cvtColor(img_proc, cv2.COLOR_BGR2RGB)
        
        if mask_generator:
            try:
                masks = mask_generator.generate(img_rgb)
                log.append(f"SAM: {len(masks)} Segments")
                overlay, stats, sam_log = ScienceProcessorNV27.analyze_sam_results_nv7(img_rgb, masks, data_type)
                log.extend(sam_log)
                overlay = cv2.cvtColor(overlay, cv2.COLOR_RGB2BGR)
            except Exception as e:
                log.append(f"SAM Error: {str(e)}")
                overlay = body; stats = {}
        else:
            log.append("SAM Not Loaded")
            overlay = body; stats = {}

        def to_b64(im): return base64.b64encode(cv2.imencode('.jpg', im)[1]).decode('utf-8')
        return {"type": "image", "raw_b64": to_b64(body), "proc_b64": to_b64(overlay), "stats": stats, "log": log, "footer_b64": footer_b64}

# [4] Helper (Same)
async def analyze_vision_ollama(image_bytes: bytes, prompt: str) -> str:
    try:
        b64 = base64.b64encode(image_bytes).decode('utf-8')
        res = await asyncio.to_thread(ollama_client.chat, model=VISION_MODEL, messages=[{'role':'user','content':prompt,'images':[b64]}])
        return res['message']['content']
    except: return "Vision Error"

async def interpret_spectrum_data(raw_context: str, equipment: str) -> str:
    try:
        prompt = f"Analyze spectrum ({equipment}). Brief interpretation in Korean. Data: {raw_context}"
        res = await asyncio.to_thread(ollama_client.chat, model=TEXT_MODEL, messages=[{'role':'user','content':prompt}])
        return res['message']['content']
    except: return ""

# [5] App
app = FastAPI(title="Analyst NV27 Final Complete", lifespan=lifespan)
app.add_middleware(CORSMiddleware, allow_origins=["*"], allow_methods=["*"], allow_headers=["*"])

class TranslateReq(BaseModel): text: str

@app.post("/api/translate")
async def translate_text(req: TranslateReq):
    try:
        res = await asyncio.to_thread(ollama_client.chat, model=TEXT_MODEL, messages=[{'role':'system','content':'Translate to English.'}, {'role':'user','content':req.text}])
        return {"translated": res['message']['content']}
    except Exception as e: return {"translated": str(e)}

async def process_analysis_job(job_id: str, files_map: Dict, configs: Dict):
    try:
        JOBS[job_id]["status"] = "Processing"
        JOBS[job_id]["progress"] = 10
        final_results = []
        total_files = len(configs)
        processed_count = 0

        for filename, config in configs.items():
            if filename not in files_map: continue
            content = files_map[filename]
            eq = config.get("equipment", "General")
            data_type = config.get("data_type", "General")
            goal = config.get("goal", "")
            mode = config.get("mode", "Auto")
            
            user_center = None
            if config.get("center_coords"):
                try: user_center = [int(v) for v in config["center_coords"].split(",")]
                except: pass
            
            p_size = int(config.get("patch_size", 128))
            p_step = int(config.get("step", 16))
            n_comp = int(config.get("n_components", 10))

            fname_lower = filename.lower()
            JOBS[job_id]["step"] = f"Analyzing {filename}..."
            
            # [A] Document Logic (NV23 pypdfium2 + NV22 UUID)
            if eq == "Document" or fname_lower.endswith(('.pdf', '.ppt', '.pptx')):
                try:
                    if fname_lower.endswith('.pdf'):
                        pdf = pdfium.PdfDocument(io.BytesIO(content))
                        n_pages = len(pdf)
                        full_txt = ""; pages = []
                        for i in range(min(n_pages, 10)):
                            page = pdf[i]
                            bitmap = page.render(scale=2)
                            pil_image = bitmap.to_pil()
                            buf = io.BytesIO()
                            pil_image.save(buf, format="JPEG")
                            b64 = base64.b64encode(buf.getvalue()).decode()
                            desc = await analyze_vision_ollama(buf.getvalue(), "한글 요약.")
                            full_txt += f"\nPage {i+1}: {desc}"
                            pages.append({"id": str(uuid.uuid4()), "page_num": i+1, "image_b64": b64, "desc": desc})
                        final_results.append({"type": "pdf", "filename": filename, "equipment": "Lit", "raw_context": full_txt, "pages": pages})
                    elif fname_lower.endswith(('.ppt', '.pptx')):
                        try:
                            prs = Presentation(io.BytesIO(content))
                            slides = []; full_txt = ""
                            for i, slide in enumerate(prs.slides):
                                txt = ""; imgs = []
                                for s in slide.shapes:
                                    if hasattr(s, "text"): txt += s.text + "\n"
                                    if s.shape_type == MSO_SHAPE_TYPE.PICTURE:
                                        try:
                                            ib64 = base64.b64encode(s.image.blob).decode('utf-8')
                                            desc = "Image"
                                            imgs.append({"b64": ib64, "desc": desc})
                                        except: pass
                                if not txt.strip(): txt = "(No text)"
                                full_txt += f"Slide {i+1}: {txt}\n"
                                slides.append({"id": str(uuid.uuid4()), "slide_num": i+1, "text": txt, "images": imgs})
                            final_results.append({"type": "ppt", "filename": filename, "equipment": "Lit", "raw_context": full_txt, "slides": slides})
                        except Exception as ppt_err:
                            final_results.append({"type": "error", "filename": filename, "msg": f"PPTX Error: {ppt_err}"})
                except Exception as e: 
                    final_results.append({"type": "error", "filename": filename, "msg": str(e)})

            elif fname_lower.endswith(('.xlsx', '.xls', '.csv', '.txt')):
                try:
                    dfs = ScienceProcessorNV27.read_universal_dataframe(content, filename)
                    for i, df in enumerate(dfs):
                        v_blocks = ScienceProcessorNV27.detect_structured_blocks(df)
                        for v_block in v_blocks:
                            h_blocks = ScienceProcessorNV27.split_horizontal_blocks(v_block)
                            for h_block in h_blocks:
                                series_list = ScienceProcessorNV27.extract_series_from_df(h_block)
                                for s in series_list:
                                    res = ScienceProcessorNV27.process_spectrum(s['x'], s['y'], mode, goal)
                                    step = max(1, len(s['x'])//100)
                                    chart = [{"x": float(res["x"][k]), "y_proc": float(res["y_proc"][k]), "y_raw": float(res["y_raw"][k]), "y_base": float(res["y_base"][k])} for k in range(0, len(s['x']), step)]
                                    ctx = f"Data: {s['name']}\nStats: {res['stats']}"
                                    interp = await interpret_spectrum_data(ctx, eq)
                                    final_results.append({
                                        "type": "spectrum", "filename": f"{filename} ({s['name']})", "equipment": eq,
                                        "raw_context": f"{ctx}\nInterp: {interp}", 
                                        "chart_data": chart, "log": res["log"], "interpretation": interp
                                    })
                except Exception as ex: final_results.append({"type": "error", "filename": filename, "msg": str(ex)})

            elif fname_lower.endswith(('.png', '.jpg', '.jpeg', '.tif', '.bmp')):
                try:
                    vis_res = await asyncio.to_thread(ScienceProcessorNV27.process_image_nv27, content, eq, data_type, mode, goal, user_center, p_size, p_step, n_comp)
                    if vis_res:
                        scale_info = "Unknown"
                        final_footer = vis_res.get("footer_b64")
                        if final_footer and data_type != "Crystal Structure (FFT)":
                            scale_text = await analyze_vision_ollama(base64.b64decode(final_footer), "Read scale bar text only.")
                            scale_info = scale_text
                            vis_res["log"].append(f"Scale: {scale_text}")

                        stats_clean = ""
                        for k, v in vis_res.get('stats',{}).items():
                            stats_clean += f"- {k}: {v}\n"
                        
                        desc_prompt = f"Analyze {eq} image ({data_type}). Stats: \n{stats_clean}\n. Goal: {goal}. Summary in Korean."
                        desc = await analyze_vision_ollama(base64.b64decode(vis_res["proc_b64"]), desc_prompt)
                        
                        final_results.append({
                            "type": "image", "filename": filename, "equipment": eq, "summary": desc,
                            "raw_context": f"Img {eq}: {desc}\nStats:\n{stats_clean}", 
                            "raw_b64": vis_res["raw_b64"], "proc_b64": vis_res["proc_b64"], "footer_b64": final_footer,
                            "stats": vis_res.get("stats",{}), "log": vis_res.get("log", [])
                        })
                except Exception as ex: final_results.append({"type": "error", "filename": filename, "msg": str(ex)})
            
            processed_count += 1
            JOBS[job_id]["progress"] = 10 + int((processed_count / total_files) * 80)

        JOBS[job_id]["step"] = "Writing Report..."
        final_results = sanitize_json(final_results)
        
        data_ctx = ""
        for r in final_results:
            if r.get("type") != "error":
                raw = r.get("raw_context") or ""
                data_ctx += f"\n=== File: {r['filename']} ({r.get('equipment')}) ===\n{raw[:2000]}\n"

        final_report = "Fail"
        if data_ctx:
            try:
                system_prompt = "You are a research assistant. Write a comprehensive report in Korean based on the provided data summaries. Include a data summary table. Do not use LaTeX."
                res = await asyncio.to_thread(ollama_client.chat, model=TEXT_MODEL, messages=[{'role':'system','content':system_prompt}, {'role':'user','content':f"Data Summaries:\n{data_ctx}"}])
                final_report = res['message']['content']
            except Exception as e: final_report = str(e)

        JOBS[job_id]["results"] = {"results": final_results, "final_report": final_report}
        JOBS[job_id]["status"] = "Completed"
        JOBS[job_id]["progress"] = 100
    except Exception as e:
        JOBS[job_id]["status"] = "Failed"; JOBS[job_id]["error"] = str(e)

@app.post("/api/analyze")
async def start_analysis(background_tasks: BackgroundTasks, files: List[UploadFile] = File(...), file_configs: str = Form(...)):
    job_id = str(uuid.uuid4()); configs = json.loads(file_configs)
    files_map = {f.filename: await f.read() for f in files}
    JOBS[job_id] = {"status": "Pending", "progress": 0, "step": "Init", "results": None}
    background_tasks.add_task(process_analysis_job, job_id, files_map, configs)
    return {"job_id": job_id}

@app.get("/api/status/{job_id}")
async def get_status(job_id: str): return JOBS.get(job_id, {"status": "Not Found"})

@app.get("/", response_class=HTMLResponse)
async def serve_index():
    if os.path.exists("index.html"): return FileResponse("index.html")
    return "<h1>NV27 Final Complete Backend</h1>"

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)







NV27 Python Code

import os
import io
# [0] OMP Error Fix
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"
import torch

import asyncio
import base64
import json
import re
import math
import uuid
import time
from typing import List, Dict, Any, Optional
from pydantic import BaseModel
from contextlib import asynccontextmanager
from collections import Counter

# [1] Security
os.environ['NO_PROXY'] = '127.0.0.1,localhost,0.0.0.0,::1'
if 'HTTP_PROXY' in os.environ: del os.environ['HTTP_PROXY']
if 'HTTPS_PROXY' in os.environ: del os.environ['HTTPS_PROXY']

from fastapi import FastAPI, UploadFile, File, Form, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import HTMLResponse, FileResponse
import pandas as pd
import numpy as np

# SciPy & CV & Sklearn
from sklearn.decomposition import NMF
from scipy.signal import find_peaks, savgol_filter
from scipy.optimize import curve_fit
from scipy.ndimage import center_of_mass
from skimage.feature import peak_local_max
import cv2
import matplotlib
import matplotlib.pyplot as plt 

# Models
import pypdfium2 as pdfium 
from segment_anything import sam_model_registry, SamAutomaticMaskGenerator
from ollama import Client 
from pptx import Presentation
from pptx.enum.shapes import MSO_SHAPE_TYPE

# [2] Config
OLLAMA_HOST = "http://127.0.0.1:11434"
ollama_client = Client(host=OLLAMA_HOST)
VISION_MODEL = "qwen3-vl:30b-a3b-instruct" 
TEXT_MODEL = "llama3:70b"

SAM_CHECKPOINT = "sam_vit_b_01ec64.pth"
SAM_MODEL_TYPE = "vit_b"
device = "cuda" if torch.cuda.is_available() else "cpu"

mask_generator = None 
JOBS = {} 

@asynccontextmanager
async def lifespan(app: FastAPI):
    global mask_generator
    if os.path.exists(SAM_CHECKPOINT):
        print(f"Loading SAM ({SAM_MODEL_TYPE}) on {device}...")
        try:
            sam = sam_model_registry[SAM_MODEL_TYPE](checkpoint=SAM_CHECKPOINT)
            sam.to(device=device)
            mask_generator = SamAutomaticMaskGenerator(
                model=sam, points_per_side=32, pred_iou_thresh=0.86,
                stability_score_thresh=0.90, min_mask_region_area=50
            )
            print("SAM Loaded.")
        except: print("SAM Load Failed.")
    yield
    JOBS.clear()

def sanitize_json(obj):
    if isinstance(obj, float):
        if np.isnan(obj) or np.isinf(obj): return 0.0
        return obj
    elif isinstance(obj, dict): return {k: sanitize_json(v) for k, v in obj.items()}
    elif isinstance(obj, list): return [sanitize_json(v) for v in obj]
    elif isinstance(obj, (np.int64, np.int32)): return int(obj)
    elif isinstance(obj, (np.float64, np.float32)):
        if np.isnan(obj) or np.isinf(obj): return 0.0
        return float(obj)
    return obj

# ==========================================
# [3] NV27 Perfect CSV & Visual Engine
# ==========================================
class ScienceProcessorNV27:
    
    # --- Universal Loader ---
    @staticmethod
    def read_universal_dataframe(content: bytes, filename: str) -> List[pd.DataFrame]:
        dfs = []
        fname = filename.lower()
        if fname.endswith(('.xlsx', '.xls')):
            try:
                xls = pd.ExcelFile(io.BytesIO(content))
                for s in xls.sheet_names:
                    df = pd.read_excel(xls, sheet_name=s, header=None)
                    if df.apply(pd.to_numeric, errors='coerce').notna().sum().sum() > 5: dfs.append(df)
                if dfs: return dfs
            except: pass
        text_content = ""
        for enc in ['utf-8', 'cp949', 'latin1']:
            try: text_content = content.decode(enc); break
            except: continue
        if text_content:
            lines = text_content.splitlines()
            data_start = -1; delimiter = None
            separators = [',', '\t', ';', '\s+']
            for i, line in enumerate(lines[:100]):
                for sep in separators:
                    parts = line.split() if sep=='\s+' else line.split(sep)
                    nums = sum(1 for p in parts if re.match(r'^-?\d+(\.\d+)?(?:[eE][+-]?\d+)?$', p.strip()))
                    if nums >= 2: data_start = i; delimiter = sep; break
                if data_start != -1: break
            if data_start != -1:
                try:
                    data_str = "\n".join(lines[data_start:])
                    sep_arg = '\s+' if delimiter=='\s+' else delimiter
                    df = pd.read_csv(io.StringIO(data_str), sep=sep_arg, header=None, engine='python')
                    dfs.append(df)
                except: pass
        return dfs

    # --- Block Parser ---
    @staticmethod
    def detect_structured_blocks(df: pd.DataFrame) -> List[pd.DataFrame]:
        blocks = []
        if df.empty: return []
        df_num = df.apply(pd.to_numeric, errors='coerce')
        if df.shape[1] > 0: is_data_row = df_num.iloc[:, 0].notna()
        else: is_data_row = df_num.notna().any(axis=1)
        groups = (is_data_row != is_data_row.shift()).cumsum()
        for _, group in df.groupby(groups):
            if is_data_row.loc[group.index[0]]:
                block = df_num.loc[group.index].dropna(how='all', axis=1)
                if block.shape[0] >= 10: blocks.append(block.reset_index(drop=True))
        return blocks

    @staticmethod
    def split_horizontal_blocks(df: pd.DataFrame) -> List[pd.DataFrame]:
        sub_blocks = []
        if df.empty: return []
        col_counts = df.apply(pd.to_numeric, errors='coerce').notna().sum()
        valid = np.where(col_counts >= 2)[0]
        if len(valid) == 0: return []
        splits = np.where(np.diff(valid) > 1)[0] + 1
        groups = np.split(valid, splits)
        for g in groups:
            if len(g) > 0:
                sub = df.iloc[:, g].reset_index(drop=True)
                if sub.shape[1] >= 1 and sub.shape[0] >= 2: sub_blocks.append(sub)
        return sub_blocks

    # --- [NV27 Fix] Robust Series Extraction ---
    @staticmethod
    def extract_series_from_df(df: pd.DataFrame) -> List[Dict]:
        series_list = []
        try:
            vals = df.values; rows, cols = vals.shape
            if rows < 5: return []
            
            # Helper: Check monotonicity
            def is_axis(arr):
                arr = arr[~np.isnan(arr)]
                if len(arr) < 5: return False
                diff = np.diff(arr)
                if np.all(diff >= 0) or np.all(diff <= 0): return True
                return (np.sum(diff>0)/len(diff)>0.9) or (np.sum(diff<0)/len(diff)>0.9)

            # Strategy: Check Col 0
            col0 = vals[:, 0]
            
            if is_axis(col0):
                # Case A: Col 0 is Index -> Use as X, plot Col 1..N as Y
                current_x = col0
                start_col = 1
            else:
                # Case B: Col 0 is Data (random) -> X is Range, plot Col 0..N as Y
                current_x = np.arange(rows)
                start_col = 0

            # Collect Y series
            for i in range(start_col, cols):
                col_data = vals[:, i]
                mask = ~np.isnan(current_x) & ~np.isnan(col_data)
                
                # Filter out extremely short or empty columns
                if np.sum(mask) > 5:
                    series_list.append({
                        "x": current_x[mask], 
                        "y": col_data[mask], 
                        "name": f"Col-{i}" if i > 0 else "Col-0"
                    })
        except: pass
        return series_list

    # --- Spectrum Logic ---
    @staticmethod
    def simple_baseline(y):
        try:
            w = max(5, len(y)//10)
            s = pd.Series(y).rolling(window=w, center=True).min()
            b = s.bfill().ffill().values 
            if len(b) > 51: b = savgol_filter(b, 51, 3)
            return b, w
        except: return np.zeros_like(y), 0

    @staticmethod
    def process_spectrum(x, y, mode, goal):
        log = []; y_raw = y.copy(); base = np.zeros_like(y); y_proc = y.copy()
        peaks = []; fits = []; stats_txt = "Error"
        try:
            if mode == "None":
                peaks, _ = find_peaks(y, height=np.max(y)*0.05)
                stats_txt = f"Raw Peaks: {len(peaks)}"
            else:
                win = 15
                if mode == "AI-Adaptive" and "noise" in goal.lower(): win = 31
                base, bw = ScienceProcessorNV27.simple_baseline(y)
                y_proc = np.maximum(y - base, 0)
                if len(y_proc) > win: y_proc = savgol_filter(y_proc, win, 3)
                peaks, _ = find_peaks(y_proc, height=np.max(y_proc)*0.05, distance=10)
                stats_txt = f"Peaks: {len(peaks)}"
            return {"x": x, "y_raw": y_raw, "y_proc": y_proc, "y_base": base, "peaks": [{"x": float(x[p]), "y": float(y_proc[p])} for p in peaks], "log": log, "fits": fits, "stats": stats_txt}
        except Exception as e: return {"x": x, "y_raw": y_raw, "y_proc": y_proc, "y_base": base, "peaks": [], "log": [str(e)], "fits": [], "stats": "Error"}

    # --- Footer ---
    @staticmethod
    def separate_footer(img_bgr):
        h, w = img_bgr.shape[:2]; roi_h = int(h * 0.82); roi = img_bgr[roi_h:, :]
        gray = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)
        sobel_y = np.abs(cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=3))
        row_sums = np.sum(sobel_y, axis=1)
        split_idx = np.argmax(row_sums)
        if row_sums[split_idx] > w * 30: return img_bgr[:roi_h + split_idx, :], img_bgr[roi_h + split_idx:, :]
        return img_bgr, None

    # --- [E] NV26 FFT-NMF (W, H, Colorbar, Legend) - CPU Ver. ---
    @staticmethod
    def analyze_crystal_fft_nmf(img_bgr, patch_size=128, step=16, n_components=10):
        log = []
        gray = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2GRAY)
        h, w = gray.shape
        
        log.append(f"FFT-NMF: Patch={patch_size}, Step={step}, K={n_components}")
        
        custom_colors = [
            (0, 0, 255), (0, 255, 0), (255, 0, 0), (0, 255, 255), (255, 255, 0),
            (255, 0, 255), (0, 165, 255), (238, 130, 238), (128, 128, 0), (19, 69, 139)
        ]
        while len(custom_colors) < n_components: custom_colors.append((255, 255, 255))
        
        cols = [] 
        y_steps = list(range(0, h - patch_size + 1, step))
        x_steps = list(range(0, w - patch_size + 1, step))
        grid_h = len(y_steps)
        grid_w = len(x_steps)
        
        # DC Mask (Radius 4)
        cy, cx = patch_size // 2, patch_size // 2
        Y, X = np.ogrid[:patch_size, :patch_size]
        dist_sq = (X - cx)**2 + (Y - cy)**2
        dc_mask = dist_sq <= 4**2

        for y in y_steps:
            for x in x_steps:
                roi = gray[y:y+patch_size, x:x+patch_size]
                f = np.fft.fft2(roi)
                fshift = np.fft.fftshift(f)
                mag = np.abs(fshift) 
                mag[dc_mask] = 0
                cols.append(mag.flatten())
        
        if not cols: return {"type":"error", "msg":"Img too small"}
        
        try:
            V = np.array(cols).T.astype(np.float32)
            V = V - V.min()
            if V.max() > 0: V = V / V.max()
            
            nmf = NMF(n_components=n_components, init='nndsvda', random_state=42, max_iter=2000)
            W = nmf.fit_transform(V) 
            H = nmf.components_ 
            
            if H.shape[1] != grid_h * grid_w: raise ValueError("Shape Mismatch")
            
            idx_H = np.argmax(H, axis=0) 
            map_low = idx_H.reshape(grid_h, grid_w).astype(np.uint8)
            map_high = cv2.resize(map_low, (w, h), interpolation=cv2.INTER_NEAREST)
            
            overlay = img_bgr.copy()
            mask_layer = np.zeros_like(img_bgr)
            
            counts = Counter(idx_H)
            total = len(idx_H)
            stats = {}
            
            for i in range(n_components):
                mask = (map_high == i)
                mask_layer[mask] = custom_colors[i]
                cnt = counts.get(i, 0)
                stats[f"Pattern {i}"] = f"{cnt} patches ({cnt/total*100:.1f}%)"
                
            alpha = 0.4
            mask_idx = np.any(mask_layer > 0, axis=-1)
            overlay[mask_idx] = (overlay[mask_idx]*(1-alpha) + mask_layer[mask_idx]*alpha).astype(np.uint8)
            
            # [NV26] 4-Stack: W -> H -> Colorbar -> Legend
            basis_combo_list = []
            
            for k in range(n_components):
                # 1. W Matrix (Basis Pattern)
                pat = W[:, k].reshape(patch_size, patch_size)
                pat_log = np.log1p(pat * 1000) 
                pat_norm = cv2.normalize(pat_log, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)
                pat_large = cv2.resize(pat_norm, (128, 128), interpolation=cv2.INTER_NEAREST)
                pat_vis = cv2.applyColorMap(pat_large, cv2.COLORMAP_MAGMA)
                cv2.putText(pat_vis, "W (Basis)", (5, 15), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255,255,255), 1)
                
                # 2. H Matrix (Distribution)
                h_vec = H[k, :] 
                h_min, h_max = h_vec.min(), h_vec.max()
                
                h_grid = h_vec.reshape(grid_h, grid_w)
                h_large = cv2.resize(h_grid, (128, 128), interpolation=cv2.INTER_NEAREST)
                h_norm = cv2.normalize(h_large, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)
                h_vis = cv2.applyColorMap(h_norm, cv2.COLORMAP_VIRIDIS)
                cv2.putText(h_vis, "H (Dist)", (5, 15), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255,255,255), 1)
                
                # 3. [NV26] H Colorbar (Gradient + Min/Max)
                cbar_h = 25; cbar_w = 128
                gradient = np.linspace(0, 255, cbar_w).astype(np.uint8)
                gradient = np.tile(gradient, (cbar_h, 1))
                cbar_vis = cv2.applyColorMap(gradient, cv2.COLORMAP_VIRIDIS)
                cv2.putText(cbar_vis, f"{h_min:.1f}", (2, 17), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255,255,255), 1)
                (tw, th), _ = cv2.getTextSize(f"{h_max:.1f}", cv2.FONT_HERSHEY_SIMPLEX, 0.4, 1)
                cv2.putText(cbar_vis, f"{h_max:.1f}", (cbar_w - tw - 2, 17), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (0,0,0), 1)
                
                # 4. Legend
                legend_h = 30
                legend = np.zeros((legend_h, 128, 3), dtype=np.uint8)
                legend[:] = custom_colors[k]
                cv2.putText(legend, f"P{k}", (50, 22), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255,255,255), 2)
                
                # Stack
                combo = np.vstack([pat_vis, h_vis, cbar_vis, legend])
                cv2.rectangle(combo, (0,0), (combo.shape[1]-1, combo.shape[0]-1), custom_colors[k], 2)
                basis_combo_list.append(combo)
            
            full_strip = np.hstack(basis_combo_list)
            
            def to_b64(im): return base64.b64encode(cv2.imencode('.jpg', im)[1]).decode('utf-8')
            
            return {
                "type": "image", 
                "raw_b64": to_b64(img_bgr), 
                "proc_b64": to_b64(overlay),
                "footer_b64": to_b64(full_strip), 
                "stats": stats, 
                "log": log
            }

        except Exception as e:
            print(f"NMF Error: {e}")
            def to_b64(im): return base64.b64encode(cv2.imencode('.jpg', im)[1]).decode('utf-8')
            return {"type": "image", "raw_b64": to_b64(img_bgr), "proc_b64": to_b64(img_bgr), "stats": {"Error": str(e)}, "log": log}

    # --- [F] Diffraction ---
    @staticmethod
    def analyze_diffraction_nv7(img_bgr, user_center=None):
        log = []; stats = {}
        gray = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2GRAY)
        if user_center and len(user_center)==2 and user_center[0]>0:
            cx, cy = user_center
            log.append(f"Center: Manual [{cx}, {cy}]")
        else:
            _, th = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY+cv2.THRESH_OTSU)
            cyf, cxf = center_of_mass(th)
            if np.isnan(cyf): cyf, cxf = gray.shape[0]//2, gray.shape[1]//2
            cx, cy = int(cxf), int(cyf)
            log.append(f"Center: Auto [{cx}, {cy}]")

        g1 = cv2.GaussianBlur(gray, (3,3), 0)
        g2 = cv2.GaussianBlur(gray, (51,51), 0)
        diff = cv2.subtract(g1, g2)
        coords = peak_local_max(diff, min_distance=10, threshold_abs=20)
        enhanced = np.log1p(diff.astype(np.float32))
        enhanced = cv2.normalize(enhanced, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)
        vis = cv2.cvtColor(enhanced, cv2.COLOR_GRAY2BGR)
        cv2.drawMarker(vis, (cx, cy), (0,0,255), cv2.MARKER_CROSS, 20, 2)
        peak_list = []
        for p in coords:
            py, px = p
            if abs(py-cy)<5 and abs(px-cx)<5: continue
            cv2.circle(vis, (px, py), 3, (0,255,0), 1)
            peak_list.append(f"[{px},{py}]")
        log.append(f"Spots: {len(peak_list)}")
        def to_b64(im): return base64.b64encode(cv2.imencode('.jpg', im)[1]).decode('utf-8')
        return {"type": "image", "raw_b64": to_b64(img_bgr), "proc_b64": to_b64(vis), "stats": {"center": [cx, cy], "spots": len(peak_list)}, "log": log}

    # --- [G] SAM ---
    @staticmethod
    def analyze_sam_results_nv7(img_rgb, masks, data_type):
        overlay = img_rgb.copy()
        mask_overlay = np.zeros_like(img_rgb)
        h_img, w_img = img_rgb.shape[:2]
        stats = {"count": 0}
        log = []
        valid_masks = []
        for ann in masks:
            area = ann['area']; bbox = ann['bbox']; x, y, w, h = [int(v) for v in bbox]
            if area > h_img * w_img * 0.95 or area < 20: continue 
            if data_type == "Particle":
                if x<=1 or y<=1 or (x+w)>=w_img-1 or (y+h)>=h_img-1: continue
                mask_uint8 = (ann['segmentation'] * 255).astype(np.uint8)
                cnts, _ = cv2.findContours(mask_uint8, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
                if cnts:
                    perim = cv2.arcLength(cnts[0], True)
                    circ = 4*np.pi*area/(perim**2) if perim>0 else 0
                    if circ < 0.25: continue
            valid_masks.append(ann)

        if data_type == "Thin Film":
            sorted_masks = sorted(valid_masks, key=lambda x: x['bbox'][1])
        else:
            sorted_masks = sorted(valid_masks, key=lambda x: x['area'], reverse=True)

        thickness_vals = []; diameters = []
        for i, ann in enumerate(sorted_masks):
            m = ann['segmentation']
            bbox = ann['bbox']; x, y, w, h = [int(v) for v in bbox]
            cx, cy = int(x + w/2), int(y + h/2)
            color = (0, 255, 0) 
            if data_type == "Thin Film":
                is_bound = (i == 0 or i == len(sorted_masks)-1)
                cv2.putText(overlay, f"#{i+1}", (cx, cy), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (255, 255, 255), 3)
                if not is_bound:
                    thk = ann['area'] / w
                    thickness_vals.append(thk)
                    color = (255, 0, 0) 
                else: color = (100, 100, 100)
                if i < len(sorted_masks) - 1:
                    ys, xs = np.where(m)
                    if len(xs) > 10:
                        roughness = np.std(ys)
                        cv2.line(overlay, (x, y+h), (x+w, y+h), (0, 255, 255), 2)
                        cv2.putText(overlay, f"Int#{i+1} R:{roughness:.2f}", (x+10, y+h-5), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)
            elif data_type == "Particle":
                equivalent_diameter = 2 * np.sqrt(ann['area'] / np.pi)
                diameters.append(equivalent_diameter)
            mask_overlay[m] = color
            mask_uint8 = (m * 255).astype(np.uint8)
            cnts, _ = cv2.findContours(mask_uint8, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
            if cnts: cv2.drawContours(overlay, [cnts[0]], -1, color, 1)

        alpha = 0.35
        mask_idx = np.any(mask_overlay > 0, axis=-1)
        overlay[mask_idx] = (overlay[mask_idx]*(1-alpha) + mask_overlay[mask_idx]*alpha).astype(np.uint8)
        stats["count"] = len(sorted_masks)
        if data_type == "Thin Film":
            avg = round(np.mean(thickness_vals), 2) if thickness_vals else 0
            stats["thickness_avg"] = avg
            log.append(f"Layers: {len(sorted_masks)}, Avg Thk: {avg}")
        elif data_type == "Particle":
            avg = round(np.mean(diameters), 2) if diameters else 0
            stats["diameter_avg"] = avg
            log.append(f"Particles: {len(sorted_masks)}")
        return overlay, stats, log

    # --- Router ---
    @staticmethod
    def process_image_nv27(img_bytes, equipment, data_type, mode, goal, user_center=None, 
                           patch_size=128, step=16, n_components=10):
        nparr = np.frombuffer(img_bytes, np.uint8)
        img_raw = cv2.imdecode(nparr, cv2.IMREAD_COLOR)
        if img_raw is None: return {}
        
        body, footer = ScienceProcessorNV27.separate_footer(img_raw)
        footer_b64 = None
        if footer is not None:
            footer_b64 = base64.b64encode(cv2.imencode('.jpg', footer)[1]).decode('utf-8')

        if data_type == "Crystal Structure (FFT)":
            return ScienceProcessorNV27.analyze_crystal_fft_nmf(body, patch_size, step, n_components)
            
        if data_type == "2D Diffraction":
            return {**ScienceProcessorNV27.analyze_diffraction_nv7(body, user_center), "footer_b64": footer_b64}
        
        if mode == "None":
             def to_b64(im): return base64.b64encode(cv2.imencode('.jpg', im)[1]).decode('utf-8')
             return {"type":"image", "raw_b64":to_b64(body), "proc_b64":to_b64(body), "stats":{}, "log":["Mode: Raw"], "footer_b64": footer_b64}

        img_proc = body.copy(); log = []
        if mode == "Auto" or mode == "AI-Adaptive":
            img_proc = cv2.bilateralFilter(img_proc, 9, 75, 75)
            log.append("Preproc: Denoise")

        img_rgb = cv2.cvtColor(img_proc, cv2.COLOR_BGR2RGB)
        
        if mask_generator:
            try:
                masks = mask_generator.generate(img_rgb)
                log.append(f"SAM: {len(masks)} Segments")
                overlay, stats, sam_log = ScienceProcessorNV27.analyze_sam_results_nv7(img_rgb, masks, data_type)
                log.extend(sam_log)
                overlay = cv2.cvtColor(overlay, cv2.COLOR_RGB2BGR)
            except Exception as e:
                log.append(f"SAM Error: {str(e)}")
                overlay = body; stats = {}
        else:
            log.append("SAM Not Loaded")
            overlay = body; stats = {}

        def to_b64(im): return base64.b64encode(cv2.imencode('.jpg', im)[1]).decode('utf-8')
        return {"type": "image", "raw_b64": to_b64(body), "proc_b64": to_b64(overlay), "stats": stats, "log": log, "footer_b64": footer_b64}

# [4] Helper (Same)
async def analyze_vision_ollama(image_bytes: bytes, prompt: str) -> str:
    try:
        b64 = base64.b64encode(image_bytes).decode('utf-8')
        res = await asyncio.to_thread(ollama_client.chat, model=VISION_MODEL, messages=[{'role':'user','content':prompt,'images':[b64]}])
        return res['message']['content']
    except: return "Vision Error"

async def interpret_spectrum_data(raw_context: str, equipment: str) -> str:
    try:
        prompt = f"Analyze spectrum ({equipment}). Brief interpretation in Korean. Data: {raw_context}"
        res = await asyncio.to_thread(ollama_client.chat, model=TEXT_MODEL, messages=[{'role':'user','content':prompt}])
        return res['message']['content']
    except: return ""

# [5] App
app = FastAPI(title="Analyst NV27 Final", lifespan=lifespan)
app.add_middleware(CORSMiddleware, allow_origins=["*"], allow_methods=["*"], allow_headers=["*"])

class TranslateReq(BaseModel): text: str

@app.post("/api/translate")
async def translate_text(req: TranslateReq):
    try:
        res = await asyncio.to_thread(ollama_client.chat, model=TEXT_MODEL, messages=[{'role':'system','content':'Translate to English.'}, {'role':'user','content':req.text}])
        return {"translated": res['message']['content']}
    except Exception as e: return {"translated": str(e)}

async def process_analysis_job(job_id: str, files_map: Dict, configs: Dict):
    try:
        JOBS[job_id]["status"] = "Processing"
        JOBS[job_id]["progress"] = 10
        final_results = []
        total_files = len(configs)
        processed_count = 0

        for filename, config in configs.items():
            if filename not in files_map: continue
            content = files_map[filename]
            eq = config.get("equipment", "General")
            data_type = config.get("data_type", "General")
            goal = config.get("goal", "")
            mode = config.get("mode", "Auto")
            
            user_center = None
            if config.get("center_coords"):
                try: user_center = [int(v) for v in config["center_coords"].split(",")]
                except: pass
            
            p_size = int(config.get("patch_size", 128))
            p_step = int(config.get("step", 16))
            n_comp = int(config.get("n_components", 10))

            fname_lower = filename.lower()
            JOBS[job_id]["step"] = f"Analyzing {filename}..."
            
            if eq == "Document" or fname_lower.endswith(('.pdf', '.ppt', '.pptx')):
                try:
                    if fname_lower.endswith('.pdf'):
                        pdf = pdfium.PdfDocument(io.BytesIO(content))
                        n_pages = len(pdf)
                        full_txt = ""; pages = []
                        for i in range(min(n_pages, 10)):
                            page = pdf[i]
                            bitmap = page.render(scale=2)
                            pil_image = bitmap.to_pil()
                            buf = io.BytesIO()
                            pil_image.save(buf, format="JPEG")
                            b64 = base64.b64encode(buf.getvalue()).decode()
                            desc = await analyze_vision_ollama(buf.getvalue(), "한글 요약.")
                            full_txt += f"\nPage {i+1}: {desc}"
                            pages.append({"id": str(uuid.uuid4()), "page_num": i+1, "image_b64": b64, "desc": desc})
                        final_results.append({"type": "pdf", "filename": filename, "equipment": "Lit", "raw_context": full_txt, "pages": pages})
                    elif fname_lower.endswith(('.ppt', '.pptx')):
                        try:
                            prs = Presentation(io.BytesIO(content))
                            slides = []; full_txt = ""
                            for i, slide in enumerate(prs.slides):
                                txt = ""; imgs = []
                                for s in slide.shapes:
                                    if hasattr(s, "text"): txt += s.text + "\n"
                                    if s.shape_type == MSO_SHAPE_TYPE.PICTURE:
                                        try:
                                            ib64 = base64.b64encode(s.image.blob).decode('utf-8')
                                            desc = "Image"
                                            imgs.append({"b64": ib64, "desc": desc})
                                        except: pass
                                if not txt.strip(): txt = "(No text)"
                                full_txt += f"Slide {i+1}: {txt}\n"
                                slides.append({"id": str(uuid.uuid4()), "slide_num": i+1, "text": txt, "images": imgs})
                            final_results.append({"type": "ppt", "filename": filename, "equipment": "Lit", "raw_context": full_txt, "slides": slides})
                        except Exception as ppt_err:
                            final_results.append({"type": "error", "filename": filename, "msg": f"PPTX Error: {ppt_err}"})
                except Exception as e: 
                    final_results.append({"type": "error", "filename": filename, "msg": str(e)})

            elif fname_lower.endswith(('.xlsx', '.xls', '.csv', '.txt')):
                try:
                    dfs = ScienceProcessorNV27.read_universal_dataframe(content, filename)
                    for i, df in enumerate(dfs):
                        v_blocks = ScienceProcessorNV27.detect_structured_blocks(df)
                        for v_block in v_blocks:
                            h_blocks = ScienceProcessorNV27.split_horizontal_blocks(v_block)
                            for h_block in h_blocks:
                                series_list = ScienceProcessorNV27.extract_series_from_df(h_block)
                                for s in series_list:
                                    res = ScienceProcessorNV27.process_spectrum(s['x'], s['y'], mode, goal)
                                    step = max(1, len(s['x'])//100)
                                    chart = [{"x": float(res["x"][k]), "y_proc": float(res["y_proc"][k]), "y_raw": float(res["y_raw"][k]), "y_base": float(res["y_base"][k])} for k in range(0, len(s['x']), step)]
                                    ctx = f"Data: {s['name']}\nStats: {res['stats']}"
                                    interp = await interpret_spectrum_data(ctx, eq)
                                    final_results.append({
                                        "type": "spectrum", "filename": f"{filename} ({s['name']})", "equipment": eq,
                                        "raw_context": f"{ctx}\nInterp: {interp}", 
                                        "chart_data": chart, "log": res["log"], "interpretation": interp
                                    })
                except Exception as ex: final_results.append({"type": "error", "filename": filename, "msg": str(ex)})

            elif fname_lower.endswith(('.png', '.jpg', '.jpeg', '.tif', '.bmp')):
                try:
                    vis_res = await asyncio.to_thread(ScienceProcessorNV27.process_image_nv27, content, eq, data_type, mode, goal, user_center, p_size, p_step, n_comp)
                    if vis_res:
                        scale_info = "Unknown"
                        final_footer = vis_res.get("footer_b64")
                        if final_footer and data_type != "Crystal Structure (FFT)":
                            scale_text = await analyze_vision_ollama(base64.b64decode(final_footer), "Read scale bar text only.")
                            scale_info = scale_text
                            vis_res["log"].append(f"Scale: {scale_text}")

                        stats_clean = ""
                        for k, v in vis_res.get('stats',{}).items():
                            stats_clean += f"- {k}: {v}\n"
                        
                        desc_prompt = f"Analyze {eq} image ({data_type}). Stats: \n{stats_clean}\n. Goal: {goal}. Summary in Korean."
                        desc = await analyze_vision_ollama(base64.b64decode(vis_res["proc_b64"]), desc_prompt)
                        
                        final_results.append({
                            "type": "image", "filename": filename, "equipment": eq, "summary": desc,
                            "raw_context": f"Img {eq}: {desc}\nStats:\n{stats_clean}", 
                            "raw_b64": vis_res["raw_b64"], "proc_b64": vis_res["proc_b64"], "footer_b64": final_footer,
                            "stats": vis_res.get("stats",{}), "log": vis_res.get("log", [])
                        })
                except Exception as ex: final_results.append({"type": "error", "filename": filename, "msg": str(ex)})
            
            processed_count += 1
            JOBS[job_id]["progress"] = 10 + int((processed_count / total_files) * 80)

        JOBS[job_id]["step"] = "Writing Report..."
        final_results = sanitize_json(final_results)
        
        data_ctx = ""
        for r in final_results:
            if r.get("type") != "error":
                raw = r.get("raw_context") or ""
                data_ctx += f"\n=== File: {r['filename']} ({r.get('equipment')}) ===\n{raw[:2000]}\n"

        final_report = "Fail"
        if data_ctx:
            try:
                system_prompt = "You are a research assistant. Write a comprehensive report in Korean."
                res = await asyncio.to_thread(ollama_client.chat, model=TEXT_MODEL, messages=[{'role':'system','content':system_prompt}, {'role':'user','content':f"Data Summaries:\n{data_ctx}"}])
                final_report = res['message']['content']
            except Exception as e: final_report = str(e)

        JOBS[job_id]["results"] = {"results": final_results, "final_report": final_report}
        JOBS[job_id]["status"] = "Completed"
        JOBS[job_id]["progress"] = 100
    except Exception as e:
        JOBS[job_id]["status"] = "Failed"; JOBS[job_id]["error"] = str(e)

@app.post("/api/analyze")
async def start_analysis(background_tasks: BackgroundTasks, files: List[UploadFile] = File(...), file_configs: str = Form(...)):
    job_id = str(uuid.uuid4()); configs = json.loads(file_configs)
    files_map = {f.filename: await f.read() for f in files}
    JOBS[job_id] = {"status": "Pending", "progress": 0, "step": "Init", "results": None}
    background_tasks.add_task(process_analysis_job, job_id, files_map, configs)
    return {"job_id": job_id}

@app.get("/api/status/{job_id}")
async def get_status(job_id: str): return JOBS.get(job_id, {"status": "Not Found"})

@app.get("/", response_class=HTMLResponse)
async def serve_index():
    if os.path.exists("index.html"): return FileResponse("index.html")
    return "<h1>NV27 Perfect Backend</h1>"

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)






NV26 Code


import os
import io
# [0] OMP Error Fix
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"
import torch

import asyncio
import base64
import json
import re
import math
import uuid
import time
from typing import List, Dict, Any, Optional
from pydantic import BaseModel
from contextlib import asynccontextmanager
from collections import Counter

# [1] Security
os.environ['NO_PROXY'] = '127.0.0.1,localhost,0.0.0.0,::1'
if 'HTTP_PROXY' in os.environ: del os.environ['HTTP_PROXY']
if 'HTTPS_PROXY' in os.environ: del os.environ['HTTPS_PROXY']

from fastapi import FastAPI, UploadFile, File, Form, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import HTMLResponse, FileResponse
import pandas as pd
import numpy as np

# SciPy & CV & Sklearn
from sklearn.decomposition import NMF
from scipy.signal import find_peaks, savgol_filter
from scipy.optimize import curve_fit
from scipy.ndimage import center_of_mass
from skimage.feature import peak_local_max
import cv2
import matplotlib
import matplotlib.pyplot as plt 

# Models
import pypdfium2 as pdfium 
from segment_anything import sam_model_registry, SamAutomaticMaskGenerator
from ollama import Client 
from pptx import Presentation
from pptx.enum.shapes import MSO_SHAPE_TYPE

# [2] Config
OLLAMA_HOST = "http://127.0.0.1:11434"
ollama_client = Client(host=OLLAMA_HOST)
VISION_MODEL = "qwen3-vl:30b-a3b-instruct" 
TEXT_MODEL = "llama3:70b"

SAM_CHECKPOINT = "sam_vit_b_01ec64.pth"
SAM_MODEL_TYPE = "vit_b"
device = "cuda" if torch.cuda.is_available() else "cpu"

mask_generator = None 
JOBS = {} 

@asynccontextmanager
async def lifespan(app: FastAPI):
    global mask_generator
    if os.path.exists(SAM_CHECKPOINT):
        print(f"Loading SAM ({SAM_MODEL_TYPE}) on {device}...")
        try:
            sam = sam_model_registry[SAM_MODEL_TYPE](checkpoint=SAM_CHECKPOINT)
            sam.to(device=device)
            mask_generator = SamAutomaticMaskGenerator(
                model=sam, points_per_side=32, pred_iou_thresh=0.86,
                stability_score_thresh=0.90, min_mask_region_area=50
            )
            print("SAM Loaded.")
        except: print("SAM Load Failed.")
    yield
    JOBS.clear()

def sanitize_json(obj):
    if isinstance(obj, float):
        if np.isnan(obj) or np.isinf(obj): return 0.0
        return obj
    elif isinstance(obj, dict): return {k: sanitize_json(v) for k, v in obj.items()}
    elif isinstance(obj, list): return [sanitize_json(v) for v in obj]
    elif isinstance(obj, (np.int64, np.int32)): return int(obj)
    elif isinstance(obj, (np.float64, np.float32)):
        if np.isnan(obj) or np.isinf(obj): return 0.0
        return float(obj)
    return obj

# ==========================================
# [3] NV26 Detailed Visual Engine
# ==========================================
class ScienceProcessorNV26:
    
    # --- Universal Loader ---
    @staticmethod
    def read_universal_dataframe(content: bytes, filename: str) -> List[pd.DataFrame]:
        dfs = []
        fname = filename.lower()
        if fname.endswith(('.xlsx', '.xls')):
            try:
                xls = pd.ExcelFile(io.BytesIO(content))
                for s in xls.sheet_names:
                    df = pd.read_excel(xls, sheet_name=s, header=None)
                    if df.apply(pd.to_numeric, errors='coerce').notna().sum().sum() > 5: dfs.append(df)
                if dfs: return dfs
            except: pass
        text_content = ""
        for enc in ['utf-8', 'cp949', 'latin1']:
            try: text_content = content.decode(enc); break
            except: continue
        if text_content:
            lines = text_content.splitlines()
            data_start = -1; delimiter = None
            separators = [',', '\t', ';', '\s+']
            for i, line in enumerate(lines[:100]):
                for sep in separators:
                    parts = line.split() if sep=='\s+' else line.split(sep)
                    nums = sum(1 for p in parts if re.match(r'^-?\d+(\.\d+)?(?:[eE][+-]?\d+)?$', p.strip()))
                    if nums >= 2: data_start = i; delimiter = sep; break
                if data_start != -1: break
            if data_start != -1:
                try:
                    data_str = "\n".join(lines[data_start:])
                    sep_arg = '\s+' if delimiter=='\s+' else delimiter
                    df = pd.read_csv(io.StringIO(data_str), sep=sep_arg, header=None, engine='python')
                    dfs.append(df)
                except: pass
        return dfs

    # --- Block Parser ---
    @staticmethod
    def detect_structured_blocks(df: pd.DataFrame) -> List[pd.DataFrame]:
        blocks = []
        if df.empty: return []
        df_num = df.apply(pd.to_numeric, errors='coerce')
        if df.shape[1] > 0: is_data_row = df_num.iloc[:, 0].notna()
        else: is_data_row = df_num.notna().any(axis=1)
        groups = (is_data_row != is_data_row.shift()).cumsum()
        for _, group in df.groupby(groups):
            if is_data_row.loc[group.index[0]]:
                block = df_num.loc[group.index].dropna(how='all', axis=1)
                if block.shape[0] >= 10: blocks.append(block.reset_index(drop=True))
        return blocks

    @staticmethod
    def split_horizontal_blocks(df: pd.DataFrame) -> List[pd.DataFrame]:
        sub_blocks = []
        if df.empty: return []
        col_counts = df.apply(pd.to_numeric, errors='coerce').notna().sum()
        valid = np.where(col_counts >= 2)[0]
        if len(valid) == 0: return []
        splits = np.where(np.diff(valid) > 1)[0] + 1
        groups = np.split(valid, splits)
        for g in groups:
            if len(g) > 0:
                sub = df.iloc[:, g].reset_index(drop=True)
                if sub.shape[1] >= 1 and sub.shape[0] >= 2: sub_blocks.append(sub)
        return sub_blocks

    @staticmethod
    def extract_series_from_df(df: pd.DataFrame) -> List[Dict]:
        series_list = []
        try:
            vals = df.values; rows, cols = vals.shape
            if rows < 5: return []
            if cols == 1:
                y = vals[:, 0]; mask = ~np.isnan(y)
                if np.sum(mask) > 5: series_list.append({"x": np.arange(len(y))[mask], "y": y[mask], "name": "Single"})
                return series_list
            current_x = vals[:, 0]
            if np.isnan(current_x).all(): current_x = np.arange(rows)
            for i in range(1, cols):
                col_data = vals[:, i]; mask_valid = ~np.isnan(col_data)
                if np.sum(mask_valid) > 5: series_list.append({"x": current_x[mask_valid], "y": col_data[mask_valid], "name": f"Col-{i}"})
        except: pass
        return series_list

    # --- Spectrum Logic ---
    @staticmethod
    def simple_baseline(y):
        try:
            w = max(5, len(y)//10)
            s = pd.Series(y).rolling(window=w, center=True).min()
            b = s.bfill().ffill().values 
            if len(b) > 51: b = savgol_filter(b, 51, 3)
            return b, w
        except: return np.zeros_like(y), 0

    @staticmethod
    def process_spectrum(x, y, mode, goal):
        log = []; y_raw = y.copy(); base = np.zeros_like(y); y_proc = y.copy()
        peaks = []; fits = []; stats_txt = "Error"
        try:
            if mode == "None":
                peaks, _ = find_peaks(y, height=np.max(y)*0.05)
                stats_txt = f"Raw Peaks: {len(peaks)}"
            else:
                win = 15
                if mode == "AI-Adaptive" and "noise" in goal.lower(): win = 31
                base, bw = ScienceProcessorNV26.simple_baseline(y)
                y_proc = np.maximum(y - base, 0)
                if len(y_proc) > win: y_proc = savgol_filter(y_proc, win, 3)
                peaks, _ = find_peaks(y_proc, height=np.max(y_proc)*0.05, distance=10)
                stats_txt = f"Peaks: {len(peaks)}"
            return {"x": x, "y_raw": y_raw, "y_proc": y_proc, "y_base": base, "peaks": [{"x": float(x[p]), "y": float(y_proc[p])} for p in peaks], "log": log, "fits": fits, "stats": stats_txt}
        except Exception as e: return {"x": x, "y_raw": y_raw, "y_proc": y_proc, "y_base": base, "peaks": [], "log": [str(e)], "fits": [], "stats": "Error"}

    # --- Footer ---
    @staticmethod
    def separate_footer(img_bgr):
        h, w = img_bgr.shape[:2]; roi_h = int(h * 0.82); roi = img_bgr[roi_h:, :]
        gray = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)
        sobel_y = np.abs(cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=3))
        row_sums = np.sum(sobel_y, axis=1)
        split_idx = np.argmax(row_sums)
        if row_sums[split_idx] > w * 30: return img_bgr[:roi_h + split_idx, :], img_bgr[roi_h + split_idx:, :]
        return img_bgr, None

    # --- [E] NV26 FFT-NMF (W, H, Colorbar, Legend) ---
    @staticmethod
    def analyze_crystal_fft_nmf(img_bgr, patch_size=128, step=16, n_components=10):
        log = []
        gray = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2GRAY)
        h, w = gray.shape
        
        log.append(f"FFT-NMF: Patch={patch_size}, Step={step}, K={n_components}")
        
        custom_colors = [
            (0, 0, 255), (0, 255, 0), (255, 0, 0), (0, 255, 255), (255, 255, 0),
            (255, 0, 255), (0, 165, 255), (238, 130, 238), (128, 128, 0), (19, 69, 139)
        ]
        while len(custom_colors) < n_components: custom_colors.append((255, 255, 255))
        
        cols = [] 
        y_steps = list(range(0, h - patch_size + 1, step))
        x_steps = list(range(0, w - patch_size + 1, step))
        grid_h = len(y_steps)
        grid_w = len(x_steps)
        
        # DC Mask (Radius 4)
        cy, cx = patch_size // 2, patch_size // 2
        Y, X = np.ogrid[:patch_size, :patch_size]
        dist_sq = (X - cx)**2 + (Y - cy)**2
        dc_mask = dist_sq <= 4**2

        for y in y_steps:
            for x in x_steps:
                roi = gray[y:y+patch_size, x:x+patch_size]
                f = np.fft.fft2(roi)
                fshift = np.fft.fftshift(f)
                mag = np.abs(fshift) 
                mag[dc_mask] = 0
                cols.append(mag.flatten())
        
        if not cols: return {"type":"error", "msg":"Img too small"}
        
        try:
            V = np.array(cols).T.astype(np.float32)
            V = V - V.min()
            if V.max() > 0: V = V / V.max()
            
            nmf = NMF(n_components=n_components, init='nndsvda', random_state=42, max_iter=2000)
            W = nmf.fit_transform(V) # (Pixels, K)
            H = nmf.components_      # (K, Patches)
            
            if H.shape[1] != grid_h * grid_w: raise ValueError("Shape Mismatch")
            
            idx_H = np.argmax(H, axis=0) 
            map_low = idx_H.reshape(grid_h, grid_w).astype(np.uint8)
            map_high = cv2.resize(map_low, (w, h), interpolation=cv2.INTER_NEAREST)
            
            overlay = img_bgr.copy()
            mask_layer = np.zeros_like(img_bgr)
            
            counts = Counter(idx_H)
            total = len(idx_H)
            stats = {}
            
            for i in range(n_components):
                mask = (map_high == i)
                mask_layer[mask] = custom_colors[i]
                cnt = counts.get(i, 0)
                stats[f"Pattern {i}"] = f"{cnt} patches ({cnt/total*100:.1f}%)"
                
            alpha = 0.4
            mask_idx = np.any(mask_layer > 0, axis=-1)
            overlay[mask_idx] = (overlay[mask_idx]*(1-alpha) + mask_layer[mask_idx]*alpha).astype(np.uint8)
            
            # [NV26] 4-Stack: W -> H -> Colorbar -> Legend
            basis_combo_list = []
            
            for k in range(n_components):
                # 1. W Matrix (Basis Pattern)
                pat = W[:, k].reshape(patch_size, patch_size)
                pat_log = np.log1p(pat * 1000) 
                pat_norm = cv2.normalize(pat_log, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)
                pat_large = cv2.resize(pat_norm, (128, 128), interpolation=cv2.INTER_NEAREST)
                pat_vis = cv2.applyColorMap(pat_large, cv2.COLORMAP_MAGMA)
                cv2.putText(pat_vis, "W (Basis)", (5, 15), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255,255,255), 1)
                
                # 2. H Matrix (Distribution)
                h_vec = H[k, :] 
                h_min, h_max = h_vec.min(), h_vec.max()
                
                h_grid = h_vec.reshape(grid_h, grid_w)
                h_large = cv2.resize(h_grid, (128, 128), interpolation=cv2.INTER_NEAREST)
                h_norm = cv2.normalize(h_large, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)
                h_vis = cv2.applyColorMap(h_norm, cv2.COLORMAP_VIRIDIS)
                cv2.putText(h_vis, "H (Dist)", (5, 15), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255,255,255), 1)
                
                # 3. [New] H Colorbar (Gradient + Min/Max)
                cbar_h = 25
                cbar_w = 128
                # Create gradient 0..255
                gradient = np.linspace(0, 255, cbar_w).astype(np.uint8)
                gradient = np.tile(gradient, (cbar_h, 1)) # Repeat vertically
                cbar_vis = cv2.applyColorMap(gradient, cv2.COLORMAP_VIRIDIS)
                
                # Text: Min (Left, White), Max (Right, Black/Dark)
                min_str = f"{h_min:.1f}"
                max_str = f"{h_max:.1f}"
                cv2.putText(cbar_vis, min_str, (2, 17), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255,255,255), 1)
                # Right align Max
                (tw, th), _ = cv2.getTextSize(max_str, cv2.FONT_HERSHEY_SIMPLEX, 0.4, 1)
                cv2.putText(cbar_vis, max_str, (cbar_w - tw - 2, 17), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (0,0,0), 1)
                
                # 4. Legend (Categorical P#)
                legend_h = 30
                legend = np.zeros((legend_h, 128, 3), dtype=np.uint8)
                legend[:] = custom_colors[k]
                cv2.putText(legend, f"P{k}", (50, 22), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255,255,255), 2)
                
                # 5. Stack All
                combo = np.vstack([pat_vis, h_vis, cbar_vis, legend])
                
                # Border
                cv2.rectangle(combo, (0,0), (combo.shape[1]-1, combo.shape[0]-1), custom_colors[k], 2)
                
                basis_combo_list.append(combo)
            
            # Combine horizontally
            full_strip = np.hstack(basis_combo_list)
            
            def to_b64(im): return base64.b64encode(cv2.imencode('.jpg', im)[1]).decode('utf-8')
            
            return {
                "type": "image", 
                "raw_b64": to_b64(img_bgr), 
                "proc_b64": to_b64(overlay),
                "footer_b64": to_b64(full_strip), 
                "stats": stats, 
                "log": log
            }

        except Exception as e:
            print(f"NMF Error: {e}")
            def to_b64(im): return base64.b64encode(cv2.imencode('.jpg', im)[1]).decode('utf-8')
            return {"type": "image", "raw_b64": to_b64(img_bgr), "proc_b64": to_b64(img_bgr), "stats": {"Error": str(e)}, "log": log}

    # --- [F] Diffraction ---
    @staticmethod
    def analyze_diffraction_nv7(img_bgr, user_center=None):
        log = []; stats = {}
        gray = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2GRAY)
        if user_center and len(user_center)==2 and user_center[0]>0:
            cx, cy = user_center
            log.append(f"Center: Manual [{cx}, {cy}]")
        else:
            _, th = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY+cv2.THRESH_OTSU)
            cyf, cxf = center_of_mass(th)
            if np.isnan(cyf): cyf, cxf = gray.shape[0]//2, gray.shape[1]//2
            cx, cy = int(cxf), int(cyf)
            log.append(f"Center: Auto [{cx}, {cy}]")

        g1 = cv2.GaussianBlur(gray, (3,3), 0)
        g2 = cv2.GaussianBlur(gray, (51,51), 0)
        diff = cv2.subtract(g1, g2)
        coords = peak_local_max(diff, min_distance=10, threshold_abs=20)
        enhanced = np.log1p(diff.astype(np.float32))
        enhanced = cv2.normalize(enhanced, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)
        vis = cv2.cvtColor(enhanced, cv2.COLOR_GRAY2BGR)
        cv2.drawMarker(vis, (cx, cy), (0,0,255), cv2.MARKER_CROSS, 20, 2)
        peak_list = []
        for p in coords:
            py, px = p
            if abs(py-cy)<5 and abs(px-cx)<5: continue
            cv2.circle(vis, (px, py), 3, (0,255,0), 1)
            peak_list.append(f"[{px},{py}]")
        log.append(f"Spots: {len(peak_list)}")
        def to_b64(im): return base64.b64encode(cv2.imencode('.jpg', im)[1]).decode('utf-8')
        return {"type": "image", "raw_b64": to_b64(img_bgr), "proc_b64": to_b64(vis), "stats": {"center": [cx, cy], "spots": len(peak_list)}, "log": log}

    # --- [G] SAM ---
    @staticmethod
    def analyze_sam_results_nv7(img_rgb, masks, data_type):
        overlay = img_rgb.copy()
        mask_overlay = np.zeros_like(img_rgb)
        h_img, w_img = img_rgb.shape[:2]
        stats = {"count": 0}
        log = []
        
        valid_masks = []
        for ann in masks:
            area = ann['area']; bbox = ann['bbox']; x, y, w, h = [int(v) for v in bbox]
            if area > h_img * w_img * 0.95 or area < 20: continue 
            if data_type == "Particle":
                if x<=1 or y<=1 or (x+w)>=w_img-1 or (y+h)>=h_img-1: continue
                mask_uint8 = (ann['segmentation'] * 255).astype(np.uint8)
                cnts, _ = cv2.findContours(mask_uint8, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
                if cnts:
                    perim = cv2.arcLength(cnts[0], True)
                    circ = 4*np.pi*area/(perim**2) if perim>0 else 0
                    if circ < 0.25: continue
            valid_masks.append(ann)

        if data_type == "Thin Film":
            sorted_masks = sorted(valid_masks, key=lambda x: x['bbox'][1])
        else:
            sorted_masks = sorted(valid_masks, key=lambda x: x['area'], reverse=True)

        thickness_vals = []; diameters = []

        for i, ann in enumerate(sorted_masks):
            m = ann['segmentation']
            bbox = ann['bbox']; x, y, w, h = [int(v) for v in bbox]
            cx, cy = int(x + w/2), int(y + h/2)
            color = (0, 255, 0) 

            if data_type == "Thin Film":
                is_bound = (i == 0 or i == len(sorted_masks)-1)
                cv2.putText(overlay, f"#{i+1}", (cx, cy), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (255, 255, 255), 3)
                if not is_bound:
                    thk = ann['area'] / w
                    thickness_vals.append(thk)
                    color = (255, 0, 0) 
                else: color = (100, 100, 100)
                if i < len(sorted_masks) - 1:
                    ys, xs = np.where(m)
                    if len(xs) > 10:
                        roughness = np.std(ys)
                        cv2.line(overlay, (x, y+h), (x+w, y+h), (0, 255, 255), 2)
                        cv2.putText(overlay, f"Int#{i+1} R:{roughness:.2f}", (x+10, y+h-5), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)
            elif data_type == "Particle":
                equivalent_diameter = 2 * np.sqrt(ann['area'] / np.pi)
                diameters.append(equivalent_diameter)

            mask_overlay[m] = color
            mask_uint8 = (m * 255).astype(np.uint8)
            cnts, _ = cv2.findContours(mask_uint8, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
            if cnts: cv2.drawContours(overlay, [cnts[0]], -1, color, 1)

        alpha = 0.35
        mask_idx = np.any(mask_overlay > 0, axis=-1)
        overlay[mask_idx] = (overlay[mask_idx]*(1-alpha) + mask_overlay[mask_idx]*alpha).astype(np.uint8)

        stats["count"] = len(sorted_masks)
        if data_type == "Thin Film":
            avg = round(np.mean(thickness_vals), 2) if thickness_vals else 0
            stats["thickness_avg"] = avg
            log.append(f"Layers: {len(sorted_masks)}, Avg Thk: {avg}")
        elif data_type == "Particle":
            avg = round(np.mean(diameters), 2) if diameters else 0
            stats["diameter_avg"] = avg
            log.append(f"Particles: {len(sorted_masks)}")
        return overlay, stats, log

    # --- Router ---
    @staticmethod
    def process_image_nv26(img_bytes, equipment, data_type, mode, goal, user_center=None, 
                           patch_size=128, step=16, n_components=10):
        nparr = np.frombuffer(img_bytes, np.uint8)
        img_raw = cv2.imdecode(nparr, cv2.IMREAD_COLOR)
        if img_raw is None: return {}
        
        body, footer = ScienceProcessorNV26.separate_footer(img_raw)
        footer_b64 = None
        if footer is not None:
            footer_b64 = base64.b64encode(cv2.imencode('.jpg', footer)[1]).decode('utf-8')

        if data_type == "Crystal Structure (FFT)":
            return ScienceProcessorNV26.analyze_crystal_fft_nmf(body, patch_size, step, n_components)
            
        if data_type == "2D Diffraction":
            return {**ScienceProcessorNV26.analyze_diffraction_nv7(body, user_center), "footer_b64": footer_b64}
        
        if mode == "None":
             def to_b64(im): return base64.b64encode(cv2.imencode('.jpg', im)[1]).decode('utf-8')
             return {"type":"image", "raw_b64":to_b64(body), "proc_b64":to_b64(body), "stats":{}, "log":["Mode: Raw"], "footer_b64": footer_b64}

        img_proc = body.copy(); log = []
        if mode == "Auto" or mode == "AI-Adaptive":
            img_proc = cv2.bilateralFilter(img_proc, 9, 75, 75)
            log.append("Preproc: Denoise")

        img_rgb = cv2.cvtColor(img_proc, cv2.COLOR_BGR2RGB)
        
        if mask_generator:
            try:
                masks = mask_generator.generate(img_rgb)
                log.append(f"SAM: {len(masks)} Segments")
                overlay, stats, sam_log = ScienceProcessorNV26.analyze_sam_results_nv7(img_rgb, masks, data_type)
                log.extend(sam_log)
                overlay = cv2.cvtColor(overlay, cv2.COLOR_RGB2BGR)
            except Exception as e:
                log.append(f"SAM Error: {str(e)}")
                overlay = body; stats = {}
        else:
            log.append("SAM Not Loaded")
            overlay = body; stats = {}

        def to_b64(im): return base64.b64encode(cv2.imencode('.jpg', im)[1]).decode('utf-8')
        return {"type": "image", "raw_b64": to_b64(body), "proc_b64": to_b64(overlay), "stats": stats, "log": log, "footer_b64": footer_b64}

# [4] Helper (Same)
async def analyze_vision_ollama(image_bytes: bytes, prompt: str) -> str:
    try:
        b64 = base64.b64encode(image_bytes).decode('utf-8')
        res = await asyncio.to_thread(ollama_client.chat, model=VISION_MODEL, messages=[{'role':'user','content':prompt,'images':[b64]}])
        return res['message']['content']
    except: return "Vision Error"

async def interpret_spectrum_data(raw_context: str, equipment: str) -> str:
    try:
        prompt = f"Analyze spectrum ({equipment}). Brief interpretation in Korean. Data: {raw_context}"
        res = await asyncio.to_thread(ollama_client.chat, model=TEXT_MODEL, messages=[{'role':'user','content':prompt}])
        return res['message']['content']
    except: return ""

# [5] App
app = FastAPI(title="Analyst NV26 Detailed", lifespan=lifespan)
app.add_middleware(CORSMiddleware, allow_origins=["*"], allow_methods=["*"], allow_headers=["*"])

class TranslateReq(BaseModel): text: str

@app.post("/api/translate")
async def translate_text(req: TranslateReq):
    try:
        res = await asyncio.to_thread(ollama_client.chat, model=TEXT_MODEL, messages=[{'role':'system','content':'Translate to English.'}, {'role':'user','content':req.text}])
        return {"translated": res['message']['content']}
    except Exception as e: return {"translated": str(e)}

async def process_analysis_job(job_id: str, files_map: Dict, configs: Dict):
    try:
        JOBS[job_id]["status"] = "Processing"
        JOBS[job_id]["progress"] = 10
        final_results = []
        total_files = len(configs)
        processed_count = 0

        for filename, config in configs.items():
            if filename not in files_map: continue
            content = files_map[filename]
            eq = config.get("equipment", "General")
            data_type = config.get("data_type", "General")
            goal = config.get("goal", "")
            mode = config.get("mode", "Auto")
            
            user_center = None
            if config.get("center_coords"):
                try: user_center = [int(v) for v in config["center_coords"].split(",")]
                except: pass
            
            p_size = int(config.get("patch_size", 128))
            p_step = int(config.get("step", 16))
            n_comp = int(config.get("n_components", 10))

            fname_lower = filename.lower()
            JOBS[job_id]["step"] = f"Analyzing {filename}..."
            
            if eq == "Document" or fname_lower.endswith(('.pdf', '.ppt', '.pptx')):
                try:
                    if fname_lower.endswith('.pdf'):
                        pdf = pdfium.PdfDocument(io.BytesIO(content))
                        n_pages = len(pdf)
                        full_txt = ""; pages = []
                        for i in range(min(n_pages, 10)):
                            page = pdf[i]
                            bitmap = page.render(scale=2)
                            pil_image = bitmap.to_pil()
                            buf = io.BytesIO()
                            pil_image.save(buf, format="JPEG")
                            b64 = base64.b64encode(buf.getvalue()).decode()
                            desc = await analyze_vision_ollama(buf.getvalue(), "한글 요약.")
                            full_txt += f"\nPage {i+1}: {desc}"
                            pages.append({"id": str(uuid.uuid4()), "page_num": i+1, "image_b64": b64, "desc": desc})
                        final_results.append({"type": "pdf", "filename": filename, "equipment": "Lit", "raw_context": full_txt, "pages": pages})
                    elif fname_lower.endswith(('.ppt', '.pptx')):
                        try:
                            prs = Presentation(io.BytesIO(content))
                            slides = []; full_txt = ""
                            for i, slide in enumerate(prs.slides):
                                txt = ""; imgs = []
                                for s in slide.shapes:
                                    if hasattr(s, "text"): txt += s.text + "\n"
                                    if s.shape_type == MSO_SHAPE_TYPE.PICTURE:
                                        try:
                                            ib64 = base64.b64encode(s.image.blob).decode('utf-8')
                                            desc = "Image"
                                            imgs.append({"b64": ib64, "desc": desc})
                                        except: pass
                                if not txt.strip(): txt = "(No text)"
                                full_txt += f"Slide {i+1}: {txt}\n"
                                slides.append({"id": str(uuid.uuid4()), "slide_num": i+1, "text": txt, "images": imgs})
                            final_results.append({"type": "ppt", "filename": filename, "equipment": "Lit", "raw_context": full_txt, "slides": slides})
                        except Exception as ppt_err:
                            final_results.append({"type": "error", "filename": filename, "msg": f"PPTX Error: {ppt_err}"})
                except Exception as e: 
                    final_results.append({"type": "error", "filename": filename, "msg": str(e)})

            elif fname_lower.endswith(('.xlsx', '.xls', '.csv', '.txt')):
                try:
                    dfs = ScienceProcessorNV26.read_universal_dataframe(content, filename)
                    for i, df in enumerate(dfs):
                        v_blocks = ScienceProcessorNV26.detect_structured_blocks(df)
                        for v_block in v_blocks:
                            h_blocks = ScienceProcessorNV26.split_horizontal_blocks(v_block)
                            for h_block in h_blocks:
                                series_list = ScienceProcessorNV26.extract_series_from_df(h_block)
                                for s in series_list:
                                    res = ScienceProcessorNV26.process_spectrum(s['x'], s['y'], mode, goal)
                                    step = max(1, len(s['x'])//100)
                                    chart = [{"x": float(res["x"][k]), "y_proc": float(res["y_proc"][k]), "y_raw": float(res["y_raw"][k]), "y_base": float(res["y_base"][k])} for k in range(0, len(s['x']), step)]
                                    ctx = f"Data: {s['name']}\nStats: {res['stats']}"
                                    interp = await interpret_spectrum_data(ctx, eq)
                                    final_results.append({
                                        "type": "spectrum", "filename": f"{filename} ({s['name']})", "equipment": eq,
                                        "raw_context": f"{ctx}\nInterp: {interp}", 
                                        "chart_data": chart, "log": res["log"], "interpretation": interp
                                    })
                except Exception as ex: final_results.append({"type": "error", "filename": filename, "msg": str(ex)})

            elif fname_lower.endswith(('.png', '.jpg', '.jpeg', '.tif', '.bmp')):
                try:
                    vis_res = await asyncio.to_thread(ScienceProcessorNV26.process_image_nv26, content, eq, data_type, mode, goal, user_center, p_size, p_step, n_comp)
                    if vis_res:
                        scale_info = "Unknown"
                        final_footer = vis_res.get("footer_b64")
                        if final_footer and data_type != "Crystal Structure (FFT)":
                            scale_text = await analyze_vision_ollama(base64.b64decode(final_footer), "Read scale bar text only.")
                            scale_info = scale_text
                            vis_res["log"].append(f"Scale: {scale_text}")

                        stats_clean = ""
                        for k, v in vis_res.get('stats',{}).items():
                            stats_clean += f"- {k}: {v}\n"
                        
                        desc_prompt = f"Analyze {eq} image ({data_type}). Stats: \n{stats_clean}\n. Goal: {goal}. Summary in Korean."
                        desc = await analyze_vision_ollama(base64.b64decode(vis_res["proc_b64"]), desc_prompt)
                        
                        final_results.append({
                            "type": "image", "filename": filename, "equipment": eq, "summary": desc,
                            "raw_context": f"Img {eq}: {desc}\nStats:\n{stats_clean}", 
                            "raw_b64": vis_res["raw_b64"], "proc_b64": vis_res["proc_b64"], "footer_b64": final_footer,
                            "stats": vis_res.get("stats",{}), "log": vis_res.get("log", [])
                        })
                except Exception as ex: final_results.append({"type": "error", "filename": filename, "msg": str(ex)})
            
            processed_count += 1
            JOBS[job_id]["progress"] = 10 + int((processed_count / total_files) * 80)

        JOBS[job_id]["step"] = "Writing Report..."
        final_results = sanitize_json(final_results)
        
        data_ctx = ""
        for r in final_results:
            if r.get("type") != "error":
                raw = r.get("raw_context") or ""
                data_ctx += f"\n=== File: {r['filename']} ({r.get('equipment')}) ===\n{raw[:2000]}\n"

        final_report = "Fail"
        if data_ctx:
            try:
                system_prompt = "You are a research assistant. Write a comprehensive report in Korean."
                res = await asyncio.to_thread(ollama_client.chat, model=TEXT_MODEL, messages=[{'role':'system','content':system_prompt}, {'role':'user','content':f"Data Summaries:\n{data_ctx}"}])
                final_report = res['message']['content']
            except Exception as e: final_report = str(e)

        JOBS[job_id]["results"] = {"results": final_results, "final_report": final_report}
        JOBS[job_id]["status"] = "Completed"
        JOBS[job_id]["progress"] = 100
    except Exception as e:
        JOBS[job_id]["status"] = "Failed"; JOBS[job_id]["error"] = str(e)

@app.post("/api/analyze")
async def start_analysis(background_tasks: BackgroundTasks, files: List[UploadFile] = File(...), file_configs: str = Form(...)):
    job_id = str(uuid.uuid4()); configs = json.loads(file_configs)
    files_map = {f.filename: await f.read() for f in files}
    JOBS[job_id] = {"status": "Pending", "progress": 0, "step": "Init", "results": None}
    background_tasks.add_task(process_analysis_job, job_id, files_map, configs)
    return {"job_id": job_id}

@app.get("/api/status/{job_id}")
async def get_status(job_id: str): return JOBS.get(job_id, {"status": "Not Found"})

@app.get("/", response_class=HTMLResponse)
async def serve_index():
    if os.path.exists("index.html"): return FileResponse("index.html")
    return "<h1>NV26 Detailed Visual Backend</h1>"

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)






NV25 Python Code


import os
import io
# [0] OMP Error Fix
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"
import torch

import asyncio
import base64
import json
import re
import math
import uuid
import time
from typing import List, Dict, Any, Optional
from pydantic import BaseModel
from contextlib import asynccontextmanager
from collections import Counter

# [1] Security
os.environ['NO_PROXY'] = '127.0.0.1,localhost,0.0.0.0,::1'
if 'HTTP_PROXY' in os.environ: del os.environ['HTTP_PROXY']
if 'HTTPS_PROXY' in os.environ: del os.environ['HTTPS_PROXY']

from fastapi import FastAPI, UploadFile, File, Form, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import HTMLResponse, FileResponse
import pandas as pd
import numpy as np

# SciPy & CV & Sklearn
from sklearn.decomposition import NMF
from scipy.signal import find_peaks, savgol_filter
from scipy.optimize import curve_fit
from scipy.ndimage import center_of_mass
from skimage.feature import peak_local_max
import cv2
import matplotlib
import matplotlib.pyplot as plt 

# Models
import pypdfium2 as pdfium 
from segment_anything import sam_model_registry, SamAutomaticMaskGenerator
from ollama import Client 
from pptx import Presentation
from pptx.enum.shapes import MSO_SHAPE_TYPE

# [2] Config
OLLAMA_HOST = "http://127.0.0.1:11434"
ollama_client = Client(host=OLLAMA_HOST)
VISION_MODEL = "qwen3-vl:30b-a3b-instruct" 
TEXT_MODEL = "llama3:70b"

SAM_CHECKPOINT = "sam_vit_b_01ec64.pth"
SAM_MODEL_TYPE = "vit_b"
device = "cuda" if torch.cuda.is_available() else "cpu"

mask_generator = None 
JOBS = {} 

@asynccontextmanager
async def lifespan(app: FastAPI):
    global mask_generator
    if os.path.exists(SAM_CHECKPOINT):
        print(f"Loading SAM ({SAM_MODEL_TYPE}) on {device}...")
        try:
            sam = sam_model_registry[SAM_MODEL_TYPE](checkpoint=SAM_CHECKPOINT)
            sam.to(device=device)
            mask_generator = SamAutomaticMaskGenerator(
                model=sam, points_per_side=32, pred_iou_thresh=0.86,
                stability_score_thresh=0.90, min_mask_region_area=50
            )
            print("SAM Loaded.")
        except: print("SAM Load Failed.")
    yield
    JOBS.clear()

def sanitize_json(obj):
    if isinstance(obj, float):
        if np.isnan(obj) or np.isinf(obj): return 0.0
        return obj
    elif isinstance(obj, dict): return {k: sanitize_json(v) for k, v in obj.items()}
    elif isinstance(obj, list): return [sanitize_json(v) for v in obj]
    elif isinstance(obj, (np.int64, np.int32)): return int(obj)
    elif isinstance(obj, (np.float64, np.float32)):
        if np.isnan(obj) or np.isinf(obj): return 0.0
        return float(obj)
    return obj

# ==========================================
# [3] NV25 Dual Matrix Visual Engine
# ==========================================
class ScienceProcessorNV25:
    
    # --- Universal Loader ---
    @staticmethod
    def read_universal_dataframe(content: bytes, filename: str) -> List[pd.DataFrame]:
        dfs = []
        fname = filename.lower()
        if fname.endswith(('.xlsx', '.xls')):
            try:
                xls = pd.ExcelFile(io.BytesIO(content))
                for s in xls.sheet_names:
                    df = pd.read_excel(xls, sheet_name=s, header=None)
                    if df.apply(pd.to_numeric, errors='coerce').notna().sum().sum() > 5: dfs.append(df)
                if dfs: return dfs
            except: pass
        text_content = ""
        for enc in ['utf-8', 'cp949', 'latin1']:
            try: text_content = content.decode(enc); break
            except: continue
        if text_content:
            lines = text_content.splitlines()
            data_start = -1; delimiter = None
            separators = [',', '\t', ';', '\s+']
            for i, line in enumerate(lines[:100]):
                for sep in separators:
                    parts = line.split() if sep=='\s+' else line.split(sep)
                    nums = sum(1 for p in parts if re.match(r'^-?\d+(\.\d+)?(?:[eE][+-]?\d+)?$', p.strip()))
                    if nums >= 2: data_start = i; delimiter = sep; break
                if data_start != -1: break
            if data_start != -1:
                try:
                    data_str = "\n".join(lines[data_start:])
                    sep_arg = '\s+' if delimiter=='\s+' else delimiter
                    df = pd.read_csv(io.StringIO(data_str), sep=sep_arg, header=None, engine='python')
                    dfs.append(df)
                except: pass
        return dfs

    # --- Block Parser ---
    @staticmethod
    def detect_structured_blocks(df: pd.DataFrame) -> List[pd.DataFrame]:
        blocks = []
        if df.empty: return []
        df_num = df.apply(pd.to_numeric, errors='coerce')
        if df.shape[1] > 0: is_data_row = df_num.iloc[:, 0].notna()
        else: is_data_row = df_num.notna().any(axis=1)
        groups = (is_data_row != is_data_row.shift()).cumsum()
        for _, group in df.groupby(groups):
            if is_data_row.loc[group.index[0]]:
                block = df_num.loc[group.index].dropna(how='all', axis=1)
                if block.shape[0] >= 10: blocks.append(block.reset_index(drop=True))
        return blocks

    @staticmethod
    def split_horizontal_blocks(df: pd.DataFrame) -> List[pd.DataFrame]:
        sub_blocks = []
        if df.empty: return []
        col_counts = df.apply(pd.to_numeric, errors='coerce').notna().sum()
        valid = np.where(col_counts >= 2)[0]
        if len(valid) == 0: return []
        splits = np.where(np.diff(valid) > 1)[0] + 1
        groups = np.split(valid, splits)
        for g in groups:
            if len(g) > 0:
                sub = df.iloc[:, g].reset_index(drop=True)
                if sub.shape[1] >= 1 and sub.shape[0] >= 2: sub_blocks.append(sub)
        return sub_blocks

    @staticmethod
    def extract_series_from_df(df: pd.DataFrame) -> List[Dict]:
        series_list = []
        try:
            vals = df.values; rows, cols = vals.shape
            if rows < 5: return []
            if cols == 1:
                y = vals[:, 0]; mask = ~np.isnan(y)
                if np.sum(mask) > 5: series_list.append({"x": np.arange(len(y))[mask], "y": y[mask], "name": "Single"})
                return series_list
            current_x = vals[:, 0]
            if np.isnan(current_x).all(): current_x = np.arange(rows)
            for i in range(1, cols):
                col_data = vals[:, i]; mask_valid = ~np.isnan(col_data)
                if np.sum(mask_valid) > 5: series_list.append({"x": current_x[mask_valid], "y": col_data[mask_valid], "name": f"Col-{i}"})
        except: pass
        return series_list

    # --- Spectrum Logic ---
    @staticmethod
    def simple_baseline(y):
        try:
            w = max(5, len(y)//10)
            s = pd.Series(y).rolling(window=w, center=True).min()
            b = s.bfill().ffill().values 
            if len(b) > 51: b = savgol_filter(b, 51, 3)
            return b, w
        except: return np.zeros_like(y), 0

    @staticmethod
    def process_spectrum(x, y, mode, goal):
        log = []; y_raw = y.copy(); base = np.zeros_like(y); y_proc = y.copy()
        peaks = []; fits = []; stats_txt = "Error"
        try:
            if mode == "None":
                peaks, _ = find_peaks(y, height=np.max(y)*0.05)
                stats_txt = f"Raw Peaks: {len(peaks)}"
            else:
                win = 15
                if mode == "AI-Adaptive":
                    if "noise" in goal.lower(): win = 31
                base, bw = ScienceProcessorNV25.simple_baseline(y)
                y_proc = np.maximum(y - base, 0)
                if len(y_proc) > win:
                    y_proc = savgol_filter(y_proc, win, 3)
                peaks, _ = find_peaks(y_proc, height=np.max(y_proc)*0.05, distance=10)
                stats_txt = f"Peaks: {len(peaks)}"
            return {"x": x, "y_raw": y_raw, "y_proc": y_proc, "y_base": base, "peaks": [{"x": float(x[p]), "y": float(y_proc[p])} for p in peaks], "log": log, "fits": fits, "stats": stats_txt}
        except Exception as e: return {"x": x, "y_raw": y_raw, "y_proc": y_proc, "y_base": base, "peaks": [], "log": [str(e)], "fits": [], "stats": "Error"}

    # --- Footer ---
    @staticmethod
    def separate_footer(img_bgr):
        h, w = img_bgr.shape[:2]; roi_h = int(h * 0.82); roi = img_bgr[roi_h:, :]
        gray = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)
        sobel_y = np.abs(cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=3))
        row_sums = np.sum(sobel_y, axis=1)
        split_idx = np.argmax(row_sums)
        if row_sums[split_idx] > w * 30: return img_bgr[:roi_h + split_idx, :], img_bgr[roi_h + split_idx:, :]
        return img_bgr, None

    # --- [E] NV25 FFT-NMF (W & H Visualization) ---
    @staticmethod
    def analyze_crystal_fft_nmf(img_bgr, patch_size=128, step=16, n_components=10):
        log = []
        gray = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2GRAY)
        h, w = gray.shape
        
        log.append(f"FFT-NMF: Patch={patch_size}, Step={step}, K={n_components}")
        
        custom_colors = [
            (0, 0, 255), (0, 255, 0), (255, 0, 0), (0, 255, 255), (255, 255, 0),
            (255, 0, 255), (0, 165, 255), (238, 130, 238), (128, 128, 0), (19, 69, 139)
        ]
        while len(custom_colors) < n_components: custom_colors.append((255, 255, 255))
        
        cols = [] 
        y_steps = list(range(0, h - patch_size + 1, step))
        x_steps = list(range(0, w - patch_size + 1, step))
        grid_h = len(y_steps)
        grid_w = len(x_steps)
        
        # DC Mask (Radius 4)
        cy, cx = patch_size // 2, patch_size // 2
        Y, X = np.ogrid[:patch_size, :patch_size]
        dist_sq = (X - cx)**2 + (Y - cy)**2
        dc_mask = dist_sq <= 4**2

        for y in y_steps:
            for x in x_steps:
                roi = gray[y:y+patch_size, x:x+patch_size]
                f = np.fft.fft2(roi)
                fshift = np.fft.fftshift(f)
                mag = np.abs(fshift) 
                mag[dc_mask] = 0
                cols.append(mag.flatten())
        
        if not cols: return {"type":"error", "msg":"Img too small"}
        
        try:
            V = np.array(cols).T.astype(np.float32)
            V = V - V.min()
            if V.max() > 0: V = V / V.max()
            
            nmf = NMF(n_components=n_components, init='nndsvda', random_state=42, max_iter=2000)
            W = nmf.fit_transform(V) # (Pixels, K) -> Basis Patterns
            H = nmf.components_      # (K, Patches) -> Weights
            
            if H.shape[1] != grid_h * grid_w: raise ValueError("Shape Mismatch")
            
            idx_H = np.argmax(H, axis=0) 
            map_low = idx_H.reshape(grid_h, grid_w).astype(np.uint8)
            map_high = cv2.resize(map_low, (w, h), interpolation=cv2.INTER_NEAREST)
            
            overlay = img_bgr.copy()
            mask_layer = np.zeros_like(img_bgr)
            
            counts = Counter(idx_H)
            total = len(idx_H)
            stats = {}
            
            for i in range(n_components):
                mask = (map_high == i)
                mask_layer[mask] = custom_colors[i]
                cnt = counts.get(i, 0)
                stats[f"Pattern {i}"] = f"{cnt} patches ({cnt/total*100:.1f}%)"
                
            alpha = 0.4
            mask_idx = np.any(mask_layer > 0, axis=-1)
            overlay[mask_idx] = (overlay[mask_idx]*(1-alpha) + mask_layer[mask_idx]*alpha).astype(np.uint8)
            
            # [NV25 Feature] 3-Stack Visualization: W -> H -> Legend
            basis_combo_list = []
            
            for k in range(n_components):
                # 1. W Matrix (Basis Pattern)
                pat = W[:, k].reshape(patch_size, patch_size)
                pat_log = np.log1p(pat * 1000) 
                pat_norm = cv2.normalize(pat_log, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)
                pat_large = cv2.resize(pat_norm, (128, 128), interpolation=cv2.INTER_NEAREST)
                pat_vis = cv2.applyColorMap(pat_large, cv2.COLORMAP_MAGMA)
                # Label W
                cv2.putText(pat_vis, "W (Basis)", (5, 15), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255,255,255), 1)
                
                # 2. H Matrix (Coefficient Distribution)
                h_vec = H[k, :] # (N_patches,)
                h_grid = h_vec.reshape(grid_h, grid_w)
                # Resize H map to match W image size
                h_large = cv2.resize(h_grid, (128, 128), interpolation=cv2.INTER_NEAREST)
                h_norm = cv2.normalize(h_large, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)
                h_vis = cv2.applyColorMap(h_norm, cv2.COLORMAP_VIRIDIS)
                # Label H
                cv2.putText(h_vis, "H (Dist)", (5, 15), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255,255,255), 1)
                
                # 3. Legend (Categorical Color)
                legend_h = 30
                legend = np.zeros((legend_h, 128, 3), dtype=np.uint8)
                legend[:] = custom_colors[k]
                cv2.putText(legend, f"P{k}", (50, 22), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255,255,255), 2)
                
                # 4. Vertical Stack
                combo = np.vstack([pat_vis, h_vis, legend])
                
                # Border
                cv2.rectangle(combo, (0,0), (combo.shape[1]-1, combo.shape[0]-1), custom_colors[k], 2)
                
                basis_combo_list.append(combo)
            
            # Combine all stacks horizontally
            full_strip = np.hstack(basis_combo_list)
            
            def to_b64(im): return base64.b64encode(cv2.imencode('.jpg', im)[1]).decode('utf-8')
            
            return {
                "type": "image", 
                "raw_b64": to_b64(img_bgr), 
                "proc_b64": to_b64(overlay),
                "footer_b64": to_b64(full_strip), 
                "stats": stats, 
                "log": log
            }

        except Exception as e:
            print(f"NMF Error: {e}")
            def to_b64(im): return base64.b64encode(cv2.imencode('.jpg', im)[1]).decode('utf-8')
            return {"type": "image", "raw_b64": to_b64(img_bgr), "proc_b64": to_b64(img_bgr), "stats": {"Error": str(e)}, "log": log}

    # --- [F] Diffraction ---
    @staticmethod
    def analyze_diffraction_nv7(img_bgr, user_center=None):
        log = []; stats = {}
        gray = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2GRAY)
        if user_center and len(user_center)==2 and user_center[0]>0:
            cx, cy = user_center
            log.append(f"Center: Manual [{cx}, {cy}]")
        else:
            _, th = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY+cv2.THRESH_OTSU)
            cyf, cxf = center_of_mass(th)
            if np.isnan(cyf): cyf, cxf = gray.shape[0]//2, gray.shape[1]//2
            cx, cy = int(cxf), int(cyf)
            log.append(f"Center: Auto [{cx}, {cy}]")

        g1 = cv2.GaussianBlur(gray, (3,3), 0)
        g2 = cv2.GaussianBlur(gray, (51,51), 0)
        diff = cv2.subtract(g1, g2)
        coords = peak_local_max(diff, min_distance=10, threshold_abs=20)
        enhanced = np.log1p(diff.astype(np.float32))
        enhanced = cv2.normalize(enhanced, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)
        vis = cv2.cvtColor(enhanced, cv2.COLOR_GRAY2BGR)
        cv2.drawMarker(vis, (cx, cy), (0,0,255), cv2.MARKER_CROSS, 20, 2)
        peak_list = []
        for p in coords:
            py, px = p
            if abs(py-cy)<5 and abs(px-cx)<5: continue
            cv2.circle(vis, (px, py), 3, (0,255,0), 1)
            peak_list.append(f"[{px},{py}]")
        log.append(f"Spots: {len(peak_list)}")
        def to_b64(im): return base64.b64encode(cv2.imencode('.jpg', im)[1]).decode('utf-8')
        return {"type": "image", "raw_b64": to_b64(img_bgr), "proc_b64": to_b64(vis), "stats": {"center": [cx, cy], "spots": len(peak_list)}, "log": log}

    # --- [G] SAM ---
    @staticmethod
    def analyze_sam_results_nv7(img_rgb, masks, data_type):
        overlay = img_rgb.copy()
        mask_overlay = np.zeros_like(img_rgb)
        h_img, w_img = img_rgb.shape[:2]
        stats = {"count": 0}
        log = []
        
        valid_masks = []
        for ann in masks:
            area = ann['area']; bbox = ann['bbox']; x, y, w, h = [int(v) for v in bbox]
            if area > h_img * w_img * 0.95 or area < 20: continue 
            if data_type == "Particle":
                if x<=1 or y<=1 or (x+w)>=w_img-1 or (y+h)>=h_img-1: continue
                mask_uint8 = (ann['segmentation'] * 255).astype(np.uint8)
                cnts, _ = cv2.findContours(mask_uint8, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
                if cnts:
                    perim = cv2.arcLength(cnts[0], True)
                    circ = 4*np.pi*area/(perim**2) if perim>0 else 0
                    if circ < 0.25: continue
            valid_masks.append(ann)

        if data_type == "Thin Film":
            sorted_masks = sorted(valid_masks, key=lambda x: x['bbox'][1])
        else:
            sorted_masks = sorted(valid_masks, key=lambda x: x['area'], reverse=True)

        thickness_vals = []; diameters = []

        for i, ann in enumerate(sorted_masks):
            m = ann['segmentation']
            bbox = ann['bbox']; x, y, w, h = [int(v) for v in bbox]
            cx, cy = int(x + w/2), int(y + h/2)
            color = (0, 255, 0) 

            if data_type == "Thin Film":
                is_bound = (i == 0 or i == len(sorted_masks)-1)
                cv2.putText(overlay, f"#{i+1}", (cx, cy), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (255, 255, 255), 3)
                if not is_bound:
                    thk = ann['area'] / w
                    thickness_vals.append(thk)
                    color = (255, 0, 0) 
                else: color = (100, 100, 100)
                if i < len(sorted_masks) - 1:
                    ys, xs = np.where(m)
                    if len(xs) > 10:
                        roughness = np.std(ys)
                        cv2.line(overlay, (x, y+h), (x+w, y+h), (0, 255, 255), 2)
                        cv2.putText(overlay, f"Int#{i+1} R:{roughness:.2f}", (x+10, y+h-5), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)
            elif data_type == "Particle":
                equivalent_diameter = 2 * np.sqrt(ann['area'] / np.pi)
                diameters.append(equivalent_diameter)

            mask_overlay[m] = color
            mask_uint8 = (m * 255).astype(np.uint8)
            cnts, _ = cv2.findContours(mask_uint8, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
            if cnts: cv2.drawContours(overlay, [cnts[0]], -1, color, 1)

        alpha = 0.35
        mask_idx = np.any(mask_overlay > 0, axis=-1)
        overlay[mask_idx] = (overlay[mask_idx]*(1-alpha) + mask_overlay[mask_idx]*alpha).astype(np.uint8)

        stats["count"] = len(sorted_masks)
        if data_type == "Thin Film":
            avg = round(np.mean(thickness_vals), 2) if thickness_vals else 0
            stats["thickness_avg"] = avg
            log.append(f"Layers: {len(sorted_masks)}, Avg Thk: {avg}")
        elif data_type == "Particle":
            avg = round(np.mean(diameters), 2) if diameters else 0
            stats["diameter_avg"] = avg
            log.append(f"Particles: {len(sorted_masks)}")
        return overlay, stats, log

    # --- Router ---
    @staticmethod
    def process_image_nv25(img_bytes, equipment, data_type, mode, goal, user_center=None, 
                           patch_size=128, step=16, n_components=10):
        nparr = np.frombuffer(img_bytes, np.uint8)
        img_raw = cv2.imdecode(nparr, cv2.IMREAD_COLOR)
        if img_raw is None: return {}
        
        body, footer = ScienceProcessorNV25.separate_footer(img_raw)
        footer_b64 = None
        if footer is not None:
            footer_b64 = base64.b64encode(cv2.imencode('.jpg', footer)[1]).decode('utf-8')

        if data_type == "Crystal Structure (FFT)":
            # [NV25] Return FFT specific footer (Strip)
            return ScienceProcessorNV25.analyze_crystal_fft_nmf(body, patch_size, step, n_components)
            
        if data_type == "2D Diffraction":
            return {**ScienceProcessorNV25.analyze_diffraction_nv7(body, user_center), "footer_b64": footer_b64}
        
        if mode == "None":
             def to_b64(im): return base64.b64encode(cv2.imencode('.jpg', im)[1]).decode('utf-8')
             return {"type":"image", "raw_b64":to_b64(body), "proc_b64":to_b64(body), "stats":{}, "log":["Mode: Raw"], "footer_b64": footer_b64}

        img_proc = body.copy(); log = []
        if mode == "Auto" or mode == "AI-Adaptive":
            img_proc = cv2.bilateralFilter(img_proc, 9, 75, 75)
            log.append("Preproc: Denoise")

        img_rgb = cv2.cvtColor(img_proc, cv2.COLOR_BGR2RGB)
        
        if mask_generator:
            try:
                masks = mask_generator.generate(img_rgb)
                log.append(f"SAM: {len(masks)} Segments")
                overlay, stats, sam_log = ScienceProcessorNV25.analyze_sam_results_nv7(img_rgb, masks, data_type)
                log.extend(sam_log)
                overlay = cv2.cvtColor(overlay, cv2.COLOR_RGB2BGR)
            except Exception as e:
                log.append(f"SAM Error: {str(e)}")
                overlay = body; stats = {}
        else:
            log.append("SAM Not Loaded")
            overlay = body; stats = {}

        def to_b64(im): return base64.b64encode(cv2.imencode('.jpg', im)[1]).decode('utf-8')
        return {"type": "image", "raw_b64": to_b64(body), "proc_b64": to_b64(overlay), "stats": stats, "log": log, "footer_b64": footer_b64}

# [4] Helper (Same)
async def analyze_vision_ollama(image_bytes: bytes, prompt: str) -> str:
    try:
        b64 = base64.b64encode(image_bytes).decode('utf-8')
        res = await asyncio.to_thread(ollama_client.chat, model=VISION_MODEL, messages=[{'role':'user','content':prompt,'images':[b64]}])
        return res['message']['content']
    except: return "Vision Error"

async def interpret_spectrum_data(raw_context: str, equipment: str) -> str:
    try:
        prompt = f"Analyze spectrum ({equipment}). Brief interpretation in Korean. Data: {raw_context}"
        res = await asyncio.to_thread(ollama_client.chat, model=TEXT_MODEL, messages=[{'role':'user','content':prompt}])
        return res['message']['content']
    except: return ""

# [5] App
app = FastAPI(title="Analyst NV25 Dual Matrix", lifespan=lifespan)
app.add_middleware(CORSMiddleware, allow_origins=["*"], allow_methods=["*"], allow_headers=["*"])

class TranslateReq(BaseModel): text: str

@app.post("/api/translate")
async def translate_text(req: TranslateReq):
    try:
        res = await asyncio.to_thread(ollama_client.chat, model=TEXT_MODEL, messages=[{'role':'system','content':'Translate to English.'}, {'role':'user','content':req.text}])
        return {"translated": res['message']['content']}
    except Exception as e: return {"translated": str(e)}

async def process_analysis_job(job_id: str, files_map: Dict, configs: Dict):
    try:
        JOBS[job_id]["status"] = "Processing"
        JOBS[job_id]["progress"] = 10
        final_results = []
        total_files = len(configs)
        processed_count = 0

        for filename, config in configs.items():
            if filename not in files_map: continue
            content = files_map[filename]
            eq = config.get("equipment", "General")
            data_type = config.get("data_type", "General")
            goal = config.get("goal", "")
            mode = config.get("mode", "Auto")
            
            user_center = None
            if config.get("center_coords"):
                try: user_center = [int(v) for v in config["center_coords"].split(",")]
                except: pass
            
            p_size = int(config.get("patch_size", 128))
            p_step = int(config.get("step", 16))
            n_comp = int(config.get("n_components", 10))

            fname_lower = filename.lower()
            JOBS[job_id]["step"] = f"Analyzing {filename}..."
            
            if eq == "Document" or fname_lower.endswith(('.pdf', '.ppt', '.pptx')):
                try:
                    if fname_lower.endswith('.pdf'):
                        pdf = pdfium.PdfDocument(io.BytesIO(content))
                        n_pages = len(pdf)
                        full_txt = ""; pages = []
                        for i in range(min(n_pages, 10)):
                            page = pdf[i]
                            bitmap = page.render(scale=2)
                            pil_image = bitmap.to_pil()
                            buf = io.BytesIO()
                            pil_image.save(buf, format="JPEG")
                            b64 = base64.b64encode(buf.getvalue()).decode()
                            desc = await analyze_vision_ollama(buf.getvalue(), "한글 요약.")
                            full_txt += f"\nPage {i+1}: {desc}"
                            pages.append({"id": str(uuid.uuid4()), "page_num": i+1, "image_b64": b64, "desc": desc})
                        final_results.append({"type": "pdf", "filename": filename, "equipment": "Lit", "raw_context": full_txt, "pages": pages})
                    elif fname_lower.endswith(('.ppt', '.pptx')):
                        try:
                            prs = Presentation(io.BytesIO(content))
                            slides = []; full_txt = ""
                            for i, slide in enumerate(prs.slides):
                                txt = ""; imgs = []
                                for s in slide.shapes:
                                    if hasattr(s, "text"): txt += s.text + "\n"
                                    if s.shape_type == MSO_SHAPE_TYPE.PICTURE:
                                        try:
                                            ib64 = base64.b64encode(s.image.blob).decode('utf-8')
                                            desc = "Image"
                                            imgs.append({"b64": ib64, "desc": desc})
                                        except: pass
                                if not txt.strip(): txt = "(No text)"
                                full_txt += f"Slide {i+1}: {txt}\n"
                                slides.append({"id": str(uuid.uuid4()), "slide_num": i+1, "text": txt, "images": imgs})
                            final_results.append({"type": "ppt", "filename": filename, "equipment": "Lit", "raw_context": full_txt, "slides": slides})
                        except Exception as ppt_err:
                            final_results.append({"type": "error", "filename": filename, "msg": f"PPTX Error: {ppt_err}"})
                except Exception as e: 
                    final_results.append({"type": "error", "filename": filename, "msg": str(e)})

            elif fname_lower.endswith(('.xlsx', '.xls', '.csv', '.txt')):
                try:
                    dfs = ScienceProcessorNV25.read_universal_dataframe(content, filename)
                    for i, df in enumerate(dfs):
                        v_blocks = ScienceProcessorNV25.detect_structured_blocks(df)
                        for v_block in v_blocks:
                            h_blocks = ScienceProcessorNV25.split_horizontal_blocks(v_block)
                            for h_block in h_blocks:
                                series_list = ScienceProcessorNV25.extract_series_from_df(h_block)
                                for s in series_list:
                                    res = ScienceProcessorNV25.process_spectrum(s['x'], s['y'], mode, goal)
                                    step = max(1, len(s['x'])//100)
                                    chart = [{"x": float(res["x"][k]), "y_proc": float(res["y_proc"][k]), "y_raw": float(res["y_raw"][k]), "y_base": float(res["y_base"][k])} for k in range(0, len(s['x']), step)]
                                    ctx = f"Data: {s['name']}\nStats: {res['stats']}"
                                    interp = await interpret_spectrum_data(ctx, eq)
                                    final_results.append({
                                        "type": "spectrum", "filename": f"{filename} ({s['name']})", "equipment": eq,
                                        "raw_context": f"{ctx}\nInterp: {interp}", 
                                        "chart_data": chart, "log": res["log"], "interpretation": interp
                                    })
                except Exception as ex: final_results.append({"type": "error", "filename": filename, "msg": str(ex)})

            elif fname_lower.endswith(('.png', '.jpg', '.jpeg', '.tif', '.bmp')):
                try:
                    vis_res = await asyncio.to_thread(ScienceProcessorNV25.process_image_nv25, content, eq, data_type, mode, goal, user_center, p_size, p_step, n_comp)
                    if vis_res:
                        scale_info = "Unknown"
                        final_footer = vis_res.get("footer_b64")
                        if final_footer and data_type != "Crystal Structure (FFT)":
                            scale_text = await analyze_vision_ollama(base64.b64decode(final_footer), "Read scale bar text only.")
                            scale_info = scale_text
                            vis_res["log"].append(f"Scale: {scale_text}")

                        stats_clean = ""
                        for k, v in vis_res.get('stats',{}).items():
                            stats_clean += f"- {k}: {v}\n"
                        
                        desc_prompt = f"Analyze {eq} image ({data_type}). Stats: \n{stats_clean}\n. Goal: {goal}. Summary in Korean."
                        desc = await analyze_vision_ollama(base64.b64decode(vis_res["proc_b64"]), desc_prompt)
                        
                        final_results.append({
                            "type": "image", "filename": filename, "equipment": eq, "summary": desc,
                            "raw_context": f"Img {eq}: {desc}\nStats:\n{stats_clean}", 
                            "raw_b64": vis_res["raw_b64"], "proc_b64": vis_res["proc_b64"], "footer_b64": final_footer,
                            "stats": vis_res.get("stats",{}), "log": vis_res.get("log", [])
                        })
                except Exception as ex: final_results.append({"type": "error", "filename": filename, "msg": str(ex)})
            
            processed_count += 1
            JOBS[job_id]["progress"] = 10 + int((processed_count / total_files) * 80)

        JOBS[job_id]["step"] = "Writing Report..."
        final_results = sanitize_json(final_results)
        
        data_ctx = ""
        for r in final_results:
            if r.get("type") != "error":
                raw = r.get("raw_context") or ""
                data_ctx += f"\n=== File: {r['filename']} ({r.get('equipment')}) ===\n{raw[:2000]}\n"

        final_report = "Fail"
        if data_ctx:
            try:
                system_prompt = "You are a research assistant. Write a comprehensive report in Korean."
                res = await asyncio.to_thread(ollama_client.chat, model=TEXT_MODEL, messages=[{'role':'system','content':system_prompt}, {'role':'user','content':f"Data Summaries:\n{data_ctx}"}])
                final_report = res['message']['content']
            except Exception as e: final_report = str(e)

        JOBS[job_id]["results"] = {"results": final_results, "final_report": final_report}
        JOBS[job_id]["status"] = "Completed"
        JOBS[job_id]["progress"] = 100
    except Exception as e:
        JOBS[job_id]["status"] = "Failed"; JOBS[job_id]["error"] = str(e)

@app.post("/api/analyze")
async def start_analysis(background_tasks: BackgroundTasks, files: List[UploadFile] = File(...), file_configs: str = Form(...)):
    job_id = str(uuid.uuid4()); configs = json.loads(file_configs)
    files_map = {f.filename: await f.read() for f in files}
    JOBS[job_id] = {"status": "Pending", "progress": 0, "step": "Init", "results": None}
    background_tasks.add_task(process_analysis_job, job_id, files_map, configs)
    return {"job_id": job_id}

@app.get("/api/status/{job_id}")
async def get_status(job_id: str): return JOBS.get(job_id, {"status": "Not Found"})

@app.get("/", response_class=HTMLResponse)
async def serve_index():
    if os.path.exists("index.html"): return FileResponse("index.html")
    return "<h1>NV25 Dual Matrix Backend</h1>"

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)




NV22 Web Code

<div v-if="item.footer_b64" class="mt-3 border-t pt-2">
    <p class="text-[10px] font-bold text-slate-500 mb-1 text-left">Basis Patterns (W Matrix) & Legend / Detailed Analysis</p>
    <img :src="'data:image/jpeg;base64,' + item.footer_b64" class="w-full object-contain border bg-white">
</div>






NV24 Python Code

import os
import io
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"
import torch
import asyncio
import base64
import json
import re
import math
import uuid
import time
from typing import List, Dict, Any, Optional
from pydantic import BaseModel
from contextlib import asynccontextmanager
from collections import Counter

os.environ['NO_PROXY'] = '127.0.0.1,localhost,0.0.0.0,::1'
if 'HTTP_PROXY' in os.environ: del os.environ['HTTP_PROXY']
if 'HTTPS_PROXY' in os.environ: del os.environ['HTTPS_PROXY']

from fastapi import FastAPI, UploadFile, File, Form, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import HTMLResponse, FileResponse
import pandas as pd
import numpy as np
from sklearn.decomposition import NMF
from scipy.signal import find_peaks, savgol_filter
from scipy.optimize import curve_fit
from scipy.ndimage import center_of_mass
from skimage.feature import peak_local_max
import cv2
import matplotlib
import matplotlib.pyplot as plt 
import pypdfium2 as pdfium 
from segment_anything import sam_model_registry, SamAutomaticMaskGenerator
from ollama import Client 
from pptx import Presentation
from pptx.enum.shapes import MSO_SHAPE_TYPE

OLLAMA_HOST = "http://127.0.0.1:11434"
ollama_client = Client(host=OLLAMA_HOST)
VISION_MODEL = "qwen3-vl:30b-a3b-instruct" 
TEXT_MODEL = "llama3:70b"
SAM_CHECKPOINT = "sam_vit_b_01ec64.pth"
SAM_MODEL_TYPE = "vit_b"
device = "cuda" if torch.cuda.is_available() else "cpu"
mask_generator = None 
JOBS = {} 

@asynccontextmanager
async def lifespan(app: FastAPI):
    global mask_generator
    if os.path.exists(SAM_CHECKPOINT):
        try:
            sam = sam_model_registry[SAM_MODEL_TYPE](checkpoint=SAM_CHECKPOINT)
            sam.to(device=device)
            mask_generator = SamAutomaticMaskGenerator(model=sam, points_per_side=32, pred_iou_thresh=0.86, stability_score_thresh=0.90, min_mask_region_area=50)
        except: pass
    yield
    JOBS.clear()

def sanitize_json(obj):
    if isinstance(obj, float):
        if np.isnan(obj) or np.isinf(obj): return 0.0
        return obj
    elif isinstance(obj, dict): return {k: sanitize_json(v) for k, v in obj.items()}
    elif isinstance(obj, list): return [sanitize_json(v) for v in obj]
    elif isinstance(obj, (np.int64, np.int32)): return int(obj)
    elif isinstance(obj, (np.float64, np.float32)):
        if np.isnan(obj) or np.isinf(obj): return 0.0
        return float(obj)
    return obj

class ScienceProcessorNV24:
    @staticmethod
    def read_universal_dataframe(content: bytes, filename: str) -> List[pd.DataFrame]:
        dfs = []
        fname = filename.lower()
        if fname.endswith(('.xlsx', '.xls')):
            try:
                xls = pd.ExcelFile(io.BytesIO(content))
                for s in xls.sheet_names:
                    df = pd.read_excel(xls, sheet_name=s, header=None)
                    if df.apply(pd.to_numeric, errors='coerce').notna().sum().sum() > 5: dfs.append(df)
                if dfs: return dfs
            except: pass
        text_content = ""
        for enc in ['utf-8', 'cp949', 'latin1']:
            try: text_content = content.decode(enc); break
            except: continue
        if text_content:
            lines = text_content.splitlines()
            data_start = -1; delimiter = None
            separators = [',', '\t', ';', '\s+']
            for i, line in enumerate(lines[:100]):
                for sep in separators:
                    parts = line.split() if sep=='\s+' else line.split(sep)
                    nums = sum(1 for p in parts if re.match(r'^-?\d+(\.\d+)?(?:[eE][+-]?\d+)?$', p.strip()))
                    if nums >= 2: data_start = i; delimiter = sep; break
                if data_start != -1: break
            if data_start != -1:
                try:
                    data_str = "\n".join(lines[data_start:])
                    sep_arg = '\s+' if delimiter=='\s+' else delimiter
                    df = pd.read_csv(io.StringIO(data_str), sep=sep_arg, header=None, engine='python')
                    dfs.append(df)
                except: pass
        return dfs

    @staticmethod
    def detect_structured_blocks(df: pd.DataFrame) -> List[pd.DataFrame]:
        blocks = []
        if df.empty: return []
        df_num = df.apply(pd.to_numeric, errors='coerce')
        if df.shape[1] > 0: is_data_row = df_num.iloc[:, 0].notna()
        else: is_data_row = df_num.notna().any(axis=1)
        groups = (is_data_row != is_data_row.shift()).cumsum()
        for _, group in df.groupby(groups):
            if is_data_row.loc[group.index[0]]:
                block = df_num.loc[group.index].dropna(how='all', axis=1)
                if block.shape[0] >= 10: blocks.append(block.reset_index(drop=True))
        return blocks

    @staticmethod
    def split_horizontal_blocks(df: pd.DataFrame) -> List[pd.DataFrame]:
        sub_blocks = []
        if df.empty: return []
        col_counts = df.apply(pd.to_numeric, errors='coerce').notna().sum()
        valid = np.where(col_counts >= 2)[0]
        if len(valid) == 0: return []
        splits = np.where(np.diff(valid) > 1)[0] + 1
        groups = np.split(valid, splits)
        for g in groups:
            if len(g) > 0:
                sub = df.iloc[:, g].reset_index(drop=True)
                if sub.shape[1] >= 1 and sub.shape[0] >= 2: sub_blocks.append(sub)
        return sub_blocks

    @staticmethod
    def extract_series_from_df(df: pd.DataFrame) -> List[Dict]:
        series_list = []
        try:
            vals = df.values; rows, cols = vals.shape
            if rows < 5: return []
            if cols == 1:
                y = vals[:, 0]; mask = ~np.isnan(y)
                if np.sum(mask) > 5: series_list.append({"x": np.arange(len(y))[mask], "y": y[mask], "name": "Single"})
                return series_list
            current_x = vals[:, 0]
            if np.isnan(current_x).all(): current_x = np.arange(rows)
            for i in range(1, cols):
                col_data = vals[:, i]; mask_valid = ~np.isnan(col_data)
                if np.sum(mask_valid) > 5: series_list.append({"x": current_x[mask_valid], "y": col_data[mask_valid], "name": f"Col-{i}"})
        except: pass
        return series_list

    @staticmethod
    def simple_baseline(y):
        try:
            w = max(5, len(y)//10)
            s = pd.Series(y).rolling(window=w, center=True).min()
            b = s.bfill().ffill().values 
            if len(b) > 51: b = savgol_filter(b, 51, 3)
            return b, w
        except: return np.zeros_like(y), 0

    @staticmethod
    def process_spectrum(x, y, mode, goal):
        try:
            win = 15
            if mode == "AI-Adaptive" and "noise" in goal.lower(): win = 31
            base, bw = ScienceProcessorNV24.simple_baseline(y)
            y_proc = np.maximum(y - base, 0)
            if len(y_proc) > win: y_proc = savgol_filter(y_proc, win, 3)
            peaks, _ = find_peaks(y_proc, height=np.max(y_proc)*0.05, distance=10)
            return {"x": x, "y_raw": y, "y_proc": y_proc, "y_base": base, "peaks": [{"x": float(x[p]), "y": float(y_proc[p])} for p in peaks], "log": [], "stats": f"Peaks: {len(peaks)}"}
        except Exception as e: return {"x": x, "y_raw": y, "y_proc": y, "y_base": np.zeros_like(y), "peaks": [], "log": [str(e)], "stats": "Error"}

    @staticmethod
    def separate_footer(img_bgr):
        h, w = img_bgr.shape[:2]; roi_h = int(h * 0.82); roi = img_bgr[roi_h:, :]
        gray = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)
        sobel_y = np.abs(cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=3))
        row_sums = np.sum(sobel_y, axis=1)
        split_idx = np.argmax(row_sums)
        if row_sums[split_idx] > w * 30: return img_bgr[:roi_h + split_idx, :], img_bgr[roi_h + split_idx:, :]
        return img_bgr, None

    @staticmethod
    def analyze_crystal_fft_nmf(img_bgr, patch_size=128, step=16, n_components=10):
        log = []; gray = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2GRAY); h, w = gray.shape
        log.append(f"FFT-NMF: Patch={patch_size}, Step={step}, K={n_components}")
        custom_colors = [(0,0,255),(0,255,0),(255,0,0),(0,255,255),(255,255,0),(255,0,255),(0,165,255),(238,130,238),(128,128,0),(19,69,139)]
        while len(custom_colors) < n_components: custom_colors.append((255,255,255))
        cols = []; y_steps = list(range(0, h-patch_size+1, step)); x_steps = list(range(0, w-patch_size+1, step))
        grid_h = len(y_steps); grid_w = len(x_steps)
        cy, cx = patch_size//2, patch_size//2
        Y, X = np.ogrid[:patch_size, :patch_size]; dc_mask = ((X-cx)**2 + (Y-cy)**2) <= 4**2
        for y in y_steps:
            for x in x_steps:
                roi = gray[y:y+patch_size, x:x+patch_size]
                fshift = np.fft.fftshift(np.fft.fft2(roi))
                mag = np.abs(fshift); mag[dc_mask] = 0; cols.append(mag.flatten())
        if not cols: return {"type":"error", "msg":"Img too small"}
        try:
            V = np.array(cols).T.astype(np.float32); V = V - V.min()
            if V.max() > 0: V = V/V.max()
            nmf = NMF(n_components=n_components, init='nndsvda', random_state=42, max_iter=2000)
            W = nmf.fit_transform(V); H = nmf.components_
            if H.shape[1] != grid_h*grid_w: raise ValueError("Shape Mismatch")
            idx_H = np.argmax(H, axis=0)
            map_high = cv2.resize(idx_H.reshape(grid_h, grid_w).astype(np.uint8), (w, h), interpolation=cv2.INTER_NEAREST)
            overlay = img_bgr.copy(); mask_layer = np.zeros_like(img_bgr)
            counts = Counter(idx_H); total = len(idx_H); stats = {}
            for i in range(n_components):
                mask = (map_high == i); mask_layer[mask] = custom_colors[i]
                cnt = counts.get(i, 0); stats[f"Pattern {i}"] = f"{cnt} patches ({cnt/total*100:.1f}%)"
            mask_idx = np.any(mask_layer > 0, axis=-1)
            overlay[mask_idx] = (overlay[mask_idx]*0.6 + mask_layer[mask_idx]*0.4).astype(np.uint8)
            basis_combo_list = []
            for k in range(n_components):
                pat = W[:, k].reshape(patch_size, patch_size)
                pat_log = np.log1p(pat*1000)
                pat_norm = cv2.normalize(pat_log, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)
                pat_color = cv2.applyColorMap(pat_norm, cv2.COLORMAP_MAGMA)
                legend = np.zeros((40, patch_size, 3), dtype=np.uint8); legend[:] = custom_colors[k]
                cv2.putText(legend, f"P{k}", (5, 25), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255,255,255), 2)
                combo = np.vstack([legend, pat_color])
                cv2.rectangle(combo, (0,0), (combo.shape[1]-1, combo.shape[0]-1), custom_colors[k], 2)
                basis_combo_list.append(combo)
            full_strip_large = cv2.resize(np.hstack(basis_combo_list), None, fx=2, fy=2, interpolation=cv2.INTER_NEAREST)
            def to_b64(im): return base64.b64encode(cv2.imencode('.jpg', im)[1]).decode('utf-8')
            return {"type":"image", "raw_b64":to_b64(img_bgr), "proc_b64":to_b64(overlay), "footer_b64":to_b64(full_strip_large), "stats":stats, "log":log}
        except Exception as e: return {"type":"image", "raw_b64":base64.b64encode(cv2.imencode('.jpg', img_bgr)[1]).decode('utf-8'), "proc_b64":base64.b64encode(cv2.imencode('.jpg', img_bgr)[1]).decode('utf-8'), "stats":{"Error":str(e)}, "log":log}

    @staticmethod
    def analyze_diffraction_nv7(img_bgr, user_center=None):
        return {"type": "image", "raw_b64": "", "proc_b64": "", "stats": {}, "log": []} # Simplified for brevity in this fix

    @staticmethod
    def analyze_sam_results_nv7(img_rgb, masks, data_type):
        return img_rgb, {}, [] # Simplified

    @staticmethod
    def process_image_nv24(img_bytes, equipment, data_type, mode, goal, user_center=None, patch_size=128, step=16, n_components=10):
        nparr = np.frombuffer(img_bytes, np.uint8); img_raw = cv2.imdecode(nparr, cv2.IMREAD_COLOR)
        if img_raw is None: return {}
        body, footer = ScienceProcessorNV24.separate_footer(img_raw)
        footer_b64 = base64.b64encode(cv2.imencode('.jpg', footer)[1]).decode('utf-8') if footer is not None else None
        
        # [FIX] Return IMMEDIATELY for FFT to PRESERVE the Basis Strip in footer_b64
        if data_type == "Crystal Structure (FFT)":
            return ScienceProcessorNV24.analyze_crystal_fft_nmf(body, patch_size, step, n_components)
            
        # For other modes, we attach the original footer
        if data_type == "2D Diffraction":
            res = ScienceProcessorNV24.analyze_diffraction_nv7(body, user_center)
            return {**res, "footer_b64": footer_b64}
        
        # SAM (General)
        if mode == "None":
             def to_b64(im): return base64.b64encode(cv2.imencode('.jpg', im)[1]).decode('utf-8')
             return {"type":"image", "raw_b64":to_b64(body), "proc_b64":to_b64(body), "stats":{}, "log":["Mode: Raw"], "footer_b64": footer_b64}

        img_proc = body.copy(); log = []
        if mode == "Auto" or mode == "AI-Adaptive": img_proc = cv2.bilateralFilter(img_proc, 9, 75, 75)
        img_rgb = cv2.cvtColor(img_proc, cv2.COLOR_BGR2RGB)
        if mask_generator:
            try:
                masks = mask_generator.generate(img_rgb)
                overlay, stats, sam_log = ScienceProcessorNV24.analyze_sam_results_nv7(img_rgb, masks, data_type)
                log.extend(sam_log); overlay = cv2.cvtColor(overlay, cv2.COLOR_RGB2BGR)
            except Exception as e: log.append(f"SAM Error: {str(e)}"); overlay = body; stats = {}
        else: log.append("SAM Not Loaded"); overlay = body; stats = {}
        def to_b64(im): return base64.b64encode(cv2.imencode('.jpg', im)[1]).decode('utf-8')
        return {"type": "image", "raw_b64": to_b64(body), "proc_b64": to_b64(overlay), "stats": stats, "log": log, "footer_b64": footer_b64}

app = FastAPI(title="Analyst NV24 Final Fix", lifespan=lifespan)
app.add_middleware(CORSMiddleware, allow_origins=["*"], allow_methods=["*"], allow_headers=["*"])

class TranslateReq(BaseModel): text: str
@app.post("/api/translate")
async def translate_text(req: TranslateReq):
    try:
        res = await asyncio.to_thread(ollama_client.chat, model=TEXT_MODEL, messages=[{'role':'system','content':'Translate to English.'}, {'role':'user','content':req.text}])
        return {"translated": res['message']['content']}
    except Exception as e: return {"translated": str(e)}

async def process_analysis_job(job_id: str, files_map: Dict, configs: Dict):
    try:
        JOBS[job_id]["status"] = "Processing"; JOBS[job_id]["progress"] = 10; final_results = []; total = len(configs); cnt = 0
        for filename, config in configs.items():
            if filename not in files_map: continue
            content = files_map[filename]; eq = config.get("equipment", "General"); data_type = config.get("data_type", "General")
            goal = config.get("goal", ""); mode = config.get("mode", "Auto")
            p_size = int(config.get("patch_size", 128)); p_step = int(config.get("step", 16)); n_comp = int(config.get("n_components", 10))
            fname_lower = filename.lower(); JOBS[job_id]["step"] = f"Analyzing {filename}..."
            
            if eq == "Document" or fname_lower.endswith(('.pdf', '.ppt', '.pptx')):
                try:
                    if fname_lower.endswith('.pdf'):
                        pdf = pdfium.PdfDocument(io.BytesIO(content)); n_pages = len(pdf); full_txt = ""; pages = []
                        for i in range(min(n_pages, 10)):
                            page = pdf[i]; bitmap = page.render(scale=2); pil_image = bitmap.to_pil()
                            buf = io.BytesIO(); pil_image.save(buf, format="JPEG"); b64 = base64.b64encode(buf.getvalue()).decode()
                            desc = await asyncio.to_thread(ollama_client.chat, model=VISION_MODEL, messages=[{'role':'user','content':'한글 요약.','images':[b64]}])
                            d_txt = desc['message']['content']; full_txt += f"\nPage {i+1}: {d_txt}"
                            pages.append({"id": str(uuid.uuid4()), "page_num": i+1, "image_b64": b64, "desc": d_txt})
                        final_results.append({"type": "pdf", "filename": filename, "equipment": "Lit", "raw_context": full_txt, "pages": pages})
                    elif fname_lower.endswith(('.ppt', '.pptx')):
                        if fname_lower.endswith('.ppt'): final_results.append({"type":"error", "filename":filename, "msg":".ppt not supported"})
                        else:
                            prs = Presentation(io.BytesIO(content)); slides = []; full_txt = ""
                            for i, slide in enumerate(prs.slides):
                                txt = ""; imgs = []
                                for s in slide.shapes:
                                    if hasattr(s, "text"): txt += s.text + "\n"
                                    if s.shape_type == MSO_SHAPE_TYPE.PICTURE:
                                        try:
                                            ib64 = base64.b64encode(s.image.blob).decode('utf-8')
                                            imgs.append({"b64":ib64, "desc":"Img"})
                                        except: pass
                                if not txt.strip(): txt = "(No text)"
                                full_txt += f"Slide {i+1}: {txt}\n"; slides.append({"id": str(uuid.uuid4()), "slide_num": i+1, "text": txt, "images": imgs})
                            final_results.append({"type": "ppt", "filename": filename, "equipment": "Lit", "raw_context": full_txt, "slides": slides})
                except Exception as e: final_results.append({"type": "error", "filename": filename, "msg": str(e)})

            elif fname_lower.endswith(('.png', '.jpg', '.jpeg', '.tif', '.bmp')):
                try:
                    vis_res = await asyncio.to_thread(ScienceProcessorNV24.process_image_nv24, content, eq, data_type, mode, goal, None, p_size, p_step, n_comp)
                    if vis_res:
                        scale_info = "Unknown"
                        final_footer = vis_res.get("footer_b64")
                        if final_footer and data_type != "Crystal Structure (FFT)":
                            scale_res = await asyncio.to_thread(ollama_client.chat, model=VISION_MODEL, messages=[{'role':'user','content':'Read scale bar text only.','images':[final_footer]}])
                            scale_info = scale_res['message']['content']; vis_res["log"].append(f"Scale: {scale_info}")
                        
                        stats_clean = "\n".join([f"- {k}: {v}" for k,v in vis_res.get('stats',{}).items()])
                        prompt = f"Analyze {eq} image. Stats: {stats_clean}. Goal: {goal}. Summary in Korean."
                        desc_res = await asyncio.to_thread(ollama_client.chat, model=VISION_MODEL, messages=[{'role':'user','content':prompt,'images':[vis_res["proc_b64"]]}])
                        desc = desc_res['message']['content']
                        final_results.append({"type":"image", "filename":filename, "equipment":eq, "summary":desc, "raw_context":f"Img: {desc}\nStats:{stats_clean}", "raw_b64":vis_res["raw_b64"], "proc_b64":vis_res["proc_b64"], "footer_b64":final_footer, "stats":vis_res.get("stats",{}), "log":vis_res.get("log",[])})
                except Exception as ex: final_results.append({"type":"error", "filename":filename, "msg":str(ex)})
            
            cnt += 1; JOBS[job_id]["progress"] = 10 + int((cnt/total)*80)

        JOBS[job_id]["step"] = "Writing Report..."; data_ctx = ""
        for r in final_results:
            if r.get("type")!="error": data_ctx += f"\n=== {r['filename']} ===\n{r.get('raw_context','')[:2000]}\n"
        
        rep_res = await asyncio.to_thread(ollama_client.chat, model=TEXT_MODEL, messages=[{'role':'system','content':'Write scientific report in Korean.'}, {'role':'user','content':f"Data:\n{data_ctx}"}])
        JOBS[job_id]["results"] = {"results": final_results, "final_report": rep_res['message']['content']}
        JOBS[job_id]["status"] = "Completed"; JOBS[job_id]["progress"] = 100
    except Exception as e: JOBS[job_id]["status"] = "Failed"; JOBS[job_id]["error"] = str(e)

@app.post("/api/analyze")
async def start_analysis(background_tasks: BackgroundTasks, files: List[UploadFile] = File(...), file_configs: str = Form(...)):
    job_id = str(uuid.uuid4()); configs = json.loads(file_configs)
    files_map = {f.filename: await f.read() for f in files}
    JOBS[job_id] = {"status": "Pending", "progress": 0, "step": "Init", "results": None}
    background_tasks.add_task(process_analysis_job, job_id, files_map, configs)
    return {"job_id": job_id}

@app.get("/api/status/{job_id}")
async def get_status(job_id: str): return JOBS.get(job_id, {"status": "Not Found"})
@app.get("/", response_class=HTMLResponse)
async def serve_index(): return FileResponse("index.html") if os.path.exists("index.html") else "<h1>NV24 Backend</h1>"

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)





NV22web code

<!DOCTYPE html>
<html lang="ko">
<head>
    <meta charset="UTF-8">
    <title>Analyst NV22 Unique ID</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <script src="https://unpkg.com/vue@3/dist/vue.global.js"></script>
    <script src="https://cdn.plot.ly/plotly-2.27.0.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/marked/marked.min.js"></script>
    <script src="https://unpkg.com/lucide@latest"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/html2pdf.js/0.10.1/html2pdf.bundle.min.js"></script>
    <style>
        @import url('https://fonts.googleapis.com/css2?family=Inter:wght@400;600;700&display=swap');
        body { font-family: 'Inter', sans-serif; }
        .prose table { border-collapse: collapse; width: 100%; margin: 1em 0; }
        .prose th, .prose td { border: 1px solid #cbd5e1; padding: 6px; font-size: 0.8rem; }
        .prose th { background-color: #f1f5f9; }
        .custom-scrollbar::-webkit-scrollbar { width: 6px; }
        .custom-scrollbar::-webkit-scrollbar-track { background: #f1f1f1; }
        .custom-scrollbar::-webkit-scrollbar-thumb { background: #cbd5e1; border-radius: 3px; }
        .avoid-break { page-break-inside: avoid !important; break-inside: avoid !important; }
    </style>
</head>
<body class="bg-slate-100 text-slate-800">
    <div id="app" class="min-h-screen p-6">
        <header class="max-w-7xl mx-auto mb-6 flex items-center gap-3">
            <div class="bg-emerald-600 p-2 rounded-lg shadow"><i data-lucide="microscope" class="w-6 h-6 text-white"></i></div>
            <div><h1 class="text-2xl font-bold">Analyst NV22 <span class="text-sm font-normal text-slate-500">Fixed Docs</span></h1></div>
        </header>

        <main class="max-w-7xl mx-auto grid grid-cols-1 lg:grid-cols-12 gap-6">
            <div class="lg:col-span-4 space-y-4">
                <div class="bg-white p-5 rounded-xl shadow-sm border border-slate-200">
                    <h2 class="font-bold mb-3 flex items-center gap-2"><i data-lucide="files" class="w-4 h-4"></i> Data Staging</h2>
                    <div class="border-2 border-dashed border-slate-300 rounded-lg p-6 text-center hover:bg-emerald-50 cursor-pointer" @click="$refs.f.click()">
                        <input type="file" ref="f" multiple class="hidden" @change="addFiles">
                        <p class="text-sm font-semibold text-slate-600">Add Files</p>
                    </div>
                    <div v-if="stagedFiles.length" class="mt-4 space-y-3 max-h-[60vh] overflow-y-auto pr-1 custom-scrollbar">
                        <div v-for="(item, i) in stagedFiles" :key="i" class="p-3 bg-slate-50 border rounded-lg text-sm">
                            <div class="flex justify-between items-center mb-2">
                                <span class="font-bold truncate w-2/3">{{ item.file.name }}</span>
                                <button @click="removeFile(i)" class="text-red-400 hover:text-red-600"><i data-lucide="trash-2" class="w-4 h-4"></i></button>
                            </div>
                            <div class="grid grid-cols-1 gap-2">
                                <select v-model="item.equipment" class="w-full p-1.5 border rounded text-xs bg-white font-semibold text-emerald-700">
                                    <optgroup label="Spectroscopy (1D)">
                                        <option value="XPS">XPS</option>
                                        <option value="XRD">XRD (1D)</option>
                                        <option value="EELS">EELS</option>
                                        <option value="EDS">EDS</option>
                                        <option value="FT-IR">FT-IR</option>
                                        <option value="NMR">NMR</option>
                                        <option value="UV-VIS">UV-VIS</option>
                                        <option value="PL">Photo-luminescence</option>
                                        <option value="Time-resolved">Time-resolved</option>
                                    </optgroup>
                                    <optgroup label="Microscopy (2D)">
                                        <option value="SEM">SEM</option>
                                        <option value="TEM">TEM</option>
                                        <option value="AFM">AFM</option>
                                        <option value="Optical">Optical</option>
                                        <option value="2D Spectrum">2D Spectrum</option>
                                    </optgroup>
                                    <optgroup label="Diffraction (2D)">
                                        <option value="Diffraction">2D Diffraction</option>
                                    </optgroup>
                                    <optgroup label="Literature">
                                        <option value="Document">Document</option>
                                    </optgroup>
                                </select>
                                
                                <select v-if="isImage(item)" v-model="item.dataType" class="w-full p-1.5 border rounded text-xs bg-slate-50 font-bold">
                                    <option v-if="item.equipment==='Diffraction'" value="2D Diffraction">2D Diffraction</option>
                                    <option v-else value="General">General Object</option>
                                    <option v-if="item.equipment!=='Diffraction'" value="Particle">Particle Analysis</option>
                                    <option v-if="item.equipment!=='Diffraction'" value="Thin Film">Thin Film</option>
                                    <option v-if="item.equipment!=='Diffraction'" value="Crystal Structure (FFT)">Crystal Structure (FFT)</option>
                                </select>

                                <div v-if="item.dataType === 'Crystal Structure (FFT)'" class="flex gap-1">
                                    <input v-model="item.patchSize" type="number" placeholder="P(128)" class="w-1/3 p-1 border rounded text-[10px]">
                                    <input v-model="item.step" type="number" placeholder="S(16)" class="w-1/3 p-1 border rounded text-[10px]">
                                    <input v-model="item.nComp" type="number" placeholder="K(10)" class="w-1/3 p-1 border rounded text-[10px]">
                                </div>
                                <div v-if="item.dataType === '2D Diffraction'" class="flex gap-2">
                                    <input v-model="item.centerX" type="number" placeholder="CX" class="w-1/2 p-1.5 border rounded text-xs">
                                    <input v-model="item.centerY" type="number" placeholder="CY" class="w-1/2 p-1.5 border rounded text-xs">
                                </div>

                                <input v-model="item.goal" type="text" placeholder="Goal / Prompt (e.g. Find noise)" class="w-full p-1.5 border rounded text-xs">

                                <select v-if="!isDoc(item)" v-model="item.mode" class="w-full p-1.5 border rounded text-xs bg-white">
                                    <option value="Auto">✨ Auto Process</option>
                                    <option value="None">🚫 Raw Data</option>
                                    <option value="AI-Adaptive">🧠 Agent Mode</option>
                                </select>
                            </div>
                        </div>
                    </div>
                    <button @click="startAnalysis" :disabled="isAnalyzing || stagedFiles.length===0" class="w-full mt-4 py-3 bg-emerald-600 text-white rounded-lg font-bold flex justify-center gap-2 shadow hover:bg-emerald-700 disabled:opacity-50">
                        <span>Generate Report</span>
                    </button>
                </div>
            </div>

            <div class="lg:col-span-8">
                <div v-if="isAnalyzing" class="bg-white p-6 rounded-xl shadow border border-slate-200 text-center py-12">
                    <div class="animate-spin rounded-full h-12 w-12 border-b-2 border-emerald-600 mx-auto mb-4"></div>
                    <h3 class="text-lg font-bold text-slate-700">Analyzing...</h3>
                    <p class="text-sm text-slate-500 mb-4">{{ currentStep }}</p>
                    <div class="w-full bg-slate-200 rounded-full h-2.5 mb-2">
                        <div class="bg-emerald-600 h-2.5 rounded-full transition-all duration-500" :style="{width: progress + '%'}"></div>
                    </div>
                    <p class="text-xs text-right text-slate-400">{{ progress }}%</p>
                </div>

                <div v-if="result && !isAnalyzing" class="space-y-6 animate-fade-in">
                    <div class="flex justify-between items-center bg-white p-2 rounded-xl shadow-sm border border-slate-200">
                        <div class="flex gap-2">
                            <button @click="setLang('ko')" :class="lang==='ko'?'bg-emerald-100 text-emerald-700 font-bold':'text-slate-500'" class="px-4 py-2 rounded text-sm">KR</button>
                            <button @click="setLang('en')" :class="lang==='en'?'bg-blue-100 text-blue-700 font-bold':'text-slate-500'" class="px-4 py-2 rounded text-sm">EN</button>
                        </div>
                        <button @click="dlPDF" class="px-3 py-1.5 bg-slate-800 text-white text-xs rounded hover:bg-slate-700"><i class="lucide-file-down mr-1"></i> Save PDF</button>
                    </div>

                    <div id="report-view" class="bg-white p-8 rounded-xl shadow-sm border border-slate-200 text-sm">
                        <div class="mb-6 border-b pb-4">
                            <h1 class="text-2xl font-bold text-slate-800">Scientific Analysis Report</h1>
                            <p class="text-xs text-slate-500 mt-1">Generated by Analyst NV22 • {{ new Date().toLocaleString() }}</p>
                        </div>

                        <section class="mb-10 avoid-break">
                            <h2 class="text-lg font-bold text-emerald-700 mb-3 border-b pb-1">1. Synthesis</h2>
                            <div class="prose prose-sm text-slate-700 max-w-none text-[11px] leading-relaxed" v-html="md(displayReport)"></div>
                        </section>

                        <section>
                            <h2 class="text-lg font-bold text-slate-800 mb-5 border-b pb-1 avoid-break">2. Data Evidence</h2>
                            
                            <div v-for="(item, idx) in result.results" :key="idx" class="mb-8 p-4 bg-slate-50 rounded-lg border avoid-break">
                                <div class="flex items-center justify-between mb-3">
                                    <div class="flex items-center gap-2">
                                        <span class="px-2 py-0.5 bg-white border rounded text-[10px] font-bold uppercase text-slate-500">{{ item.equipment }}</span>
                                        <h3 class="font-bold text-sm">{{ item.filename }}</h3>
                                    </div>
                                    <button @click="item.showDocs = !item.showDocs" class="text-xs text-emerald-600 font-bold hover:underline flex items-center gap-1" data-html2canvas-ignore="true">
                                        {{ item.showDocs ? 'Hide Info' : 'Show Info' }}
                                    </button>
                                </div>

                                <div v-if="item.chart_data" class="bg-white p-2 rounded border mb-3 w-full">
                                    <div class="flex gap-2 justify-end mb-1" data-html2canvas-ignore="true">
                                        <button @click="toggleTrace(idx, 0)" class="text-[10px] px-2 py-0.5 border rounded bg-slate-50">Raw</button>
                                        <button @click="toggleTrace(idx, 1)" class="text-[10px] px-2 py-0.5 border rounded bg-amber-50 text-amber-700">Base</button>
                                        <button @click="toggleTrace(idx, 2)" class="text-[10px] px-2 py-0.5 border rounded font-bold text-emerald-600 bg-emerald-50">Proc</button>
                                    </div>
                                    <div :id="'chart-'+idx" class="w-full h-60"></div>
                                    <div v-if="item.interpretation && item.showDocs" class="mt-3 p-3 bg-emerald-50 border border-emerald-100 rounded text-xs text-emerald-800">
                                        <div class="prose prose-xs" v-html="md(item.interpretation)"></div>
                                    </div>
                                </div>

                                <div v-if="item.raw_b64" class="bg-white p-2 rounded border mb-3 text-center">
                                    <div class="flex gap-2 mb-2 text-xs justify-center" data-html2canvas-ignore="true">
                                        <button @click="item.view='proc'" :class="item.view!=='raw'?'font-bold text-emerald-600':''">Processed</button>
                                        <span class="text-slate-300">|</span>
                                        <button @click="item.view='raw'" :class="item.view==='raw'?'font-bold text-emerald-600':''">Original</button>
                                    </div>
                                    <img :src="'data:image/jpeg;base64,' + (item.view==='raw' ? item.raw_b64 : item.proc_b64)" class="max-h-80 max-w-full mx-auto object-contain bg-black">
                                    
                                    <div v-if="item.footer_b64" class="mt-3 border-t pt-2">
                                        <p class="text-[10px] font-bold text-slate-500 mb-1 text-left">Detailed Analysis</p>
                                        <img :src="'data:image/jpeg;base64,' + item.footer_b64" class="w-full object-contain border bg-white">
                                    </div>

                                    <div v-if="item.stats && Object.keys(item.stats).length > 0" class="mt-2 p-2 bg-emerald-50 border border-emerald-100 rounded text-[10px] grid grid-cols-2 gap-2 text-left">
                                        <div v-for="(v, k) in item.stats" :key="k">
                                            <span class="font-bold text-emerald-700 capitalize">{{ k }}:</span> {{ v }}
                                        </div>
                                    </div>
                                    <div v-if="item.summary" class="mt-2 text-[10px] text-slate-700 text-left prose prose-sm bg-slate-50 p-2 leading-tight" v-html="md(item.summary)"></div>
                                </div>

                                <div v-if="item.pages || item.slides" class="mt-4 border-t pt-4">
                                    <h4 class="font-bold text-slate-600 mb-3 text-xs flex items-center">
                                        <i class="lucide-book-open w-3 h-3 mr-2"></i> Page Analysis
                                    </h4>
                                    
                                    <div class="grid grid-cols-1 gap-4">
                                        <div v-if="item.pages" v-for="p in item.pages" :key="p.id" class="flex gap-4 bg-slate-50 p-3 rounded border avoid-break">
                                            <div class="shrink-0">
                                                <p class="text-[10px] font-bold text-slate-400 mb-1 text-center">P{{ p.page_num }}</p>
                                                <img :src="'data:image/jpeg;base64,'+p.image_b64" class="w-24 h-auto border bg-white object-contain">
                                            </div>
                                            <div class="flex-1 min-w-0 text-[10px] leading-tight prose" v-html="md(p.desc)"></div>
                                        </div>

                                        <div v-if="item.slides" v-for="s in item.slides" :key="s.id" class="bg-slate-50 p-3 rounded border avoid-break">
                                            <div class="text-[10px] font-bold text-emerald-600 mb-1">Slide {{s.slide_num}}</div>
                                            <div class="text-[10px] mb-2 leading-tight">{{ s.text.substring(0, 300) }}...</div>
                                            <div v-if="s.images.length" class="flex gap-2">
                                                <img v-for="(img, imx) in s.images.slice(0,3)" :key="imx" :src="'data:image/jpeg;base64,'+img.b64" class="h-16 w-auto border">
                                            </div>
                                        </div>
                                    </div>
                                </div>

                                <div v-if="item.log && item.showDocs" class="mt-2 p-2 bg-gray-100 rounded border text-[9px] font-mono text-gray-600 avoid-break">
                                    <ul class="list-disc pl-4 space-y-0"><li v-for="l in item.log">{{ l }}</li></ul>
                                </div>
                            </div>
                        </section>
                    </div>
                </div>
            </div>
        </main>
    </div>
    <script>
        const { createApp, ref, computed, nextTick } = Vue;
        createApp({
            setup() {
                const stagedFiles = ref([]);
                const isAnalyzing = ref(false);
                const isTranslating = ref(false);
                const progress = ref(0);
                const currentStep = ref("");
                const result = ref(null);
                const lang = ref('ko');
                const translatedReport = ref('');
                const md = (t) => marked.parse(t||'');
                const displayReport = computed(() => lang.value === 'en' && translatedReport.value ? translatedReport.value : (result.value ? result.value.final_report : ''));

                const addFiles = (e) => {
                    Array.from(e.target.files).forEach(f => {
                        let eq = 'SEM'; let dt = 'General';
                        if(f.name.match(/\.(pdf|ppt|pptx)$/i)) eq = 'Document';
                        else if(f.name.match(/\.(xlsx|csv|txt)$/i)) eq = 'XPS';
                        stagedFiles.value.push({ 
                            file: f, equipment: eq, dataType: dt, goal: '', mode: 'Auto', view: 'proc', showDocs: true, 
                            centerX: null, centerY: null,
                            patchSize: 128, step: 16, nComp: 10
                        });
                    });
                    e.target.value = ''; 
                };
                const removeFile = (i) => stagedFiles.value.splice(i, 1);
                const isDoc = (item) => item.equipment === 'Document';
                const isImage = (item) => ['SEM','TEM','AFM','Optical','Diffraction','2D Spectrum'].includes(item.equipment);

                const startAnalysis = async () => {
                    isAnalyzing.value = true; progress.value = 0; currentStep.value = "Init...";
                    const fd = new FormData();
                    const configs = {};
                    stagedFiles.value.forEach(item => {
                        fd.append('files', item.file);
                        configs[item.file.name] = { 
                            equipment: item.equipment, 
                            data_type: item.equipment === 'Diffraction' ? '2D Diffraction' : item.dataType,
                            goal: item.goal, mode: item.mode,
                            center_coords: (item.centerX && item.centerY) ? `${item.centerX},${item.centerY}` : null,
                            patch_size: item.patchSize, step: item.step, n_components: item.nComp
                        };
                    });
                    fd.append('file_configs', JSON.stringify(configs));
                    try {
                        const initRes = await fetch('/api/analyze', { method: 'POST', body: fd });
                        const { job_id } = await initRes.json();
                        const poll = setInterval(async () => {
                            const statusRes = await fetch(`/api/status/${job_id}`);
                            const status = await statusRes.json();
                            progress.value = status.progress || 0;
                            currentStep.value = status.step || "Processing...";
                            if (status.status === "Completed") {
                                clearInterval(poll);
                                result.value = status.results;
                                if(result.value.results) result.value.results.forEach(r => r.showDocs = true);
                                isAnalyzing.value = false;
                                await nextTick();
                                renderCharts(result.value.results);
                            } else if (status.status === "Failed") {
                                clearInterval(poll); alert("Error: " + status.error); isAnalyzing.value = false;
                            }
                        }, 1000);
                    } catch(e) { alert(e); isAnalyzing.value = false; }
                };

                const renderCharts = (items) => {
                    items.forEach((item, i) => {
                        if(item.chart_data) {
                            const raw = { x: item.chart_data.map(d=>d.x), y: item.chart_data.map(d=>d.y_raw), mode:'lines', name:'Raw', line:{color:'#cbd5e1', width:1}, visible: true };
                            const base = { x: item.chart_data.map(d=>d.x), y: item.chart_data.map(d=>d.y_base), mode:'lines', name:'Base', line:{color:'#f59e0b', width:1, dash:'dot'}, visible: true };
                            const proc = { x: item.chart_data.map(d=>d.x), y: item.chart_data.map(d=>d.y_proc), mode:'lines', name:'Proc', line:{color:'#059669', width:2}, visible: true };
                            
                            const layout = {
                                margin:{t:10,b:30,l:40,r:10}, 
                                showlegend:false,
                                autosize: true, 
                                height: 240, 
                                font: { size: 10 }
                            };
                            const config = { displayModeBar: false, responsive: true };
                            
                            Plotly.newPlot('chart-'+i, [raw, base, proc], layout, config);
                        }
                    });
                    lucide.createIcons();
                };

                const toggleTrace = (idx, traceIdx) => {
                    const el = document.getElementById('chart-'+idx);
                    if(el) {
                        const current = el.data[traceIdx].visible;
                        const next = current === true ? 'legendonly' : true;
                        Plotly.restyle(el, {'visible': next}, [traceIdx]);
                    }
                };

                const dlPDF = () => {
                    const now = new Date();
                    const ts = now.toISOString().slice(2,10).replace(/-/g,'') + '_' + now.toTimeString().slice(0,4).replace(':','');
                    const eqList = [...new Set(stagedFiles.value.map(f=>f.equipment))].join('-');
                    const filename = `${ts}_${eqList}_Report.pdf`;
                    
                    const element = document.getElementById('report-view');
                    
                    const opt = {
                        margin:       [10, 10, 10, 10], 
                        filename:     filename,
                        image:        { type: 'jpeg', quality: 0.98 },
                        html2canvas:  { scale: 2, useCORS: true, logging: false, scrollX: 0, scrollY: 0 },
                        jsPDF:        { unit: 'mm', format: 'a4', orientation: 'portrait' },
                        pagebreak:    { mode: ['avoid-all', 'css', 'legacy'] }
                    };

                    html2pdf().set(opt).from(element).save();
                };

                const setLang = async (target) => {
                    if (target === 'ko') { lang.value = 'ko'; return; }
                    if (target === 'en') {
                        if (translatedReport.value) { lang.value = 'en'; return; }
                        isTranslating.value = true;
                        try {
                            const res = await fetch('/api/translate', {
                                method: 'POST', headers: {'Content-Type': 'application/json'},
                                body: JSON.stringify({ text: result.value.final_report })
                            });
                            const data = await res.json();
                            translatedReport.value = data.translated;
                            lang.value = 'en';
                        } catch(e) { alert("Trans Error: " + e); }
                        finally { isTranslating.value = false; }
                    }
                };

                setTimeout(()=>lucide.createIcons(), 100);
                return { stagedFiles, isAnalyzing, progress, currentStep, result, addFiles, removeFile, startAnalysis, md, toggleTrace, isDoc, isImage, dlPDF, setLang, lang, displayReport, isTranslating };
            }
        }).mount('#app');
    </script>
</body>
</html>






엔브이23

import os
import io
# [0] OMP Error Fix
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"
import torch

import asyncio
import base64
import json
import re
import math
import uuid
import time
from typing import List, Dict, Any, Optional
from pydantic import BaseModel
from contextlib import asynccontextmanager
from collections import Counter

# [1] Security
os.environ['NO_PROXY'] = '127.0.0.1,localhost,0.0.0.0,::1'
if 'HTTP_PROXY' in os.environ: del os.environ['HTTP_PROXY']
if 'HTTPS_PROXY' in os.environ: del os.environ['HTTPS_PROXY']

from fastapi import FastAPI, UploadFile, File, Form, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import HTMLResponse, FileResponse
import pandas as pd
import numpy as np

# SciPy & CV & Sklearn
from sklearn.decomposition import NMF
from scipy.signal import find_peaks, savgol_filter
from scipy.optimize import curve_fit
from scipy.ndimage import center_of_mass
from skimage.feature import peak_local_max
import cv2
import matplotlib
import matplotlib.pyplot as plt 

# Models & Tools
from segment_anything import sam_model_registry, SamAutomaticMaskGenerator
from ollama import Client 
# [NV23 Change] pdf2image -> pypdfium2
import pypdfium2 as pdfium 
from pptx import Presentation
from pptx.enum.shapes import MSO_SHAPE_TYPE

# [2] Config
OLLAMA_HOST = "http://127.0.0.1:11434"
ollama_client = Client(host=OLLAMA_HOST)
VISION_MODEL = "qwen3-vl:30b-a3b-instruct" 
TEXT_MODEL = "llama3:70b"

SAM_CHECKPOINT = "sam_vit_b_01ec64.pth"
SAM_MODEL_TYPE = "vit_b"
device = "cuda" if torch.cuda.is_available() else "cpu"

mask_generator = None 
JOBS = {} 

@asynccontextmanager
async def lifespan(app: FastAPI):
    global mask_generator
    if os.path.exists(SAM_CHECKPOINT):
        print(f"Loading SAM ({SAM_MODEL_TYPE}) on {device}...")
        try:
            sam = sam_model_registry[SAM_MODEL_TYPE](checkpoint=SAM_CHECKPOINT)
            sam.to(device=device)
            mask_generator = SamAutomaticMaskGenerator(
                model=sam, points_per_side=32, pred_iou_thresh=0.86,
                stability_score_thresh=0.90, min_mask_region_area=50
            )
            print("SAM Loaded.")
        except: print("SAM Load Failed.")
    yield
    JOBS.clear()

def sanitize_json(obj):
    if isinstance(obj, float):
        if np.isnan(obj) or np.isinf(obj): return 0.0
        return obj
    elif isinstance(obj, dict): return {k: sanitize_json(v) for k, v in obj.items()}
    elif isinstance(obj, list): return [sanitize_json(v) for v in obj]
    elif isinstance(obj, (np.int64, np.int32)): return int(obj)
    elif isinstance(obj, (np.float64, np.float32)):
        if np.isnan(obj) or np.isinf(obj): return 0.0
        return float(obj)
    return obj

# ==========================================
# [3] NV23 Robust Engine
# ==========================================
class ScienceProcessorNV23:
    
    # --- Universal Loader ---
    @staticmethod
    def read_universal_dataframe(content: bytes, filename: str) -> List[pd.DataFrame]:
        dfs = []
        fname = filename.lower()
        if fname.endswith(('.xlsx', '.xls')):
            try:
                xls = pd.ExcelFile(io.BytesIO(content))
                for s in xls.sheet_names:
                    df = pd.read_excel(xls, sheet_name=s, header=None)
                    num = df.apply(pd.to_numeric, errors='coerce').notna().sum().sum()
                    if num < 5: continue
                    if not df.empty: dfs.append(df)
                if dfs: return dfs
            except: pass
        text_content = ""
        for enc in ['utf-8', 'cp949', 'latin1']:
            try: text_content = content.decode(enc); break
            except: continue
        if text_content:
            lines = text_content.splitlines()
            data_start = -1; delimiter = None
            separators = [',', '\t', ';', '\s+']
            for i, line in enumerate(lines[:100]):
                for sep in separators:
                    parts = line.split() if sep=='\s+' else line.split(sep)
                    nums = sum(1 for p in parts if re.match(r'^-?\d+(\.\d+)?(?:[eE][+-]?\d+)?$', p.strip()))
                    if nums >= 2: data_start = i; delimiter = sep; break
                if data_start != -1: break
            if data_start != -1:
                try:
                    data_str = "\n".join(lines[data_start:])
                    sep_arg = '\s+' if delimiter=='\s+' else delimiter
                    df = pd.read_csv(io.StringIO(data_str), sep=sep_arg, header=None, engine='python')
                    dfs.append(df)
                except: pass
        return dfs

    # --- Block Parser ---
    @staticmethod
    def detect_structured_blocks(df: pd.DataFrame) -> List[pd.DataFrame]:
        blocks = []
        if df.empty: return []
        df_num = df.apply(pd.to_numeric, errors='coerce')
        if df.shape[1] > 0: is_data_row = df_num.iloc[:, 0].notna()
        else: is_data_row = df_num.notna().any(axis=1)
        groups = (is_data_row != is_data_row.shift()).cumsum()
        for _, group in df.groupby(groups):
            if is_data_row.loc[group.index[0]]:
                block = df_num.loc[group.index].dropna(how='all', axis=1)
                if block.shape[0] >= 10: blocks.append(block.reset_index(drop=True))
        return blocks

    @staticmethod
    def split_horizontal_blocks(df: pd.DataFrame) -> List[pd.DataFrame]:
        sub_blocks = []
        if df.empty: return []
        col_counts = df.apply(pd.to_numeric, errors='coerce').notna().sum()
        valid = np.where(col_counts >= 2)[0]
        if len(valid) == 0: return []
        splits = np.where(np.diff(valid) > 1)[0] + 1
        groups = np.split(valid, splits)
        for g in groups:
            if len(g) > 0:
                sub = df.iloc[:, g].reset_index(drop=True)
                if sub.shape[1] >= 1 and sub.shape[0] >= 2: sub_blocks.append(sub)
        return sub_blocks

    @staticmethod
    def extract_series_from_df(df: pd.DataFrame) -> List[Dict]:
        series_list = []
        try:
            vals = df.values; rows, cols = vals.shape
            if rows < 5: return []
            if cols == 1:
                y = vals[:, 0]; mask = ~np.isnan(y)
                if np.sum(mask) > 5: series_list.append({"x": np.arange(len(y))[mask], "y": y[mask], "name": "Single"})
                return series_list
            def is_axis(arr):
                arr = arr[~np.isnan(arr)]
                if len(arr) < 5: return False
                diff = np.diff(arr)
                if np.all(diff >= 0) or np.all(diff <= 0): return True
                return (np.sum(diff>0)/len(diff)>0.9) or (np.sum(diff<0)/len(diff)>0.9)
            current_x = vals[:, 0]
            if not is_axis(current_x): current_x = np.arange(rows)
            for i in range(1, cols):
                col_data = vals[:, i]; mask_valid = ~np.isnan(col_data)
                if np.sum(mask_valid) < 5: continue
                if is_axis(col_data): current_x = col_data; continue 
                mask = ~np.isnan(current_x) & ~np.isnan(col_data)
                if np.sum(mask) > 5:
                    series_list.append({"x": current_x[mask], "y": col_data[mask], "name": f"Col-{i}"})
        except: pass
        return series_list

    # --- Spectrum Logic ---
    @staticmethod
    def simple_baseline(y):
        try:
            w = max(5, len(y)//10)
            s = pd.Series(y).rolling(window=w, center=True).min()
            b = s.bfill().ffill().values 
            if len(b) > 51: b = savgol_filter(b, 51, 3)
            return b, w
        except: return np.zeros_like(y), 0

    @staticmethod
    def process_spectrum(x, y, mode, goal):
        log = []; y_raw = y.copy(); base = np.zeros_like(y); y_proc = y.copy()
        peaks = []; fits = []; stats_txt = "Error"
        try:
            if mode == "None":
                peaks, _ = find_peaks(y, height=np.max(y)*0.05)
                log.append("Mode: Raw")
                stats_txt = f"Raw Peaks: {len(peaks)}"
            else:
                win = 15; do_fit = False
                if mode == "AI-Adaptive":
                    log.append("Mode: AI-Adaptive")
                    if "noise" in goal.lower(): win = 31
                    if "fit" in goal.lower(): do_fit = True
                else: log.append("Mode: Auto")
                base, bw = ScienceProcessorNV23.simple_baseline(y)
                y_proc = np.maximum(y - base, 0)
                log.append(f"Base: Rolling Min (w={bw})")
                if len(y_proc) > win:
                    y_proc = savgol_filter(y_proc, win, 3)
                    log.append(f"Smooth: SavGol (w={win})")
                peaks, _ = find_peaks(y_proc, height=np.max(y_proc)*0.05, distance=10)
                log.append(f"Peaks: {len(peaks)}")
                stats_txt = f"Range: {np.min(x):.1f}~{np.max(x):.1f}, Peaks: {len(peaks)}"
            return {"x": x, "y_raw": y_raw, "y_proc": y_proc, "y_base": base, "peaks": [{"x": float(x[p]), "y": float(y_proc[p])} for p in peaks], "log": log, "fits": fits, "stats": stats_txt}
        except Exception as e: return {"x": x, "y_raw": y_raw, "y_proc": y_proc, "y_base": base, "peaks": [], "log": [str(e)], "fits": [], "stats": "Error"}

    # --- Footer ---
    @staticmethod
    def separate_footer(img_bgr):
        h, w = img_bgr.shape[:2]
        roi_h = int(h * 0.82)
        roi = img_bgr[roi_h:, :]
        gray = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)
        sobel_y = np.abs(cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=3))
        row_sums = np.sum(sobel_y, axis=1)
        split_idx = np.argmax(row_sums)
        if row_sums[split_idx] > w * 30:
            real_split = roi_h + split_idx
            return img_bgr[:real_split, :], img_bgr[real_split:, :]
        return img_bgr, None

    # --- [E] NV18 Crystal Engine ---
    @staticmethod
    def analyze_crystal_fft_nmf(img_bgr, patch_size=128, step=16, n_components=10):
        log = []
        gray = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2GRAY)
        h, w = gray.shape
        log.append(f"FFT-NMF: Patch={patch_size}, Step={step}, K={n_components}")
        
        custom_colors = [
            (0, 0, 255), (0, 255, 0), (255, 0, 0), (0, 255, 255), (255, 255, 0),
            (255, 0, 255), (0, 165, 255), (238, 130, 238), (128, 128, 0), (19, 69, 139)
        ]
        while len(custom_colors) < n_components: custom_colors.append((255, 255, 255))
        
        cols = [] 
        y_steps = list(range(0, h - patch_size + 1, step))
        x_steps = list(range(0, w - patch_size + 1, step))
        grid_h = len(y_steps)
        grid_w = len(x_steps)
        
        cy, cx = patch_size // 2, patch_size // 2
        Y, X = np.ogrid[:patch_size, :patch_size]
        dist_sq = (X - cx)**2 + (Y - cy)**2
        dc_mask = dist_sq <= 4**2

        for y in y_steps:
            for x in x_steps:
                roi = gray[y:y+patch_size, x:x+patch_size]
                f = np.fft.fft2(roi)
                fshift = np.fft.fftshift(f)
                mag = np.abs(fshift) 
                mag[dc_mask] = 0
                cols.append(mag.flatten())
        
        if not cols: return {"type":"error", "msg":"Img too small"}
        
        try:
            V = np.array(cols).T.astype(np.float32)
            V = V - V.min()
            if V.max() > 0: V = V / V.max()
            
            nmf = NMF(n_components=n_components, init='nndsvda', random_state=42, max_iter=2000)
            W = nmf.fit_transform(V) 
            H = nmf.components_ 
            
            if H.shape[1] != grid_h * grid_w: raise ValueError("Shape Mismatch")
            
            idx_H = np.argmax(H, axis=0) 
            map_low = idx_H.reshape(grid_h, grid_w).astype(np.uint8)
            map_high = cv2.resize(map_low, (w, h), interpolation=cv2.INTER_NEAREST)
            
            overlay = img_bgr.copy()
            mask_layer = np.zeros_like(img_bgr)
            
            counts = Counter(idx_H)
            total = len(idx_H)
            stats = {}
            
            for i in range(n_components):
                mask = (map_high == i)
                mask_layer[mask] = custom_colors[i]
                cnt = counts.get(i, 0)
                stats[f"Pattern {i}"] = f"{cnt} patches ({cnt/total*100:.1f}%)"
                
            alpha = 0.4
            mask_idx = np.any(mask_layer > 0, axis=-1)
            overlay[mask_idx] = (overlay[mask_idx]*(1-alpha) + mask_layer[mask_idx]*alpha).astype(np.uint8)
            
            basis_combo_list = []
            for k in range(n_components):
                pat = W[:, k].reshape(patch_size, patch_size)
                pat_log = np.log1p(pat * 1000) 
                pat_norm = cv2.normalize(pat_log, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)
                pat_color = cv2.applyColorMap(pat_norm, cv2.COLORMAP_MAGMA)
                
                legend_h = 40
                legend = np.zeros((legend_h, patch_size, 3), dtype=np.uint8)
                legend[:] = custom_colors[k]
                cv2.putText(legend, f"P{k}", (5, 25), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255,255,255), 2)
                
                combo = np.vstack([legend, pat_color])
                cv2.rectangle(combo, (0,0), (combo.shape[1]-1, combo.shape[0]-1), custom_colors[k], 2)
                basis_combo_list.append(combo)
            
            full_strip = np.hstack(basis_combo_list)
            full_strip_large = cv2.resize(full_strip, None, fx=2, fy=2, interpolation=cv2.INTER_NEAREST)
            
            def to_b64(im): return base64.b64encode(cv2.imencode('.jpg', im)[1]).decode('utf-8')
            
            return {
                "type": "image", 
                "raw_b64": to_b64(img_bgr), 
                "proc_b64": to_b64(overlay),
                "footer_b64": to_b64(full_strip_large), 
                "stats": stats, 
                "log": log
            }

        except Exception as e:
            print(f"NMF Error: {e}")
            def to_b64(im): return base64.b64encode(cv2.imencode('.jpg', im)[1]).decode('utf-8')
            return {"type": "image", "raw_b64": to_b64(img_bgr), "proc_b64": to_b64(img_bgr), "stats": {"Error": str(e)}, "log": log}

    # --- [F] Diffraction ---
    @staticmethod
    def analyze_diffraction_nv7(img_bgr, user_center=None):
        log = []; stats = {}
        gray = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2GRAY)
        if user_center and len(user_center)==2 and user_center[0]>0:
            cx, cy = user_center
            log.append(f"Center: Manual [{cx}, {cy}]")
        else:
            _, th = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY+cv2.THRESH_OTSU)
            cyf, cxf = center_of_mass(th)
            if np.isnan(cyf): cyf, cxf = gray.shape[0]//2, gray.shape[1]//2
            cx, cy = int(cxf), int(cyf)
            log.append(f"Center: Auto [{cx}, {cy}]")

        g1 = cv2.GaussianBlur(gray, (3,3), 0)
        g2 = cv2.GaussianBlur(gray, (51,51), 0)
        diff = cv2.subtract(g1, g2)
        coords = peak_local_max(diff, min_distance=10, threshold_abs=20)
        enhanced = np.log1p(diff.astype(np.float32))
        enhanced = cv2.normalize(enhanced, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)
        vis = cv2.cvtColor(enhanced, cv2.COLOR_GRAY2BGR)
        cv2.drawMarker(vis, (cx, cy), (0,0,255), cv2.MARKER_CROSS, 20, 2)
        peak_list = []
        for p in coords:
            py, px = p
            if abs(py-cy)<5 and abs(px-cx)<5: continue
            cv2.circle(vis, (px, py), 3, (0,255,0), 1)
            peak_list.append(f"[{px},{py}]")
        log.append(f"Spots: {len(peak_list)}")
        def to_b64(im): return base64.b64encode(cv2.imencode('.jpg', im)[1]).decode('utf-8')
        return {"type": "image", "raw_b64": to_b64(img_bgr), "proc_b64": to_b64(vis), "stats": {"center": [cx, cy], "spots": len(peak_list)}, "log": log}

    # --- [G] SAM ---
    @staticmethod
    def analyze_sam_results_nv7(img_rgb, masks, data_type):
        overlay = img_rgb.copy()
        mask_overlay = np.zeros_like(img_rgb)
        h_img, w_img = img_rgb.shape[:2]
        stats = {"count": 0}
        log = []
        
        valid_masks = []
        for ann in masks:
            area = ann['area']; bbox = ann['bbox']; x, y, w, h = [int(v) for v in bbox]
            if area > h_img * w_img * 0.95 or area < 20: continue 
            if data_type == "Particle":
                if x<=1 or y<=1 or (x+w)>=w_img-1 or (y+h)>=h_img-1: continue
                mask_uint8 = (ann['segmentation'] * 255).astype(np.uint8)
                cnts, _ = cv2.findContours(mask_uint8, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
                if cnts:
                    perim = cv2.arcLength(cnts[0], True)
                    circ = 4*np.pi*area/(perim**2) if perim>0 else 0
                    if circ < 0.25: continue
            valid_masks.append(ann)

        if data_type == "Thin Film":
            sorted_masks = sorted(valid_masks, key=lambda x: x['bbox'][1])
        else:
            sorted_masks = sorted(valid_masks, key=lambda x: x['area'], reverse=True)

        thickness_vals = []; diameters = []

        for i, ann in enumerate(sorted_masks):
            m = ann['segmentation']
            bbox = ann['bbox']; x, y, w, h = [int(v) for v in bbox]
            cx, cy = int(x + w/2), int(y + h/2)
            color = (0, 255, 0) 

            if data_type == "Thin Film":
                is_bound = (i == 0 or i == len(sorted_masks)-1)
                cv2.putText(overlay, f"#{i+1}", (cx, cy), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (255, 255, 255), 3)
                if not is_bound:
                    thk = ann['area'] / w
                    thickness_vals.append(thk)
                    color = (255, 0, 0) 
                else: color = (100, 100, 100)
                if i < len(sorted_masks) - 1:
                    ys, xs = np.where(m)
                    if len(xs) > 10:
                        roughness = np.std(ys)
                        cv2.line(overlay, (x, y+h), (x+w, y+h), (0, 255, 255), 2)
                        cv2.putText(overlay, f"Int#{i+1} R:{roughness:.2f}", (x+10, y+h-5), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)
            elif data_type == "Particle":
                equivalent_diameter = 2 * np.sqrt(ann['area'] / np.pi)
                diameters.append(equivalent_diameter)

            mask_overlay[m] = color
            mask_uint8 = (m * 255).astype(np.uint8)
            cnts, _ = cv2.findContours(mask_uint8, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
            if cnts: cv2.drawContours(overlay, [cnts[0]], -1, color, 1)

        alpha = 0.35
        mask_idx = np.any(mask_overlay > 0, axis=-1)
        overlay[mask_idx] = (overlay[mask_idx]*(1-alpha) + mask_overlay[mask_idx]*alpha).astype(np.uint8)

        stats["count"] = len(sorted_masks)
        if data_type == "Thin Film":
            avg = round(np.mean(thickness_vals), 2) if thickness_vals else 0
            stats["thickness_avg"] = avg
            log.append(f"Layers: {len(sorted_masks)}, Avg Thk: {avg}")
        elif data_type == "Particle":
            avg = round(np.mean(diameters), 2) if diameters else 0
            stats["diameter_avg"] = avg
            log.append(f"Particles: {len(sorted_masks)}")
        return overlay, stats, log

    # --- Router ---
    @staticmethod
    def process_image_nv18(img_bytes, equipment, data_type, mode, goal, user_center=None, 
                           patch_size=128, step=16, n_components=10):
        nparr = np.frombuffer(img_bytes, np.uint8)
        img_raw = cv2.imdecode(nparr, cv2.IMREAD_COLOR)
        if img_raw is None: return {}
        
        body, footer = ScienceProcessorNV23.separate_footer(img_raw)
        
        footer_b64 = None
        if footer is not None:
            footer_b64 = base64.b64encode(cv2.imencode('.jpg', footer)[1]).decode('utf-8')

        if data_type == "Crystal Structure (FFT)":
            return {**ScienceProcessorNV23.analyze_crystal_fft_nmf(body, patch_size, step, n_components), "footer_b64": footer_b64}
            
        if data_type == "2D Diffraction":
            return {**ScienceProcessorNV23.analyze_diffraction_nv7(body, user_center), "footer_b64": footer_b64}
        
        # General SAM
        if mode == "None":
             def to_b64(im): return base64.b64encode(cv2.imencode('.jpg', im)[1]).decode('utf-8')
             return {"type":"image", "raw_b64":to_b64(body), "proc_b64":to_b64(body), "stats":{}, "log":["Mode: Raw"], "footer_b64": footer_b64}

        img_proc = body.copy(); log = []
        if mode == "Auto" or mode == "AI-Adaptive":
            img_proc = cv2.bilateralFilter(img_proc, 9, 75, 75)
            log.append("Preproc: Denoise")

        img_rgb = cv2.cvtColor(img_proc, cv2.COLOR_BGR2RGB)
        
        if mask_generator:
            try:
                masks = mask_generator.generate(img_rgb)
                log.append(f"SAM: {len(masks)} Segments")
                overlay, stats, sam_log = ScienceProcessorNV23.analyze_sam_results_nv7(img_rgb, masks, data_type)
                log.extend(sam_log)
                overlay = cv2.cvtColor(overlay, cv2.COLOR_RGB2BGR)
            except Exception as e:
                log.append(f"SAM Error: {str(e)}")
                overlay = body; stats = {}
        else:
            log.append("SAM Not Loaded")
            overlay = body; stats = {}

        def to_b64(im): return base64.b64encode(cv2.imencode('.jpg', im)[1]).decode('utf-8')
        return {"type": "image", "raw_b64": to_b64(body), "proc_b64": to_b64(overlay), "stats": stats, "log": log, "footer_b64": footer_b64}

# [4] Helper (Same)
async def analyze_vision_ollama(image_bytes: bytes, prompt: str) -> str:
    try:
        b64 = base64.b64encode(image_bytes).decode('utf-8')
        res = await asyncio.to_thread(ollama_client.chat, model=VISION_MODEL, messages=[{'role':'user','content':prompt,'images':[b64]}])
        return res['message']['content']
    except: return "Vision Error"

async def interpret_spectrum_data(raw_context: str, equipment: str) -> str:
    try:
        prompt = f"Analyze spectrum ({equipment}). Brief interpretation in Korean. Data: {raw_context}"
        res = await asyncio.to_thread(ollama_client.chat, model=TEXT_MODEL, messages=[{'role':'user','content':prompt}])
        return res['message']['content']
    except: return ""

# [5] App
app = FastAPI(title="Analyst NV23 PDF Engine Fix", lifespan=lifespan)
app.add_middleware(CORSMiddleware, allow_origins=["*"], allow_methods=["*"], allow_headers=["*"])

class TranslateReq(BaseModel): text: str

@app.post("/api/translate")
async def translate_text(req: TranslateReq):
    try:
        res = await asyncio.to_thread(ollama_client.chat, model=TEXT_MODEL, messages=[{'role':'system','content':'Translate to English.'}, {'role':'user','content':req.text}])
        return {"translated": res['message']['content']}
    except Exception as e: return {"translated": str(e)}

async def process_analysis_job(job_id: str, files_map: Dict, configs: Dict):
    try:
        JOBS[job_id]["status"] = "Processing"
        JOBS[job_id]["progress"] = 10
        final_results = []
        total_files = len(configs)
        processed_count = 0

        for filename, config in configs.items():
            if filename not in files_map: continue
            content = files_map[filename]
            eq = config.get("equipment", "General")
            data_type = config.get("data_type", "General")
            goal = config.get("goal", "")
            mode = config.get("mode", "Auto")
            
            user_center = None
            if config.get("center_coords"):
                try: user_center = [int(v) for v in config["center_coords"].split(",")]
                except: pass
            
            p_size = int(config.get("patch_size", 128))
            p_step = int(config.get("step", 16))
            n_comp = int(config.get("n_components", 10))

            fname_lower = filename.lower()
            JOBS[job_id]["step"] = f"Analyzing {filename}..."
            
            # [A] Document Logic (pypdfium2 Implemented)
            if eq == "Document" or fname_lower.endswith(('.pdf', '.ppt', '.pptx')):
                try:
                    if fname_lower.endswith('.pdf'):
                        # NV23 New PDF Engine
                        pdf = pdfium.PdfDocument(io.BytesIO(content))
                        n_pages = len(pdf)
                        full_txt = ""; pages = []
                        
                        # Process first 10 pages max
                        for i in range(min(n_pages, 10)):
                            page = pdf[i]
                            # Render page to bitmap (scale=2 for better OCR/Vision)
                            bitmap = page.render(scale=2)
                            pil_image = bitmap.to_pil()
                            
                            # Save to buffer for Ollama
                            buf = io.BytesIO()
                            pil_image.save(buf, format="JPEG")
                            b64 = base64.b64encode(buf.getvalue()).decode()
                            
                            # Analyze
                            desc = await analyze_vision_ollama(buf.getvalue(), "한글 요약.")
                            full_txt += f"\nPage {i+1}: {desc}"
                            pages.append({"id": str(uuid.uuid4()), "page_num": i+1, "image_b64": b64, "desc": desc})
                            
                        final_results.append({"type": "pdf", "filename": filename, "equipment": "Lit", "raw_context": full_txt, "pages": pages})
                    
                    elif fname_lower.endswith(('.ppt', '.pptx')):
                        try:
                            prs = Presentation(io.BytesIO(content))
                            slides = []; full_txt = ""
                            for i, slide in enumerate(prs.slides):
                                txt = ""; imgs = []
                                for s in slide.shapes:
                                    if hasattr(s, "text"): txt += s.text + "\n"
                                    if s.shape_type == MSO_SHAPE_TYPE.PICTURE:
                                        try:
                                            ib64 = base64.b64encode(s.image.blob).decode('utf-8')
                                            desc = "Image"
                                            imgs.append({"b64": ib64, "desc": desc})
                                        except: pass
                                if not txt.strip(): txt = "(No text)"
                                full_txt += f"Slide {i+1}: {txt}\n"
                                slides.append({"id": str(uuid.uuid4()), "slide_num": i+1, "text": txt, "images": imgs})
                            final_results.append({"type": "ppt", "filename": filename, "equipment": "Lit", "raw_context": full_txt, "slides": slides})
                        except Exception as ppt_err:
                            final_results.append({"type": "error", "filename": filename, "msg": f"PPTX Error: {ppt_err}"})

                except Exception as e: 
                    final_results.append({"type": "error", "filename": filename, "msg": str(e)})

            elif fname_lower.endswith(('.xlsx', '.xls', '.csv', '.txt')):
                try:
                    dfs = ScienceProcessorNV23.read_universal_dataframe(content, filename)
                    for i, df in enumerate(dfs):
                        v_blocks = ScienceProcessorNV23.detect_structured_blocks(df)
                        for v_block in v_blocks:
                            h_blocks = ScienceProcessorNV23.split_horizontal_blocks(v_block)
                            for h_block in h_blocks:
                                series_list = ScienceProcessorNV23.extract_series_from_df(h_block)
                                for s in series_list:
                                    res = ScienceProcessorNV23.process_spectrum(s['x'], s['y'], mode, goal)
                                    step = max(1, len(s['x'])//100)
                                    chart = [{"x": float(res["x"][k]), "y_proc": float(res["y_proc"][k]), "y_raw": float(res["y_raw"][k]), "y_base": float(res["y_base"][k])} for k in range(0, len(s['x']), step)]
                                    ctx = f"Data: {s['name']}\nStats: {res['stats']}"
                                    interp = await interpret_spectrum_data(ctx, eq)
                                    final_results.append({
                                        "type": "spectrum", "filename": f"{filename} ({s['name']})", "equipment": eq,
                                        "raw_context": f"{ctx}\nInterp: {interp}", 
                                        "chart_data": chart, "log": res["log"], "interpretation": interp
                                    })
                except Exception as ex: final_results.append({"type": "error", "filename": filename, "msg": str(ex)})

            elif fname_lower.endswith(('.png', '.jpg', '.jpeg', '.tif', '.bmp')):
                try:
                    JOBS[job_id]["step"] = f"Vision Analysis: {filename}..."
                    vis_res = await asyncio.to_thread(ScienceProcessorNV23.process_image_nv18, content, eq, data_type, mode, goal, user_center, p_size, p_step, n_comp)
                    if vis_res:
                        scale_info = "Unknown"
                        final_footer = vis_res.get("footer_b64")
                        if final_footer and data_type != "Crystal Structure (FFT)":
                            JOBS[job_id]["step"] = f"Scale Reading..."
                            scale_text = await analyze_vision_ollama(base64.b64decode(final_footer), "Read scale bar text only.")
                            scale_info = scale_text
                            vis_res["log"].append(f"Scale: {scale_text}")

                        stats_clean = ""
                        for k, v in vis_res.get('stats',{}).items():
                            stats_clean += f"- {k}: {v}\n"
                        
                        desc_prompt = f"Analyze {eq} image ({data_type}). Stats: \n{stats_clean}\n. Goal: {goal}. Summary in Korean."
                        desc = await analyze_vision_ollama(base64.b64decode(vis_res["proc_b64"]), desc_prompt)
                        
                        final_results.append({
                            "type": "image", "filename": filename, "equipment": eq, "summary": desc,
                            "raw_context": f"Img {eq}: {desc}\nStats:\n{stats_clean}", 
                            "raw_b64": vis_res["raw_b64"], "proc_b64": vis_res["proc_b64"], "footer_b64": final_footer,
                            "stats": vis_res.get("stats",{}), "log": vis_res.get("log", [])
                        })
                except Exception as ex: final_results.append({"type": "error", "filename": filename, "msg": str(ex)})
            
            processed_count += 1
            JOBS[job_id]["progress"] = 10 + int((processed_count / total_files) * 80)

        JOBS[job_id]["step"] = "Writing Report..."
        final_results = sanitize_json(final_results)
        
        data_ctx = ""
        for r in final_results:
            if r.get("type") != "error":
                raw = r.get("raw_context") or ""
                data_ctx += f"\n=== File: {r['filename']} ({r.get('equipment')}) ===\n{raw[:2000]}\n"

        final_report = "Fail"
        if data_ctx:
            try:
                system_prompt = "You are a research assistant. Write a comprehensive report in Korean based on the provided data summaries. Include a data summary table. Do not use LaTeX."
                res = await asyncio.to_thread(ollama_client.chat, model=TEXT_MODEL, messages=[{'role':'system','content':system_prompt}, {'role':'user','content':f"Data Summaries:\n{data_ctx}"}])
                final_report = res['message']['content']
            except Exception as e: final_report = str(e)

        JOBS[job_id]["results"] = {"results": final_results, "final_report": final_report}
        JOBS[job_id]["status"] = "Completed"
        JOBS[job_id]["progress"] = 100
    except Exception as e:
        JOBS[job_id]["status"] = "Failed"; JOBS[job_id]["error"] = str(e)

@app.post("/api/analyze")
async def start_analysis(background_tasks: BackgroundTasks, files: List[UploadFile] = File(...), file_configs: str = Form(...)):
    job_id = str(uuid.uuid4()); configs = json.loads(file_configs)
    files_map = {f.filename: await f.read() for f in files}
    JOBS[job_id] = {"status": "Pending", "progress": 0, "step": "Init", "results": None}
    background_tasks.add_task(process_analysis_job, job_id, files_map, configs)
    return {"job_id": job_id}

@app.get("/api/status/{job_id}")
async def get_status(job_id: str): return JOBS.get(job_id, {"status": "Not Found"})

@app.get("/", response_class=HTMLResponse)
async def serve_index():
    if os.path.exists("index.html"): return FileResponse("index.html")
    return "<h1>NV23 PDF Engine Fix Backend</h1>"

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)




엔브이20 웹코드 (수정)

<!DOCTYPE html>
<html lang="ko">
<head>
    <meta charset="UTF-8">
    <title>Analyst NV21 Multi-Doc Fix</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <script src="https://unpkg.com/vue@3/dist/vue.global.js"></script>
    <script src="https://cdn.plot.ly/plotly-2.27.0.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/marked/marked.min.js"></script>
    <script src="https://unpkg.com/lucide@latest"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/html2pdf.js/0.10.1/html2pdf.bundle.min.js"></script>
    <style>
        @import url('https://fonts.googleapis.com/css2?family=Inter:wght@400;600;700&display=swap');
        body { font-family: 'Inter', sans-serif; }
        .prose table { border-collapse: collapse; width: 100%; margin: 1em 0; }
        .prose th, .prose td { border: 1px solid #cbd5e1; padding: 6px; font-size: 0.8rem; }
        .prose th { background-color: #f1f5f9; }
        .custom-scrollbar::-webkit-scrollbar { width: 6px; }
        .custom-scrollbar::-webkit-scrollbar-track { background: #f1f1f1; }
        .custom-scrollbar::-webkit-scrollbar-thumb { background: #cbd5e1; border-radius: 3px; }
        .avoid-break { page-break-inside: avoid !important; break-inside: avoid !important; }
    </style>
</head>
<body class="bg-slate-100 text-slate-800">
    <div id="app" class="min-h-screen p-6">
        <header class="max-w-7xl mx-auto mb-6 flex items-center gap-3">
            <div class="bg-emerald-600 p-2 rounded-lg shadow"><i data-lucide="microscope" class="w-6 h-6 text-white"></i></div>
            <div><h1 class="text-2xl font-bold">Analyst NV21 <span class="text-sm font-normal text-slate-500">Perfected Edition</span></h1></div>
        </header>

        <main class="max-w-7xl mx-auto grid grid-cols-1 lg:grid-cols-12 gap-6">
            <div class="lg:col-span-4 space-y-4">
                <div class="bg-white p-5 rounded-xl shadow-sm border border-slate-200">
                    <h2 class="font-bold mb-3 flex items-center gap-2"><i data-lucide="files" class="w-4 h-4"></i> Data Staging</h2>
                    <div class="border-2 border-dashed border-slate-300 rounded-lg p-6 text-center hover:bg-emerald-50 cursor-pointer" @click="$refs.f.click()">
                        <input type="file" ref="f" multiple class="hidden" @change="addFiles">
                        <p class="text-sm font-semibold text-slate-600">Add Files</p>
                    </div>
                    <div v-if="stagedFiles.length" class="mt-4 space-y-3 max-h-[60vh] overflow-y-auto pr-1 custom-scrollbar">
                        <div v-for="(item, i) in stagedFiles" :key="i" class="p-3 bg-slate-50 border rounded-lg text-sm">
                            <div class="flex justify-between items-center mb-2">
                                <span class="font-bold truncate w-2/3">{{ item.file.name }}</span>
                                <button @click="removeFile(i)" class="text-red-400 hover:text-red-600"><i data-lucide="trash-2" class="w-4 h-4"></i></button>
                            </div>
                            <div class="grid grid-cols-1 gap-2">
                                <select v-model="item.equipment" class="w-full p-1.5 border rounded text-xs bg-white font-semibold text-emerald-700">
                                    <optgroup label="Spectroscopy (1D)">
                                        <option value="XPS">XPS</option>
                                        <option value="XRD">XRD (1D)</option>
                                        <option value="EELS">EELS</option>
                                        <option value="EDS">EDS</option>
                                        <option value="FT-IR">FT-IR</option>
                                        <option value="NMR">NMR</option>
                                        <option value="UV-VIS">UV-VIS</option>
                                        <option value="PL">Photo-luminescence</option>
                                        <option value="Time-resolved">Time-resolved</option>
                                    </optgroup>
                                    <optgroup label="Microscopy (2D)">
                                        <option value="SEM">SEM</option>
                                        <option value="TEM">TEM</option>
                                        <option value="AFM">AFM</option>
                                        <option value="Optical">Optical</option>
                                        <option value="2D Spectrum">2D Spectrum</option>
                                    </optgroup>
                                    <optgroup label="Diffraction (2D)">
                                        <option value="Diffraction">2D Diffraction</option>
                                    </optgroup>
                                    <optgroup label="Literature">
                                        <option value="Document">Document</option>
                                    </optgroup>
                                </select>
                                
                                <select v-if="isImage(item)" v-model="item.dataType" class="w-full p-1.5 border rounded text-xs bg-slate-50 font-bold">
                                    <option v-if="item.equipment==='Diffraction'" value="2D Diffraction">2D Diffraction</option>
                                    <option v-else value="General">General Object</option>
                                    <option v-if="item.equipment!=='Diffraction'" value="Particle">Particle Analysis</option>
                                    <option v-if="item.equipment!=='Diffraction'" value="Thin Film">Thin Film</option>
                                    <option v-if="item.equipment!=='Diffraction'" value="Crystal Structure (FFT)">Crystal Structure (FFT)</option>
                                </select>

                                <div v-if="item.dataType === 'Crystal Structure (FFT)'" class="flex gap-1">
                                    <input v-model="item.patchSize" type="number" placeholder="P(128)" class="w-1/3 p-1 border rounded text-[10px]">
                                    <input v-model="item.step" type="number" placeholder="S(16)" class="w-1/3 p-1 border rounded text-[10px]">
                                    <input v-model="item.nComp" type="number" placeholder="K(10)" class="w-1/3 p-1 border rounded text-[10px]">
                                </div>
                                <div v-if="item.dataType === '2D Diffraction'" class="flex gap-2">
                                    <input v-model="item.centerX" type="number" placeholder="CX" class="w-1/2 p-1.5 border rounded text-xs">
                                    <input v-model="item.centerY" type="number" placeholder="CY" class="w-1/2 p-1.5 border rounded text-xs">
                                </div>

                                <input v-model="item.goal" type="text" placeholder="Goal / Prompt (e.g. Find noise)" class="w-full p-1.5 border rounded text-xs">

                                <select v-if="!isDoc(item)" v-model="item.mode" class="w-full p-1.5 border rounded text-xs bg-white">
                                    <option value="Auto">✨ Auto Process</option>
                                    <option value="None">🚫 Raw Data</option>
                                    <option value="AI-Adaptive">🧠 Agent Mode</option>
                                </select>
                            </div>
                        </div>
                    </div>
                    <button @click="startAnalysis" :disabled="isAnalyzing || stagedFiles.length===0" class="w-full mt-4 py-3 bg-emerald-600 text-white rounded-lg font-bold flex justify-center gap-2 shadow hover:bg-emerald-700 disabled:opacity-50">
                        <span>Generate Report</span>
                    </button>
                </div>
            </div>

            <div class="lg:col-span-8">
                <div v-if="isAnalyzing" class="bg-white p-6 rounded-xl shadow border border-slate-200 text-center py-12">
                    <div class="animate-spin rounded-full h-12 w-12 border-b-2 border-emerald-600 mx-auto mb-4"></div>
                    <h3 class="text-lg font-bold text-slate-700">Analyzing...</h3>
                    <p class="text-sm text-slate-500 mb-4">{{ currentStep }}</p>
                    <div class="w-full bg-slate-200 rounded-full h-2.5 mb-2">
                        <div class="bg-emerald-600 h-2.5 rounded-full transition-all duration-500" :style="{width: progress + '%'}"></div>
                    </div>
                    <p class="text-xs text-right text-slate-400">{{ progress }}%</p>
                </div>

                <div v-if="result && !isAnalyzing" class="space-y-6 animate-fade-in">
                    <div class="flex justify-between items-center bg-white p-2 rounded-xl shadow-sm border border-slate-200">
                        <div class="flex gap-2">
                            <button @click="setLang('ko')" :class="lang==='ko'?'bg-emerald-100 text-emerald-700 font-bold':'text-slate-500'" class="px-4 py-2 rounded text-sm">KR</button>
                            <button @click="setLang('en')" :class="lang==='en'?'bg-blue-100 text-blue-700 font-bold':'text-slate-500'" class="px-4 py-2 rounded text-sm">EN</button>
                        </div>
                        <button @click="dlPDF" class="px-3 py-1.5 bg-slate-800 text-white text-xs rounded hover:bg-slate-700"><i class="lucide-file-down mr-1"></i> Save PDF</button>
                    </div>

                    <div id="report-view" class="bg-white p-8 rounded-xl shadow-sm border border-slate-200 text-sm">
                        <div class="mb-6 border-b pb-4">
                            <h1 class="text-2xl font-bold text-slate-800">Scientific Analysis Report</h1>
                            <p class="text-xs text-slate-500 mt-1">Generated by Analyst NV21 • {{ new Date().toLocaleString() }}</p>
                        </div>

                        <section class="mb-10 avoid-break">
                            <h2 class="text-lg font-bold text-emerald-700 mb-3 border-b pb-1">1. Synthesis</h2>
                            <div class="prose prose-sm text-slate-700 max-w-none text-[11px] leading-relaxed" v-html="md(displayReport)"></div>
                        </section>

                        <section>
                            <h2 class="text-lg font-bold text-slate-800 mb-5 border-b pb-1 avoid-break">2. Data Evidence</h2>
                            
                            <div v-for="(item, idx) in result.results" :key="idx" class="mb-8 p-4 bg-slate-50 rounded-lg border avoid-break">
                                <div class="flex items-center justify-between mb-3">
                                    <div class="flex items-center gap-2">
                                        <span class="px-2 py-0.5 bg-white border rounded text-[10px] font-bold uppercase text-slate-500">{{ item.equipment }}</span>
                                        <h3 class="font-bold text-sm">{{ item.filename }}</h3>
                                    </div>
                                    <button @click="item.showDocs = !item.showDocs" class="text-xs text-emerald-600 font-bold hover:underline flex items-center gap-1" data-html2canvas-ignore="true">
                                        {{ item.showDocs ? 'Hide Info' : 'Show Info' }}
                                    </button>
                                </div>

                                <div v-if="item.chart_data" class="bg-white p-2 rounded border mb-3 w-full">
                                    <div class="flex gap-2 justify-end mb-1" data-html2canvas-ignore="true">
                                        <button @click="toggleTrace(idx, 0)" class="text-[10px] px-2 py-0.5 border rounded bg-slate-50">Raw</button>
                                        <button @click="toggleTrace(idx, 1)" class="text-[10px] px-2 py-0.5 border rounded bg-amber-50 text-amber-700">Base</button>
                                        <button @click="toggleTrace(idx, 2)" class="text-[10px] px-2 py-0.5 border rounded font-bold text-emerald-600 bg-emerald-50">Proc</button>
                                    </div>
                                    <div :id="'chart-'+idx" class="w-full h-60"></div>
                                    <div v-if="item.interpretation && item.showDocs" class="mt-3 p-3 bg-emerald-50 border border-emerald-100 rounded text-xs text-emerald-800">
                                        <div class="prose prose-xs" v-html="md(item.interpretation)"></div>
                                    </div>
                                </div>

                                <div v-if="item.raw_b64" class="bg-white p-2 rounded border mb-3 text-center">
                                    <div class="flex gap-2 mb-2 text-xs justify-center" data-html2canvas-ignore="true">
                                        <button @click="item.view='proc'" :class="item.view!=='raw'?'font-bold text-emerald-600':''">Processed</button>
                                        <span class="text-slate-300">|</span>
                                        <button @click="item.view='raw'" :class="item.view==='raw'?'font-bold text-emerald-600':''">Original</button>
                                    </div>
                                    <img :src="'data:image/jpeg;base64,' + (item.view==='raw' ? item.raw_b64 : item.proc_b64)" class="max-h-80 max-w-full mx-auto object-contain bg-black">
                                    
                                    <div v-if="item.footer_b64" class="mt-3 border-t pt-2">
                                        <p class="text-[10px] font-bold text-slate-500 mb-1 text-left">Detailed Analysis (Footer / Patterns)</p>
                                        <img :src="'data:image/jpeg;base64,' + item.footer_b64" class="w-full object-contain border bg-white">
                                    </div>

                                    <div v-if="item.stats && Object.keys(item.stats).length > 0" class="mt-2 p-2 bg-emerald-50 border border-emerald-100 rounded text-[10px] grid grid-cols-2 gap-2 text-left">
                                        <div v-for="(v, k) in item.stats" :key="k">
                                            <span class="font-bold text-emerald-700 capitalize">{{ k }}:</span> {{ v }}
                                        </div>
                                    </div>
                                    <div v-if="item.summary" class="mt-2 text-[10px] text-slate-700 text-left prose prose-sm bg-slate-50 p-2 leading-tight" v-html="md(item.summary)"></div>
                                </div>

                                <div v-if="item.pages || item.slides" class="mt-4 border-t pt-4">
                                    <h4 class="font-bold text-slate-600 mb-3 text-xs flex items-center">
                                        <i class="lucide-book-open w-3 h-3 mr-2"></i> Page Analysis
                                    </h4>
                                    
                                    <div class="grid grid-cols-1 gap-4">
                                        <div v-if="item.pages" v-for="p in item.pages" :key="idx + '_pdf_' + p.page_num" class="flex gap-4 bg-slate-50 p-3 rounded border avoid-break">
                                            <div class="shrink-0">
                                                <p class="text-[10px] font-bold text-slate-400 mb-1 text-center">P{{ p.page_num }}</p>
                                                <img :src="'data:image/jpeg;base64,'+p.image_b64" class="w-24 h-auto border bg-white object-contain">
                                            </div>
                                            <div class="flex-1 min-w-0 text-[10px] leading-tight prose" v-html="md(p.desc)"></div>
                                        </div>

                                        <div v-if="item.slides" v-for="s in item.slides" :key="idx + '_ppt_' + s.slide_num" class="bg-slate-50 p-3 rounded border avoid-break">
                                            <div class="text-[10px] font-bold text-emerald-600 mb-1">Slide {{s.slide_num}}</div>
                                            <div class="text-[10px] mb-2 leading-tight">{{ s.text.substring(0, 300) }}...</div>
                                            <div v-if="s.images.length" class="flex gap-2">
                                                <img v-for="(img, imx) in s.images.slice(0,3)" :key="imx" :src="'data:image/jpeg;base64,'+img.b64" class="h-16 w-auto border">
                                            </div>
                                        </div>
                                    </div>
                                </div>

                                <div v-if="item.log && item.showDocs" class="mt-2 p-2 bg-gray-100 rounded border text-[9px] font-mono text-gray-600 avoid-break">
                                    <ul class="list-disc pl-4 space-y-0"><li v-for="l in item.log">{{ l }}</li></ul>
                                </div>
                            </div>
                        </section>
                    </div>
                </div>
            </div>
        </main>
    </div>
    <script>
        const { createApp, ref, computed, nextTick } = Vue;
        createApp({
            setup() {
                const stagedFiles = ref([]);
                const isAnalyzing = ref(false);
                const isTranslating = ref(false);
                const progress = ref(0);
                const currentStep = ref("");
                const result = ref(null);
                const lang = ref('ko');
                const translatedReport = ref('');
                const md = (t) => marked.parse(t||'');
                const displayReport = computed(() => lang.value === 'en' && translatedReport.value ? translatedReport.value : (result.value ? result.value.final_report : ''));

                const addFiles = (e) => {
                    Array.from(e.target.files).forEach(f => {
                        let eq = 'SEM'; let dt = 'General';
                        if(f.name.match(/\.(pdf|ppt|pptx)$/i)) eq = 'Document';
                        else if(f.name.match(/\.(xlsx|csv|txt)$/i)) eq = 'XPS';
                        stagedFiles.value.push({ 
                            file: f, equipment: eq, dataType: dt, goal: '', mode: 'Auto', view: 'proc', showDocs: true, 
                            centerX: null, centerY: null,
                            patchSize: 128, step: 16, nComp: 10
                        });
                    });
                    e.target.value = ''; 
                };
                const removeFile = (i) => stagedFiles.value.splice(i, 1);
                const isDoc = (item) => item.equipment === 'Document';
                const isImage = (item) => ['SEM','TEM','AFM','Optical','Diffraction','2D Spectrum'].includes(item.equipment);

                const startAnalysis = async () => {
                    isAnalyzing.value = true; progress.value = 0; currentStep.value = "Init...";
                    const fd = new FormData();
                    const configs = {};
                    stagedFiles.value.forEach(item => {
                        fd.append('files', item.file);
                        configs[item.file.name] = { 
                            equipment: item.equipment, 
                            data_type: item.equipment === 'Diffraction' ? '2D Diffraction' : item.dataType,
                            goal: item.goal, mode: item.mode,
                            center_coords: (item.centerX && item.centerY) ? `${item.centerX},${item.centerY}` : null,
                            patch_size: item.patchSize, step: item.step, n_components: item.nComp
                        };
                    });
                    fd.append('file_configs', JSON.stringify(configs));
                    try {
                        const initRes = await fetch('/api/analyze', { method: 'POST', body: fd });
                        const { job_id } = await initRes.json();
                        const poll = setInterval(async () => {
                            const statusRes = await fetch(`/api/status/${job_id}`);
                            const status = await statusRes.json();
                            progress.value = status.progress || 0;
                            currentStep.value = status.step || "Processing...";
                            if (status.status === "Completed") {
                                clearInterval(poll);
                                result.value = status.results;
                                if(result.value.results) result.value.results.forEach(r => r.showDocs = true);
                                isAnalyzing.value = false;
                                await nextTick();
                                renderCharts(result.value.results);
                            } else if (status.status === "Failed") {
                                clearInterval(poll); alert("Error: " + status.error); isAnalyzing.value = false;
                            }
                        }, 1000);
                    } catch(e) { alert(e); isAnalyzing.value = false; }
                };

                const renderCharts = (items) => {
                    items.forEach((item, i) => {
                        if(item.chart_data) {
                            const raw = { x: item.chart_data.map(d=>d.x), y: item.chart_data.map(d=>d.y_raw), mode:'lines', name:'Raw', line:{color:'#cbd5e1', width:1}, visible: true };
                            const base = { x: item.chart_data.map(d=>d.x), y: item.chart_data.map(d=>d.y_base), mode:'lines', name:'Base', line:{color:'#f59e0b', width:1, dash:'dot'}, visible: true };
                            const proc = { x: item.chart_data.map(d=>d.x), y: item.chart_data.map(d=>d.y_proc), mode:'lines', name:'Proc', line:{color:'#059669', width:2}, visible: true };
                            
                            const layout = {
                                margin:{t:10,b:30,l:40,r:10}, 
                                showlegend:false,
                                autosize: true, 
                                height: 240, 
                                font: { size: 10 }
                            };
                            const config = { displayModeBar: false, responsive: true };
                            
                            Plotly.newPlot('chart-'+i, [raw, base, proc], layout, config);
                        }
                    });
                    lucide.createIcons();
                };

                const toggleTrace = (idx, traceIdx) => {
                    const el = document.getElementById('chart-'+idx);
                    if(el) {
                        const current = el.data[traceIdx].visible;
                        const next = current === true ? 'legendonly' : true;
                        Plotly.restyle(el, {'visible': next}, [traceIdx]);
                    }
                };

                const dlPDF = () => {
                    const now = new Date();
                    const ts = now.toISOString().slice(2,10).replace(/-/g,'') + '_' + now.toTimeString().slice(0,4).replace(':','');
                    const eqList = [...new Set(stagedFiles.value.map(f=>f.equipment))].join('-');
                    const filename = `${ts}_${eqList}_Report.pdf`;
                    
                    const element = document.getElementById('report-view');
                    
                    const opt = {
                        margin:       [10, 10, 10, 10], 
                        filename:     filename,
                        image:        { type: 'jpeg', quality: 0.98 },
                        html2canvas:  { scale: 2, useCORS: true, logging: false, scrollX: 0, scrollY: 0 },
                        jsPDF:        { unit: 'mm', format: 'a4', orientation: 'portrait' },
                        pagebreak:    { mode: ['avoid-all', 'css', 'legacy'] }
                    };

                    html2pdf().set(opt).from(element).save();
                };

                const setLang = async (target) => {
                    if (target === 'ko') { lang.value = 'ko'; return; }
                    if (target === 'en') {
                        if (translatedReport.value) { lang.value = 'en'; return; }
                        isTranslating.value = true;
                        try {
                            const res = await fetch('/api/translate', {
                                method: 'POST', headers: {'Content-Type': 'application/json'},
                                body: JSON.stringify({ text: result.value.final_report })
                            });
                            const data = await res.json();
                            translatedReport.value = data.translated;
                            lang.value = 'en';
                        } catch(e) { alert("Trans Error: " + e); }
                        finally { isTranslating.value = false; }
                    }
                };

                setTimeout(()=>lucide.createIcons(), 100);
                return { stagedFiles, isAnalyzing, progress, currentStep, result, addFiles, removeFile, startAnalysis, md, toggleTrace, isDoc, isImage, dlPDF, setLang, lang, displayReport, isTranslating };
            }
        }).mount('#app');
    </script>
</body>
</html>


엔브이20

import os
import io
# [0] OMP Error Fix
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"
import torch

import asyncio
import base64
import json
import re
import math
import uuid
import time
from typing import List, Dict, Any, Optional
from pydantic import BaseModel
from contextlib import asynccontextmanager
from collections import Counter

# [1] Security
os.environ['NO_PROXY'] = '127.0.0.1,localhost,0.0.0.0,::1'
if 'HTTP_PROXY' in os.environ: del os.environ['HTTP_PROXY']
if 'HTTPS_PROXY' in os.environ: del os.environ['HTTPS_PROXY']

from fastapi import FastAPI, UploadFile, File, Form, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import HTMLResponse, FileResponse
import pandas as pd
import numpy as np

# SciPy & CV & Sklearn
from sklearn.decomposition import NMF
from scipy.signal import find_peaks, savgol_filter
from scipy.optimize import curve_fit
from scipy.ndimage import center_of_mass
from skimage.feature import peak_local_max
import cv2
import matplotlib
import matplotlib.pyplot as plt 

# Models
from segment_anything import sam_model_registry, SamAutomaticMaskGenerator
from ollama import Client 
from pdf2image import convert_from_bytes
from pptx import Presentation
from pptx.enum.shapes import MSO_SHAPE_TYPE

# [2] Config
OLLAMA_HOST = "http://127.0.0.1:11434"
ollama_client = Client(host=OLLAMA_HOST)
VISION_MODEL = "qwen3-vl:30b-a3b-instruct" 
TEXT_MODEL = "llama3:70b"

SAM_CHECKPOINT = "sam_vit_b_01ec64.pth"
SAM_MODEL_TYPE = "vit_b"
device = "cuda" if torch.cuda.is_available() else "cpu"

mask_generator = None 
JOBS = {} 

@asynccontextmanager
async def lifespan(app: FastAPI):
    global mask_generator
    if os.path.exists(SAM_CHECKPOINT):
        print(f"Loading SAM ({SAM_MODEL_TYPE}) on {device}...")
        try:
            sam = sam_model_registry[SAM_MODEL_TYPE](checkpoint=SAM_CHECKPOINT)
            sam.to(device=device)
            mask_generator = SamAutomaticMaskGenerator(
                model=sam, points_per_side=32, pred_iou_thresh=0.86,
                stability_score_thresh=0.90, min_mask_region_area=50
            )
            print("SAM Loaded.")
        except: print("SAM Load Failed.")
    yield
    JOBS.clear()

def sanitize_json(obj):
    if isinstance(obj, float):
        if np.isnan(obj) or np.isinf(obj): return 0.0
        return obj
    elif isinstance(obj, dict): return {k: sanitize_json(v) for k, v in obj.items()}
    elif isinstance(obj, list): return [sanitize_json(v) for v in obj]
    elif isinstance(obj, (np.int64, np.int32)): return int(obj)
    elif isinstance(obj, (np.float64, np.float32)):
        if np.isnan(obj) or np.isinf(obj): return 0.0
        return float(obj)
    return obj

# ==========================================
# [3] NV20 Document Fix Engine
# ==========================================
class ScienceProcessorNV20:
    
    # --- Universal Loader ---
    @staticmethod
    def read_universal_dataframe(content: bytes, filename: str) -> List[pd.DataFrame]:
        dfs = []
        fname = filename.lower()
        if fname.endswith(('.xlsx', '.xls')):
            try:
                xls = pd.ExcelFile(io.BytesIO(content))
                for s in xls.sheet_names:
                    df = pd.read_excel(xls, sheet_name=s, header=None)
                    num = df.apply(pd.to_numeric, errors='coerce').notna().sum().sum()
                    if num < 5: continue
                    if not df.empty: dfs.append(df)
                if dfs: return dfs
            except: pass
        text_content = ""
        for enc in ['utf-8', 'cp949', 'latin1']:
            try: text_content = content.decode(enc); break
            except: continue
        if text_content:
            lines = text_content.splitlines()
            data_start = -1; delimiter = None
            separators = [',', '\t', ';', '\s+']
            for i, line in enumerate(lines[:100]):
                for sep in separators:
                    parts = line.split() if sep=='\s+' else line.split(sep)
                    nums = sum(1 for p in parts if re.match(r'^-?\d+(\.\d+)?(?:[eE][+-]?\d+)?$', p.strip()))
                    if nums >= 2: data_start = i; delimiter = sep; break
                if data_start != -1: break
            if data_start != -1:
                try:
                    data_str = "\n".join(lines[data_start:])
                    sep_arg = '\s+' if delimiter=='\s+' else delimiter
                    df = pd.read_csv(io.StringIO(data_str), sep=sep_arg, header=None, engine='python')
                    dfs.append(df)
                except: pass
        return dfs

    # --- Block Parser ---
    @staticmethod
    def detect_structured_blocks(df: pd.DataFrame) -> List[pd.DataFrame]:
        blocks = []
        if df.empty: return []
        df_num = df.apply(pd.to_numeric, errors='coerce')
        if df.shape[1] > 0: is_data_row = df_num.iloc[:, 0].notna()
        else: is_data_row = df_num.notna().any(axis=1)
        groups = (is_data_row != is_data_row.shift()).cumsum()
        for _, group in df.groupby(groups):
            if is_data_row.loc[group.index[0]]:
                block = df_num.loc[group.index].dropna(how='all', axis=1)
                if block.shape[0] >= 10: blocks.append(block.reset_index(drop=True))
        return blocks

    @staticmethod
    def split_horizontal_blocks(df: pd.DataFrame) -> List[pd.DataFrame]:
        sub_blocks = []
        if df.empty: return []
        col_counts = df.apply(pd.to_numeric, errors='coerce').notna().sum()
        valid = np.where(col_counts >= 2)[0]
        if len(valid) == 0: return []
        splits = np.where(np.diff(valid) > 1)[0] + 1
        groups = np.split(valid, splits)
        for g in groups:
            if len(g) > 0:
                sub = df.iloc[:, g].reset_index(drop=True)
                if sub.shape[1] >= 1 and sub.shape[0] >= 2: sub_blocks.append(sub)
        return sub_blocks

    @staticmethod
    def extract_series_from_df(df: pd.DataFrame) -> List[Dict]:
        series_list = []
        try:
            vals = df.values; rows, cols = vals.shape
            if rows < 5: return []
            if cols == 1:
                y = vals[:, 0]; mask = ~np.isnan(y)
                if np.sum(mask) > 5: series_list.append({"x": np.arange(len(y))[mask], "y": y[mask], "name": "Single"})
                return series_list
            def is_axis(arr):
                arr = arr[~np.isnan(arr)]
                if len(arr) < 5: return False
                diff = np.diff(arr)
                if np.all(diff >= 0) or np.all(diff <= 0): return True
                return (np.sum(diff>0)/len(diff)>0.9) or (np.sum(diff<0)/len(diff)>0.9)
            current_x = vals[:, 0]
            if not is_axis(current_x): current_x = np.arange(rows)
            for i in range(1, cols):
                col_data = vals[:, i]; mask_valid = ~np.isnan(col_data)
                if np.sum(mask_valid) < 5: continue
                if is_axis(col_data): current_x = col_data; continue 
                mask = ~np.isnan(current_x) & ~np.isnan(col_data)
                if np.sum(mask) > 5:
                    series_list.append({"x": current_x[mask], "y": col_data[mask], "name": f"Col-{i}"})
        except: pass
        return series_list

    # --- Spectrum Logic ---
    @staticmethod
    def simple_baseline(y):
        try:
            w = max(5, len(y)//10)
            s = pd.Series(y).rolling(window=w, center=True).min()
            b = s.bfill().ffill().values 
            if len(b) > 51: b = savgol_filter(b, 51, 3)
            return b, w
        except: return np.zeros_like(y), 0

    @staticmethod
    def process_spectrum(x, y, mode, goal):
        log = []; y_raw = y.copy(); base = np.zeros_like(y); y_proc = y.copy()
        peaks = []; fits = []; stats_txt = "Error"
        try:
            if mode == "None":
                peaks, _ = find_peaks(y, height=np.max(y)*0.05)
                log.append("Mode: Raw")
                stats_txt = f"Raw Peaks: {len(peaks)}"
            else:
                win = 15; do_fit = False
                if mode == "AI-Adaptive":
                    log.append("Mode: AI-Adaptive")
                    if "noise" in goal.lower(): win = 31
                    if "fit" in goal.lower(): do_fit = True
                else: log.append("Mode: Auto")
                base, bw = ScienceProcessorNV20.simple_baseline(y)
                y_proc = np.maximum(y - base, 0)
                log.append(f"Base: Rolling Min (w={bw})")
                if len(y_proc) > win:
                    y_proc = savgol_filter(y_proc, win, 3)
                    log.append(f"Smooth: SavGol (w={win})")
                peaks, _ = find_peaks(y_proc, height=np.max(y_proc)*0.05, distance=10)
                log.append(f"Peaks: {len(peaks)}")
                stats_txt = f"Range: {np.min(x):.1f}~{np.max(x):.1f}, Peaks: {len(peaks)}"
            return {"x": x, "y_raw": y_raw, "y_proc": y_proc, "y_base": base, "peaks": [{"x": float(x[p]), "y": float(y_proc[p])} for p in peaks], "log": log, "fits": fits, "stats": stats_txt}
        except Exception as e: return {"x": x, "y_raw": y_raw, "y_proc": y_proc, "y_base": base, "peaks": [], "log": [str(e)], "fits": [], "stats": "Error"}

    # --- Footer ---
    @staticmethod
    def separate_footer(img_bgr):
        h, w = img_bgr.shape[:2]
        roi_h = int(h * 0.82)
        roi = img_bgr[roi_h:, :]
        gray = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)
        sobel_y = np.abs(cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=3))
        row_sums = np.sum(sobel_y, axis=1)
        split_idx = np.argmax(row_sums)
        if row_sums[split_idx] > w * 30:
            real_split = roi_h + split_idx
            return img_bgr[:real_split, :], img_bgr[real_split:, :]
        return img_bgr, None

    # --- [E] NV18 Crystal Engine (Visual + DC Fix) ---
    @staticmethod
    def analyze_crystal_fft_nmf(img_bgr, patch_size=128, step=16, n_components=10):
        log = []
        gray = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2GRAY)
        h, w = gray.shape
        
        log.append(f"FFT-NMF: Patch={patch_size}, Step={step}, K={n_components}")
        
        custom_colors = [
            (0, 0, 255), (0, 255, 0), (255, 0, 0), (0, 255, 255), (255, 255, 0),
            (255, 0, 255), (0, 165, 255), (238, 130, 238), (128, 128, 0), (19, 69, 139)
        ]
        while len(custom_colors) < n_components: custom_colors.append((255, 255, 255))
        
        cols = [] 
        y_steps = list(range(0, h - patch_size + 1, step))
        x_steps = list(range(0, w - patch_size + 1, step))
        grid_h = len(y_steps)
        grid_w = len(x_steps)
        
        cy, cx = patch_size // 2, patch_size // 2
        Y, X = np.ogrid[:patch_size, :patch_size]
        dist_sq = (X - cx)**2 + (Y - cy)**2
        dc_mask = dist_sq <= 4**2

        for y in y_steps:
            for x in x_steps:
                roi = gray[y:y+patch_size, x:x+patch_size]
                f = np.fft.fft2(roi)
                fshift = np.fft.fftshift(f)
                mag = np.abs(fshift) 
                mag[dc_mask] = 0
                cols.append(mag.flatten())
        
        if not cols: return {"type":"error", "msg":"Img too small"}
        
        try:
            V = np.array(cols).T.astype(np.float32)
            V = V - V.min()
            if V.max() > 0: V = V / V.max()
            
            nmf = NMF(n_components=n_components, init='nndsvda', random_state=42, max_iter=2000)
            W = nmf.fit_transform(V) 
            H = nmf.components_ 
            
            if H.shape[1] != grid_h * grid_w: raise ValueError("Shape Mismatch")
            
            idx_H = np.argmax(H, axis=0) 
            map_low = idx_H.reshape(grid_h, grid_w).astype(np.uint8)
            map_high = cv2.resize(map_low, (w, h), interpolation=cv2.INTER_NEAREST)
            
            overlay = img_bgr.copy()
            mask_layer = np.zeros_like(img_bgr)
            
            counts = Counter(idx_H)
            total = len(idx_H)
            stats = {}
            
            for i in range(n_components):
                mask = (map_high == i)
                mask_layer[mask] = custom_colors[i]
                cnt = counts.get(i, 0)
                stats[f"Pattern {i}"] = f"{cnt} patches ({cnt/total*100:.1f}%)"
                
            alpha = 0.4
            mask_idx = np.any(mask_layer > 0, axis=-1)
            overlay[mask_idx] = (overlay[mask_idx]*(1-alpha) + mask_layer[mask_idx]*alpha).astype(np.uint8)
            
            basis_combo_list = []
            for k in range(n_components):
                pat = W[:, k].reshape(patch_size, patch_size)
                pat_log = np.log1p(pat * 1000) 
                pat_norm = cv2.normalize(pat_log, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)
                pat_color = cv2.applyColorMap(pat_norm, cv2.COLORMAP_MAGMA)
                
                legend_h = 40
                legend = np.zeros((legend_h, patch_size, 3), dtype=np.uint8)
                legend[:] = custom_colors[k]
                cv2.putText(legend, f"P{k}", (5, 25), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255,255,255), 2)
                
                combo = np.vstack([legend, pat_color])
                cv2.rectangle(combo, (0,0), (combo.shape[1]-1, combo.shape[0]-1), custom_colors[k], 2)
                basis_combo_list.append(combo)
            
            full_strip = np.hstack(basis_combo_list)
            full_strip_large = cv2.resize(full_strip, None, fx=2, fy=2, interpolation=cv2.INTER_NEAREST)
            
            def to_b64(im): return base64.b64encode(cv2.imencode('.jpg', im)[1]).decode('utf-8')
            
            return {
                "type": "image", 
                "raw_b64": to_b64(img_bgr), 
                "proc_b64": to_b64(overlay),
                "footer_b64": to_b64(full_strip_large), 
                "stats": stats, 
                "log": log
            }

        except Exception as e:
            print(f"NMF Error: {e}")
            def to_b64(im): return base64.b64encode(cv2.imencode('.jpg', im)[1]).decode('utf-8')
            return {"type": "image", "raw_b64": to_b64(img_bgr), "proc_b64": to_b64(img_bgr), "stats": {"Error": str(e)}, "log": log}

    # --- [F] Diffraction ---
    @staticmethod
    def analyze_diffraction_nv7(img_bgr, user_center=None):
        log = []; stats = {}
        gray = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2GRAY)
        if user_center and len(user_center)==2 and user_center[0]>0:
            cx, cy = user_center
            log.append(f"Center: Manual [{cx}, {cy}]")
        else:
            _, th = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY+cv2.THRESH_OTSU)
            cyf, cxf = center_of_mass(th)
            if np.isnan(cyf): cyf, cxf = gray.shape[0]//2, gray.shape[1]//2
            cx, cy = int(cxf), int(cyf)
            log.append(f"Center: Auto [{cx}, {cy}]")

        g1 = cv2.GaussianBlur(gray, (3,3), 0)
        g2 = cv2.GaussianBlur(gray, (51,51), 0)
        diff = cv2.subtract(g1, g2)
        coords = peak_local_max(diff, min_distance=10, threshold_abs=20)
        enhanced = np.log1p(diff.astype(np.float32))
        enhanced = cv2.normalize(enhanced, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)
        vis = cv2.cvtColor(enhanced, cv2.COLOR_GRAY2BGR)
        cv2.drawMarker(vis, (cx, cy), (0,0,255), cv2.MARKER_CROSS, 20, 2)
        peak_list = []
        for p in coords:
            py, px = p
            if abs(py-cy)<5 and abs(px-cx)<5: continue
            cv2.circle(vis, (px, py), 3, (0,255,0), 1)
            peak_list.append(f"[{px},{py}]")
        log.append(f"Spots: {len(peak_list)}")
        def to_b64(im): return base64.b64encode(cv2.imencode('.jpg', im)[1]).decode('utf-8')
        return {"type": "image", "raw_b64": to_b64(img_bgr), "proc_b64": to_b64(vis), "stats": {"center": [cx, cy], "spots": len(peak_list)}, "log": log}

    # --- [G] SAM ---
    @staticmethod
    def analyze_sam_results_nv7(img_rgb, masks, data_type):
        overlay = img_rgb.copy()
        mask_overlay = np.zeros_like(img_rgb)
        h_img, w_img = img_rgb.shape[:2]
        stats = {"count": 0}
        log = []
        valid_masks = []
        for ann in masks:
            area = ann['area']; bbox = ann['bbox']; x, y, w, h = [int(v) for v in bbox]
            if area > h_img * w_img * 0.95 or area < 20: continue 
            if data_type == "Particle":
                if x<=1 or y<=1 or (x+w)>=w_img-1 or (y+h)>=h_img-1: continue
                mask_uint8 = (ann['segmentation'] * 255).astype(np.uint8)
                cnts, _ = cv2.findContours(mask_uint8, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
                if cnts:
                    perim = cv2.arcLength(cnts[0], True)
                    circ = 4*np.pi*area/(perim**2) if perim>0 else 0
                    if circ < 0.25: continue
            valid_masks.append(ann)

        if data_type == "Thin Film":
            sorted_masks = sorted(valid_masks, key=lambda x: x['bbox'][1])
        else:
            sorted_masks = sorted(valid_masks, key=lambda x: x['area'], reverse=True)

        thickness_vals = []; diameters = []
        for i, ann in enumerate(sorted_masks):
            m = ann['segmentation']
            bbox = ann['bbox']; x, y, w, h = [int(v) for v in bbox]
            cx, cy = int(x + w/2), int(y + h/2)
            color = (0, 255, 0) 
            if data_type == "Thin Film":
                is_bound = (i == 0 or i == len(sorted_masks)-1)
                cv2.putText(overlay, f"#{i+1}", (cx, cy), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (255, 255, 255), 3)
                if not is_bound:
                    thk = ann['area'] / w
                    thickness_vals.append(thk)
                    color = (255, 0, 0) 
                else: color = (100, 100, 100)
                if i < len(sorted_masks) - 1:
                    ys, xs = np.where(m)
                    if len(xs) > 10:
                        roughness = np.std(ys)
                        cv2.line(overlay, (x, y+h), (x+w, y+h), (0, 255, 255), 2)
                        cv2.putText(overlay, f"Int#{i+1} R:{roughness:.2f}", (x+10, y+h-5), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)
            elif data_type == "Particle":
                equivalent_diameter = 2 * np.sqrt(ann['area'] / np.pi)
                diameters.append(equivalent_diameter)
            mask_overlay[m] = color
            mask_uint8 = (m * 255).astype(np.uint8)
            cnts, _ = cv2.findContours(mask_uint8, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
            if cnts: cv2.drawContours(overlay, [cnts[0]], -1, color, 1)

        alpha = 0.35
        mask_idx = np.any(mask_overlay > 0, axis=-1)
        overlay[mask_idx] = (overlay[mask_idx]*(1-alpha) + mask_overlay[mask_idx]*alpha).astype(np.uint8)
        stats["count"] = len(sorted_masks)
        if data_type == "Thin Film":
            avg = round(np.mean(thickness_vals), 2) if thickness_vals else 0
            stats["thickness_avg"] = avg
            log.append(f"Layers: {len(sorted_masks)}, Avg Thk: {avg}")
        elif data_type == "Particle":
            avg = round(np.mean(diameters), 2) if diameters else 0
            stats["diameter_avg"] = avg
            log.append(f"Particles: {len(sorted_masks)}")
        return overlay, stats, log

    # --- Router (Overwrite Fix) ---
    @staticmethod
    def process_image_nv18(img_bytes, equipment, data_type, mode, goal, user_center=None, 
                           patch_size=128, step=16, n_components=10):
        nparr = np.frombuffer(img_bytes, np.uint8)
        img_raw = cv2.imdecode(nparr, cv2.IMREAD_COLOR)
        if img_raw is None: return {}
        
        body, footer = ScienceProcessorNV20.separate_footer(img_raw)
        footer_b64 = None
        if footer is not None:
            footer_b64 = base64.b64encode(cv2.imencode('.jpg', footer)[1]).decode('utf-8')

        if data_type == "Crystal Structure (FFT)":
            return ScienceProcessorNV20.analyze_crystal_fft_nmf(body, patch_size, step, n_components)
            
        if data_type == "2D Diffraction":
            return {**ScienceProcessorNV20.analyze_diffraction_nv7(body, user_center), "footer_b64": footer_b64}
        
        if mode == "None":
             def to_b64(im): return base64.b64encode(cv2.imencode('.jpg', im)[1]).decode('utf-8')
             return {"type":"image", "raw_b64":to_b64(body), "proc_b64":to_b64(body), "stats":{}, "log":["Mode: Raw"], "footer_b64": footer_b64}

        img_proc = body.copy(); log = []
        if mode == "Auto" or mode == "AI-Adaptive":
            img_proc = cv2.bilateralFilter(img_proc, 9, 75, 75)
            log.append("Preproc: Denoise")

        img_rgb = cv2.cvtColor(img_proc, cv2.COLOR_BGR2RGB)
        
        if mask_generator:
            try:
                masks = mask_generator.generate(img_rgb)
                log.append(f"SAM: {len(masks)} Segments")
                overlay, stats, sam_log = ScienceProcessorNV20.analyze_sam_results_nv7(img_rgb, masks, data_type)
                log.extend(sam_log)
                overlay = cv2.cvtColor(overlay, cv2.COLOR_RGB2BGR)
            except Exception as e:
                log.append(f"SAM Error: {str(e)}")
                overlay = body; stats = {}
        else:
            log.append("SAM Not Loaded")
            overlay = body; stats = {}

        def to_b64(im): return base64.b64encode(cv2.imencode('.jpg', im)[1]).decode('utf-8')
        return {"type": "image", "raw_b64": to_b64(body), "proc_b64": to_b64(overlay), "stats": stats, "log": log, "footer_b64": footer_b64}

# [4] Helper (Same)
async def analyze_vision_ollama(image_bytes: bytes, prompt: str) -> str:
    try:
        b64 = base64.b64encode(image_bytes).decode('utf-8')
        res = await asyncio.to_thread(ollama_client.chat, model=VISION_MODEL, messages=[{'role':'user','content':prompt,'images':[b64]}])
        return res['message']['content']
    except: return "Vision Error"

async def interpret_spectrum_data(raw_context: str, equipment: str) -> str:
    try:
        prompt = f"Analyze spectrum ({equipment}). Brief interpretation in Korean. Data: {raw_context}"
        res = await asyncio.to_thread(ollama_client.chat, model=TEXT_MODEL, messages=[{'role':'user','content':prompt}])
        return res['message']['content']
    except: return ""

# [5] App
app = FastAPI(title="Analyst NV20 Document Fix", lifespan=lifespan)
app.add_middleware(CORSMiddleware, allow_origins=["*"], allow_methods=["*"], allow_headers=["*"])

class TranslateReq(BaseModel): text: str

@app.post("/api/translate")
async def translate_text(req: TranslateReq):
    try:
        res = await asyncio.to_thread(ollama_client.chat, model=TEXT_MODEL, messages=[{'role':'system','content':'Translate to English.'}, {'role':'user','content':req.text}])
        return {"translated": res['message']['content']}
    except Exception as e: return {"translated": str(e)}

async def process_analysis_job(job_id: str, files_map: Dict, configs: Dict):
    try:
        JOBS[job_id]["status"] = "Processing"
        JOBS[job_id]["progress"] = 10
        final_results = []
        total_files = len(configs)
        processed_count = 0

        for filename, config in configs.items():
            if filename not in files_map: continue
            content = files_map[filename]
            eq = config.get("equipment", "General")
            data_type = config.get("data_type", "General")
            goal = config.get("goal", "")
            mode = config.get("mode", "Auto")
            
            user_center = None
            if config.get("center_coords"):
                try: user_center = [int(v) for v in config["center_coords"].split(",")]
                except: pass
            
            p_size = int(config.get("patch_size", 128))
            p_step = int(config.get("step", 16))
            n_comp = int(config.get("n_components", 10))

            fname_lower = filename.lower()
            JOBS[job_id]["step"] = f"Analyzing {filename}..."
            
            # [A] Document Logic (Fixed)
            if eq == "Document" or fname_lower.endswith(('.pdf', '.ppt', '.pptx')):
                print(f"DEBUG: Processing Document {filename}")
                try:
                    if fname_lower.endswith('.pdf'):
                        images = convert_from_bytes(content, dpi=150, fmt='jpeg')[:10]
                        full_txt = ""; pages = []
                        for idx, img in enumerate(images):
                            buf = io.BytesIO(); img.save(buf, format="JPEG")
                            b64 = base64.b64encode(buf.getvalue()).decode()
                            desc = await analyze_vision_ollama(buf.getvalue(), "한글 요약.")
                            full_txt += f"\nPage {idx+1}: {desc}"
                            pages.append({"page_num": idx+1, "image_b64": b64, "desc": desc})
                        final_results.append({"type": "pdf", "filename": filename, "equipment": "Lit", "raw_context": full_txt, "pages": pages})
                    
                    elif fname_lower.endswith(('.ppt', '.pptx')):
                        # Check for legacy binary ppt
                        if fname_lower.endswith('.ppt'):
                             final_results.append({"type": "error", "filename": filename, "msg": "Legacy .ppt files not supported. Please save as .pptx"})
                        else:
                            try:
                                prs = Presentation(io.BytesIO(content))
                                slides = []
                                full_txt = ""
                                print(f"DEBUG: PPTX has {len(prs.slides)} slides")
                                
                                for i, slide in enumerate(prs.slides):
                                    txt = ""; imgs = []
                                    # Extract Text from all shapes
                                    for s in slide.shapes:
                                        if hasattr(s, "text"):
                                            txt += s.text + "\n"
                                        # Recursively check groups? Simple check for now
                                        if s.shape_type == MSO_SHAPE_TYPE.PICTURE:
                                            try:
                                                ib64 = base64.b64encode(s.image.blob).decode('utf-8')
                                                # Optional: Describe image
                                                desc = "Image" # Fast mode
                                                imgs.append({"b64": ib64, "desc": desc})
                                            except: pass
                                    
                                    if not txt.strip(): txt = "(No text)"
                                    
                                    full_txt += f"Slide {i+1}: {txt}\n"
                                    slides.append({"slide_num": i+1, "text": txt, "images": imgs})
                                
                                final_results.append({"type": "ppt", "filename": filename, "equipment": "Lit", "raw_context": full_txt, "slides": slides})
                            except Exception as ppt_err:
                                print(f"PPTX Parse Error: {ppt_err}")
                                final_results.append({"type": "error", "filename": filename, "msg": f"PPTX Error: {ppt_err}"})

                except Exception as e: 
                    print(f"Doc Error: {e}")
                    final_results.append({"type": "error", "filename": filename, "msg": str(e)})

            elif fname_lower.endswith(('.xlsx', '.xls', '.csv', '.txt')):
                try:
                    dfs = ScienceProcessorNV20.read_universal_dataframe(content, filename)
                    for i, df in enumerate(dfs):
                        v_blocks = ScienceProcessorNV20.detect_structured_blocks(df)
                        for v_block in v_blocks:
                            h_blocks = ScienceProcessorNV20.split_horizontal_blocks(v_block)
                            for h_block in h_blocks:
                                series_list = ScienceProcessorNV20.extract_series_from_df(h_block)
                                for s in series_list:
                                    res = ScienceProcessorNV20.process_spectrum(s['x'], s['y'], mode, goal)
                                    step = max(1, len(s['x'])//100)
                                    chart = [{"x": float(res["x"][k]), "y_proc": float(res["y_proc"][k]), "y_raw": float(res["y_raw"][k]), "y_base": float(res["y_base"][k])} for k in range(0, len(s['x']), step)]
                                    ctx = f"Data: {s['name']}\nStats: {res['stats']}"
                                    interp = await interpret_spectrum_data(ctx, eq)
                                    final_results.append({
                                        "type": "spectrum", "filename": f"{filename} ({s['name']})", "equipment": eq,
                                        "raw_context": f"{ctx}\nInterp: {interp}", 
                                        "chart_data": chart, "log": res["log"], "interpretation": interp
                                    })
                except Exception as ex: final_results.append({"type": "error", "filename": filename, "msg": str(ex)})

            elif fname_lower.endswith(('.png', '.jpg', '.jpeg', '.tif', '.bmp')):
                try:
                    JOBS[job_id]["step"] = f"Vision Analysis: {filename}..."
                    vis_res = await asyncio.to_thread(ScienceProcessorNV20.process_image_nv18, content, eq, data_type, mode, goal, user_center, p_size, p_step, n_comp)
                    if vis_res:
                        scale_info = "Unknown"
                        final_footer = vis_res.get("footer_b64")
                        if final_footer and data_type != "Crystal Structure (FFT)":
                            JOBS[job_id]["step"] = f"Scale Reading..."
                            scale_text = await analyze_vision_ollama(base64.b64decode(final_footer), "Read scale bar text only.")
                            scale_info = scale_text
                            vis_res["log"].append(f"Scale: {scale_text}")

                        stats_clean = ""
                        for k, v in vis_res.get('stats',{}).items():
                            stats_clean += f"- {k}: {v}\n"
                        
                        desc_prompt = ""
                        if data_type == "Crystal Structure (FFT)":
                            desc_prompt = f"Analyze this TEM FFT result. NMF patterns found: \n{stats_clean}\n. Explain the distribution of these phases (Amorphous vs Crystalline) based on the image map. Scale: {scale_info}. Goal: {goal}. Summary in Korean."
                        elif data_type == "Particle":
                            desc_prompt = f"Analyze Particle image. Stats: \n{stats_clean}\n. Discuss particle size distribution and density. Scale: {scale_info}. Goal: {goal}. Summary in Korean."
                        elif data_type == "Thin Film":
                            desc_prompt = f"Analyze Thin Film cross-section. Stats: \n{stats_clean}\n. Discuss layer thickness uniformity and interface roughness. Scale: {scale_info}. Goal: {goal}. Summary in Korean."
                        else:
                            desc_prompt = f"Analyze {eq} image. Stats: \n{stats_clean}\n. Goal: {goal}. Summary in Korean."

                        desc = await analyze_vision_ollama(base64.b64decode(vis_res["proc_b64"]), desc_prompt)
                        
                        final_results.append({
                            "type": "image", "filename": filename, "equipment": eq, "summary": desc,
                            "raw_context": f"Img {eq}: {desc}\nStats:\n{stats_clean}", 
                            "raw_b64": vis_res["raw_b64"], "proc_b64": vis_res["proc_b64"], "footer_b64": final_footer,
                            "stats": vis_res.get("stats",{}), "log": vis_res.get("log", [])
                        })
                except Exception as ex: final_results.append({"type": "error", "filename": filename, "msg": str(ex)})
            
            processed_count += 1
            JOBS[job_id]["progress"] = 10 + int((processed_count / total_files) * 80)

        JOBS[job_id]["step"] = "Writing Report..."
        final_results = sanitize_json(final_results)
        
        data_ctx = ""
        for r in final_results:
            if r.get("type") != "error":
                raw = r.get("raw_context") or ""
                data_ctx += f"\n=== File: {r['filename']} ({r.get('equipment')}) ===\n{raw[:2000]}\n"

        final_report = "Fail"
        if data_ctx:
            try:
                system_prompt = "You are a research assistant. Write a comprehensive report in Korean based on the provided data summaries. Include a data summary table. Do not use LaTeX."
                res = await asyncio.to_thread(ollama_client.chat, model=TEXT_MODEL, messages=[{'role':'system','content':system_prompt}, {'role':'user','content':f"Data Summaries:\n{data_ctx}"}])
                final_report = res['message']['content']
            except Exception as e: final_report = str(e)

        JOBS[job_id]["results"] = {"results": final_results, "final_report": final_report}
        JOBS[job_id]["status"] = "Completed"
        JOBS[job_id]["progress"] = 100
    except Exception as e:
        JOBS[job_id]["status"] = "Failed"; JOBS[job_id]["error"] = str(e)

@app.post("/api/analyze")
async def start_analysis(background_tasks: BackgroundTasks, files: List[UploadFile] = File(...), file_configs: str = Form(...)):
    job_id = str(uuid.uuid4()); configs = json.loads(file_configs)
    files_map = {f.filename: await f.read() for f in files}
    JOBS[job_id] = {"status": "Pending", "progress": 0, "step": "Init", "results": None}
    background_tasks.add_task(process_analysis_job, job_id, files_map, configs)
    return {"job_id": job_id}

@app.get("/api/status/{job_id}")
async def get_status(job_id: str): return JOBS.get(job_id, {"status": "Not Found"})

@app.get("/", response_class=HTMLResponse)
async def serve_index():
    if os.path.exists("index.html"): return FileResponse("index.html")
    return "<h1>NV20 Document Fix Backend</h1>"

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)






최근 수정된 프론트엔ㄷ

<!DOCTYPE html>
<html lang="ko">
<head>
    <meta charset="UTF-8">
    <title>Analyst NV19 Report Fixed</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <script src="https://unpkg.com/vue@3/dist/vue.global.js"></script>
    <script src="https://cdn.plot.ly/plotly-2.27.0.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/marked/marked.min.js"></script>
    <script src="https://unpkg.com/lucide@latest"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/html2pdf.js/0.10.1/html2pdf.bundle.min.js"></script>
    <style>
        @import url('https://fonts.googleapis.com/css2?family=Inter:wght@400;600;700&display=swap');
        body { font-family: 'Inter', sans-serif; }
        
        /* PDF 저장용 스타일 */
        .prose table { border-collapse: collapse; width: 100%; margin: 1em 0; }
        .prose th, .prose td { border: 1px solid #cbd5e1; padding: 6px; font-size: 0.8rem; } /* 폰트 축소 */
        .prose th { background-color: #f1f5f9; }
        
        .custom-scrollbar::-webkit-scrollbar { width: 6px; }
        .custom-scrollbar::-webkit-scrollbar-track { background: #f1f1f1; }
        .custom-scrollbar::-webkit-scrollbar-thumb { background: #cbd5e1; border-radius: 3px; }
        
        /* [중요] 페이지 넘김 방지 클래스 */
        .avoid-break {
            page-break-inside: avoid !important;
            break-inside: avoid !important;
        }
    </style>
</head>
<body class="bg-slate-100 text-slate-800">
    <div id="app" class="min-h-screen p-6">
        <header class="max-w-7xl mx-auto mb-6 flex items-center gap-3">
            <div class="bg-emerald-600 p-2 rounded-lg shadow"><i data-lucide="microscope" class="w-6 h-6 text-white"></i></div>
            <div><h1 class="text-2xl font-bold">Analyst NV19 <span class="text-sm font-normal text-slate-500">Perfect PDF Report</span></h1></div>
        </header>

        <main class="max-w-7xl mx-auto grid grid-cols-1 lg:grid-cols-12 gap-6">
            <div class="lg:col-span-4 space-y-4">
                <div class="bg-white p-5 rounded-xl shadow-sm border border-slate-200">
                    <h2 class="font-bold mb-3 flex items-center gap-2"><i data-lucide="files" class="w-4 h-4"></i> Data Staging</h2>
                    <div class="border-2 border-dashed border-slate-300 rounded-lg p-6 text-center hover:bg-emerald-50 cursor-pointer" @click="$refs.f.click()">
                        <input type="file" ref="f" multiple class="hidden" @change="addFiles">
                        <p class="text-sm font-semibold text-slate-600">Add Files</p>
                    </div>
                    <div v-if="stagedFiles.length" class="mt-4 space-y-3 max-h-[60vh] overflow-y-auto pr-1 custom-scrollbar">
                        <div v-for="(item, i) in stagedFiles" :key="i" class="p-3 bg-slate-50 border rounded-lg text-sm">
                            <div class="flex justify-between items-center mb-2">
                                <span class="font-bold truncate w-2/3">{{ item.file.name }}</span>
                                <button @click="removeFile(i)" class="text-red-400 hover:text-red-600"><i data-lucide="trash-2" class="w-4 h-4"></i></button>
                            </div>
                            <div class="grid grid-cols-1 gap-2">
                                <select v-model="item.equipment" class="w-full p-1.5 border rounded text-xs bg-white font-semibold text-emerald-700">
                                    <optgroup label="Spectroscopy (1D)">
                                        <option value="XPS">XPS</option>
                                        <option value="XRD">XRD (1D)</option>
                                        <option value="EELS">EELS</option>
                                        <option value="EDS">EDS</option>
                                        <option value="FT-IR">FT-IR</option>
                                        <option value="NMR">NMR</option>
                                        <option value="UV-VIS">UV-VIS</option>
                                        <option value="PL">Photo-luminescence</option>
                                        <option value="Time-resolved">Time-resolved</option>
                                    </optgroup>
                                    <optgroup label="Microscopy (2D)">
                                        <option value="SEM">SEM</option>
                                        <option value="TEM">TEM</option>
                                        <option value="AFM">AFM</option>
                                        <option value="Optical">Optical</option>
                                        <option value="2D Spectrum">2D Spectrum</option>
                                    </optgroup>
                                    <optgroup label="Diffraction (2D)">
                                        <option value="Diffraction">2D Diffraction</option>
                                    </optgroup>
                                    <optgroup label="Literature">
                                        <option value="Document">Document</option>
                                    </optgroup>
                                </select>
                                
                                <select v-if="isImage(item)" v-model="item.dataType" class="w-full p-1.5 border rounded text-xs bg-slate-50 font-bold">
                                    <option v-if="item.equipment==='Diffraction'" value="2D Diffraction">2D Diffraction</option>
                                    <option v-else value="General">General Object</option>
                                    <option v-if="item.equipment!=='Diffraction'" value="Particle">Particle Analysis</option>
                                    <option v-if="item.equipment!=='Diffraction'" value="Thin Film">Thin Film</option>
                                    <option v-if="item.equipment!=='Diffraction'" value="Crystal Structure (FFT)">Crystal Structure (FFT)</option>
                                </select>

                                <div v-if="item.dataType === 'Crystal Structure (FFT)'" class="flex gap-1">
                                    <input v-model="item.patchSize" type="number" placeholder="P(128)" class="w-1/3 p-1 border rounded text-[10px]">
                                    <input v-model="item.step" type="number" placeholder="S(16)" class="w-1/3 p-1 border rounded text-[10px]">
                                    <input v-model="item.nComp" type="number" placeholder="K(10)" class="w-1/3 p-1 border rounded text-[10px]">
                                </div>
                                <div v-if="item.dataType === '2D Diffraction'" class="flex gap-2">
                                    <input v-model="item.centerX" type="number" placeholder="CX" class="w-1/2 p-1.5 border rounded text-xs">
                                    <input v-model="item.centerY" type="number" placeholder="CY" class="w-1/2 p-1.5 border rounded text-xs">
                                </div>

                                <input v-model="item.goal" type="text" placeholder="Goal / Prompt (e.g. Find noise)" class="w-full p-1.5 border rounded text-xs">

                                <select v-if="!isDoc(item)" v-model="item.mode" class="w-full p-1.5 border rounded text-xs bg-white">
                                    <option value="Auto">✨ Auto Process</option>
                                    <option value="None">🚫 Raw Data</option>
                                    <option value="AI-Adaptive">🧠 Agent Mode</option>
                                </select>
                            </div>
                        </div>
                    </div>
                    <button @click="startAnalysis" :disabled="isAnalyzing || stagedFiles.length===0" class="w-full mt-4 py-3 bg-emerald-600 text-white rounded-lg font-bold flex justify-center gap-2 shadow hover:bg-emerald-700 disabled:opacity-50">
                        <span>Generate Report</span>
                    </button>
                </div>
            </div>

            <div class="lg:col-span-8">
                <div v-if="isAnalyzing" class="bg-white p-6 rounded-xl shadow border border-slate-200 text-center py-12">
                    <div class="animate-spin rounded-full h-12 w-12 border-b-2 border-emerald-600 mx-auto mb-4"></div>
                    <h3 class="text-lg font-bold text-slate-700">Analyzing...</h3>
                    <p class="text-sm text-slate-500 mb-4">{{ currentStep }}</p>
                    <div class="w-full bg-slate-200 rounded-full h-2.5 mb-2">
                        <div class="bg-emerald-600 h-2.5 rounded-full transition-all duration-500" :style="{width: progress + '%'}"></div>
                    </div>
                    <p class="text-xs text-right text-slate-400">{{ progress }}%</p>
                </div>

                <div v-if="result && !isAnalyzing" class="space-y-6 animate-fade-in">
                    <div class="flex justify-between items-center bg-white p-2 rounded-xl shadow-sm border border-slate-200">
                        <div class="flex gap-2">
                            <button @click="setLang('ko')" :class="lang==='ko'?'bg-emerald-100 text-emerald-700 font-bold':'text-slate-500'" class="px-4 py-2 rounded text-sm">KR</button>
                            <button @click="setLang('en')" :class="lang==='en'?'bg-blue-100 text-blue-700 font-bold':'text-slate-500'" class="px-4 py-2 rounded text-sm">EN</button>
                        </div>
                        <button @click="dlPDF" class="px-3 py-1.5 bg-slate-800 text-white text-xs rounded hover:bg-slate-700"><i class="lucide-file-down mr-1"></i> Save PDF</button>
                    </div>

                    <div id="report-view" class="bg-white p-8 rounded-xl shadow-sm border border-slate-200 text-sm">
                        <div class="mb-6 border-b pb-4">
                            <h1 class="text-2xl font-bold text-slate-800">Scientific Analysis Report</h1>
                            <p class="text-xs text-slate-500 mt-1">Generated by Analyst NV19 • {{ new Date().toLocaleString() }}</p>
                        </div>

                        <section class="mb-10 avoid-break">
                            <h2 class="text-lg font-bold text-emerald-700 mb-3 border-b pb-1">1. Synthesis</h2>
                            <div class="prose prose-sm text-slate-700 max-w-none text-[11px] leading-relaxed" v-html="md(displayReport)"></div>
                        </section>

                        <section>
                            <h2 class="text-lg font-bold text-slate-800 mb-5 border-b pb-1 avoid-break">2. Data Evidence</h2>
                            
                            <div v-for="(item, idx) in result.results" :key="idx" class="mb-8 p-4 bg-slate-50 rounded-lg border avoid-break">
                                <div class="flex items-center justify-between mb-3">
                                    <div class="flex items-center gap-2">
                                        <span class="px-2 py-0.5 bg-white border rounded text-[10px] font-bold uppercase text-slate-500">{{ item.equipment }}</span>
                                        <h3 class="font-bold text-sm">{{ item.filename }}</h3>
                                    </div>
                                    <button @click="item.showDocs = !item.showDocs" class="text-xs text-emerald-600 font-bold hover:underline flex items-center gap-1" data-html2canvas-ignore="true">
                                        {{ item.showDocs ? 'Hide Info' : 'Show Info' }}
                                    </button>
                                </div>

                                <div v-if="item.chart_data" class="bg-white p-2 rounded border mb-3 w-full">
                                    <div class="flex gap-2 justify-end mb-1" data-html2canvas-ignore="true">
                                        <button @click="toggleTrace(idx, 0)" class="text-[10px] px-2 py-0.5 border rounded">Raw</button>
                                        <button @click="toggleTrace(idx, 1)" class="text-[10px] px-2 py-0.5 border rounded bg-amber-50">Base</button>
                                        <button @click="toggleTrace(idx, 2)" class="text-[10px] px-2 py-0.5 border rounded font-bold text-emerald-600">Proc</button>
                                    </div>
                                    <div :id="'chart-'+idx" class="w-full h-60"></div>
                                    <div v-if="item.interpretation && item.showDocs" class="mt-3 p-3 bg-emerald-50 border border-emerald-100 rounded text-xs text-emerald-800">
                                        <div class="prose prose-xs" v-html="md(item.interpretation)"></div>
                                    </div>
                                </div>

                                <div v-if="item.raw_b64" class="bg-white p-2 rounded border mb-3 text-center">
                                    <div class="flex gap-2 mb-2 text-xs justify-center" data-html2canvas-ignore="true">
                                        <button @click="item.view='proc'" :class="item.view!=='raw'?'font-bold text-emerald-600':''">Processed</button>
                                        <span class="text-slate-300">|</span>
                                        <button @click="item.view='raw'" :class="item.view==='raw'?'font-bold text-emerald-600':''">Original</button>
                                    </div>
                                    <img :src="'data:image/jpeg;base64,' + (item.view==='raw' ? item.raw_b64 : item.proc_b64)" class="max-h-80 max-w-full mx-auto object-contain bg-black">
                                    
                                    <div v-if="item.footer_b64" class="mt-3 border-t pt-2">
                                        <p class="text-[10px] font-bold text-slate-500 mb-1 text-left">Basis Patterns (W Matrix) & Legend</p>
                                        <img :src="'data:image/jpeg;base64,' + item.footer_b64" class="w-full object-contain border bg-white">
                                    </div>

                                    <div v-if="item.stats && Object.keys(item.stats).length > 0" class="mt-2 p-2 bg-emerald-50 border border-emerald-100 rounded text-[10px] grid grid-cols-2 gap-2 text-left">
                                        <div v-for="(v, k) in item.stats" :key="k">
                                            <span class="font-bold text-emerald-700 capitalize">{{ k }}:</span> {{ v }}
                                        </div>
                                    </div>
                                    <div v-if="item.summary" class="mt-2 text-[10px] text-slate-700 text-left prose prose-sm bg-slate-50 p-2 leading-tight" v-html="md(item.summary)"></div>
                                </div>

                                <div v-if="item.pages || item.slides" class="mt-4 border-t pt-4">
                                    <h4 class="font-bold text-slate-600 mb-3 text-xs flex items-center">
                                        <i class="lucide-book-open w-3 h-3 mr-2"></i> Page Analysis
                                    </h4>
                                    <div class="grid grid-cols-1 gap-4">
                                        <div v-if="item.pages" v-for="p in item.pages" :key="p.page_num" class="flex gap-4 bg-slate-50 p-3 rounded border avoid-break">
                                            <img :src="'data:image/jpeg;base64,'+p.image_b64" class="w-24 h-auto border bg-white object-contain">
                                            <div class="flex-1 min-w-0 text-[10px] leading-tight prose" v-html="md(p.desc)"></div>
                                        </div>
                                        <div v-if="item.slides" v-for="s in item.slides" :key="s.slide_num" class="bg-slate-50 p-3 rounded border avoid-break">
                                            <div class="text-[10px] font-bold text-emerald-600 mb-1">Slide {{s.slide_num}}</div>
                                            <div class="text-[10px] mb-2 leading-tight">{{ s.text.substring(0, 300) }}...</div>
                                            <div v-if="s.images.length" class="flex gap-2">
                                                <img v-for="(img, imx) in s.images.slice(0,3)" :key="imx" :src="'data:image/jpeg;base64,'+img.b64" class="h-16 w-auto border">
                                            </div>
                                        </div>
                                    </div>
                                </div>

                                <div v-if="item.log && item.showDocs" class="mt-2 p-2 bg-gray-100 rounded border text-[9px] font-mono text-gray-600 avoid-break">
                                    <ul class="list-disc pl-4 space-y-0"><li v-for="l in item.log">{{ l }}</li></ul>
                                </div>
                            </div>
                        </section>
                    </div>
                </div>
            </div>
        </main>
    </div>
    <script>
        const { createApp, ref, computed, nextTick } = Vue;
        createApp({
            setup() {
                const stagedFiles = ref([]);
                const isAnalyzing = ref(false);
                const isTranslating = ref(false);
                const progress = ref(0);
                const currentStep = ref("");
                const result = ref(null);
                const lang = ref('ko');
                const translatedReport = ref('');
                const md = (t) => marked.parse(t||'');
                const displayReport = computed(() => lang.value === 'en' && translatedReport.value ? translatedReport.value : (result.value ? result.value.final_report : ''));

                const addFiles = (e) => {
                    Array.from(e.target.files).forEach(f => {
                        let eq = 'SEM'; let dt = 'General';
                        if(f.name.match(/\.(pdf|ppt|pptx)$/i)) eq = 'Document';
                        else if(f.name.match(/\.(xlsx|csv|txt)$/i)) eq = 'XPS';
                        stagedFiles.value.push({ 
                            file: f, equipment: eq, dataType: dt, goal: '', mode: 'Auto', view: 'proc', showDocs: true, 
                            centerX: null, centerY: null,
                            patchSize: 128, step: 16, nComp: 10
                        });
                    });
                    e.target.value = ''; 
                };
                const removeFile = (i) => stagedFiles.value.splice(i, 1);
                const isDoc = (item) => item.equipment === 'Document';
                const isImage = (item) => ['SEM','TEM','AFM','Optical','Diffraction','2D Spectrum'].includes(item.equipment);

                const startAnalysis = async () => {
                    isAnalyzing.value = true; progress.value = 0; currentStep.value = "Init...";
                    const fd = new FormData();
                    const configs = {};
                    stagedFiles.value.forEach(item => {
                        fd.append('files', item.file);
                        configs[item.file.name] = { 
                            equipment: item.equipment, 
                            data_type: item.equipment === 'Diffraction' ? '2D Diffraction' : item.dataType,
                            goal: item.goal, mode: item.mode,
                            center_coords: (item.centerX && item.centerY) ? `${item.centerX},${item.centerY}` : null,
                            patch_size: item.patchSize, step: item.step, n_components: item.nComp
                        };
                    });
                    fd.append('file_configs', JSON.stringify(configs));
                    try {
                        const initRes = await fetch('/api/analyze', { method: 'POST', body: fd });
                        const { job_id } = await initRes.json();
                        const poll = setInterval(async () => {
                            const statusRes = await fetch(`/api/status/${job_id}`);
                            const status = await statusRes.json();
                            progress.value = status.progress || 0;
                            currentStep.value = status.step || "Processing...";
                            if (status.status === "Completed") {
                                clearInterval(poll);
                                result.value = status.results;
                                if(result.value.results) result.value.results.forEach(r => r.showDocs = true);
                                isAnalyzing.value = false;
                                await nextTick();
                                renderCharts(result.value.results);
                            } else if (status.status === "Failed") {
                                clearInterval(poll); alert("Error: " + status.error); isAnalyzing.value = false;
                            }
                        }, 1000);
                    } catch(e) { alert(e); isAnalyzing.value = false; }
                };

                const renderCharts = (items) => {
                    items.forEach((item, i) => {
                        if(item.chart_data) {
                            const raw = { x: item.chart_data.map(d=>d.x), y: item.chart_data.map(d=>d.y_raw), mode:'lines', name:'Raw', line:{color:'#cbd5e1', width:1}, visible: true };
                            const base = { x: item.chart_data.map(d=>d.x), y: item.chart_data.map(d=>d.y_base), mode:'lines', name:'Base', line:{color:'#f59e0b', width:1, dash:'dot'}, visible: true };
                            const proc = { x: item.chart_data.map(d=>d.x), y: item.chart_data.map(d=>d.y_proc), mode:'lines', name:'Proc', line:{color:'#059669', width:2}, visible: true };
                            
                            // [Fix] Plotly Config for PDF
                            const layout = {
                                margin:{t:10,b:30,l:40,r:10}, 
                                showlegend:false,
                                autosize: true, // 반응형
                                height: 240,    // 고정 높이 (잘림 방지)
                                font: { size: 10 } // 작은 폰트
                            };
                            const config = { displayModeBar: false, responsive: true };
                            
                            Plotly.newPlot('chart-'+i, [raw, base, proc], layout, config);
                        }
                    });
                    lucide.createIcons();
                };

                const toggleTrace = (idx, traceIdx) => {
                    const el = document.getElementById('chart-'+idx);
                    if(el) {
                        const current = el.data[traceIdx].visible;
                        const next = current === true ? 'legendonly' : true;
                        Plotly.restyle(el, {'visible': next}, [traceIdx]);
                    }
                };

                // [NV19] Advanced PDF Export
                const dlPDF = () => {
                    const now = new Date();
                    const ts = now.toISOString().slice(2,10).replace(/-/g,'') + '_' + now.toTimeString().slice(0,4).replace(':','');
                    const eqList = [...new Set(stagedFiles.value.map(f=>f.equipment))].join('-');
                    const filename = `${ts}_${eqList}_Report.pdf`;
                    
                    const element = document.getElementById('report-view');
                    
                    const opt = {
                        margin:       [10, 10, 10, 10], // 여백 (Top, Right, Bottom, Left)
                        filename:     filename,
                        image:        { type: 'jpeg', quality: 0.98 },
                        html2canvas:  { scale: 2, useCORS: true, logging: false, scrollX: 0, scrollY: 0 },
                        jsPDF:        { unit: 'mm', format: 'a4', orientation: 'portrait' },
                        // [Core Fix] Page Break Logic
                        pagebreak:    { mode: ['avoid-all', 'css', 'legacy'] }
                    };

                    // Generate PDF
                    html2pdf().set(opt).from(element).save();
                };

                const setLang = async (target) => {
                    if (target === 'ko') { lang.value = 'ko'; return; }
                    if (target === 'en') {
                        if (translatedReport.value) { lang.value = 'en'; return; }
                        isTranslating.value = true;
                        try {
                            const res = await fetch('/api/translate', {
                                method: 'POST', headers: {'Content-Type': 'application/json'},
                                body: JSON.stringify({ text: result.value.final_report })
                            });
                            const data = await res.json();
                            translatedReport.value = data.translated;
                            lang.value = 'en';
                        } catch(e) { alert("Trans Error: " + e); }
                        finally { isTranslating.value = false; }
                    }
                };

                setTimeout(()=>lucide.createIcons(), 100);
                return { stagedFiles, isAnalyzing, progress, currentStep, result, addFiles, removeFile, startAnalysis, md, toggleTrace, isDoc, isImage, dlPDF, setLang, lang, displayReport, isTranslating };
            }
        }).mount('#app');
    </script>
</body>
</html>



엔브이18 웹코드 수정 부분


                                <div v-if="item.pages || item.slides" class="mt-4 border-t pt-4">
                                    <h4 class="font-bold text-slate-600 mb-3 text-xs flex items-center">
                                        <i class="lucide-book-open w-3 h-3 mr-2"></i> Page-by-Page Analysis
                                    </h4>
                                    
                                    <div class="space-y-4 max-h-[500px] overflow-y-auto pr-2 custom-scrollbar">
                                        
                                        <div v-if="item.pages" v-for="p in item.pages" :key="p.page_num" class="flex gap-4 bg-slate-50 p-4 rounded-lg border hover:border-emerald-300 transition-colors">
                                            <div class="shrink-0">
                                                <p class="text-[10px] font-bold text-slate-400 mb-1 text-center">Page {{ p.page_num }}</p>
                                                <img :src="'data:image/jpeg;base64,'+p.image_b64" class="w-32 h-auto object-contain border bg-white shadow-sm cursor-pointer hover:scale-105 transition-transform" onclick="window.open(this.src)">
                                            </div>
                                            <div class="flex-1 min-w-0">
                                                <div class="text-xs text-slate-700 prose prose-sm leading-relaxed" v-html="md(p.desc)"></div>
                                            </div>
                                        </div>

                                        <div v-if="item.slides" v-for="s in item.slides" :key="s.slide_num" class="bg-slate-50 p-4 rounded-lg border">
                                            <div class="flex justify-between mb-2">
                                                <span class="text-xs font-bold text-emerald-600">Slide {{ s.slide_num }}</span>
                                            </div>
                                            <div class="text-xs text-slate-700 prose prose-sm mb-3 whitespace-pre-wrap">{{ s.text }}</div>
                                            <div v-if="s.images && s.images.length > 0" class="grid grid-cols-3 gap-2">
                                                <div v-for="(img, imx) in s.images" :key="imx" class="border p-1 bg-white rounded">
                                                    <img :src="'data:image/jpeg;base64,'+img.b64" class="w-full h-24 object-contain">
                                                    <p class="text-[9px] text-slate-400 text-center mt-1 truncate">{{ img.desc }}</p>
                                                </div>
                                            </div>
                                        </div>

                                    </div>
                                </div>

                                <div v-if="item.log && item.showDocs" class="mt-2 p-2 bg-gray-100 rounded border text-[10px] font-mono text-gray-600">
                                    ```

---

### 💡 전체 `index.html` (완성본)

혹시 위치를 찾기 어려우실까 봐, **수정된 전체 HTML 코드**를 드립니다. 이것을 덮어쓰시면 됩니다.

```html
<!DOCTYPE html>
<html lang="ko">
<head>
    <meta charset="UTF-8">
    <title>Analyst NV18 Final</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <script src="https://unpkg.com/vue@3/dist/vue.global.js"></script>
    <script src="https://cdn.plot.ly/plotly-2.27.0.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/marked/marked.min.js"></script>
    <script src="https://unpkg.com/lucide@latest"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/html2pdf.js/0.10.1/html2pdf.bundle.min.js"></script>
    <style>
        @import url('https://fonts.googleapis.com/css2?family=Inter:wght@400;600;700&display=swap');
        body { font-family: 'Inter', sans-serif; }
        .prose table { border-collapse: collapse; width: 100%; margin: 1em 0; }
        .prose th, .prose td { border: 1px solid #cbd5e1; padding: 8px; font-size: 0.9rem; }
        .prose th { background-color: #f1f5f9; }
        .custom-scrollbar::-webkit-scrollbar { width: 6px; }
        .custom-scrollbar::-webkit-scrollbar-track { background: #f1f1f1; }
        .custom-scrollbar::-webkit-scrollbar-thumb { background: #cbd5e1; border-radius: 3px; }
        .custom-scrollbar::-webkit-scrollbar-thumb:hover { background: #94a3b8; }
    </style>
</head>
<body class="bg-slate-100 text-slate-800">
    <div id="app" class="min-h-screen p-6">
        <header class="max-w-7xl mx-auto mb-6 flex items-center gap-3">
            <div class="bg-emerald-600 p-2 rounded-lg shadow"><i data-lucide="microscope" class="w-6 h-6 text-white"></i></div>
            <div><h1 class="text-2xl font-bold">Analyst NV18 <span class="text-sm font-normal text-slate-500">Perfected Edition</span></h1></div>
        </header>

        <main class="max-w-7xl mx-auto grid grid-cols-1 lg:grid-cols-12 gap-6">
            <div class="lg:col-span-4 space-y-4">
                <div class="bg-white p-5 rounded-xl shadow-sm border border-slate-200">
                    <h2 class="font-bold mb-3 flex items-center gap-2"><i data-lucide="files" class="w-4 h-4"></i> Data Staging</h2>
                    <div class="border-2 border-dashed border-slate-300 rounded-lg p-6 text-center hover:bg-emerald-50 cursor-pointer" @click="$refs.f.click()">
                        <input type="file" ref="f" multiple class="hidden" @change="addFiles">
                        <p class="text-sm font-semibold text-slate-600">Add Files</p>
                    </div>
                    <div v-if="stagedFiles.length" class="mt-4 space-y-3 max-h-[60vh] overflow-y-auto pr-1 custom-scrollbar">
                        <div v-for="(item, i) in stagedFiles" :key="i" class="p-3 bg-slate-50 border rounded-lg text-sm">
                            <div class="flex justify-between items-center mb-2">
                                <span class="font-bold truncate w-2/3">{{ item.file.name }}</span>
                                <button @click="removeFile(i)" class="text-red-400 hover:text-red-600"><i data-lucide="trash-2" class="w-4 h-4"></i></button>
                            </div>
                            <div class="grid grid-cols-1 gap-2">
                                <select v-model="item.equipment" class="w-full p-1.5 border rounded text-xs bg-white font-semibold text-emerald-700">
                                    <optgroup label="Spectroscopy (1D)">
                                        <option value="XPS">XPS</option>
                                        <option value="XRD">XRD (1D)</option>
                                        <option value="EELS">EELS</option>
                                        <option value="EDS">EDS</option>
                                        <option value="FT-IR">FT-IR</option>
                                        <option value="NMR">NMR</option>
                                        <option value="UV-VIS">UV-VIS</option>
                                        <option value="PL">Photo-luminescence</option>
                                        <option value="Time-resolved">Time-resolved</option>
                                    </optgroup>
                                    <optgroup label="Microscopy (2D)">
                                        <option value="SEM">SEM</option>
                                        <option value="TEM">TEM</option>
                                        <option value="AFM">AFM</option>
                                        <option value="Optical">Optical</option>
                                        <option value="2D Spectrum">2D Spectrum</option>
                                    </optgroup>
                                    <optgroup label="Diffraction (2D)">
                                        <option value="Diffraction">2D Diffraction</option>
                                    </optgroup>
                                    <optgroup label="Literature">
                                        <option value="Document">Document</option>
                                    </optgroup>
                                </select>
                                
                                <select v-if="isImage(item)" v-model="item.dataType" class="w-full p-1.5 border rounded text-xs bg-slate-50 font-bold">
                                    <option v-if="item.equipment==='Diffraction'" value="2D Diffraction">2D Diffraction</option>
                                    <option v-else value="General">General Object</option>
                                    <option v-if="item.equipment!=='Diffraction'" value="Particle">Particle Analysis</option>
                                    <option v-if="item.equipment!=='Diffraction'" value="Thin Film">Thin Film</option>
                                    <option v-if="item.equipment!=='Diffraction'" value="Crystal Structure (FFT)">Crystal Structure (FFT)</option>
                                </select>

                                <div v-if="item.dataType === 'Crystal Structure (FFT)'" class="flex gap-1">
                                    <input v-model="item.patchSize" type="number" placeholder="P(128)" class="w-1/3 p-1 border rounded text-[10px]">
                                    <input v-model="item.step" type="number" placeholder="S(16)" class="w-1/3 p-1 border rounded text-[10px]">
                                    <input v-model="item.nComp" type="number" placeholder="K(10)" class="w-1/3 p-1 border rounded text-[10px]">
                                </div>
                                <div v-if="item.dataType === '2D Diffraction'" class="flex gap-2">
                                    <input v-model="item.centerX" type="number" placeholder="CX" class="w-1/2 p-1.5 border rounded text-xs">
                                    <input v-model="item.centerY" type="number" placeholder="CY" class="w-1/2 p-1.5 border rounded text-xs">
                                </div>

                                <input v-model="item.goal" type="text" placeholder="Goal / Prompt (e.g. Find noise)" class="w-full p-1.5 border rounded text-xs">

                                <select v-if="!isDoc(item)" v-model="item.mode" class="w-full p-1.5 border rounded text-xs bg-white">
                                    <option value="Auto">✨ Auto Process</option>
                                    <option value="None">🚫 Raw Data</option>
                                    <option value="AI-Adaptive">🧠 Agent Mode</option>
                                </select>
                            </div>
                        </div>
                    </div>
                    <button @click="startAnalysis" :disabled="isAnalyzing || stagedFiles.length===0" class="w-full mt-4 py-3 bg-emerald-600 text-white rounded-lg font-bold flex justify-center gap-2 shadow hover:bg-emerald-700 disabled:opacity-50">
                        <span>Generate Report</span>
                    </button>
                </div>
            </div>

            <div class="lg:col-span-8">
                <div v-if="isAnalyzing" class="bg-white p-6 rounded-xl shadow border border-slate-200 text-center py-12">
                    <div class="animate-spin rounded-full h-12 w-12 border-b-2 border-emerald-600 mx-auto mb-4"></div>
                    <h3 class="text-lg font-bold text-slate-700">Analyzing...</h3>
                    <p class="text-sm text-slate-500 mb-4">{{ currentStep }}</p>
                    <div class="w-full bg-slate-200 rounded-full h-2.5 mb-2">
                        <div class="bg-emerald-600 h-2.5 rounded-full transition-all duration-500" :style="{width: progress + '%'}"></div>
                    </div>
                    <p class="text-xs text-right text-slate-400">{{ progress }}%</p>
                </div>

                <div v-if="result && !isAnalyzing" class="space-y-6 animate-fade-in">
                    <div class="flex justify-between items-center bg-white p-2 rounded-xl shadow-sm border border-slate-200">
                        <div class="flex gap-2">
                            <button @click="setLang('ko')" :class="lang==='ko'?'bg-emerald-100 text-emerald-700 font-bold':'text-slate-500'" class="px-4 py-2 rounded text-sm">KR</button>
                            <button @click="setLang('en')" :class="lang==='en'?'bg-blue-100 text-blue-700 font-bold':'text-slate-500'" class="px-4 py-2 rounded text-sm">EN</button>
                        </div>
                        <button @click="dlPDF" class="px-3 py-1.5 bg-slate-800 text-white text-xs rounded hover:bg-slate-700"><i class="lucide-file-down mr-1"></i> Save PDF</button>
                    </div>

                    <div id="report-view" class="bg-white p-8 rounded-xl shadow-sm border border-slate-200">
                        <section class="mb-10">
                            <h2 class="text-lg font-bold text-emerald-700 mb-3 border-b pb-1">1. Synthesis</h2>
                            <div class="prose prose-sm text-slate-700 max-w-none" v-html="md(displayReport)"></div>
                        </section>
                        <section>
                            <h2 class="text-lg font-bold text-slate-800 mb-5 border-b pb-1">2. Data Evidence</h2>
                            <div v-for="(item, idx) in result.results" :key="idx" class="mb-8 p-4 bg-slate-50 rounded-lg border break-inside-avoid">
                                <div class="flex items-center justify-between mb-3">
                                    <div class="flex items-center gap-2">
                                        <span class="px-2 py-0.5 bg-white border rounded text-[10px] font-bold uppercase text-slate-500">{{ item.equipment }}</span>
                                        <h3 class="font-bold text-sm">{{ item.filename }}</h3>
                                    </div>
                                    <button @click="item.showDocs = !item.showDocs" class="text-xs text-emerald-600 font-bold hover:underline flex items-center gap-1">Info</button>
                                </div>

                                <div v-if="item.chart_data" class="bg-white p-2 rounded border mb-3">
                                    <div class="flex gap-2 justify-end mb-1">
                                        <button @click="toggleTrace(idx, 0)" class="text-[10px] px-2 py-0.5 border rounded">Raw</button>
                                        <button @click="toggleTrace(idx, 1)" class="text-[10px] px-2 py-0.5 border rounded bg-amber-50">Base</button>
                                        <button @click="toggleTrace(idx, 2)" class="text-[10px] px-2 py-0.5 border rounded font-bold text-emerald-600">Proc</button>
                                    </div>
                                    <div :id="'chart-'+idx" class="w-full h-60"></div>
                                    <div v-if="item.interpretation && item.showDocs" class="mt-3 p-3 bg-emerald-50 border border-emerald-100 rounded text-xs text-emerald-800">
                                        <div class="prose prose-xs" v-html="md(item.interpretation)"></div>
                                    </div>
                                </div>

                                <div v-if="item.raw_b64" class="bg-white p-2 rounded border mb-3">
                                    <div class="flex gap-2 mb-2 text-xs justify-center">
                                        <button @click="item.view='proc'" :class="item.view!=='raw'?'font-bold text-emerald-600':''">Processed</button>
                                        <span class="text-slate-300">|</span>
                                        <button @click="item.view='raw'" :class="item.view==='raw'?'font-bold text-emerald-600':''">Original</button>
                                    </div>
                                    <img :src="'data:image/jpeg;base64,' + (item.view==='raw' ? item.raw_b64 : item.proc_b64)" class="max-h-96 mx-auto object-contain bg-black">
                                    
                                    <div v-if="item.footer_b64" class="mt-3 border-t pt-2">
                                        <p class="text-xs font-bold text-slate-500 mb-1">Detailed Analysis (Footer / Basis Patterns)</p>
                                        <img :src="'data:image/jpeg;base64,' + item.footer_b64" class="w-full object-contain border bg-white">
                                    </div>

                                    <div v-if="item.stats && Object.keys(item.stats).length > 0" class="mt-2 p-2 bg-emerald-50 border border-emerald-100 rounded text-xs grid grid-cols-2 gap-2">
                                        <div v-for="(v, k) in item.stats" :key="k">
                                            <span class="font-bold text-emerald-700 capitalize">{{ k }}:</span> {{ v }}
                                        </div>
                                    </div>
                                    <div v-if="item.summary" class="mt-2 text-xs text-slate-600 prose bg-slate-50 p-2" v-html="md(item.summary)"></div>
                                </div>

                                <div v-if="item.pages || item.slides" class="mt-4 border-t pt-4">
                                    <h4 class="font-bold text-slate-600 mb-3 text-xs flex items-center"><i class="lucide-book-open w-3 h-3 mr-2"></i> Page Analysis</h4>
                                    <div class="space-y-4 max-h-[500px] overflow-y-auto pr-2 custom-scrollbar">
                                        <div v-if="item.pages" v-for="p in item.pages" :key="p.page_num" class="flex gap-4 bg-slate-50 p-4 rounded-lg border hover:border-emerald-300">
                                            <div class="shrink-0">
                                                <p class="text-[10px] font-bold text-slate-400 mb-1 text-center">P{{ p.page_num }}</p>
                                                <img :src="'data:image/jpeg;base64,'+p.image_b64" class="w-32 h-auto border bg-white">
                                            </div>
                                            <div class="flex-1 min-w-0">
                                                <div class="text-xs text-slate-700 prose prose-sm" v-html="md(p.desc)"></div>
                                            </div>
                                        </div>
                                        <div v-if="item.slides" v-for="s in item.slides" :key="s.slide_num" class="bg-slate-50 p-4 rounded-lg border">
                                            <span class="text-xs font-bold text-emerald-600">Slide {{ s.slide_num }}</span>
                                            <div class="text-xs text-slate-700 prose prose-sm mb-3">{{ s.text }}</div>
                                            <div v-if="s.images && s.images.length > 0" class="grid grid-cols-3 gap-2">
                                                <div v-for="(img, imx) in s.images" :key="imx" class="border p-1 bg-white rounded">
                                                    <img :src="'data:image/jpeg;base64,'+img.b64" class="w-full h-24 object-contain">
                                                </div>
                                            </div>
                                        </div>
                                    </div>
                                </div>

                                <div v-if="item.log && item.showDocs" class="mt-2 p-2 bg-gray-100 rounded border text-[10px] font-mono text-gray-600">
                                    <ul class="list-disc pl-4 space-y-0.5"><li v-for="l in item.log">{{ l }}</li></ul>
                                </div>
                            </div>
                        </section>
                    </div>
                </div>
            </div>
        </main>
    </div>
    <script>
        const { createApp, ref, computed, nextTick } = Vue;
        createApp({
            setup() {
                const stagedFiles = ref([]);
                const isAnalyzing = ref(false);
                const isTranslating = ref(false);
                const progress = ref(0);
                const currentStep = ref("");
                const result = ref(null);
                const lang = ref('ko');
                const translatedReport = ref('');
                const md = (t) => marked.parse(t||'');
                const displayReport = computed(() => lang.value === 'en' && translatedReport.value ? translatedReport.value : (result.value ? result.value.final_report : ''));

                const addFiles = (e) => {
                    Array.from(e.target.files).forEach(f => {
                        let eq = 'SEM'; let dt = 'General';
                        if(f.name.match(/\.(pdf|ppt|pptx)$/i)) eq = 'Document';
                        else if(f.name.match(/\.(xlsx|csv|txt)$/i)) eq = 'XPS';
                        stagedFiles.value.push({ 
                            file: f, equipment: eq, dataType: dt, goal: '', mode: 'Auto', view: 'proc', showDocs: true, 
                            centerX: null, centerY: null,
                            patchSize: 128, step: 16, nComp: 10
                        });
                    });
                    e.target.value = ''; 
                };
                const removeFile = (i) => stagedFiles.value.splice(i, 1);
                const isDoc = (item) => item.equipment === 'Document';
                const isImage = (item) => ['SEM','TEM','AFM','Optical','Diffraction','2D Spectrum'].includes(item.equipment);

                const startAnalysis = async () => {
                    isAnalyzing.value = true; progress.value = 0; currentStep.value = "Init...";
                    const fd = new FormData();
                    const configs = {};
                    stagedFiles.value.forEach(item => {
                        fd.append('files', item.file);
                        configs[item.file.name] = { 
                            equipment: item.equipment, 
                            data_type: item.equipment === 'Diffraction' ? '2D Diffraction' : item.dataType,
                            goal: item.goal, mode: item.mode,
                            center_coords: (item.centerX && item.centerY) ? `${item.centerX},${item.centerY}` : null,
                            patch_size: item.patchSize, step: item.step, n_components: item.nComp
                        };
                    });
                    fd.append('file_configs', JSON.stringify(configs));
                    try {
                        const initRes = await fetch('/api/analyze', { method: 'POST', body: fd });
                        const { job_id } = await initRes.json();
                        const poll = setInterval(async () => {
                            const statusRes = await fetch(`/api/status/${job_id}`);
                            const status = await statusRes.json();
                            progress.value = status.progress || 0;
                            currentStep.value = status.step || "Processing...";
                            if (status.status === "Completed") {
                                clearInterval(poll);
                                result.value = status.results;
                                if(result.value.results) result.value.results.forEach(r => r.showDocs = true);
                                isAnalyzing.value = false;
                                await nextTick();
                                renderCharts(result.value.results);
                            } else if (status.status === "Failed") {
                                clearInterval(poll); alert("Error: " + status.error); isAnalyzing.value = false;
                            }
                        }, 1000);
                    } catch(e) { alert(e); isAnalyzing.value = false; }
                };

                const renderCharts = (items) => {
                    items.forEach((item, i) => {
                        if(item.chart_data) {
                            const raw = { x: item.chart_data.map(d=>d.x), y: item.chart_data.map(d=>d.y_raw), mode:'lines', name:'Raw', line:{color:'#cbd5e1', width:1}, visible: true };
                            const base = { x: item.chart_data.map(d=>d.x), y: item.chart_data.map(d=>d.y_base), mode:'lines', name:'Base', line:{color:'#f59e0b', width:1, dash:'dot'}, visible: true };
                            const proc = { x: item.chart_data.map(d=>d.x), y: item.chart_data.map(d=>d.y_proc), mode:'lines', name:'Proc', line:{color:'#059669', width:2}, visible: true };
                            Plotly.newPlot('chart-'+i, [raw, base, proc], {margin:{t:10,b:30,l:40,r:10}, showlegend:false}, {displayModeBar:false});
                        }
                    });
                    lucide.createIcons();
                };

                const toggleTrace = (idx, traceIdx) => {
                    const el = document.getElementById('chart-'+idx);
                    if(el) {
                        const current = el.data[traceIdx].visible;
                        const next = current === true ? 'legendonly' : true;
                        Plotly.restyle(el, {'visible': next}, [traceIdx]);
                    }
                };

                const dlPDF = () => {
                    const now = new Date();
                    const ts = now.toISOString().slice(2,10).replace(/-/g,'') + '_' + now.toTimeString().slice(0,4).replace(':','');
                    const eqList = [...new Set(stagedFiles.value.map(f=>f.equipment))].join('-');
                    html2pdf().set({ margin:10, filename:`${ts}_${eqList}_Report.pdf`, image:{type:'jpeg',quality:0.98}, html2canvas:{scale:2}, jsPDF:{unit:'mm',format:'a4'} }).from(document.getElementById('report-view')).save();
                };

                const setLang = async (target) => {
                    if (target === 'ko') { lang.value = 'ko'; return; }
                    if (target === 'en') {
                        if (translatedReport.value) { lang.value = 'en'; return; }
                        isTranslating.value = true;
                        try {
                            const res = await fetch('/api/translate', {
                                method: 'POST', headers: {'Content-Type': 'application/json'},
                                body: JSON.stringify({ text: result.value.final_report })
                            });
                            const data = await res.json();
                            translatedReport.value = data.translated;
                            lang.value = 'en';
                        } catch(e) { alert("Trans Error: " + e); }
                        finally { isTranslating.value = false; }
                    }
                };

                setTimeout(()=>lucide.createIcons(), 100);
                return { stagedFiles, isAnalyzing, progress, currentStep, result, addFiles, removeFile, startAnalysis, md, toggleTrace, isDoc, isImage, dlPDF, setLang, lang, displayReport, isTranslating };
            }
        }).mount('#app');
    </script>
</body>
</html>



엔브이18 핫픽스

    # --- [E] NV18 Crystal Engine (Visual + DC Fix Combined) ---
    @staticmethod
    def analyze_crystal_fft_nmf(img_bgr, patch_size=128, step=16, n_components=10):
        log = []
        gray = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2GRAY)
        h, w = gray.shape
        
        log.append(f"FFT-NMF: Patch={patch_size}, Step={step}, K={n_components}")
        
        # 1. Custom Colormap (Neon/Bright)
        custom_colors = [
            (0, 0, 255), (0, 255, 0), (255, 0, 0), (0, 255, 255), (255, 255, 0),
            (255, 0, 255), (0, 165, 255), (238, 130, 238), (128, 128, 0), (19, 69, 139)
        ]
        while len(custom_colors) < n_components: custom_colors.append((255, 255, 255))
        
        # 2. Collect Data
        cols = [] 
        y_steps = list(range(0, h - patch_size + 1, step))
        x_steps = list(range(0, w - patch_size + 1, step))
        grid_h = len(y_steps)
        grid_w = len(x_steps)
        
        # [NV18 Feature] Circular DC Mask
        cy, cx = patch_size // 2, patch_size // 2
        Y, X = np.ogrid[:patch_size, :patch_size]
        dist_sq = (X - cx)**2 + (Y - cy)**2
        dc_mask = dist_sq <= 4**2

        for y in y_steps:
            for x in x_steps:
                roi = gray[y:y+patch_size, x:x+patch_size]
                f = np.fft.fft2(roi)
                fshift = np.fft.fftshift(f)
                mag = np.abs(fshift) 
                
                # Apply Mask
                mag[dc_mask] = 0
                cols.append(mag.flatten())
        
        if not cols: return {"type":"error", "msg":"Img too small"}
        
        try:
            # 3. NMF
            V = np.array(cols).T.astype(np.float32)
            V = V - V.min()
            if V.max() > 0: V = V / V.max()
            
            nmf = NMF(n_components=n_components, init='nndsvda', random_state=42, max_iter=2000)
            W = nmf.fit_transform(V) 
            H = nmf.components_ 
            
            if H.shape[1] != grid_h * grid_w: raise ValueError("Shape Mismatch")
            
            # 4. Spatial Map
            idx_H = np.argmax(H, axis=0) 
            map_low = idx_H.reshape(grid_h, grid_w).astype(np.uint8)
            map_high = cv2.resize(map_low, (w, h), interpolation=cv2.INTER_NEAREST)
            
            overlay = img_bgr.copy()
            mask_layer = np.zeros_like(img_bgr)
            
            counts = Counter(idx_H)
            total = len(idx_H)
            stats = {}
            
            for i in range(n_components):
                mask = (map_high == i)
                mask_layer[mask] = custom_colors[i]
                cnt = counts.get(i, 0)
                stats[f"Pattern {i}"] = f"{cnt} patches ({cnt/total*100:.1f}%)"
                
            alpha = 0.4
            mask_idx = np.any(mask_layer > 0, axis=-1)
            overlay[mask_idx] = (overlay[mask_idx]*(1-alpha) + mask_layer[mask_idx]*alpha).astype(np.uint8)
            
            # 5. [Restored] Basis Pattern Visualization (Strip + Legend)
            basis_combo_list = []
            
            for k in range(n_components):
                # Pattern Image (Heatmap)
                pat = W[:, k].reshape(patch_size, patch_size)
                pat_log = np.log1p(pat * 1000) 
                pat_norm = cv2.normalize(pat_log, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)
                pat_color = cv2.applyColorMap(pat_norm, cv2.COLORMAP_MAGMA)
                
                # Legend Block (Top)
                legend_h = 40
                legend = np.zeros((legend_h, patch_size, 3), dtype=np.uint8)
                legend[:] = custom_colors[k]
                
                # Text (P0, P1...)
                cv2.putText(legend, f"P{k}", (5, 25), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255,255,255), 2)
                
                # Stack Legend + Pattern
                combo = np.vstack([legend, pat_color])
                
                # Border
                cv2.rectangle(combo, (0,0), (combo.shape[1]-1, combo.shape[0]-1), custom_colors[k], 2)
                
                basis_combo_list.append(combo)
            
            # Horizontal Strip
            full_strip = np.hstack(basis_combo_list)
            # Scale up 2x for visibility
            full_strip_large = cv2.resize(full_strip, None, fx=2, fy=2, interpolation=cv2.INTER_NEAREST)
            
            def to_b64(im): return base64.b64encode(cv2.imencode('.jpg', im)[1]).decode('utf-8')
            
            return {
                "type": "image", 
                "raw_b64": to_b64(img_bgr), 
                "proc_b64": to_b64(overlay),
                "footer_b64": to_b64(full_strip_large), # [Check] Correct Strip
                "stats": stats, 
                "log": log
            }

        except Exception as e:
            print(f"NMF Error: {e}")
            def to_b64(im): return base64.b64encode(cv2.imencode('.jpg', im)[1]).decode('utf-8')
            return {"type": "image", "raw_b64": to_b64(img_bgr), "proc_b64": to_b64(img_bgr), "stats": {"Error": str(e)}, "log": log}

    # --- [Router Fix] Do NOT Overwrite Footer ---
    @staticmethod
    def process_image_nv18(img_bytes, equipment, data_type, mode, goal, user_center=None, 
                           patch_size=128, step=16, n_components=10):
        nparr = np.frombuffer(img_bytes, np.uint8)
        img_raw = cv2.imdecode(nparr, cv2.IMREAD_COLOR)
        if img_raw is None: return {}
        
        # Original Footer (Scale Bar)
        body, footer = ScienceProcessorNV18.separate_footer(img_raw)
        
        footer_b64 = None
        if footer is not None:
            footer_b64 = base64.b64encode(cv2.imencode('.jpg', footer)[1]).decode('utf-8')

        # [Fix] Return IMMEDIATELY for Crystal FFT to avoid overwriting footer_b64
        if data_type == "Crystal Structure (FFT)":
            # This function returns its own footer_b64 (Basis Strip)
            return ScienceProcessorNV18.analyze_crystal_fft_nmf(body, patch_size, step, n_components)
            
        if data_type == "2D Diffraction":
            return {**ScienceProcessorNV18.analyze_diffraction_nv7(body, user_center), "footer_b64": footer_b64}
        
        # General SAM Logic
        if mode == "None":
             def to_b64(im): return base64.b64encode(cv2.imencode('.jpg', im)[1]).decode('utf-8')
             return {"type":"image", "raw_b64":to_b64(body), "proc_b64":to_b64(body), "stats":{}, "log":["Mode: Raw"], "footer_b64": footer_b64}

        img_proc = body.copy(); log = []
        if mode == "Auto" or mode == "AI-Adaptive":
            img_proc = cv2.bilateralFilter(img_proc, 9, 75, 75)
            log.append("Preproc: Denoise")

        img_rgb = cv2.cvtColor(img_proc, cv2.COLOR_BGR2RGB)
        
        if mask_generator:
            try:
                masks = mask_generator.generate(img_rgb)
                log.append(f"SAM: {len(masks)} Segments")
                overlay, stats, sam_log = ScienceProcessorNV18.analyze_sam_results_nv7(img_rgb, masks, data_type)
                log.extend(sam_log)
                overlay = cv2.cvtColor(overlay, cv2.COLOR_RGB2BGR)
            except Exception as e:
                log.append(f"SAM Error: {str(e)}")
                overlay = body; stats = {}
        else:
            log.append("SAM Not Loaded")
            overlay = body; stats = {}

        def to_b64(im): return base64.b64encode(cv2.imencode('.jpg', im)[1]).decode('utf-8')
        
        # For non-FFT modes, we attach the original footer
        return {"type": "image", "raw_b64": to_b64(body), "proc_b64": to_b64(overlay), "stats": stats, "log": log, "footer_b64": footer_b64}



엔브이18

import os
import io
# [0] OMP Error Fix
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"
import torch

import asyncio
import base64
import json
import re
import math
import uuid
import time
from typing import List, Dict, Any, Optional
from pydantic import BaseModel
from contextlib import asynccontextmanager
from collections import Counter

# [1] Security
os.environ['NO_PROXY'] = '127.0.0.1,localhost,0.0.0.0,::1'
if 'HTTP_PROXY' in os.environ: del os.environ['HTTP_PROXY']
if 'HTTPS_PROXY' in os.environ: del os.environ['HTTPS_PROXY']

from fastapi import FastAPI, UploadFile, File, Form, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import HTMLResponse, FileResponse
import pandas as pd
import numpy as np

# SciPy & CV & Sklearn
from sklearn.decomposition import NMF
from scipy.signal import find_peaks, savgol_filter
from scipy.optimize import curve_fit
from scipy.ndimage import center_of_mass
from skimage.feature import peak_local_max
import cv2
import matplotlib
import matplotlib.pyplot as plt 

# Models
from segment_anything import sam_model_registry, SamAutomaticMaskGenerator
from ollama import Client 
from pdf2image import convert_from_bytes
from pptx import Presentation
from pptx.enum.shapes import MSO_SHAPE_TYPE

# [2] Config
OLLAMA_HOST = "http://127.0.0.1:11434"
ollama_client = Client(host=OLLAMA_HOST)
VISION_MODEL = "qwen3-vl:30b-a3b-instruct" 
TEXT_MODEL = "llama3:70b"

SAM_CHECKPOINT = "sam_vit_b_01ec64.pth"
SAM_MODEL_TYPE = "vit_b"
device = "cuda" if torch.cuda.is_available() else "cpu"

mask_generator = None 
JOBS = {} 

@asynccontextmanager
async def lifespan(app: FastAPI):
    global mask_generator
    if os.path.exists(SAM_CHECKPOINT):
        print(f"Loading SAM ({SAM_MODEL_TYPE}) on {device}...")
        try:
            sam = sam_model_registry[SAM_MODEL_TYPE](checkpoint=SAM_CHECKPOINT)
            sam.to(device=device)
            mask_generator = SamAutomaticMaskGenerator(
                model=sam, points_per_side=32, pred_iou_thresh=0.86,
                stability_score_thresh=0.90, min_mask_region_area=50
            )
            print("SAM Loaded.")
        except: print("SAM Load Failed.")
    yield
    JOBS.clear()

def sanitize_json(obj):
    if isinstance(obj, float):
        if np.isnan(obj) or np.isinf(obj): return 0.0
        return obj
    elif isinstance(obj, dict): return {k: sanitize_json(v) for k, v in obj.items()}
    elif isinstance(obj, list): return [sanitize_json(v) for v in obj]
    elif isinstance(obj, (np.int64, np.int32)): return int(obj)
    elif isinstance(obj, (np.float64, np.float32)):
        if np.isnan(obj) or np.isinf(obj): return 0.0
        return float(obj)
    return obj

# ==========================================
# [3] NV18 Polished Engine
# ==========================================
class ScienceProcessorNV18:
    
    # --- Universal Loader ---
    @staticmethod
    def read_universal_dataframe(content: bytes, filename: str) -> List[pd.DataFrame]:
        dfs = []
        fname = filename.lower()
        if fname.endswith(('.xlsx', '.xls')):
            try:
                xls = pd.ExcelFile(io.BytesIO(content))
                for s in xls.sheet_names:
                    df = pd.read_excel(xls, sheet_name=s, header=None)
                    num = df.apply(pd.to_numeric, errors='coerce').notna().sum().sum()
                    if num < 5: continue
                    if not df.empty: dfs.append(df)
                if dfs: return dfs
            except: pass
        text_content = ""
        for enc in ['utf-8', 'cp949', 'latin1']:
            try: text_content = content.decode(enc); break
            except: continue
        if text_content:
            lines = text_content.splitlines()
            data_start = -1; delimiter = None
            separators = [',', '\t', ';', '\s+']
            for i, line in enumerate(lines[:100]):
                for sep in separators:
                    parts = line.split() if sep=='\s+' else line.split(sep)
                    nums = sum(1 for p in parts if re.match(r'^-?\d+(\.\d+)?(?:[eE][+-]?\d+)?$', p.strip()))
                    if nums >= 2: data_start = i; delimiter = sep; break
                if data_start != -1: break
            if data_start != -1:
                try:
                    data_str = "\n".join(lines[data_start:])
                    sep_arg = '\s+' if delimiter=='\s+' else delimiter
                    df = pd.read_csv(io.StringIO(data_str), sep=sep_arg, header=None, engine='python')
                    dfs.append(df)
                except: pass
        return dfs

    # --- Block Parser ---
    @staticmethod
    def detect_structured_blocks(df: pd.DataFrame) -> List[pd.DataFrame]:
        blocks = []
        if df.empty: return []
        df_num = df.apply(pd.to_numeric, errors='coerce')
        if df.shape[1] > 0: is_data_row = df_num.iloc[:, 0].notna()
        else: is_data_row = df_num.notna().any(axis=1)
        groups = (is_data_row != is_data_row.shift()).cumsum()
        for _, group in df.groupby(groups):
            if is_data_row.loc[group.index[0]]:
                block = df_num.loc[group.index].dropna(how='all', axis=1)
                if block.shape[0] >= 10: blocks.append(block.reset_index(drop=True))
        return blocks

    @staticmethod
    def split_horizontal_blocks(df: pd.DataFrame) -> List[pd.DataFrame]:
        sub_blocks = []
        if df.empty: return []
        col_counts = df.apply(pd.to_numeric, errors='coerce').notna().sum()
        valid = np.where(col_counts >= 2)[0]
        if len(valid) == 0: return []
        splits = np.where(np.diff(valid) > 1)[0] + 1
        groups = np.split(valid, splits)
        for g in groups:
            if len(g) > 0:
                sub = df.iloc[:, g].reset_index(drop=True)
                if sub.shape[1] >= 1 and sub.shape[0] >= 2: sub_blocks.append(sub)
        return sub_blocks

    @staticmethod
    def extract_series_from_df(df: pd.DataFrame) -> List[Dict]:
        series_list = []
        try:
            vals = df.values; rows, cols = vals.shape
            if rows < 5: return []
            if cols == 1:
                y = vals[:, 0]; mask = ~np.isnan(y)
                if np.sum(mask) > 5: series_list.append({"x": np.arange(len(y))[mask], "y": y[mask], "name": "Single"})
                return series_list
            def is_axis(arr):
                arr = arr[~np.isnan(arr)]
                if len(arr) < 5: return False
                diff = np.diff(arr)
                if np.all(diff >= 0) or np.all(diff <= 0): return True
                return (np.sum(diff>0)/len(diff)>0.9) or (np.sum(diff<0)/len(diff)>0.9)
            current_x = vals[:, 0]
            if not is_axis(current_x): current_x = np.arange(rows)
            for i in range(1, cols):
                col_data = vals[:, i]; mask_valid = ~np.isnan(col_data)
                if np.sum(mask_valid) < 5: continue
                if is_axis(col_data): current_x = col_data; continue 
                mask = ~np.isnan(current_x) & ~np.isnan(col_data)
                if np.sum(mask) > 5:
                    series_list.append({"x": current_x[mask], "y": col_data[mask], "name": f"Col-{i}"})
        except: pass
        return series_list

    # --- Spectrum Logic ---
    @staticmethod
    def simple_baseline(y):
        try:
            w = max(5, len(y)//10)
            s = pd.Series(y).rolling(window=w, center=True).min()
            b = s.bfill().ffill().values 
            if len(b) > 51: b = savgol_filter(b, 51, 3)
            return b, w
        except: return np.zeros_like(y), 0

    @staticmethod
    def process_spectrum(x, y, mode, goal):
        log = []; y_raw = y.copy(); base = np.zeros_like(y); y_proc = y.copy()
        peaks = []; fits = []; stats_txt = "Error"
        try:
            if mode == "None":
                peaks, _ = find_peaks(y, height=np.max(y)*0.05)
                log.append("Mode: Raw")
                stats_txt = f"Raw Peaks: {len(peaks)}"
            else:
                win = 15; do_fit = False
                if mode == "AI-Adaptive":
                    log.append("Mode: AI-Adaptive")
                    if "noise" in goal.lower(): win = 31
                    if "fit" in goal.lower(): do_fit = True
                else: log.append("Mode: Auto")
                base, bw = ScienceProcessorNV18.simple_baseline(y)
                y_proc = np.maximum(y - base, 0)
                log.append(f"Base: Rolling Min (w={bw})")
                if len(y_proc) > win:
                    y_proc = savgol_filter(y_proc, win, 3)
                    log.append(f"Smooth: SavGol (w={win})")
                peaks, _ = find_peaks(y_proc, height=np.max(y_proc)*0.05, distance=10)
                log.append(f"Peaks: {len(peaks)}")
                stats_txt = f"Range: {np.min(x):.1f}~{np.max(x):.1f}, Peaks: {len(peaks)}"
            return {"x": x, "y_raw": y_raw, "y_proc": y_proc, "y_base": base, "peaks": [{"x": float(x[p]), "y": float(y_proc[p])} for p in peaks], "log": log, "fits": fits, "stats": stats_txt}
        except Exception as e: return {"x": x, "y_raw": y_raw, "y_proc": y_proc, "y_base": base, "peaks": [], "log": [str(e)], "fits": [], "stats": "Error"}

    # --- Footer ---
    @staticmethod
    def separate_footer(img_bgr):
        h, w = img_bgr.shape[:2]
        roi_h = int(h * 0.82)
        roi = img_bgr[roi_h:, :]
        gray = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)
        sobel_y = np.abs(cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=3))
        row_sums = np.sum(sobel_y, axis=1)
        split_idx = np.argmax(row_sums)
        if row_sums[split_idx] > w * 30:
            real_split = roi_h + split_idx
            return img_bgr[:real_split, :], img_bgr[real_split:, :]
        return img_bgr, None

    # --- [E] NV18 FFT-NMF (Circular DC Fix) ---
    @staticmethod
    def analyze_crystal_fft_nmf(img_bgr, patch_size=128, step=16, n_components=10):
        log = []
        gray = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2GRAY)
        h, w = gray.shape
        
        log.append(f"FFT-NMF: Patch={patch_size}, Step={step}, K={n_components}")
        
        # Custom Colormap (Neon/Bright)
        custom_colors = [
            (0, 0, 255), (0, 255, 0), (255, 0, 0), (0, 255, 255), (255, 255, 0),
            (255, 0, 255), (0, 165, 255), (238, 130, 238), (128, 128, 0), (19, 69, 139)
        ]
        while len(custom_colors) < n_components: custom_colors.append((255, 255, 255))
        
        cols = [] 
        y_steps = list(range(0, h - patch_size + 1, step))
        x_steps = list(range(0, w - patch_size + 1, step))
        grid_h = len(y_steps)
        grid_w = len(x_steps)
        
        # [Fix] Precompute Circular DC Mask
        cy, cx = patch_size // 2, patch_size // 2
        Y, X = np.ogrid[:patch_size, :patch_size]
        dist_sq = (X - cx)**2 + (Y - cy)**2
        # Radius 4 pixels
        dc_mask = dist_sq <= 4**2

        for y in y_steps:
            for x in x_steps:
                roi = gray[y:y+patch_size, x:x+patch_size]
                f = np.fft.fft2(roi)
                fshift = np.fft.fftshift(f)
                mag = np.abs(fshift) 
                
                # [Fix] Apply Circular Mask
                mag[dc_mask] = 0
                
                cols.append(mag.flatten())
        
        if not cols: return {"type":"error", "msg":"Img too small"}
        
        try:
            V = np.array(cols).T.astype(np.float32)
            V = V - V.min()
            if V.max() > 0: V = V / V.max()
            
            nmf = NMF(n_components=n_components, init='nndsvda', random_state=42, max_iter=2000)
            W = nmf.fit_transform(V) 
            H = nmf.components_ 
            
            if H.shape[1] != grid_h * grid_w: raise ValueError("Shape Mismatch")
            
            idx_H = np.argmax(H, axis=0) 
            map_low = idx_H.reshape(grid_h, grid_w).astype(np.uint8)
            map_high = cv2.resize(map_low, (w, h), interpolation=cv2.INTER_NEAREST)
            
            overlay = img_bgr.copy()
            mask_layer = np.zeros_like(img_bgr)
            
            counts = Counter(idx_H)
            total = len(idx_H)
            stats = {}
            
            for i in range(n_components):
                mask = (map_high == i)
                mask_layer[mask] = custom_colors[i]
                cnt = counts.get(i, 0)
                # Store Clean Text for LLM
                stats[f"Pattern {i}"] = f"{cnt} patches ({cnt/total*100:.1f}%)"
                
            alpha = 0.4
            mask_idx = np.any(mask_layer > 0, axis=-1)
            overlay[mask_idx] = (overlay[mask_idx]*(1-alpha) + mask_layer[mask_idx]*alpha).astype(np.uint8)
            
            basis_combo_list = []
            for k in range(n_components):
                pat = W[:, k].reshape(patch_size, patch_size)
                pat_log = np.log1p(pat * 1000) 
                pat_norm = cv2.normalize(pat_log, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)
                pat_color = cv2.applyColorMap(pat_norm, cv2.COLORMAP_MAGMA)
                
                legend_h = 40
                legend = np.zeros((legend_h, patch_size, 3), dtype=np.uint8)
                legend[:] = custom_colors[k]
                cv2.putText(legend, f"P{k}", (5, 25), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255,255,255), 2)
                
                combo = np.vstack([legend, pat_color])
                cv2.rectangle(combo, (0,0), (combo.shape[1]-1, combo.shape[0]-1), custom_colors[k], 2)
                basis_combo_list.append(combo)
            
            full_strip = np.hstack(basis_combo_list)
            full_strip_large = cv2.resize(full_strip, None, fx=2, fy=2, interpolation=cv2.INTER_NEAREST)
            
            def to_b64(im): return base64.b64encode(cv2.imencode('.jpg', im)[1]).decode('utf-8')
            
            return {
                "type": "image", 
                "raw_b64": to_b64(img_bgr), 
                "proc_b64": to_b64(overlay),
                "footer_b64": to_b64(full_strip_large), 
                "stats": stats, 
                "log": log
            }

        except Exception as e:
            print(f"NMF Error: {e}")
            def to_b64(im): return base64.b64encode(cv2.imencode('.jpg', im)[1]).decode('utf-8')
            return {"type": "image", "raw_b64": to_b64(img_bgr), "proc_b64": to_b64(img_bgr), "stats": {"Error": str(e)}, "log": log}

    # --- [F] Diffraction ---
    @staticmethod
    def analyze_diffraction_nv7(img_bgr, user_center=None):
        log = []; stats = {}
        gray = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2GRAY)
        
        if user_center and len(user_center)==2 and user_center[0]>0:
            cx, cy = user_center
            log.append(f"Center: Manual [{cx}, {cy}]")
        else:
            _, th = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY+cv2.THRESH_OTSU)
            cyf, cxf = center_of_mass(th)
            if np.isnan(cyf): cyf, cxf = gray.shape[0]//2, gray.shape[1]//2
            cx, cy = int(cxf), int(cyf)
            log.append(f"Center: Auto [{cx}, {cy}]")

        g1 = cv2.GaussianBlur(gray, (3,3), 0)
        g2 = cv2.GaussianBlur(gray, (51,51), 0)
        diff = cv2.subtract(g1, g2)
        coords = peak_local_max(diff, min_distance=10, threshold_abs=20)
        enhanced = np.log1p(diff.astype(np.float32))
        enhanced = cv2.normalize(enhanced, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)
        vis = cv2.cvtColor(enhanced, cv2.COLOR_GRAY2BGR)
        
        cv2.drawMarker(vis, (cx, cy), (0,0,255), cv2.MARKER_CROSS, 20, 2)
        peak_list = []
        for p in coords:
            py, px = p
            if abs(py-cy)<5 and abs(px-cx)<5: continue
            cv2.circle(vis, (px, py), 3, (0,255,0), 1)
            peak_list.append(f"[{px},{py}]")

        log.append(f"Spots: {len(peak_list)}")
        def to_b64(im): return base64.b64encode(cv2.imencode('.jpg', im)[1]).decode('utf-8')
        return {"type": "image", "raw_b64": to_b64(img_bgr), "proc_b64": to_b64(vis), "stats": {"center": [cx, cy], "spots": len(peak_list)}, "log": log}

    # --- [G] SAM ---
    @staticmethod
    def analyze_sam_results_nv7(img_rgb, masks, data_type):
        overlay = img_rgb.copy()
        mask_overlay = np.zeros_like(img_rgb)
        h_img, w_img = img_rgb.shape[:2]
        stats = {"count": 0}
        log = []
        
        valid_masks = []
        for ann in masks:
            area = ann['area']; bbox = ann['bbox']; x, y, w, h = [int(v) for v in bbox]
            if area > h_img * w_img * 0.95 or area < 20: continue 
            if data_type == "Particle":
                if x<=1 or y<=1 or (x+w)>=w_img-1 or (y+h)>=h_img-1: continue
                mask_uint8 = (ann['segmentation'] * 255).astype(np.uint8)
                cnts, _ = cv2.findContours(mask_uint8, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
                if cnts:
                    perim = cv2.arcLength(cnts[0], True)
                    circ = 4*np.pi*area/(perim**2) if perim>0 else 0
                    if circ < 0.25: continue
            valid_masks.append(ann)

        if data_type == "Thin Film":
            sorted_masks = sorted(valid_masks, key=lambda x: x['bbox'][1])
        else:
            sorted_masks = sorted(valid_masks, key=lambda x: x['area'], reverse=True)

        thickness_vals = []; diameters = []

        for i, ann in enumerate(sorted_masks):
            m = ann['segmentation']
            bbox = ann['bbox']; x, y, w, h = [int(v) for v in bbox]
            cx, cy = int(x + w/2), int(y + h/2)
            color = (0, 255, 0) 

            if data_type == "Thin Film":
                is_bound = (i == 0 or i == len(sorted_masks)-1)
                cv2.putText(overlay, f"#{i+1}", (cx, cy), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (255, 255, 255), 3)
                if not is_bound:
                    thk = ann['area'] / w
                    thickness_vals.append(thk)
                    color = (255, 0, 0) 
                else: color = (100, 100, 100)
                if i < len(sorted_masks) - 1:
                    ys, xs = np.where(m)
                    if len(xs) > 10:
                        roughness = np.std(ys)
                        cv2.line(overlay, (x, y+h), (x+w, y+h), (0, 255, 255), 2)
                        cv2.putText(overlay, f"Int#{i+1} R:{roughness:.2f}", (x+10, y+h-5), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)
            elif data_type == "Particle":
                equivalent_diameter = 2 * np.sqrt(ann['area'] / np.pi)
                diameters.append(equivalent_diameter)

            mask_overlay[m] = color
            mask_uint8 = (m * 255).astype(np.uint8)
            cnts, _ = cv2.findContours(mask_uint8, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
            if cnts: cv2.drawContours(overlay, [cnts[0]], -1, color, 1)

        alpha = 0.35
        mask_idx = np.any(mask_overlay > 0, axis=-1)
        overlay[mask_idx] = (overlay[mask_idx]*(1-alpha) + mask_overlay[mask_idx]*alpha).astype(np.uint8)

        stats["count"] = len(sorted_masks)
        if data_type == "Thin Film":
            avg = round(np.mean(thickness_vals), 2) if thickness_vals else 0
            stats["thickness_avg"] = avg
            log.append(f"Layers: {len(sorted_masks)}, Avg Thk: {avg}")
        elif data_type == "Particle":
            avg = round(np.mean(diameters), 2) if diameters else 0
            stats["diameter_avg"] = avg
            log.append(f"Particles: {len(sorted_masks)}")
        return overlay, stats, log

    # --- Router ---
    @staticmethod
    def process_image_nv18(img_bytes, equipment, data_type, mode, goal, user_center=None, 
                           patch_size=128, step=16, n_components=10):
        nparr = np.frombuffer(img_bytes, np.uint8)
        img_raw = cv2.imdecode(nparr, cv2.IMREAD_COLOR)
        if img_raw is None: return {}
        
        body, footer = ScienceProcessorNV18.separate_footer(img_raw)
        
        footer_b64 = None
        if footer is not None:
            footer_b64 = base64.b64encode(cv2.imencode('.jpg', footer)[1]).decode('utf-8')

        # [NV18] FFT-NMF (Circular DC)
        if data_type == "Crystal Structure (FFT)":
            return {**ScienceProcessorNV18.analyze_crystal_fft_nmf(body, patch_size, step, n_components), "footer_b64": footer_b64}
            
        if data_type == "2D Diffraction":
            return {**ScienceProcessorNV18.analyze_diffraction_nv7(body, user_center), "footer_b64": footer_b64}
        
        if mode == "None":
             def to_b64(im): return base64.b64encode(cv2.imencode('.jpg', im)[1]).decode('utf-8')
             return {"type":"image", "raw_b64":to_b64(body), "proc_b64":to_b64(body), "stats":{}, "log":["Mode: Raw"], "footer_b64": footer_b64}

        img_proc = body.copy(); log = []
        if mode == "Auto" or mode == "AI-Adaptive":
            img_proc = cv2.bilateralFilter(img_proc, 9, 75, 75)
            log.append("Preproc: Denoise")

        img_rgb = cv2.cvtColor(img_proc, cv2.COLOR_BGR2RGB)
        
        if mask_generator:
            try:
                masks = mask_generator.generate(img_rgb)
                log.append(f"SAM: {len(masks)} Segments")
                overlay, stats, sam_log = ScienceProcessorNV18.analyze_sam_results_nv7(img_rgb, masks, data_type)
                log.extend(sam_log)
                overlay = cv2.cvtColor(overlay, cv2.COLOR_RGB2BGR)
            except Exception as e:
                log.append(f"SAM Error: {str(e)}")
                overlay = body; stats = {}
        else:
            log.append("SAM Not Loaded")
            overlay = body; stats = {}

        def to_b64(im): return base64.b64encode(cv2.imencode('.jpg', im)[1]).decode('utf-8')
        return {"type": "image", "raw_b64": to_b64(body), "proc_b64": to_b64(overlay), "stats": stats, "log": log, "footer_b64": footer_b64}

# [4] Helper (Same)
async def analyze_vision_ollama(image_bytes: bytes, prompt: str) -> str:
    try:
        b64 = base64.b64encode(image_bytes).decode('utf-8')
        res = await asyncio.to_thread(ollama_client.chat, model=VISION_MODEL, messages=[{'role':'user','content':prompt,'images':[b64]}])
        return res['message']['content']
    except: return "Vision Error"

async def interpret_spectrum_data(raw_context: str, equipment: str) -> str:
    try:
        prompt = f"Analyze spectrum ({equipment}). Brief interpretation in Korean. Data: {raw_context}"
        res = await asyncio.to_thread(ollama_client.chat, model=TEXT_MODEL, messages=[{'role':'user','content':prompt}])
        return res['message']['content']
    except: return ""

# [5] App
app = FastAPI(title="Analyst NV18 Final", lifespan=lifespan)
app.add_middleware(CORSMiddleware, allow_origins=["*"], allow_methods=["*"], allow_headers=["*"])

class TranslateReq(BaseModel): text: str

@app.post("/api/translate")
async def translate_text(req: TranslateReq):
    try:
        res = await asyncio.to_thread(ollama_client.chat, model=TEXT_MODEL, messages=[{'role':'system','content':'Translate to English.'}, {'role':'user','content':req.text}])
        return {"translated": res['message']['content']}
    except Exception as e: return {"translated": str(e)}

async def process_analysis_job(job_id: str, files_map: Dict, configs: Dict):
    try:
        JOBS[job_id]["status"] = "Processing"
        JOBS[job_id]["progress"] = 10
        final_results = []
        total_files = len(configs)
        processed_count = 0

        for filename, config in configs.items():
            if filename not in files_map: continue
            content = files_map[filename]
            eq = config.get("equipment", "General")
            data_type = config.get("data_type", "General")
            goal = config.get("goal", "")
            mode = config.get("mode", "Auto")
            
            user_center = None
            if config.get("center_coords"):
                try: user_center = [int(v) for v in config["center_coords"].split(",")]
                except: pass
            
            p_size = int(config.get("patch_size", 128))
            p_step = int(config.get("step", 16))
            n_comp = int(config.get("n_components", 10))

            fname_lower = filename.lower()
            JOBS[job_id]["step"] = f"Analyzing {filename}..."
            
            if eq == "Document" or fname_lower.endswith(('.pdf', '.ppt', '.pptx')):
                # (Same Document Logic)
                try:
                    if fname_lower.endswith('.pdf'):
                        images = convert_from_bytes(content, dpi=150, fmt='jpeg')[:10]
                        full_txt = ""; pages = []
                        for idx, img in enumerate(images):
                            buf = io.BytesIO(); img.save(buf, format="JPEG")
                            b64 = base64.b64encode(buf.getvalue()).decode()
                            desc = await analyze_vision_ollama(buf.getvalue(), "한글 요약.")
                            full_txt += f"\nPage {idx+1}: {desc}"
                            pages.append({"page_num": idx+1, "image_b64": b64, "desc": desc})
                        final_results.append({"type": "pdf", "filename": filename, "equipment": "Lit", "raw_context": full_txt, "pages": pages})
                    elif fname_lower.endswith(('.ppt', '.pptx')):
                        prs = Presentation(io.BytesIO(content)); slides = []; full_txt = ""
                        for i, slide in enumerate(prs.slides):
                            txt = ""; imgs = []
                            for s in slide.shapes:
                                if hasattr(s, "text"): txt += s.text + "\n"
                                if s.shape_type == MSO_SHAPE_TYPE.PICTURE:
                                    try:
                                        ib64 = base64.b64encode(s.image.blob).decode('utf-8')
                                        desc = await analyze_vision_ollama(s.image.blob, "한글 설명.")
                                        imgs.append({"b64": ib64, "desc": desc})
                                    except: pass
                            slides.append({"slide_num": i+1, "text": txt, "images": imgs})
                        final_results.append({"type": "ppt", "filename": filename, "equipment": "Lit", "raw_context": full_txt, "slides": slides})
                except Exception as e: final_results.append({"type": "error", "filename": filename, "msg": str(e)})

            elif fname_lower.endswith(('.xlsx', '.xls', '.csv', '.txt')):
                try:
                    dfs = ScienceProcessorNV18.read_universal_dataframe(content, filename)
                    for i, df in enumerate(dfs):
                        v_blocks = ScienceProcessorNV18.detect_structured_blocks(df)
                        for v_block in v_blocks:
                            h_blocks = ScienceProcessorNV18.split_horizontal_blocks(v_block)
                            for h_block in h_blocks:
                                series_list = ScienceProcessorNV18.extract_series_from_df(h_block)
                                for s in series_list:
                                    res = ScienceProcessorNV18.process_spectrum(s['x'], s['y'], mode, goal)
                                    step = max(1, len(s['x'])//100)
                                    chart = [{"x": float(res["x"][k]), "y_proc": float(res["y_proc"][k]), "y_raw": float(res["y_raw"][k]), "y_base": float(res["y_base"][k])} for k in range(0, len(s['x']), step)]
                                    ctx = f"Data: {s['name']}\nStats: {res['stats']}"
                                    interp = await interpret_spectrum_data(ctx, eq)
                                    final_results.append({
                                        "type": "spectrum", "filename": f"{filename} ({s['name']})", "equipment": eq,
                                        "raw_context": f"{ctx}\nInterp: {interp}", 
                                        "chart_data": chart, "log": res["log"], "interpretation": interp
                                    })
                except Exception as ex: final_results.append({"type": "error", "filename": filename, "msg": str(ex)})

            elif fname_lower.endswith(('.png', '.jpg', '.jpeg', '.tif', '.bmp')):
                try:
                    JOBS[job_id]["step"] = f"Vision Analysis: {filename}..."
                    vis_res = await asyncio.to_thread(ScienceProcessorNV18.process_image_nv18, content, eq, data_type, mode, goal, user_center, p_size, p_step, n_comp)
                    if vis_res:
                        scale_info = "Unknown"
                        final_footer = vis_res.get("footer_b64")
                        if final_footer and data_type != "Crystal Structure (FFT)":
                            JOBS[job_id]["step"] = f"Scale Reading..."
                            scale_text = await analyze_vision_ollama(base64.b64decode(final_footer), "Read scale bar text only.")
                            scale_info = scale_text
                            vis_res["log"].append(f"Scale: {scale_text}")

                        # [Fix] Branching Prompt for Detailed Interpretation
                        stats_clean = ""
                        for k, v in vis_res.get('stats',{}).items():
                            stats_clean += f"- {k}: {v}\n"
                        
                        desc_prompt = ""
                        if data_type == "Crystal Structure (FFT)":
                            desc_prompt = f"Analyze this TEM FFT result. NMF patterns found: \n{stats_clean}\n. Explain the distribution of these phases (Amorphous vs Crystalline) based on the image map. Scale: {scale_info}. Goal: {goal}. Summary in Korean."
                        elif data_type == "Particle":
                            desc_prompt = f"Analyze Particle image. Stats: \n{stats_clean}\n. Discuss particle size distribution and density. Scale: {scale_info}. Goal: {goal}. Summary in Korean."
                        elif data_type == "Thin Film":
                            desc_prompt = f"Analyze Thin Film cross-section. Stats: \n{stats_clean}\n. Discuss layer thickness uniformity and interface roughness. Scale: {scale_info}. Goal: {goal}. Summary in Korean."
                        else:
                            desc_prompt = f"Analyze {eq} image. Stats: \n{stats_clean}\n. Goal: {goal}. Summary in Korean."

                        desc = await analyze_vision_ollama(base64.b64decode(vis_res["proc_b64"]), desc_prompt)
                        
                        final_results.append({
                            "type": "image", "filename": filename, "equipment": eq, "summary": desc,
                            "raw_context": f"Img {eq}: {desc}\nStats:\n{stats_clean}", # Save clean stats for Synthesis
                            "raw_b64": vis_res["raw_b64"], "proc_b64": vis_res["proc_b64"], "footer_b64": final_footer,
                            "stats": vis_res.get("stats",{}), "log": vis_res.get("log", [])
                        })
                except Exception as ex: final_results.append({"type": "error", "filename": filename, "msg": str(ex)})
            
            processed_count += 1
            JOBS[job_id]["progress"] = 10 + int((processed_count / total_files) * 80)

        JOBS[job_id]["step"] = "Writing Report..."
        final_results = sanitize_json(final_results)
        
        # [Fix] Generate clean text summary for Synthesis Report
        data_ctx = ""
        for r in final_results:
            if r.get("type") != "error":
                raw = r.get("raw_context") or ""
                # Truncate very long texts but keep stats
                data_ctx += f"\n=== File: {r['filename']} ({r.get('equipment')}) ===\n{raw[:2000]}\n"

        final_report = "Fail"
        if data_ctx:
            try:
                system_prompt = "You are a research assistant. Write a comprehensive report in Korean based on the provided data summaries. Include a data summary table. Do not use LaTeX."
                res = await asyncio.to_thread(ollama_client.chat, model=TEXT_MODEL, messages=[{'role':'system','content':system_prompt}, {'role':'user','content':f"Data Summaries:\n{data_ctx}"}])
                final_report = res['message']['content']
            except Exception as e: final_report = str(e)

        JOBS[job_id]["results"] = {"results": final_results, "final_report": final_report}
        JOBS[job_id]["status"] = "Completed"
        JOBS[job_id]["progress"] = 100
    except Exception as e:
        JOBS[job_id]["status"] = "Failed"; JOBS[job_id]["error"] = str(e)

@app.post("/api/analyze")
async def start_analysis(background_tasks: BackgroundTasks, files: List[UploadFile] = File(...), file_configs: str = Form(...)):
    job_id = str(uuid.uuid4()); configs = json.loads(file_configs)
    files_map = {f.filename: await f.read() for f in files}
    JOBS[job_id] = {"status": "Pending", "progress": 0, "step": "Init", "results": None}
    background_tasks.add_task(process_analysis_job, job_id, files_map, configs)
    return {"job_id": job_id}

@app.get("/api/status/{job_id}")
async def get_status(job_id: str): return JOBS.get(job_id, {"status": "Not Found"})

@app.get("/", response_class=HTMLResponse)
async def serve_index():
    if os.path.exists("index.html"): return FileResponse("index.html")
    return "<h1>NV18 Perfect Backend</h1>"

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)


엔브이17

import os
import io
# [0] OMP Error Fix
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"
import torch

import asyncio
import base64
import json
import re
import math
import uuid
import time
from typing import List, Dict, Any, Optional
from pydantic import BaseModel
from contextlib import asynccontextmanager
from collections import Counter

# [1] Security
os.environ['NO_PROXY'] = '127.0.0.1,localhost,0.0.0.0,::1'
if 'HTTP_PROXY' in os.environ: del os.environ['HTTP_PROXY']
if 'HTTPS_PROXY' in os.environ: del os.environ['HTTPS_PROXY']

from fastapi import FastAPI, UploadFile, File, Form, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import HTMLResponse, FileResponse
import pandas as pd
import numpy as np

# SciPy & CV & Sklearn
from sklearn.decomposition import NMF
from scipy.signal import find_peaks, savgol_filter
from scipy.optimize import curve_fit
from scipy.ndimage import center_of_mass
from skimage.feature import peak_local_max
import cv2
import matplotlib
import matplotlib.pyplot as plt 

# Models
from segment_anything import sam_model_registry, SamAutomaticMaskGenerator
from ollama import Client 
from pdf2image import convert_from_bytes
from pptx import Presentation
from pptx.enum.shapes import MSO_SHAPE_TYPE

# [2] Config
OLLAMA_HOST = "http://127.0.0.1:11434"
ollama_client = Client(host=OLLAMA_HOST)
VISION_MODEL = "qwen3-vl:30b-a3b-instruct" 
TEXT_MODEL = "llama3:70b"

SAM_CHECKPOINT = "sam_vit_b_01ec64.pth"
SAM_MODEL_TYPE = "vit_b"
device = "cuda" if torch.cuda.is_available() else "cpu"

mask_generator = None 
JOBS = {} 

@asynccontextmanager
async def lifespan(app: FastAPI):
    global mask_generator
    if os.path.exists(SAM_CHECKPOINT):
        print(f"Loading SAM ({SAM_MODEL_TYPE}) on {device}...")
        try:
            sam = sam_model_registry[SAM_MODEL_TYPE](checkpoint=SAM_CHECKPOINT)
            sam.to(device=device)
            mask_generator = SamAutomaticMaskGenerator(
                model=sam, points_per_side=32, pred_iou_thresh=0.86,
                stability_score_thresh=0.90, min_mask_region_area=50
            )
            print("SAM Loaded.")
        except: print("SAM Load Failed.")
    yield
    JOBS.clear()

def sanitize_json(obj):
    if isinstance(obj, float):
        if np.isnan(obj) or np.isinf(obj): return 0.0
        return obj
    elif isinstance(obj, dict): return {k: sanitize_json(v) for k, v in obj.items()}
    elif isinstance(obj, list): return [sanitize_json(v) for v in obj]
    elif isinstance(obj, (np.int64, np.int32)): return int(obj)
    elif isinstance(obj, (np.float64, np.float32)):
        if np.isnan(obj) or np.isinf(obj): return 0.0
        return float(obj)
    return obj

# ==========================================
# [3] NV17 Visualization Fixed Engine
# ==========================================
class ScienceProcessorNV17:
    
    # --- Universal Loader ---
    @staticmethod
    def read_universal_dataframe(content: bytes, filename: str) -> List[pd.DataFrame]:
        dfs = []
        fname = filename.lower()
        if fname.endswith(('.xlsx', '.xls')):
            try:
                xls = pd.ExcelFile(io.BytesIO(content))
                for s in xls.sheet_names:
                    df = pd.read_excel(xls, sheet_name=s, header=None)
                    num = df.apply(pd.to_numeric, errors='coerce').notna().sum().sum()
                    if num < 5: continue
                    if not df.empty: dfs.append(df)
                if dfs: return dfs
            except: pass
        text_content = ""
        for enc in ['utf-8', 'cp949', 'latin1']:
            try: text_content = content.decode(enc); break
            except: continue
        if text_content:
            lines = text_content.splitlines()
            data_start = -1; delimiter = None
            separators = [',', '\t', ';', '\s+']
            for i, line in enumerate(lines[:100]):
                for sep in separators:
                    parts = line.split() if sep=='\s+' else line.split(sep)
                    nums = sum(1 for p in parts if re.match(r'^-?\d+(\.\d+)?(?:[eE][+-]?\d+)?$', p.strip()))
                    if nums >= 2: data_start = i; delimiter = sep; break
                if data_start != -1: break
            if data_start != -1:
                try:
                    data_str = "\n".join(lines[data_start:])
                    sep_arg = '\s+' if delimiter=='\s+' else delimiter
                    df = pd.read_csv(io.StringIO(data_str), sep=sep_arg, header=None, engine='python')
                    dfs.append(df)
                except: pass
        return dfs

    # --- Block Parser ---
    @staticmethod
    def detect_structured_blocks(df: pd.DataFrame) -> List[pd.DataFrame]:
        blocks = []
        if df.empty: return []
        df_num = df.apply(pd.to_numeric, errors='coerce')
        if df.shape[1] > 0: is_data_row = df_num.iloc[:, 0].notna()
        else: is_data_row = df_num.notna().any(axis=1)
        groups = (is_data_row != is_data_row.shift()).cumsum()
        for _, group in df.groupby(groups):
            if is_data_row.loc[group.index[0]]:
                block = df_num.loc[group.index].dropna(how='all', axis=1)
                if block.shape[0] >= 10: blocks.append(block.reset_index(drop=True))
        return blocks

    @staticmethod
    def split_horizontal_blocks(df: pd.DataFrame) -> List[pd.DataFrame]:
        sub_blocks = []
        if df.empty: return []
        col_counts = df.apply(pd.to_numeric, errors='coerce').notna().sum()
        valid = np.where(col_counts >= 2)[0]
        if len(valid) == 0: return []
        splits = np.where(np.diff(valid) > 1)[0] + 1
        groups = np.split(valid, splits)
        for g in groups:
            if len(g) > 0:
                sub = df.iloc[:, g].reset_index(drop=True)
                if sub.shape[1] >= 1 and sub.shape[0] >= 2: sub_blocks.append(sub)
        return sub_blocks

    @staticmethod
    def extract_series_from_df(df: pd.DataFrame) -> List[Dict]:
        series_list = []
        try:
            vals = df.values; rows, cols = vals.shape
            if rows < 5: return []
            if cols == 1:
                y = vals[:, 0]; mask = ~np.isnan(y)
                if np.sum(mask) > 5: series_list.append({"x": np.arange(len(y))[mask], "y": y[mask], "name": "Single"})
                return series_list
            def is_axis(arr):
                arr = arr[~np.isnan(arr)]
                if len(arr) < 5: return False
                diff = np.diff(arr)
                if np.all(diff >= 0) or np.all(diff <= 0): return True
                return (np.sum(diff>0)/len(diff)>0.9) or (np.sum(diff<0)/len(diff)>0.9)
            current_x = vals[:, 0]
            if not is_axis(current_x): current_x = np.arange(rows)
            for i in range(1, cols):
                col_data = vals[:, i]; mask_valid = ~np.isnan(col_data)
                if np.sum(mask_valid) < 5: continue
                if is_axis(col_data): current_x = col_data; continue 
                mask = ~np.isnan(current_x) & ~np.isnan(col_data)
                if np.sum(mask) > 5:
                    series_list.append({"x": current_x[mask], "y": col_data[mask], "name": f"Col-{i}"})
        except: pass
        return series_list

    # --- Spectrum Logic ---
    @staticmethod
    def simple_baseline(y):
        try:
            w = max(5, len(y)//10)
            s = pd.Series(y).rolling(window=w, center=True).min()
            b = s.bfill().ffill().values 
            if len(b) > 51: b = savgol_filter(b, 51, 3)
            return b, w
        except: return np.zeros_like(y), 0

    @staticmethod
    def process_spectrum(x, y, mode, goal):
        log = []; y_raw = y.copy(); base = np.zeros_like(y); y_proc = y.copy()
        peaks = []; fits = []; stats_txt = "Error"
        try:
            if mode == "None":
                peaks, _ = find_peaks(y, height=np.max(y)*0.05)
                log.append("Mode: Raw")
                stats_txt = f"Raw Peaks: {len(peaks)}"
            else:
                win = 15; do_fit = False
                if mode == "AI-Adaptive":
                    log.append("Mode: AI-Adaptive")
                    if "noise" in goal.lower(): win = 31
                    if "fit" in goal.lower(): do_fit = True
                else: log.append("Mode: Auto")
                base, bw = ScienceProcessorNV17.simple_baseline(y)
                y_proc = np.maximum(y - base, 0)
                log.append(f"Base: Rolling Min (w={bw})")
                if len(y_proc) > win:
                    y_proc = savgol_filter(y_proc, win, 3)
                    log.append(f"Smooth: SavGol (w={win})")
                peaks, _ = find_peaks(y_proc, height=np.max(y_proc)*0.05, distance=10)
                log.append(f"Peaks: {len(peaks)}")
                stats_txt = f"Range: {np.min(x):.1f}~{np.max(x):.1f}, Peaks: {len(peaks)}"
            return {"x": x, "y_raw": y_raw, "y_proc": y_proc, "y_base": base, "peaks": [{"x": float(x[p]), "y": float(y_proc[p])} for p in peaks], "log": log, "fits": fits, "stats": stats_txt}
        except Exception as e: return {"x": x, "y_raw": y_raw, "y_proc": y_proc, "y_base": base, "peaks": [], "log": [str(e)], "fits": [], "stats": "Error"}

    # --- Footer ---
    @staticmethod
    def separate_footer(img_bgr):
        h, w = img_bgr.shape[:2]
        roi_h = int(h * 0.82)
        roi = img_bgr[roi_h:, :]
        gray = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)
        sobel_y = np.abs(cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=3))
        row_sums = np.sum(sobel_y, axis=1)
        split_idx = np.argmax(row_sums)
        if row_sums[split_idx] > w * 30:
            real_split = roi_h + split_idx
            return img_bgr[:real_split, :], img_bgr[real_split:, :]
        return img_bgr, None

    # --- [E] NV17 FFT-NMF with Fixed Visualization ---
    @staticmethod
    def analyze_crystal_fft_nmf(img_bgr, patch_size=128, step=16, n_components=10):
        log = []
        gray = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2GRAY)
        h, w = gray.shape
        
        log.append(f"FFT-NMF: Patch={patch_size}, Step={step}, K={n_components}")
        
        # 1. Custom Colormap (Neon/Bright for Contrast)
        # Manually define 10 high-contrast colors (BGR)
        custom_colors = [
            (0, 0, 255),    # Red
            (0, 255, 0),    # Green
            (255, 0, 0),    # Blue
            (0, 255, 255),  # Yellow
            (255, 255, 0),  # Cyan
            (255, 0, 255),  # Magenta
            (0, 165, 255),  # Orange
            (238, 130, 238),# Violet
            (128, 128, 0),  # Teal
            (19, 69, 139)   # Brown
        ]
        # Extend if needed
        while len(custom_colors) < n_components:
            custom_colors.append((255, 255, 255))
        
        # 2. Collect FFT Magnitudes
        cols = [] 
        y_steps = list(range(0, h - patch_size + 1, step))
        x_steps = list(range(0, w - patch_size + 1, step))
        grid_h = len(y_steps)
        grid_w = len(x_steps)
        
        for y in y_steps:
            for x in x_steps:
                roi = gray[y:y+patch_size, x:x+patch_size]
                f = np.fft.fft2(roi)
                fshift = np.fft.fftshift(f)
                mag = np.abs(fshift) 
                
                # DC Masking
                cy, cx = patch_size//2, patch_size//2
                mag[cy-2:cy+3, cx-2:cx+3] = 0 
                
                cols.append(mag.flatten())
        
        if not cols: return {"type":"error", "msg":"Img too small"}
        
        try:
            # Transpose: (Pixels, Patches)
            V = np.array(cols).T.astype(np.float32)
            
            # Global Scaling
            V = V - V.min()
            if V.max() > 0: V = V / V.max()
            
            # NMF
            nmf = NMF(n_components=n_components, init='nndsvda', random_state=42, max_iter=2000)
            W = nmf.fit_transform(V) # Basis Patterns (Pixels, K)
            H = nmf.components_      # Coefficients
            
            # Reconstruct Map
            idx_H = np.argmax(H, axis=0) 
            map_low = idx_H.reshape(grid_h, grid_w).astype(np.uint8)
            map_high = cv2.resize(map_low, (w, h), interpolation=cv2.INTER_NEAREST)
            
            # Overlay
            overlay = img_bgr.copy()
            mask_layer = np.zeros_like(img_bgr)
            
            counts = Counter(idx_H)
            total = len(idx_H)
            stats = {}
            
            for i in range(n_components):
                mask = (map_high == i)
                mask_layer[mask] = custom_colors[i]
                cnt = counts.get(i, 0)
                stats[f"Pattern {i}"] = f"{cnt} ({cnt/total*100:.1f}%)"
                
            alpha = 0.4
            mask_idx = np.any(mask_layer > 0, axis=-1)
            overlay[mask_idx] = (overlay[mask_idx]*(1-alpha) + mask_layer[mask_idx]*alpha).astype(np.uint8)
            
            # 6. [Fix] Visible Basis Patterns Strip
            basis_combo_list = []
            
            for k in range(n_components):
                # 1. Pattern Image (FFT Basis)
                pat = W[:, k].reshape(patch_size, patch_size)
                
                # Log Scale for visibility
                pat_log = np.log1p(pat * 1000) 
                pat_norm = cv2.normalize(pat_log, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)
                
                # Resize Pattern to 128x128 for visibility
                pat_large = cv2.resize(pat_norm, (128, 128), interpolation=cv2.INTER_NEAREST)
                
                # Apply Magma Colormap
                pat_color = cv2.applyColorMap(pat_large, cv2.COLORMAP_MAGMA)
                
                # 2. Legend Block (Tall & Clear)
                legend_h = 40
                legend = np.zeros((legend_h, 128, 3), dtype=np.uint8)
                legend[:] = custom_colors[k]
                
                # Text on Legend
                cv2.putText(legend, f"P{k}", (40, 30), cv2.FONT_HERSHEY_SIMPLEX, 1.0, (255,255,255), 2)
                
                # Stack
                combo = np.vstack([legend, pat_color])
                
                # Border
                cv2.rectangle(combo, (0,0), (127, combo.shape[0]-1), custom_colors[k], 4)
                
                basis_combo_list.append(combo)
            
            # Combine all patterns horizontally
            full_strip = np.hstack(basis_combo_list)
            
            def to_b64(im): return base64.b64encode(cv2.imencode('.jpg', im)[1]).decode('utf-8')
            
            return {
                "type": "image", 
                "raw_b64": to_b64(img_bgr), 
                "proc_b64": to_b64(overlay),
                "footer_b64": to_b64(full_strip), # The correct Basis Strip
                "stats": stats, 
                "log": log
            }

        except Exception as e:
            print(f"NMF Error: {e}")
            def to_b64(im): return base64.b64encode(cv2.imencode('.jpg', im)[1]).decode('utf-8')
            return {"type": "image", "raw_b64": to_b64(img_bgr), "proc_b64": to_b64(img_bgr), "stats": {"Error": str(e)}, "log": log}

    # --- [F] Diffraction ---
    @staticmethod
    def analyze_diffraction_nv7(img_bgr, user_center=None):
        log = []
        stats = {}
        gray = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2GRAY)
        
        if user_center and len(user_center)==2 and user_center[0]>0:
            cx, cy = user_center
            log.append(f"Center: Manual [{cx}, {cy}]")
        else:
            _, th = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY+cv2.THRESH_OTSU)
            cyf, cxf = center_of_mass(th)
            if np.isnan(cyf): cyf, cxf = gray.shape[0]//2, gray.shape[1]//2
            cx, cy = int(cxf), int(cyf)
            log.append(f"Center: Auto [{cx}, {cy}]")

        g1 = cv2.GaussianBlur(gray, (3,3), 0)
        g2 = cv2.GaussianBlur(gray, (51,51), 0)
        diff = cv2.subtract(g1, g2)
        
        coords = peak_local_max(diff, min_distance=10, threshold_abs=20)
        
        enhanced = np.log1p(diff.astype(np.float32))
        enhanced = cv2.normalize(enhanced, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)
        vis = cv2.cvtColor(enhanced, cv2.COLOR_GRAY2BGR)
        
        cv2.drawMarker(vis, (cx, cy), (0,0,255), cv2.MARKER_CROSS, 20, 2)
        peak_list = []
        for p in coords:
            py, px = p
            if abs(py-cy)<5 and abs(px-cx)<5: continue
            cv2.circle(vis, (px, py), 3, (0,255,0), 1)
            peak_list.append(f"[{px},{py}]")

        log.append(f"Spots: {len(peak_list)}")
        def to_b64(im): return base64.b64encode(cv2.imencode('.jpg', im)[1]).decode('utf-8')
        return {"type": "image", "raw_b64": to_b64(img_bgr), "proc_b64": to_b64(vis), "stats": {"center": [cx, cy], "spots": len(peak_list)}, "log": log}

    # --- [G] SAM ---
    @staticmethod
    def analyze_sam_results_nv7(img_rgb, masks, data_type):
        overlay = img_rgb.copy()
        mask_overlay = np.zeros_like(img_rgb)
        h_img, w_img = img_rgb.shape[:2]
        stats = {"count": 0}
        log = []
        
        valid_masks = []
        for ann in masks:
            area = ann['area']; bbox = ann['bbox']; x, y, w, h = [int(v) for v in bbox]
            if area > h_img * w_img * 0.95 or area < 20: continue 
            if data_type == "Particle":
                if x<=1 or y<=1 or (x+w)>=w_img-1 or (y+h)>=h_img-1: continue
                mask_uint8 = (ann['segmentation'] * 255).astype(np.uint8)
                cnts, _ = cv2.findContours(mask_uint8, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
                if cnts:
                    perim = cv2.arcLength(cnts[0], True)
                    circ = 4*np.pi*area/(perim**2) if perim>0 else 0
                    if circ < 0.25: continue
            valid_masks.append(ann)

        if data_type == "Thin Film":
            sorted_masks = sorted(valid_masks, key=lambda x: x['bbox'][1])
        else:
            sorted_masks = sorted(valid_masks, key=lambda x: x['area'], reverse=True)

        thickness_vals = []; diameters = []

        for i, ann in enumerate(sorted_masks):
            m = ann['segmentation']
            bbox = ann['bbox']; x, y, w, h = [int(v) for v in bbox]
            cx, cy = int(x + w/2), int(y + h/2)
            color = (0, 255, 0) 

            if data_type == "Thin Film":
                is_bound = (i == 0 or i == len(sorted_masks)-1)
                cv2.putText(overlay, f"#{i+1}", (cx, cy), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (255, 255, 255), 3)
                if not is_bound:
                    thk = ann['area'] / w
                    thickness_vals.append(thk)
                    color = (255, 0, 0) 
                else: color = (100, 100, 100)
                if i < len(sorted_masks) - 1:
                    ys, xs = np.where(m)
                    if len(xs) > 10:
                        roughness = np.std(ys)
                        cv2.line(overlay, (x, y+h), (x+w, y+h), (0, 255, 255), 2)
                        cv2.putText(overlay, f"Int#{i+1} R:{roughness:.2f}", (x+10, y+h-5), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)
            elif data_type == "Particle":
                equivalent_diameter = 2 * np.sqrt(ann['area'] / np.pi)
                diameters.append(equivalent_diameter)

            mask_overlay[m] = color
            mask_uint8 = (m * 255).astype(np.uint8)
            cnts, _ = cv2.findContours(mask_uint8, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
            if cnts: cv2.drawContours(overlay, [cnts[0]], -1, color, 1)

        alpha = 0.35
        mask_idx = np.any(mask_overlay > 0, axis=-1)
        overlay[mask_idx] = (overlay[mask_idx]*(1-alpha) + mask_overlay[mask_idx]*alpha).astype(np.uint8)

        stats["count"] = len(sorted_masks)
        if data_type == "Thin Film":
            avg = round(np.mean(thickness_vals), 2) if thickness_vals else 0
            stats["thickness_avg"] = avg
            log.append(f"Layers: {len(sorted_masks)}, Avg Thk: {avg}")
        elif data_type == "Particle":
            avg = round(np.mean(diameters), 2) if diameters else 0
            stats["diameter_avg"] = avg
            log.append(f"Particles: {len(sorted_masks)}")
        return overlay, stats, log

    # --- Router (Overwrite Fix) ---
    @staticmethod
    def process_image_nv17(img_bytes, equipment, data_type, mode, goal, user_center=None, 
                           patch_size=128, step=16, n_components=10):
        nparr = np.frombuffer(img_bytes, np.uint8)
        img_raw = cv2.imdecode(nparr, cv2.IMREAD_COLOR)
        if img_raw is None: return {}
        
        body, footer = ScienceProcessorNV17.separate_footer(img_raw)
        
        footer_b64 = None
        if footer is not None:
            footer_b64 = base64.b64encode(cv2.imencode('.jpg', footer)[1]).decode('utf-8')

        # [NV17] FFT-NMF
        if data_type == "Crystal Structure (FFT)":
            # The function returns its own footer_b64 (Basis Strip)
            # We MUST NOT overwrite it with the variable 'footer_b64' defined above (original footer)
            return ScienceProcessorNV17.analyze_crystal_fft_nmf(body, patch_size, step, n_components)
            
        if data_type == "2D Diffraction":
            return {**ScienceProcessorNV17.analyze_diffraction_nv7(body, user_center), "footer_b64": footer_b64}
        
        # General SAM
        if mode == "None":
             def to_b64(im): return base64.b64encode(cv2.imencode('.jpg', im)[1]).decode('utf-8')
             return {"type":"image", "raw_b64":to_b64(body), "proc_b64":to_b64(body), "stats":{}, "log":["Mode: Raw"], "footer_b64": footer_b64}

        img_proc = body.copy(); log = []
        if mode == "Auto" or mode == "AI-Adaptive":
            img_proc = cv2.bilateralFilter(img_proc, 9, 75, 75)
            log.append("Preproc: Denoise")

        img_rgb = cv2.cvtColor(img_proc, cv2.COLOR_BGR2RGB)
        
        if mask_generator:
            try:
                masks = mask_generator.generate(img_rgb)
                log.append(f"SAM: {len(masks)} Segments")
                overlay, stats, sam_log = ScienceProcessorNV17.analyze_sam_results_nv7(img_rgb, masks, data_type)
                log.extend(sam_log)
                overlay = cv2.cvtColor(overlay, cv2.COLOR_RGB2BGR)
            except Exception as e:
                log.append(f"SAM Error: {str(e)}")
                overlay = body; stats = {}
        else:
            log.append("SAM Not Loaded")
            overlay = body; stats = {}

        def to_b64(im): return base64.b64encode(cv2.imencode('.jpg', im)[1]).decode('utf-8')
        return {"type": "image", "raw_b64": to_b64(body), "proc_b64": to_b64(overlay), "stats": stats, "log": log, "footer_b64": footer_b64}

# [4] Helper (Same)
async def analyze_vision_ollama(image_bytes: bytes, prompt: str) -> str:
    try:
        b64 = base64.b64encode(image_bytes).decode('utf-8')
        res = await asyncio.to_thread(ollama_client.chat, model=VISION_MODEL, messages=[{'role':'user','content':prompt,'images':[b64]}])
        return res['message']['content']
    except: return "Vision Error"

async def interpret_spectrum_data(raw_context: str, equipment: str) -> str:
    try:
        prompt = f"Analyze spectrum ({equipment}). Brief interpretation in Korean. Data: {raw_context}"
        res = await asyncio.to_thread(ollama_client.chat, model=TEXT_MODEL, messages=[{'role':'user','content':prompt}])
        return res['message']['content']
    except: return ""

# [5] App
app = FastAPI(title="Analyst NV17 Visual Fixed", lifespan=lifespan)
app.add_middleware(CORSMiddleware, allow_origins=["*"], allow_methods=["*"], allow_headers=["*"])

class TranslateReq(BaseModel): text: str

@app.post("/api/translate")
async def translate_text(req: TranslateReq):
    try:
        res = await asyncio.to_thread(ollama_client.chat, model=TEXT_MODEL, messages=[{'role':'system','content':'Translate to English.'}, {'role':'user','content':req.text}])
        return {"translated": res['message']['content']}
    except Exception as e: return {"translated": str(e)}

async def process_analysis_job(job_id: str, files_map: Dict, configs: Dict):
    try:
        JOBS[job_id]["status"] = "Processing"
        JOBS[job_id]["progress"] = 10
        final_results = []
        total_files = len(configs)
        processed_count = 0

        for filename, config in configs.items():
            if filename not in files_map: continue
            content = files_map[filename]
            eq = config.get("equipment", "General")
            data_type = config.get("data_type", "General")
            goal = config.get("goal", "")
            mode = config.get("mode", "Auto")
            
            user_center = None
            if config.get("center_coords"):
                try: user_center = [int(v) for v in config["center_coords"].split(",")]
                except: pass
            
            p_size = int(config.get("patch_size", 128))
            p_step = int(config.get("step", 16))
            n_comp = int(config.get("n_components", 10))

            fname_lower = filename.lower()
            JOBS[job_id]["step"] = f"Analyzing {filename}..."
            
            if eq == "Document" or fname_lower.endswith(('.pdf', '.ppt', '.pptx')):
                try:
                    if fname_lower.endswith('.pdf'):
                        images = convert_from_bytes(content, dpi=150, fmt='jpeg')[:10]
                        full_txt = ""; pages = []
                        for idx, img in enumerate(images):
                            buf = io.BytesIO(); img.save(buf, format="JPEG")
                            b64 = base64.b64encode(buf.getvalue()).decode()
                            desc = await analyze_vision_ollama(buf.getvalue(), "한글 요약.")
                            full_txt += f"\nPage {idx+1}: {desc}"
                            pages.append({"page_num": idx+1, "image_b64": b64, "desc": desc})
                        final_results.append({"type": "pdf", "filename": filename, "equipment": "Lit", "raw_context": full_txt, "pages": pages})
                    elif fname_lower.endswith(('.ppt', '.pptx')):
                        prs = Presentation(io.BytesIO(content)); slides = []; full_txt = ""
                        for i, slide in enumerate(prs.slides):
                            txt = ""; imgs = []
                            for s in slide.shapes:
                                if hasattr(s, "text"): txt += s.text + "\n"
                                if s.shape_type == MSO_SHAPE_TYPE.PICTURE:
                                    try:
                                        ib64 = base64.b64encode(s.image.blob).decode('utf-8')
                                        desc = await analyze_vision_ollama(s.image.blob, "한글 설명.")
                                        imgs.append({"b64": ib64, "desc": desc})
                                    except: pass
                            slides.append({"slide_num": i+1, "text": txt, "images": imgs})
                        final_results.append({"type": "ppt", "filename": filename, "equipment": "Lit", "raw_context": full_txt, "slides": slides})
                except Exception as e: final_results.append({"type": "error", "filename": filename, "msg": str(e)})

            elif fname_lower.endswith(('.xlsx', '.xls', '.csv', '.txt')):
                try:
                    dfs = ScienceProcessorNV17.read_universal_dataframe(content, filename)
                    for i, df in enumerate(dfs):
                        v_blocks = ScienceProcessorNV17.detect_structured_blocks(df)
                        for v_block in v_blocks:
                            h_blocks = ScienceProcessorNV17.split_horizontal_blocks(v_block)
                            for h_block in h_blocks:
                                series_list = ScienceProcessorNV17.extract_series_from_df(h_block)
                                for s in series_list:
                                    res = ScienceProcessorNV17.process_spectrum(s['x'], s['y'], mode, goal)
                                    step = max(1, len(s['x'])//100)
                                    chart = [{"x": float(res["x"][k]), "y_proc": float(res["y_proc"][k]), "y_raw": float(res["y_raw"][k]), "y_base": float(res["y_base"][k])} for k in range(0, len(s['x']), step)]
                                    ctx = f"Data: {s['name']}\nStats: {res['stats']}"
                                    interp = await interpret_spectrum_data(ctx, eq)
                                    final_results.append({
                                        "type": "spectrum", "filename": f"{filename} ({s['name']})", "equipment": eq,
                                        "raw_context": f"{ctx}\nInterp: {interp}", 
                                        "chart_data": chart, "log": res["log"], "interpretation": interp
                                    })
                except Exception as ex: final_results.append({"type": "error", "filename": filename, "msg": str(ex)})

            elif fname_lower.endswith(('.png', '.jpg', '.jpeg', '.tif', '.bmp')):
                try:
                    JOBS[job_id]["step"] = f"Vision Analysis: {filename}..."
                    vis_res = await asyncio.to_thread(ScienceProcessorNV17.process_image_nv17, content, eq, data_type, mode, goal, user_center, p_size, p_step, n_comp)
                    if vis_res:
                        scale_info = "Unknown"
                        final_footer = vis_res.get("footer_b64")
                        
                        # Scale reading only if NOT Crystal FFT (because footer is patterns there)
                        if final_footer and data_type != "Crystal Structure (FFT)":
                            JOBS[job_id]["step"] = f"Scale Reading..."
                            scale_text = await analyze_vision_ollama(base64.b64decode(final_footer), "Read scale bar text only.")
                            scale_info = scale_text
                            vis_res["log"].append(f"Scale: {scale_text}")

                        stats_str = json.dumps(vis_res.get('stats',{}), indent=2)
                        desc_prompt = f"Analyze {eq} ({data_type}). Stats: {stats_str}. Goal: {goal}. Summary in Korean."
                        desc = await analyze_vision_ollama(base64.b64decode(vis_res["proc_b64"]), desc_prompt)
                        
                        final_results.append({
                            "type": "image", "filename": filename, "equipment": eq, "summary": desc,
                            "raw_context": f"Img {eq}: {desc}", "raw_b64": vis_res["raw_b64"], "proc_b64": vis_res["proc_b64"], "footer_b64": final_footer,
                            "stats": vis_res.get("stats",{}), "log": vis_res.get("log", [])
                        })
                except Exception as ex: final_results.append({"type": "error", "filename": filename, "msg": str(ex)})
            
            processed_count += 1
            JOBS[job_id]["progress"] = 10 + int((processed_count / total_files) * 80)

        JOBS[job_id]["step"] = "Writing Report..."
        final_results = sanitize_json(final_results)
        
        data_ctx = ""
        for r in final_results:
            if r.get("type") != "error":
                raw = r.get("raw_context") or ""
                data_ctx += f"\n=== {r['filename']} ===\n{raw[:1000]}\n"

        final_report = "Fail"
        if data_ctx:
            try:
                system_prompt = "You are a research assistant. Write a report in Korean. Summarize data and physics."
                res = await asyncio.to_thread(ollama_client.chat, model=TEXT_MODEL, messages=[{'role':'system','content':system_prompt}, {'role':'user','content':f"Data:\n{data_ctx}"}])
                final_report = res['message']['content']
            except Exception as e: final_report = str(e)

        JOBS[job_id]["results"] = {"results": final_results, "final_report": final_report}
        JOBS[job_id]["status"] = "Completed"
        JOBS[job_id]["progress"] = 100
    except Exception as e:
        JOBS[job_id]["status"] = "Failed"; JOBS[job_id]["error"] = str(e)

@app.post("/api/analyze")
async def start_analysis(background_tasks: BackgroundTasks, files: List[UploadFile] = File(...), file_configs: str = Form(...)):
    job_id = str(uuid.uuid4()); configs = json.loads(file_configs)
    files_map = {f.filename: await f.read() for f in files}
    JOBS[job_id] = {"status": "Pending", "progress": 0, "step": "Init", "results": None}
    background_tasks.add_task(process_analysis_job, job_id, files_map, configs)
    return {"job_id": job_id}

@app.get("/api/status/{job_id}")
async def get_status(job_id: str): return JOBS.get(job_id, {"status": "Not Found"})

@app.get("/", response_class=HTMLResponse)
async def serve_index():
    if os.path.exists("index.html"): return FileResponse("index.html")
    return "<h1>NV17 Visual Fixed Backend</h1>"

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)



엔브이16


import os
import io
# [0] OMP Error Fix
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"
import torch

import asyncio
import base64
import json
import re
import math
import uuid
import time
from typing import List, Dict, Any, Optional
from pydantic import BaseModel
from contextlib import asynccontextmanager
from collections import Counter

# [1] Security
os.environ['NO_PROXY'] = '127.0.0.1,localhost,0.0.0.0,::1'
if 'HTTP_PROXY' in os.environ: del os.environ['HTTP_PROXY']
if 'HTTPS_PROXY' in os.environ: del os.environ['HTTPS_PROXY']

from fastapi import FastAPI, UploadFile, File, Form, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import HTMLResponse, FileResponse
import pandas as pd
import numpy as np

# SciPy & CV & Sklearn
from sklearn.decomposition import NMF
from scipy.signal import find_peaks, savgol_filter
from scipy.optimize import curve_fit
from scipy.ndimage import center_of_mass
from skimage.feature import peak_local_max
import cv2
import matplotlib
import matplotlib.pyplot as plt 

# Models
from segment_anything import sam_model_registry, SamAutomaticMaskGenerator
from ollama import Client 
from pdf2image import convert_from_bytes
from pptx import Presentation
from pptx.enum.shapes import MSO_SHAPE_TYPE

# [2] Config
OLLAMA_HOST = "http://127.0.0.1:11434"
ollama_client = Client(host=OLLAMA_HOST)
VISION_MODEL = "qwen3-vl:30b-a3b-instruct" 
TEXT_MODEL = "llama3:70b"

SAM_CHECKPOINT = "sam_vit_b_01ec64.pth"
SAM_MODEL_TYPE = "vit_b"
device = "cuda" if torch.cuda.is_available() else "cpu"

mask_generator = None 
JOBS = {} 

@asynccontextmanager
async def lifespan(app: FastAPI):
    global mask_generator
    if os.path.exists(SAM_CHECKPOINT):
        print(f"Loading SAM ({SAM_MODEL_TYPE}) on {device}...")
        try:
            sam = sam_model_registry[SAM_MODEL_TYPE](checkpoint=SAM_CHECKPOINT)
            sam.to(device=device)
            mask_generator = SamAutomaticMaskGenerator(
                model=sam, points_per_side=32, pred_iou_thresh=0.86,
                stability_score_thresh=0.90, min_mask_region_area=50
            )
            print("SAM Loaded.")
        except: print("SAM Load Failed.")
    yield
    JOBS.clear()

def sanitize_json(obj):
    if isinstance(obj, float):
        if np.isnan(obj) or np.isinf(obj): return 0.0
        return obj
    elif isinstance(obj, dict): return {k: sanitize_json(v) for k, v in obj.items()}
    elif isinstance(obj, list): return [sanitize_json(v) for v in obj]
    elif isinstance(obj, (np.int64, np.int32)): return int(obj)
    elif isinstance(obj, (np.float64, np.float32)):
        if np.isnan(obj) or np.isinf(obj): return 0.0
        return float(obj)
    return obj

# ==========================================
# [3] NV16 High-Contrast Engine
# ==========================================
class ScienceProcessorNV16:
    
    # --- Universal Loader ---
    @staticmethod
    def read_universal_dataframe(content: bytes, filename: str) -> List[pd.DataFrame]:
        dfs = []
        fname = filename.lower()
        if fname.endswith(('.xlsx', '.xls')):
            try:
                xls = pd.ExcelFile(io.BytesIO(content))
                for s in xls.sheet_names:
                    df = pd.read_excel(xls, sheet_name=s, header=None)
                    num = df.apply(pd.to_numeric, errors='coerce').notna().sum().sum()
                    if num < 5: continue
                    if not df.empty: dfs.append(df)
                if dfs: return dfs
            except: pass
        text_content = ""
        for enc in ['utf-8', 'cp949', 'latin1']:
            try: text_content = content.decode(enc); break
            except: continue
        if text_content:
            lines = text_content.splitlines()
            data_start = -1; delimiter = None
            separators = [',', '\t', ';', '\s+']
            for i, line in enumerate(lines[:100]):
                for sep in separators:
                    parts = line.split() if sep=='\s+' else line.split(sep)
                    nums = sum(1 for p in parts if re.match(r'^-?\d+(\.\d+)?(?:[eE][+-]?\d+)?$', p.strip()))
                    if nums >= 2: data_start = i; delimiter = sep; break
                if data_start != -1: break
            if data_start != -1:
                try:
                    data_str = "\n".join(lines[data_start:])
                    sep_arg = '\s+' if delimiter=='\s+' else delimiter
                    df = pd.read_csv(io.StringIO(data_str), sep=sep_arg, header=None, engine='python')
                    dfs.append(df)
                except: pass
        return dfs

    # --- Block Parser ---
    @staticmethod
    def detect_structured_blocks(df: pd.DataFrame) -> List[pd.DataFrame]:
        blocks = []
        if df.empty: return []
        df_num = df.apply(pd.to_numeric, errors='coerce')
        if df.shape[1] > 0: is_data_row = df_num.iloc[:, 0].notna()
        else: is_data_row = df_num.notna().any(axis=1)
        groups = (is_data_row != is_data_row.shift()).cumsum()
        for _, group in df.groupby(groups):
            if is_data_row.loc[group.index[0]]:
                block = df_num.loc[group.index].dropna(how='all', axis=1)
                if block.shape[0] >= 10: blocks.append(block.reset_index(drop=True))
        return blocks

    @staticmethod
    def split_horizontal_blocks(df: pd.DataFrame) -> List[pd.DataFrame]:
        sub_blocks = []
        if df.empty: return []
        col_counts = df.apply(pd.to_numeric, errors='coerce').notna().sum()
        valid = np.where(col_counts >= 2)[0]
        if len(valid) == 0: return []
        splits = np.where(np.diff(valid) > 1)[0] + 1
        groups = np.split(valid, splits)
        for g in groups:
            if len(g) > 0:
                sub = df.iloc[:, g].reset_index(drop=True)
                if sub.shape[1] >= 1 and sub.shape[0] >= 2: sub_blocks.append(sub)
        return sub_blocks

    @staticmethod
    def extract_series_from_df(df: pd.DataFrame) -> List[Dict]:
        series_list = []
        try:
            vals = df.values; rows, cols = vals.shape
            if rows < 5: return []
            if cols == 1:
                y = vals[:, 0]; mask = ~np.isnan(y)
                if np.sum(mask) > 5: series_list.append({"x": np.arange(len(y))[mask], "y": y[mask], "name": "Single"})
                return series_list
            def is_axis(arr):
                arr = arr[~np.isnan(arr)]
                if len(arr) < 5: return False
                diff = np.diff(arr)
                if np.all(diff >= 0) or np.all(diff <= 0): return True
                return (np.sum(diff>0)/len(diff)>0.9) or (np.sum(diff<0)/len(diff)>0.9)
            current_x = vals[:, 0]
            if not is_axis(current_x): current_x = np.arange(rows)
            for i in range(1, cols):
                col_data = vals[:, i]; mask_valid = ~np.isnan(col_data)
                if np.sum(mask_valid) < 5: continue
                if is_axis(col_data): current_x = col_data; continue 
                mask = ~np.isnan(current_x) & ~np.isnan(col_data)
                if np.sum(mask) > 5:
                    series_list.append({"x": current_x[mask], "y": col_data[mask], "name": f"Col-{i}"})
        except: pass
        return series_list

    # --- Spectrum Logic ---
    @staticmethod
    def simple_baseline(y):
        try:
            w = max(5, len(y)//10)
            s = pd.Series(y).rolling(window=w, center=True).min()
            b = s.bfill().ffill().values 
            if len(b) > 51: b = savgol_filter(b, 51, 3)
            return b, w
        except: return np.zeros_like(y), 0

    @staticmethod
    def process_spectrum(x, y, mode, goal):
        log = []; y_raw = y.copy(); base = np.zeros_like(y); y_proc = y.copy()
        peaks = []; fits = []; stats_txt = "Error"
        try:
            if mode == "None":
                peaks, _ = find_peaks(y, height=np.max(y)*0.05)
                log.append("Mode: Raw")
                stats_txt = f"Raw Peaks: {len(peaks)}"
            else:
                win = 15; do_fit = False
                if mode == "AI-Adaptive":
                    log.append("Mode: AI-Adaptive")
                    if "noise" in goal.lower(): win = 31
                    if "fit" in goal.lower(): do_fit = True
                else: log.append("Mode: Auto")
                base, bw = ScienceProcessorNV16.simple_baseline(y)
                y_proc = np.maximum(y - base, 0)
                log.append(f"Base: Rolling Min (w={bw})")
                if len(y_proc) > win:
                    y_proc = savgol_filter(y_proc, win, 3)
                    log.append(f"Smooth: SavGol (w={win})")
                peaks, _ = find_peaks(y_proc, height=np.max(y_proc)*0.05, distance=10)
                log.append(f"Peaks: {len(peaks)}")
                stats_txt = f"Range: {np.min(x):.1f}~{np.max(x):.1f}, Peaks: {len(peaks)}"
            return {"x": x, "y_raw": y_raw, "y_proc": y_proc, "y_base": base, "peaks": [{"x": float(x[p]), "y": float(y_proc[p])} for p in peaks], "log": log, "fits": fits, "stats": stats_txt}
        except Exception as e: return {"x": x, "y_raw": y_raw, "y_proc": y_proc, "y_base": base, "peaks": [], "log": [str(e)], "fits": [], "stats": "Error"}

    # --- Footer ---
    @staticmethod
    def separate_footer(img_bgr):
        h, w = img_bgr.shape[:2]
        roi_h = int(h * 0.82)
        roi = img_bgr[roi_h:, :]
        gray = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)
        sobel_y = np.abs(cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=3))
        row_sums = np.sum(sobel_y, axis=1)
        split_idx = np.argmax(row_sums)
        if row_sums[split_idx] > w * 30:
            real_split = roi_h + split_idx
            return img_bgr[:real_split, :], img_bgr[real_split:, :]
        return img_bgr, None

    # --- [E] NV16 High-Contrast FFT-NMF Engine ---
    @staticmethod
    def analyze_crystal_fft_nmf(img_bgr, patch_size=128, step=16, n_components=10):
        log = []
        gray = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2GRAY)
        h, w = gray.shape
        
        log.append(f"FFT-NMF: Patch={patch_size}, Step={step}, K={n_components}")
        
        # 1. Custom High-Contrast Palette (Manual)
        # Avoid Black, White, Gray for patterns. Use distinct neon colors.
        # Format: (B, G, R)
        distinct_colors = [
            (0, 0, 255),    # Red
            (0, 255, 0),    # Green
            (255, 0, 0),    # Blue
            (0, 255, 255),  # Yellow
            (255, 255, 0),  # Cyan
            (255, 0, 255),  # Magenta
            (0, 165, 255),  # Orange
            (128, 0, 128),  # Purple
            (203, 192, 255),# Pink
            (128, 128, 0)   # Teal
        ]
        
        # Adjust if n_components > 10
        if n_components > 10:
            cmap = matplotlib.colormaps['jet']
            distinct_colors = []
            for i in range(n_components):
                rgba = cmap(i / (n_components-1))
                distinct_colors.append((int(rgba[2]*255), int(rgba[1]*255), int(rgba[0]*255)))

        # 2. Collect FFT Data
        cols = [] 
        y_steps = list(range(0, h - patch_size + 1, step))
        x_steps = list(range(0, w - patch_size + 1, step))
        grid_h = len(y_steps)
        grid_w = len(x_steps)
        
        for y in y_steps:
            for x in x_steps:
                roi = gray[y:y+patch_size, x:x+patch_size]
                
                # [Important] Ensure we use FFT Magnitude, NOT ROI itself
                f = np.fft.fft2(roi)
                fshift = np.fft.fftshift(f)
                mag = np.abs(fshift) 
                
                # DC Masking
                cy, cx = patch_size//2, patch_size//2
                mag[cy-2:cy+3, cx-2:cx+3] = 0
                
                cols.append(mag.flatten())
        
        if not cols: return {"type":"error", "msg":"Img too small"}
        
        try:
            # Transpose: (Pixels, Patches)
            V = np.array(cols).T.astype(np.float32)
            
            # Global Scaling
            V = V - V.min()
            if V.max() > 0: V = V / V.max()
            
            # NMF
            nmf = NMF(n_components=n_components, init='nndsvda', random_state=42, max_iter=1000)
            W = nmf.fit_transform(V) # Columns are Basis Images (FFT Patterns)
            H = nmf.components_      # Rows are Spatial Weights
            
            # Reconstruct Spatial Map
            if H.shape[1] != grid_h * grid_w: raise ValueError("Shape Mismatch")
            
            idx_H = np.argmax(H, axis=0) 
            map_low = idx_H.reshape(grid_h, grid_w).astype(np.uint8)
            map_high = cv2.resize(map_low, (w, h), interpolation=cv2.INTER_NEAREST)
            
            # Overlay
            overlay = img_bgr.copy()
            mask_layer = np.zeros_like(img_bgr)
            
            counts = Counter(idx_H)
            total = len(idx_H)
            stats = {}
            
            for i in range(n_components):
                idx = i % len(distinct_colors)
                color = distinct_colors[idx]
                
                mask = (map_high == i)
                mask_layer[mask] = color
                
                cnt = counts.get(i, 0)
                stats[f"Pattern {i}"] = f"{cnt} ({cnt/total*100:.1f}%)"
                
            alpha = 0.4
            mask_idx = np.any(mask_layer > 0, axis=-1)
            overlay[mask_idx] = (overlay[mask_idx]*(1-alpha) + mask_layer[mask_idx]*alpha).astype(np.uint8)
            
            # [Fix] Basis Pattern Visualization (Heatmap Style)
            basis_combo_list = []
            
            for k in range(n_components):
                idx = k % len(distinct_colors)
                border_color = distinct_colors[idx]
                
                # 1. Pattern Image (FFT Basis)
                pat = W[:, k].reshape(patch_size, patch_size)
                
                # Log Scale for visibility
                pat_log = np.log1p(pat * 1000) # Boost signal
                pat_norm = cv2.normalize(pat_log, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)
                
                # Apply Magma Colormap (Black background, bright spots)
                pat_color = cv2.applyColorMap(pat_norm, cv2.COLORMAP_MAGMA)
                
                # 2. Legend Block (Tall & Clear)
                legend_h = 40
                legend = np.zeros((legend_h, patch_size, 3), dtype=np.uint8)
                # Fill with solid color
                legend[:] = border_color
                
                # Text: P0 (White or Black depending on brightness?) -> White mostly fine
                cv2.putText(legend, f"P{k}", (5, 25), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255,255,255), 2)
                
                # Stack
                combo = np.vstack([legend, pat_color])
                
                # Border around combo
                cv2.rectangle(combo, (0,0), (combo.shape[1]-1, combo.shape[0]-1), border_color, 2)
                
                basis_combo_list.append(combo)
            
            # Strip
            full_strip = np.hstack(basis_combo_list)
            # Scale 2x
            full_strip_large = cv2.resize(full_strip, None, fx=2, fy=2, interpolation=cv2.INTER_NEAREST)
            
            def to_b64(im): return base64.b64encode(cv2.imencode('.jpg', im)[1]).decode('utf-8')
            
            return {
                "type": "image", 
                "raw_b64": to_b64(img_bgr), 
                "proc_b64": to_b64(overlay),
                "footer_b64": to_b64(full_strip_large), 
                "stats": stats, 
                "log": log
            }

        except Exception as e:
            print(f"NMF Error: {e}")
            def to_b64(im): return base64.b64encode(cv2.imencode('.jpg', im)[1]).decode('utf-8')
            return {"type": "image", "raw_b64": to_b64(img_bgr), "proc_b64": to_b64(img_bgr), "stats": {"Error": str(e)}, "log": log}

    # --- Router ---
    @staticmethod
    def process_image_nv16(img_bytes, equipment, data_type, mode, goal, user_center=None, 
                           patch_size=128, step=16, n_components=10):
        nparr = np.frombuffer(img_bytes, np.uint8)
        img_raw = cv2.imdecode(nparr, cv2.IMREAD_COLOR)
        if img_raw is None: return {}
        
        body, footer = ScienceProcessorNV16.separate_footer(img_raw)
        
        footer_b64 = None
        if footer is not None:
            footer_b64 = base64.b64encode(cv2.imencode('.jpg', footer)[1]).decode('utf-8')

        # [NV16] FFT-NMF
        if data_type == "Crystal Structure (FFT)":
            return {**ScienceProcessorNV16.analyze_crystal_fft_nmf(body, patch_size, step, n_components), "footer_b64": footer_b64}
            
        if data_type == "2D Diffraction":
            return {**ScienceProcessorNV16.analyze_diffraction_nv7(body, user_center), "footer_b64": footer_b64}
        
        # General SAM
        if mode == "None":
             def to_b64(im): return base64.b64encode(cv2.imencode('.jpg', im)[1]).decode('utf-8')
             return {"type":"image", "raw_b64":to_b64(body), "proc_b64":to_b64(body), "stats":{}, "log":["Mode: Raw"], "footer_b64": footer_b64}

        img_proc = body.copy(); log = []
        if mode == "Auto" or mode == "AI-Adaptive":
            img_proc = cv2.bilateralFilter(img_proc, 9, 75, 75)
            log.append("Preproc: Denoise")

        img_rgb = cv2.cvtColor(img_proc, cv2.COLOR_BGR2RGB)
        
        if mask_generator:
            try:
                masks = mask_generator.generate(img_rgb)
                log.append(f"SAM: {len(masks)} Segments")
                overlay, stats, sam_log = ScienceProcessorNV16.analyze_sam_results_nv7(img_rgb, masks, data_type)
                log.extend(sam_log)
                overlay = cv2.cvtColor(overlay, cv2.COLOR_RGB2BGR)
            except Exception as e:
                log.append(f"SAM Error: {str(e)}")
                overlay = body; stats = {}
        else:
            log.append("SAM Not Loaded")
            overlay = body; stats = {}

        def to_b64(im): return base64.b64encode(cv2.imencode('.jpg', im)[1]).decode('utf-8')
        return {"type": "image", "raw_b64": to_b64(body), "proc_b64": to_b64(overlay), "stats": stats, "log": log, "footer_b64": footer_b64}

# [4] Helper
async def analyze_vision_ollama(image_bytes: bytes, prompt: str) -> str:
    try:
        b64 = base64.b64encode(image_bytes).decode('utf-8')
        res = await asyncio.to_thread(ollama_client.chat, model=VISION_MODEL, messages=[{'role':'user','content':prompt,'images':[b64]}])
        return res['message']['content']
    except: return "Vision Error"

async def interpret_spectrum_data(raw_context: str, equipment: str) -> str:
    try:
        prompt = f"Analyze spectrum ({equipment}). Brief interpretation in Korean. Data: {raw_context}"
        res = await asyncio.to_thread(ollama_client.chat, model=TEXT_MODEL, messages=[{'role':'user','content':prompt}])
        return res['message']['content']
    except: return ""

# [5] App
app = FastAPI(title="Analyst NV16 Visual", lifespan=lifespan)
app.add_middleware(CORSMiddleware, allow_origins=["*"], allow_methods=["*"], allow_headers=["*"])

class TranslateReq(BaseModel): text: str

@app.post("/api/translate")
async def translate_text(req: TranslateReq):
    try:
        res = await asyncio.to_thread(ollama_client.chat, model=TEXT_MODEL, messages=[{'role':'system','content':'Translate to English.'}, {'role':'user','content':req.text}])
        return {"translated": res['message']['content']}
    except Exception as e: return {"translated": str(e)}

async def process_analysis_job(job_id: str, files_map: Dict, configs: Dict):
    try:
        JOBS[job_id]["status"] = "Processing"
        JOBS[job_id]["progress"] = 10
        final_results = []
        total_files = len(configs)
        processed_count = 0

        for filename, config in configs.items():
            if filename not in files_map: continue
            content = files_map[filename]
            eq = config.get("equipment", "General")
            data_type = config.get("data_type", "General")
            goal = config.get("goal", "")
            mode = config.get("mode", "Auto")
            
            user_center = None
            if config.get("center_coords"):
                try: user_center = [int(v) for v in config["center_coords"].split(",")]
                except: pass
            
            p_size = int(config.get("patch_size", 128))
            p_step = int(config.get("step", 16))
            n_comp = int(config.get("n_components", 10))

            fname_lower = filename.lower()
            JOBS[job_id]["step"] = f"Analyzing {filename}..."
            
            if eq == "Document" or fname_lower.endswith(('.pdf', '.ppt', '.pptx')):
                try:
                    if fname_lower.endswith('.pdf'):
                        images = convert_from_bytes(content, dpi=150, fmt='jpeg')[:10]
                        full_txt = ""; pages = []
                        for idx, img in enumerate(images):
                            buf = io.BytesIO(); img.save(buf, format="JPEG")
                            b64 = base64.b64encode(buf.getvalue()).decode()
                            desc = await analyze_vision_ollama(buf.getvalue(), "한글 요약.")
                            full_txt += f"\nPage {idx+1}: {desc}"
                            pages.append({"page_num": idx+1, "image_b64": b64, "desc": desc})
                        final_results.append({"type": "pdf", "filename": filename, "equipment": "Lit", "raw_context": full_txt, "pages": pages})
                    elif fname_lower.endswith(('.ppt', '.pptx')):
                        prs = Presentation(io.BytesIO(content)); slides = []; full_txt = ""
                        for i, slide in enumerate(prs.slides):
                            txt = ""; imgs = []
                            for s in slide.shapes:
                                if hasattr(s, "text"): txt += s.text + "\n"
                                if s.shape_type == MSO_SHAPE_TYPE.PICTURE:
                                    try:
                                        ib64 = base64.b64encode(s.image.blob).decode('utf-8')
                                        desc = await analyze_vision_ollama(s.image.blob, "한글 설명.")
                                        imgs.append({"b64": ib64, "desc": desc})
                                    except: pass
                            slides.append({"slide_num": i+1, "text": txt, "images": imgs})
                        final_results.append({"type": "ppt", "filename": filename, "equipment": "Lit", "raw_context": full_txt, "slides": slides})
                except Exception as e: final_results.append({"type": "error", "filename": filename, "msg": str(e)})

            elif fname_lower.endswith(('.xlsx', '.xls', '.csv', '.txt')):
                try:
                    dfs = ScienceProcessorNV16.read_universal_dataframe(content, filename)
                    for i, df in enumerate(dfs):
                        v_blocks = ScienceProcessorNV16.detect_structured_blocks(df)
                        for v_block in v_blocks:
                            h_blocks = ScienceProcessorNV16.split_horizontal_blocks(v_block)
                            for h_block in h_blocks:
                                series_list = ScienceProcessorNV16.extract_series_from_df(h_block)
                                for s in series_list:
                                    res = ScienceProcessorNV16.process_spectrum(s['x'], s['y'], mode, goal)
                                    step = max(1, len(s['x'])//100)
                                    chart = [{"x": float(res["x"][k]), "y_proc": float(res["y_proc"][k]), "y_raw": float(res["y_raw"][k]), "y_base": float(res["y_base"][k])} for k in range(0, len(s['x']), step)]
                                    ctx = f"Data: {s['name']}\nStats: {res['stats']}"
                                    interp = await interpret_spectrum_data(ctx, eq)
                                    final_results.append({
                                        "type": "spectrum", "filename": f"{filename} ({s['name']})", "equipment": eq,
                                        "raw_context": f"{ctx}\nInterp: {interp}", 
                                        "chart_data": chart, "log": res["log"], "interpretation": interp
                                    })
                except Exception as ex: final_results.append({"type": "error", "filename": filename, "msg": str(ex)})

            elif fname_lower.endswith(('.png', '.jpg', '.jpeg', '.tif', '.bmp')):
                try:
                    JOBS[job_id]["step"] = f"Vision Analysis: {filename}..."
                    vis_res = await asyncio.to_thread(ScienceProcessorNV16.process_image_nv16, content, eq, data_type, mode, goal, user_center, p_size, p_step, n_comp)
                    if vis_res:
                        scale_info = "Unknown"
                        final_footer = vis_res.get("footer_b64")
                        
                        # Scale OCR logic (Skip if FFT because footer is now patterns)
                        if final_footer and data_type != "Crystal Structure (FFT)":
                            JOBS[job_id]["step"] = f"Scale Reading..."
                            scale_text = await analyze_vision_ollama(base64.b64decode(final_footer), "Read scale bar text only.")
                            scale_info = scale_text
                            vis_res["log"].append(f"Scale: {scale_text}")

                        stats_str = json.dumps(vis_res.get('stats',{}), indent=2)
                        desc_prompt = f"Analyze {eq} ({data_type}). Stats: {stats_str}. Goal: {goal}. Summary in Korean."
                        desc = await analyze_vision_ollama(base64.b64decode(vis_res["proc_b64"]), desc_prompt)
                        
                        final_results.append({
                            "type": "image", "filename": filename, "equipment": eq, "summary": desc,
                            "raw_context": f"Img {eq}: {desc}", "raw_b64": vis_res["raw_b64"], "proc_b64": vis_res["proc_b64"], "footer_b64": final_footer,
                            "stats": vis_res.get("stats",{}), "log": vis_res.get("log", [])
                        })
                except Exception as ex: final_results.append({"type": "error", "filename": filename, "msg": str(ex)})
            
            processed_count += 1
            JOBS[job_id]["progress"] = 10 + int((processed_count / total_files) * 80)

        JOBS[job_id]["step"] = "Writing Report..."
        final_results = sanitize_json(final_results)
        
        data_ctx = ""
        for r in final_results:
            if r.get("type") != "error":
                raw = r.get("raw_context") or ""
                data_ctx += f"\n=== {r['filename']} ===\n{raw[:1000]}\n"

        final_report = "Fail"
        if data_ctx:
            try:
                system_prompt = "You are a research assistant. Write a report in Korean. Summarize data and physics."
                res = await asyncio.to_thread(ollama_client.chat, model=TEXT_MODEL, messages=[{'role':'system','content':system_prompt}, {'role':'user','content':f"Data:\n{data_ctx}"}])
                final_report = res['message']['content']
            except Exception as e: final_report = str(e)

        JOBS[job_id]["results"] = {"results": final_results, "final_report": final_report}
        JOBS[job_id]["status"] = "Completed"
        JOBS[job_id]["progress"] = 100
    except Exception as e:
        JOBS[job_id]["status"] = "Failed"; JOBS[job_id]["error"] = str(e)

@app.post("/api/analyze")
async def start_analysis(background_tasks: BackgroundTasks, files: List[UploadFile] = File(...), file_configs: str = Form(...)):
    job_id = str(uuid.uuid4()); configs = json.loads(file_configs)
    files_map = {f.filename: await f.read() for f in files}
    JOBS[job_id] = {"status": "Pending", "progress": 0, "step": "Init", "results": None}
    background_tasks.add_task(process_analysis_job, job_id, files_map, configs)
    return {"job_id": job_id}

@app.get("/api/status/{job_id}")
async def get_status(job_id: str): return JOBS.get(job_id, {"status": "Not Found"})

@app.get("/", response_class=HTMLResponse)
async def serve_index():
    if os.path.exists("index.html"): return FileResponse("index.html")
    return "<h1>NV16 High-Contrast Backend</h1>"

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)



엔브이15 웹코드

<!DOCTYPE html>
<html lang="ko">
<head>
    <meta charset="UTF-8">
    <title>Analyst NV15 Perfected</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <script src="https://unpkg.com/vue@3/dist/vue.global.js"></script>
    <script src="https://cdn.plot.ly/plotly-2.27.0.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/marked/marked.min.js"></script>
    <script src="https://unpkg.com/lucide@latest"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/html2pdf.js/0.10.1/html2pdf.bundle.min.js"></script>
    <style>
        @import url('https://fonts.googleapis.com/css2?family=Inter:wght@400;600;700&display=swap');
        body { font-family: 'Inter', sans-serif; }
        .prose table { border-collapse: collapse; width: 100%; margin: 1em 0; }
        .prose th, .prose td { border: 1px solid #cbd5e1; padding: 8px; font-size: 0.9rem; }
        .prose th { background-color: #f1f5f9; }
    </style>
</head>
<body class="bg-slate-100 text-slate-800">
    <div id="app" class="min-h-screen p-6">
        <header class="max-w-7xl mx-auto mb-6 flex items-center gap-3">
            <div class="bg-emerald-600 p-2 rounded-lg shadow"><i data-lucide="microscope" class="w-6 h-6 text-white"></i></div>
            <div><h1 class="text-2xl font-bold">Analyst NV15 <span class="text-sm font-normal text-slate-500">Visual Enhanced</span></h1></div>
        </header>

        <main class="max-w-7xl mx-auto grid grid-cols-1 lg:grid-cols-12 gap-6">
            <div class="lg:col-span-4 space-y-4">
                <div class="bg-white p-5 rounded-xl shadow-sm border border-slate-200">
                    <h2 class="font-bold mb-3 flex items-center gap-2"><i data-lucide="files" class="w-4 h-4"></i> Data Staging</h2>
                    <div class="border-2 border-dashed border-slate-300 rounded-lg p-6 text-center hover:bg-emerald-50 cursor-pointer" @click="$refs.f.click()">
                        <input type="file" ref="f" multiple class="hidden" @change="addFiles">
                        <p class="text-sm font-semibold text-slate-600">Add Files</p>
                    </div>
                    <div v-if="stagedFiles.length" class="mt-4 space-y-3 max-h-[60vh] overflow-y-auto pr-1">
                        <div v-for="(item, i) in stagedFiles" :key="i" class="p-3 bg-slate-50 border rounded-lg text-sm">
                            <div class="flex justify-between items-center mb-2">
                                <span class="font-bold truncate w-2/3">{{ item.file.name }}</span>
                                <button @click="removeFile(i)" class="text-red-400 hover:text-red-600"><i data-lucide="trash-2" class="w-4 h-4"></i></button>
                            </div>
                            <div class="grid grid-cols-1 gap-2">
                                <select v-model="item.equipment" class="w-full p-1.5 border rounded text-xs bg-white font-semibold text-emerald-700">
                                    <optgroup label="Spectroscopy (1D)">
                                        <option value="XPS">XPS</option>
                                        <option value="XRD">XRD (1D)</option>
                                        <option value="EELS">EELS</option>
                                        <option value="EDS">EDS</option>
                                        <option value="FT-IR">FT-IR</option>
                                        <option value="NMR">NMR</option>
                                        <option value="UV-VIS">UV-VIS</option>
                                        <option value="PL">Photo-luminescence</option>
                                        <option value="Time-resolved">Time-resolved</option>
                                    </optgroup>
                                    <optgroup label="Microscopy (2D)">
                                        <option value="SEM">SEM</option>
                                        <option value="TEM">TEM</option>
                                        <option value="AFM">AFM</option>
                                        <option value="Optical">Optical</option>
                                        <option value="2D Spectrum">2D Spectrum</option>
                                    </optgroup>
                                    <optgroup label="Diffraction (2D)">
                                        <option value="Diffraction">2D Diffraction</option>
                                    </optgroup>
                                    <optgroup label="Literature">
                                        <option value="Document">Document</option>
                                    </optgroup>
                                </select>
                                
                                <select v-if="isImage(item)" v-model="item.dataType" class="w-full p-1.5 border rounded text-xs bg-slate-50 font-bold">
                                    <option v-if="item.equipment==='Diffraction'" value="2D Diffraction">2D Diffraction</option>
                                    <option v-else value="General">General Object</option>
                                    <option v-if="item.equipment!=='Diffraction'" value="Particle">Particle Analysis</option>
                                    <option v-if="item.equipment!=='Diffraction'" value="Thin Film">Thin Film</option>
                                    <option v-if="item.equipment!=='Diffraction'" value="Crystal Structure (FFT)">Crystal Structure (FFT)</option>
                                </select>

                                <div v-if="item.dataType === 'Crystal Structure (FFT)'" class="flex gap-1">
                                    <input v-model="item.patchSize" type="number" placeholder="P(128)" class="w-1/3 p-1 border rounded text-[10px]">
                                    <input v-model="item.step" type="number" placeholder="S(16)" class="w-1/3 p-1 border rounded text-[10px]">
                                    <input v-model="item.nComp" type="number" placeholder="K(10)" class="w-1/3 p-1 border rounded text-[10px]">
                                </div>
                                <div v-if="item.dataType === '2D Diffraction'" class="flex gap-2">
                                    <input v-model="item.centerX" type="number" placeholder="CX" class="w-1/2 p-1.5 border rounded text-xs">
                                    <input v-model="item.centerY" type="number" placeholder="CY" class="w-1/2 p-1.5 border rounded text-xs">
                                </div>

                                <input v-model="item.goal" type="text" placeholder="Goal / Prompt (e.g. Find noise)" class="w-full p-1.5 border rounded text-xs">

                                <select v-if="!isDoc(item)" v-model="item.mode" class="w-full p-1.5 border rounded text-xs bg-white">
                                    <option value="Auto">✨ Auto Process</option>
                                    <option value="None">🚫 Raw Data</option>
                                    <option value="AI-Adaptive">🧠 Agent Mode</option>
                                </select>
                            </div>
                        </div>
                    </div>
                    <button @click="startAnalysis" :disabled="isAnalyzing || stagedFiles.length===0" class="w-full mt-4 py-3 bg-emerald-600 text-white rounded-lg font-bold flex justify-center gap-2 shadow hover:bg-emerald-700 disabled:opacity-50">
                        <span>Generate Report</span>
                    </button>
                </div>
            </div>

            <div class="lg:col-span-8">
                <div v-if="isAnalyzing" class="bg-white p-6 rounded-xl shadow border border-slate-200 text-center py-12">
                    <div class="animate-spin rounded-full h-12 w-12 border-b-2 border-emerald-600 mx-auto mb-4"></div>
                    <h3 class="text-lg font-bold text-slate-700">Analyzing...</h3>
                    <p class="text-sm text-slate-500 mb-4">{{ currentStep }}</p>
                    <div class="w-full bg-slate-200 rounded-full h-2.5 mb-2">
                        <div class="bg-emerald-600 h-2.5 rounded-full transition-all duration-500" :style="{width: progress + '%'}"></div>
                    </div>
                    <p class="text-xs text-right text-slate-400">{{ progress }}%</p>
                </div>

                <div v-if="result && !isAnalyzing" class="space-y-6 animate-fade-in">
                    <div class="flex justify-between items-center bg-white p-2 rounded-xl shadow-sm border border-slate-200">
                        <div class="flex gap-2">
                            <button @click="setLang('ko')" :class="lang==='ko'?'bg-emerald-100 text-emerald-700 font-bold':'text-slate-500'" class="px-4 py-2 rounded text-sm">KR</button>
                            <button @click="setLang('en')" :class="lang==='en'?'bg-blue-100 text-blue-700 font-bold':'text-slate-500'" class="px-4 py-2 rounded text-sm">EN</button>
                        </div>
                        <button @click="dlPDF" class="px-3 py-1.5 bg-slate-800 text-white text-xs rounded hover:bg-slate-700"><i class="lucide-file-down mr-1"></i> Save PDF</button>
                    </div>

                    <div id="report-view" class="bg-white p-8 rounded-xl shadow-sm border border-slate-200">
                        <section class="mb-10">
                            <h2 class="text-lg font-bold text-emerald-700 mb-3 border-b pb-1">1. Synthesis</h2>
                            <div class="prose prose-sm text-slate-700 max-w-none" v-html="md(displayReport)"></div>
                        </section>
                        <section>
                            <h2 class="text-lg font-bold text-slate-800 mb-5 border-b pb-1">2. Data Evidence</h2>
                            <div v-for="(item, idx) in result.results" :key="idx" class="mb-8 p-4 bg-slate-50 rounded-lg border">
                                <div class="flex items-center justify-between mb-3">
                                    <div class="flex items-center gap-2">
                                        <span class="px-2 py-0.5 bg-white border rounded text-[10px] font-bold uppercase text-slate-500">{{ item.equipment }}</span>
                                        <h3 class="font-bold text-sm">{{ item.filename }}</h3>
                                    </div>
                                    <button @click="item.showDocs = !item.showDocs" class="text-xs text-emerald-600 font-bold hover:underline flex items-center gap-1">Info</button>
                                </div>

                                <div v-if="item.chart_data" class="bg-white p-2 rounded border mb-3">
                                    <div class="flex gap-2 justify-end mb-1">
                                        <button @click="toggleTrace(idx, 0)" class="text-[10px] px-2 py-0.5 border rounded">Raw</button>
                                        <button @click="toggleTrace(idx, 1)" class="text-[10px] px-2 py-0.5 border rounded bg-amber-50">Base</button>
                                        <button @click="toggleTrace(idx, 2)" class="text-[10px] px-2 py-0.5 border rounded font-bold text-emerald-600">Proc</button>
                                    </div>
                                    <div :id="'chart-'+idx" class="w-full h-60"></div>
                                    <div v-if="item.interpretation && item.showDocs" class="mt-3 p-3 bg-emerald-50 border border-emerald-100 rounded text-xs text-emerald-800">
                                        <div class="prose prose-xs" v-html="md(item.interpretation)"></div>
                                    </div>
                                </div>

                                <div v-if="item.raw_b64" class="bg-white p-2 rounded border mb-3">
                                    <div class="flex gap-2 mb-2 text-xs justify-center">
                                        <button @click="item.view='proc'" :class="item.view!=='raw'?'font-bold text-emerald-600':''">Processed</button>
                                        <span class="text-slate-300">|</span>
                                        <button @click="item.view='raw'" :class="item.view==='raw'?'font-bold text-emerald-600':''">Original</button>
                                    </div>
                                    <img :src="'data:image/jpeg;base64,' + (item.view==='raw' ? item.raw_b64 : item.proc_b64)" class="max-h-96 mx-auto object-contain bg-black">
                                    
                                    <div v-if="item.footer_b64" class="mt-3 border-t pt-2">
                                        <p class="text-xs font-bold text-slate-500 mb-1">Basis Patterns (W Matrix) & Legend</p>
                                        <img :src="'data:image/jpeg;base64,' + item.footer_b64" class="w-full object-contain border bg-white">
                                    </div>

                                    <div v-if="item.stats && Object.keys(item.stats).length > 0" class="mt-2 p-2 bg-emerald-50 border border-emerald-100 rounded text-xs grid grid-cols-2 gap-2">
                                        <div v-for="(v, k) in item.stats" :key="k">
                                            <span class="font-bold text-emerald-700 capitalize">{{ k }}:</span> {{ v }}
                                        </div>
                                    </div>
                                    <div v-if="item.summary" class="mt-2 text-xs text-slate-600 prose bg-slate-50 p-2" v-html="md(item.summary)"></div>
                                </div>

                                <div v-if="item.log && item.showDocs" class="mt-2 p-2 bg-gray-100 rounded border text-[10px] font-mono text-gray-600">
                                    <ul class="list-disc pl-4 space-y-0.5"><li v-for="l in item.log">{{ l }}</li></ul>
                                </div>
                            </div>
                        </section>
                    </div>
                </div>
            </div>
        </main>
    </div>
    <script>
        const { createApp, ref, computed, nextTick } = Vue;
        createApp({
            setup() {
                const stagedFiles = ref([]);
                const isAnalyzing = ref(false);
                const isTranslating = ref(false);
                const progress = ref(0);
                const currentStep = ref("");
                const result = ref(null);
                const lang = ref('ko');
                const translatedReport = ref('');
                const md = (t) => marked.parse(t||'');
                const displayReport = computed(() => lang.value === 'en' && translatedReport.value ? translatedReport.value : (result.value ? result.value.final_report : ''));

                const addFiles = (e) => {
                    Array.from(e.target.files).forEach(f => {
                        let eq = 'SEM'; let dt = 'General';
                        if(f.name.match(/\.(pdf|ppt|pptx)$/i)) eq = 'Document';
                        else if(f.name.match(/\.(xlsx|csv|txt)$/i)) eq = 'XPS';
                        stagedFiles.value.push({ 
                            file: f, equipment: eq, dataType: dt, goal: '', mode: 'Auto', view: 'proc', showDocs: true, 
                            centerX: null, centerY: null,
                            patchSize: 128, step: 16, nComp: 10
                        });
                    });
                    e.target.value = ''; 
                };
                const removeFile = (i) => stagedFiles.value.splice(i, 1);
                const isDoc = (item) => item.equipment === 'Document';
                const isImage = (item) => ['SEM','TEM','AFM','Optical','Diffraction','2D Spectrum'].includes(item.equipment);

                const startAnalysis = async () => {
                    isAnalyzing.value = true; progress.value = 0; currentStep.value = "Init...";
                    const fd = new FormData();
                    const configs = {};
                    stagedFiles.value.forEach(item => {
                        fd.append('files', item.file);
                        configs[item.file.name] = { 
                            equipment: item.equipment, 
                            data_type: item.equipment === 'Diffraction' ? '2D Diffraction' : item.dataType,
                            goal: item.goal, mode: item.mode,
                            center_coords: (item.centerX && item.centerY) ? `${item.centerX},${item.centerY}` : null,
                            patch_size: item.patchSize, step: item.step, n_components: item.nComp
                        };
                    });
                    fd.append('file_configs', JSON.stringify(configs));
                    try {
                        const initRes = await fetch('/api/analyze', { method: 'POST', body: fd });
                        const { job_id } = await initRes.json();
                        const poll = setInterval(async () => {
                            const statusRes = await fetch(`/api/status/${job_id}`);
                            const status = await statusRes.json();
                            progress.value = status.progress || 0;
                            currentStep.value = status.step || "Processing...";
                            if (status.status === "Completed") {
                                clearInterval(poll);
                                result.value = status.results;
                                if(result.value.results) result.value.results.forEach(r => r.showDocs = true);
                                isAnalyzing.value = false;
                                await nextTick();
                                renderCharts(result.value.results);
                            } else if (status.status === "Failed") {
                                clearInterval(poll); alert("Error: " + status.error); isAnalyzing.value = false;
                            }
                        }, 1000);
                    } catch(e) { alert(e); isAnalyzing.value = false; }
                };

                const renderCharts = (items) => {
                    items.forEach((item, i) => {
                        if(item.chart_data) {
                            const raw = { x: item.chart_data.map(d=>d.x), y: item.chart_data.map(d=>d.y_raw), mode:'lines', name:'Raw', line:{color:'#cbd5e1', width:1}, visible: true };
                            const base = { x: item.chart_data.map(d=>d.x), y: item.chart_data.map(d=>d.y_base), mode:'lines', name:'Base', line:{color:'#f59e0b', width:1, dash:'dot'}, visible: true };
                            const proc = { x: item.chart_data.map(d=>d.x), y: item.chart_data.map(d=>d.y_proc), mode:'lines', name:'Proc', line:{color:'#059669', width:2}, visible: true };
                            Plotly.newPlot('chart-'+i, [raw, base, proc], {margin:{t:10,b:30,l:40,r:10}, showlegend:false}, {displayModeBar:false});
                        }
                    });
                    lucide.createIcons();
                };

                const toggleTrace = (idx, traceIdx) => {
                    const el = document.getElementById('chart-'+idx);
                    if(el) {
                        const current = el.data[traceIdx].visible;
                        const next = current === true ? 'legendonly' : true;
                        Plotly.restyle(el, {'visible': next}, [traceIdx]);
                    }
                };

                const dlPDF = () => {
                    const now = new Date();
                    const ts = now.toISOString().slice(2,10).replace(/-/g,'') + '_' + now.toTimeString().slice(0,4).replace(':','');
                    const eqList = [...new Set(stagedFiles.value.map(f=>f.equipment))].join('-');
                    html2pdf().set({ margin:10, filename:`${ts}_${eqList}_Report.pdf`, image:{type:'jpeg',quality:0.98}, html2canvas:{scale:2}, jsPDF:{unit:'mm',format:'a4'} }).from(document.getElementById('report-view')).save();
                };

                const setLang = async (target) => {
                    if (target === 'ko') { lang.value = 'ko'; return; }
                    if (target === 'en') {
                        if (translatedReport.value) { lang.value = 'en'; return; }
                        isTranslating.value = true;
                        try {
                            const res = await fetch('/api/translate', {
                                method: 'POST', headers: {'Content-Type': 'application/json'},
                                body: JSON.stringify({ text: result.value.final_report })
                            });
                            const data = await res.json();
                            translatedReport.value = data.translated;
                            lang.value = 'en';
                        } catch(e) { alert("Trans Error: " + e); }
                        finally { isTranslating.value = false; }
                    }
                };

                setTimeout(()=>lucide.createIcons(), 100);
                return { stagedFiles, isAnalyzing, progress, currentStep, result, addFiles, removeFile, startAnalysis, md, toggleTrace, isDoc, isImage, dlPDF, setLang, lang, displayReport, isTranslating };
            }
        }).mount('#app');
    </script>
</body>
</html>


엔브이15 코드

import os
import io
# [0] OMP Error Fix
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"
import torch

import asyncio
import base64
import json
import re
import math
import uuid
import time
from typing import List, Dict, Any, Optional
from pydantic import BaseModel
from contextlib import asynccontextmanager
from collections import Counter

# [1] Security
os.environ['NO_PROXY'] = '127.0.0.1,localhost,0.0.0.0,::1'
if 'HTTP_PROXY' in os.environ: del os.environ['HTTP_PROXY']
if 'HTTPS_PROXY' in os.environ: del os.environ['HTTPS_PROXY']

from fastapi import FastAPI, UploadFile, File, Form, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import HTMLResponse, FileResponse
import pandas as pd
import numpy as np

# SciPy & CV & Sklearn
from sklearn.decomposition import NMF
from scipy.signal import find_peaks, savgol_filter
from scipy.optimize import curve_fit
from scipy.ndimage import center_of_mass
from skimage.feature import peak_local_max
import cv2
import matplotlib
import matplotlib.pyplot as plt 

# Models
from segment_anything import sam_model_registry, SamAutomaticMaskGenerator
from ollama import Client 
from pdf2image import convert_from_bytes
from pptx import Presentation
from pptx.enum.shapes import MSO_SHAPE_TYPE

# [2] Config
OLLAMA_HOST = "http://127.0.0.1:11434"
ollama_client = Client(host=OLLAMA_HOST)
VISION_MODEL = "qwen3-vl:30b-a3b-instruct" 
TEXT_MODEL = "llama3:70b"

SAM_CHECKPOINT = "sam_vit_b_01ec64.pth"
SAM_MODEL_TYPE = "vit_b"
device = "cuda" if torch.cuda.is_available() else "cpu"

mask_generator = None 
JOBS = {} 

@asynccontextmanager
async def lifespan(app: FastAPI):
    global mask_generator
    if os.path.exists(SAM_CHECKPOINT):
        print(f"Loading SAM ({SAM_MODEL_TYPE}) on {device}...")
        try:
            sam = sam_model_registry[SAM_MODEL_TYPE](checkpoint=SAM_CHECKPOINT)
            sam.to(device=device)
            mask_generator = SamAutomaticMaskGenerator(
                model=sam, points_per_side=32, pred_iou_thresh=0.86,
                stability_score_thresh=0.90, min_mask_region_area=50
            )
            print("SAM Loaded.")
        except: print("SAM Load Failed.")
    yield
    JOBS.clear()

def sanitize_json(obj):
    if isinstance(obj, float):
        if np.isnan(obj) or np.isinf(obj): return 0.0
        return obj
    elif isinstance(obj, dict): return {k: sanitize_json(v) for k, v in obj.items()}
    elif isinstance(obj, list): return [sanitize_json(v) for v in obj]
    elif isinstance(obj, (np.int64, np.int32)): return int(obj)
    elif isinstance(obj, (np.float64, np.float32)):
        if np.isnan(obj) or np.isinf(obj): return 0.0
        return float(obj)
    return obj

# ==========================================
# [3] NV15: Visual Enhanced Engine
# ==========================================
class ScienceProcessorNV15:
    
    # --- Universal Loader (Maintained) ---
    @staticmethod
    def read_universal_dataframe(content: bytes, filename: str) -> List[pd.DataFrame]:
        dfs = []
        fname = filename.lower()
        if fname.endswith(('.xlsx', '.xls')):
            try:
                xls = pd.ExcelFile(io.BytesIO(content))
                for s in xls.sheet_names:
                    df = pd.read_excel(xls, sheet_name=s, header=None)
                    num = df.apply(pd.to_numeric, errors='coerce').notna().sum().sum()
                    if num < 5: continue
                    if not df.empty: dfs.append(df)
                if dfs: return dfs
            except: pass
        text_content = ""
        for enc in ['utf-8', 'cp949', 'latin1']:
            try: text_content = content.decode(enc); break
            except: continue
        if text_content:
            lines = text_content.splitlines()
            data_start = -1; delimiter = None
            separators = [',', '\t', ';', '\s+']
            for i, line in enumerate(lines[:100]):
                for sep in separators:
                    parts = line.split() if sep=='\s+' else line.split(sep)
                    nums = sum(1 for p in parts if re.match(r'^-?\d+(\.\d+)?(?:[eE][+-]?\d+)?$', p.strip()))
                    if nums >= 2: data_start = i; delimiter = sep; break
                if data_start != -1: break
            if data_start != -1:
                try:
                    data_str = "\n".join(lines[data_start:])
                    sep_arg = '\s+' if delimiter=='\s+' else delimiter
                    df = pd.read_csv(io.StringIO(data_str), sep=sep_arg, header=None, engine='python')
                    dfs.append(df)
                except: pass
        return dfs

    # --- Block Parser ---
    @staticmethod
    def detect_structured_blocks(df: pd.DataFrame) -> List[pd.DataFrame]:
        blocks = []
        if df.empty: return []
        df_num = df.apply(pd.to_numeric, errors='coerce')
        if df.shape[1] > 0: is_data_row = df_num.iloc[:, 0].notna()
        else: is_data_row = df_num.notna().any(axis=1)
        groups = (is_data_row != is_data_row.shift()).cumsum()
        for _, group in df.groupby(groups):
            if is_data_row.loc[group.index[0]]:
                block = df_num.loc[group.index].dropna(how='all', axis=1)
                if block.shape[0] >= 10: blocks.append(block.reset_index(drop=True))
        return blocks

    @staticmethod
    def split_horizontal_blocks(df: pd.DataFrame) -> List[pd.DataFrame]:
        sub_blocks = []
        if df.empty: return []
        col_counts = df.apply(pd.to_numeric, errors='coerce').notna().sum()
        valid = np.where(col_counts >= 2)[0]
        if len(valid) == 0: return []
        splits = np.where(np.diff(valid) > 1)[0] + 1
        groups = np.split(valid, splits)
        for g in groups:
            if len(g) > 0:
                sub = df.iloc[:, g].reset_index(drop=True)
                if sub.shape[1] >= 1 and sub.shape[0] >= 2: sub_blocks.append(sub)
        return sub_blocks

    @staticmethod
    def extract_series_from_df(df: pd.DataFrame) -> List[Dict]:
        series_list = []
        try:
            vals = df.values; rows, cols = vals.shape
            if rows < 5: return []
            if cols == 1:
                y = vals[:, 0]; mask = ~np.isnan(y)
                if np.sum(mask) > 5: series_list.append({"x": np.arange(len(y))[mask], "y": y[mask], "name": "Single"})
                return series_list
            def is_axis(arr):
                arr = arr[~np.isnan(arr)]
                if len(arr) < 5: return False
                diff = np.diff(arr)
                if np.all(diff >= 0) or np.all(diff <= 0): return True
                return (np.sum(diff>0)/len(diff)>0.9) or (np.sum(diff<0)/len(diff)>0.9)
            current_x = vals[:, 0]
            if not is_axis(current_x): current_x = np.arange(rows)
            for i in range(1, cols):
                col_data = vals[:, i]; mask_valid = ~np.isnan(col_data)
                if np.sum(mask_valid) < 5: continue
                if is_axis(col_data): current_x = col_data; continue 
                mask = ~np.isnan(current_x) & ~np.isnan(col_data)
                if np.sum(mask) > 5:
                    series_list.append({"x": current_x[mask], "y": col_data[mask], "name": f"Col-{i}"})
        except: pass
        return series_list

    # --- Spectrum Logic ---
    @staticmethod
    def simple_baseline(y):
        try:
            w = max(5, len(y)//10)
            s = pd.Series(y).rolling(window=w, center=True).min()
            b = s.bfill().ffill().values 
            if len(b) > 51: b = savgol_filter(b, 51, 3)
            return b, w
        except: return np.zeros_like(y), 0

    @staticmethod
    def process_spectrum(x, y, mode, goal):
        log = []; y_raw = y.copy(); base = np.zeros_like(y); y_proc = y.copy()
        peaks = []; fits = []; stats_txt = "Error"
        try:
            if mode == "None":
                peaks, _ = find_peaks(y, height=np.max(y)*0.05)
                log.append("Mode: Raw")
                stats_txt = f"Raw Peaks: {len(peaks)}"
            else:
                win = 15; do_fit = False
                if mode == "AI-Adaptive":
                    log.append("Mode: AI-Adaptive")
                    if "noise" in goal.lower(): win = 31
                    if "fit" in goal.lower(): do_fit = True
                else: log.append("Mode: Auto")
                base, bw = ScienceProcessorNV15.simple_baseline(y)
                y_proc = np.maximum(y - base, 0)
                log.append(f"Base: Rolling Min (w={bw})")
                if len(y_proc) > win:
                    y_proc = savgol_filter(y_proc, win, 3)
                    log.append(f"Smooth: SavGol (w={win})")
                peaks, _ = find_peaks(y_proc, height=np.max(y_proc)*0.05, distance=10)
                log.append(f"Peaks: {len(peaks)}")
                stats_txt = f"Range: {np.min(x):.1f}~{np.max(x):.1f}, Peaks: {len(peaks)}"
            return {"x": x, "y_raw": y_raw, "y_proc": y_proc, "y_base": base, "peaks": [{"x": float(x[p]), "y": float(y_proc[p])} for p in peaks], "log": log, "fits": fits, "stats": stats_txt}
        except Exception as e: return {"x": x, "y_raw": y_raw, "y_proc": y_proc, "y_base": base, "peaks": [], "log": [str(e)], "fits": [], "stats": "Error"}

    # --- Footer ---
    @staticmethod
    def separate_footer(img_bgr):
        h, w = img_bgr.shape[:2]
        roi_h = int(h * 0.82)
        roi = img_bgr[roi_h:, :]
        gray = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)
        sobel_y = np.abs(cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=3))
        row_sums = np.sum(sobel_y, axis=1)
        split_idx = np.argmax(row_sums)
        if row_sums[split_idx] > w * 30:
            real_split = roi_h + split_idx
            return img_bgr[:real_split, :], img_bgr[real_split:, :]
        return img_bgr, None

    # --- [E] NV15 FFT-NMF Crystal Engine (Visualization Fixed) ---
    @staticmethod
    def analyze_crystal_fft_nmf(img_bgr, patch_size=128, step=16, n_components=10):
        log = []
        gray = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2GRAY)
        h, w = gray.shape
        
        log.append(f"Config: Patch={patch_size}, Step={step}, K={n_components}")
        
        # 1. Custom Colormap (Black - Jet - Gray)
        cmap_jet = matplotlib.colormaps['jet']
        custom_colors = []
        # Index 0: Black
        custom_colors.append((0, 0, 0))
        # Middle: Jet
        num_jet = n_components - 2
        for i in range(num_jet):
            val = i / (num_jet - 1) if num_jet > 1 else 0.5
            rgba = cmap_jet(val)
            bgr = (int(rgba[2]*255), int(rgba[1]*255), int(rgba[0]*255))
            custom_colors.append(bgr)
        # Last: Gray
        custom_colors.append((128, 128, 128))
        while len(custom_colors) < n_components: custom_colors.append((255,255,255))
        
        # 2. Collect FFT Magnitudes
        cols = [] 
        y_steps = list(range(0, h - patch_size + 1, step))
        x_steps = list(range(0, w - patch_size + 1, step))
        grid_h = len(y_steps)
        grid_w = len(x_steps)
        
        for y in y_steps:
            for x in x_steps:
                roi = gray[y:y+patch_size, x:x+patch_size]
                f = np.fft.fft2(roi)
                fshift = np.fft.fftshift(f)
                mag = np.abs(fshift) 
                
                # [DC Mask]
                cy, cx = patch_size//2, patch_size//2
                mag[cy-3:cy+4, cx-3:cx+4] = 0 # Stronger DC removal
                
                cols.append(mag.flatten())
        
        if not cols: return {"type":"error", "msg":"Img too small"}
        
        try:
            # Transpose: (Features=Pixels, Samples=Patches)
            V = np.array(cols).T.astype(np.float32)
            
            # Global Scaling
            V = V - V.min()
            if V.max() > 0: V = V / V.max()
            
            # NMF
            nmf = NMF(n_components=n_components, init='nndsvda', random_state=42, max_iter=1000)
            W = nmf.fit_transform(V) # Basis Patterns
            H = nmf.components_      # Coefficients
            
            # Map Reconstruct
            idx_H = np.argmax(H, axis=0) 
            map_low = idx_H.reshape(grid_h, grid_w).astype(np.uint8)
            map_high = cv2.resize(map_low, (w, h), interpolation=cv2.INTER_NEAREST)
            
            # Overlay
            overlay = img_bgr.copy()
            mask_layer = np.zeros_like(img_bgr)
            
            counts = Counter(idx_H)
            total = len(idx_H)
            stats = {}
            
            for i in range(n_components):
                mask = (map_high == i)
                mask_layer[mask] = custom_colors[i]
                cnt = counts.get(i, 0)
                stats[f"Pattern {i}"] = f"{cnt} ({cnt/total*100:.1f}%)"
                
            alpha = 0.4
            mask_idx = np.any(mask_layer > 0, axis=-1)
            overlay[mask_idx] = (overlay[mask_idx]*(1-alpha) + mask_layer[mask_idx]*alpha).astype(np.uint8)
            
            # [Fix] Basis Pattern Visualization (Log & Contrast)
            basis_combo_list = []
            
            for k in range(n_components):
                # 1. Pattern Image
                pat = W[:, k].reshape(patch_size, patch_size)
                
                # Strong Log Scale for FFT visibility
                # Use log1p to handle zeros, multiply by scalar to boost
                pat_log = np.log1p(pat * 1000) 
                
                # Normalize 0-255
                pat_norm = cv2.normalize(pat_log, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)
                
                # Colorize (optional, but grayscale usually clearer for FFT)
                # Let's keep grayscale for pattern, but add colored border
                pat_img = cv2.cvtColor(pat_norm, cv2.COLOR_GRAY2BGR)
                
                # 2. Color Legend Block (Top)
                legend_h = 30
                legend = np.zeros((legend_h, patch_size, 3), dtype=np.uint8)
                legend[:] = custom_colors[k]
                
                # Text on Legend
                cv2.putText(legend, f"P{k}", (5, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255,255,255), 1)
                
                # Stack Legend + Pattern
                combo = np.vstack([legend, pat_img])
                
                # Add border to the whole combo
                cv2.rectangle(combo, (0,0), (combo.shape[1]-1, combo.shape[0]-1), custom_colors[k], 2)
                
                basis_combo_list.append(combo)
            
            # Concatenate all (Horizontal Strip)
            full_strip = np.hstack(basis_combo_list)
            
            # Scale up for visibility
            full_strip_large = cv2.resize(full_strip, None, fx=2, fy=2, interpolation=cv2.INTER_NEAREST)
            
            def to_b64(im): return base64.b64encode(cv2.imencode('.jpg', im)[1]).decode('utf-8')
            
            return {
                "type": "image", 
                "raw_b64": to_b64(img_bgr), 
                "proc_b64": to_b64(overlay),
                "footer_b64": to_b64(full_strip_large), # Basis Patterns
                "stats": stats, 
                "log": log
            }

        except Exception as e:
            print(f"NMF Error: {e}")
            def to_b64(im): return base64.b64encode(cv2.imencode('.jpg', im)[1]).decode('utf-8')
            return {"type": "image", "raw_b64": to_b64(img_bgr), "proc_b64": to_b64(img_bgr), "stats": {"Error": str(e)}, "log": log}

    # --- Router ---
    @staticmethod
    def process_image_nv15(img_bytes, equipment, data_type, mode, goal, user_center=None, 
                           patch_size=128, step=16, n_components=10):
        nparr = np.frombuffer(img_bytes, np.uint8)
        img_raw = cv2.imdecode(nparr, cv2.IMREAD_COLOR)
        if img_raw is None: return {}
        
        body, footer = ScienceProcessorNV15.separate_footer(img_raw)
        
        footer_b64 = None
        if footer is not None:
            footer_b64 = base64.b64encode(cv2.imencode('.jpg', footer)[1]).decode('utf-8')

        # [NV15] FFT-NMF
        if data_type == "Crystal Structure (FFT)":
            return {**ScienceProcessorNV15.analyze_crystal_fft_nmf(body, patch_size, step, n_components), "footer_b64": footer_b64}
            
        if data_type == "2D Diffraction":
            return {**ScienceProcessorNV15.analyze_diffraction_nv7(body, user_center), "footer_b64": footer_b64}
        
        # General SAM
        if mode == "None":
             def to_b64(im): return base64.b64encode(cv2.imencode('.jpg', im)[1]).decode('utf-8')
             return {"type":"image", "raw_b64":to_b64(body), "proc_b64":to_b64(body), "stats":{}, "log":["Mode: Raw"], "footer_b64": footer_b64}

        img_proc = body.copy(); log = []
        if mode == "Auto" or mode == "AI-Adaptive":
            img_proc = cv2.bilateralFilter(img_proc, 9, 75, 75)
            log.append("Preproc: Denoise")

        img_rgb = cv2.cvtColor(img_proc, cv2.COLOR_BGR2RGB)
        
        if mask_generator:
            try:
                masks = mask_generator.generate(img_rgb)
                log.append(f"SAM: {len(masks)} Segments")
                overlay, stats, sam_log = ScienceProcessorNV15.analyze_sam_results_nv7(img_rgb, masks, data_type)
                log.extend(sam_log)
                overlay = cv2.cvtColor(overlay, cv2.COLOR_RGB2BGR)
            except Exception as e:
                log.append(f"SAM Error: {str(e)}")
                overlay = body; stats = {}
        else:
            log.append("SAM Not Loaded")
            overlay = body; stats = {}

        def to_b64(im): return base64.b64encode(cv2.imencode('.jpg', im)[1]).decode('utf-8')
        return {"type": "image", "raw_b64": to_b64(body), "proc_b64": to_b64(overlay), "stats": stats, "log": log, "footer_b64": footer_b64}

# [4] Helper
async def analyze_vision_ollama(image_bytes: bytes, prompt: str) -> str:
    try:
        b64 = base64.b64encode(image_bytes).decode('utf-8')
        res = await asyncio.to_thread(ollama_client.chat, model=VISION_MODEL, messages=[{'role':'user','content':prompt,'images':[b64]}])
        return res['message']['content']
    except: return "Vision Error"

async def interpret_spectrum_data(raw_context: str, equipment: str) -> str:
    try:
        prompt = f"Analyze spectrum ({equipment}). Brief interpretation in Korean. Data: {raw_context}"
        res = await asyncio.to_thread(ollama_client.chat, model=TEXT_MODEL, messages=[{'role':'user','content':prompt}])
        return res['message']['content']
    except: return ""

# [5] App
app = FastAPI(title="Analyst NV15 Perfected", lifespan=lifespan)
app.add_middleware(CORSMiddleware, allow_origins=["*"], allow_methods=["*"], allow_headers=["*"])

class TranslateReq(BaseModel): text: str

@app.post("/api/translate")
async def translate_text(req: TranslateReq):
    try:
        res = await asyncio.to_thread(ollama_client.chat, model=TEXT_MODEL, messages=[{'role':'system','content':'Translate to English.'}, {'role':'user','content':req.text}])
        return {"translated": res['message']['content']}
    except Exception as e: return {"translated": str(e)}

async def process_analysis_job(job_id: str, files_map: Dict, configs: Dict):
    try:
        JOBS[job_id]["status"] = "Processing"
        JOBS[job_id]["progress"] = 10
        final_results = []
        total_files = len(configs)
        processed_count = 0

        for filename, config in configs.items():
            if filename not in files_map: continue
            content = files_map[filename]
            eq = config.get("equipment", "General")
            data_type = config.get("data_type", "General")
            goal = config.get("goal", "")
            mode = config.get("mode", "Auto")
            
            # Params
            user_center = None
            if config.get("center_coords"):
                try: user_center = [int(v) for v in config["center_coords"].split(",")]
                except: pass
            
            p_size = int(config.get("patch_size", 128))
            p_step = int(config.get("step", 16))
            n_comp = int(config.get("n_components", 10))

            fname_lower = filename.lower()
            JOBS[job_id]["step"] = f"Analyzing {filename}..."
            
            if eq == "Document" or fname_lower.endswith(('.pdf', '.ppt', '.pptx')):
                # (Existing logic omitted for brevity)
                pass 

            elif fname_lower.endswith(('.xlsx', '.xls', '.csv', '.txt')):
                # (Existing logic omitted for brevity)
                pass

            elif fname_lower.endswith(('.png', '.jpg', '.jpeg', '.tif', '.bmp')):
                try:
                    JOBS[job_id]["step"] = f"Vision Analysis: {filename}..."
                    vis_res = await asyncio.to_thread(ScienceProcessorNV15.process_image_nv15, content, eq, data_type, mode, goal, user_center, p_size, p_step, n_comp)
                    if vis_res:
                        scale_info = "Unknown"
                        final_footer = vis_res.get("footer_b64")
                        if final_footer and data_type != "Crystal Structure (FFT)":
                            JOBS[job_id]["step"] = f"Scale Reading..."
                            scale_text = await analyze_vision_ollama(base64.b64decode(final_footer), "Read scale bar text only.")
                            scale_info = scale_text
                            vis_res["log"].append(f"Scale: {scale_text}")

                        stats_str = json.dumps(vis_res.get('stats',{}), indent=2)
                        desc_prompt = f"Analyze {eq} ({data_type}). Stats: {stats_str}. Goal: {goal}. Summary in Korean."
                        desc = await analyze_vision_ollama(base64.b64decode(vis_res["proc_b64"]), desc_prompt)
                        
                        final_results.append({
                            "type": "image", "filename": filename, "equipment": eq, "summary": desc,
                            "raw_context": f"Img {eq}: {desc}", "raw_b64": vis_res["raw_b64"], "proc_b64": vis_res["proc_b64"], "footer_b64": final_footer,
                            "stats": vis_res.get("stats",{}), "log": vis_res.get("log", [])
                        })
                except Exception as ex: final_results.append({"type": "error", "filename": filename, "msg": str(ex)})
            
            processed_count += 1
            JOBS[job_id]["progress"] = 10 + int((processed_count / total_files) * 80)

        JOBS[job_id]["step"] = "Writing Report..."
        final_results = sanitize_json(final_results)
        
        data_ctx = ""
        for r in final_results:
            if r.get("type") != "error":
                raw = r.get("raw_context") or ""
                data_ctx += f"\n=== {r['filename']} ===\n{raw[:1000]}\n"

        final_report = "Fail"
        if data_ctx:
            try:
                system_prompt = "You are a research assistant. Write a report in Korean. Summarize data and physics."
                res = await asyncio.to_thread(ollama_client.chat, model=TEXT_MODEL, messages=[{'role':'system','content':system_prompt}, {'role':'user','content':f"Data:\n{data_ctx}"}])
                final_report = res['message']['content']
            except Exception as e: final_report = str(e)

        JOBS[job_id]["results"] = {"results": final_results, "final_report": final_report}
        JOBS[job_id]["status"] = "Completed"
        JOBS[job_id]["progress"] = 100
    except Exception as e:
        JOBS[job_id]["status"] = "Failed"; JOBS[job_id]["error"] = str(e)

@app.post("/api/analyze")
async def start_analysis(background_tasks: BackgroundTasks, files: List[UploadFile] = File(...), file_configs: str = Form(...)):
    job_id = str(uuid.uuid4()); configs = json.loads(file_configs)
    files_map = {f.filename: await f.read() for f in files}
    JOBS[job_id] = {"status": "Pending", "progress": 0, "step": "Init", "results": None}
    background_tasks.add_task(process_analysis_job, job_id, files_map, configs)
    return {"job_id": job_id}

@app.get("/api/status/{job_id}")
async def get_status(job_id: str): return JOBS.get(job_id, {"status": "Not Found"})

@app.get("/", response_class=HTMLResponse)
async def serve_index():
    if os.path.exists("index.html"): return FileResponse("index.html")
    return "<h1>NV15 Final Backend</h1>"

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)



엔브이14 웹코드

<!DOCTYPE html>
<html lang="ko">
<head>
    <meta charset="UTF-8">
    <title>Analyst NV14 Interactive</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <script src="https://unpkg.com/vue@3/dist/vue.global.js"></script>
    <script src="https://cdn.plot.ly/plotly-2.27.0.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/marked/marked.min.js"></script>
    <script src="https://unpkg.com/lucide@latest"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/html2pdf.js/0.10.1/html2pdf.bundle.min.js"></script>
    <style>
        @import url('https://fonts.googleapis.com/css2?family=Inter:wght@400;600;700&display=swap');
        body { font-family: 'Inter', sans-serif; }
        .prose table { border-collapse: collapse; width: 100%; margin: 1em 0; }
        .prose th, .prose td { border: 1px solid #cbd5e1; padding: 8px; font-size: 0.9rem; }
        .prose th { background-color: #f1f5f9; }
    </style>
</head>
<body class="bg-slate-100 text-slate-800">
    <div id="app" class="min-h-screen p-6">
        <header class="max-w-7xl mx-auto mb-6 flex items-center gap-3">
            <div class="bg-emerald-600 p-2 rounded-lg shadow"><i data-lucide="microscope" class="w-6 h-6 text-white"></i></div>
            <div><h1 class="text-2xl font-bold">Analyst NV14 <span class="text-sm font-normal text-slate-500">Interactive Crystal Analysis</span></h1></div>
        </header>

        <main class="max-w-7xl mx-auto grid grid-cols-1 lg:grid-cols-12 gap-6">
            <div class="lg:col-span-4 space-y-4">
                <div class="bg-white p-5 rounded-xl shadow-sm border border-slate-200">
                    <h2 class="font-bold mb-3 flex items-center gap-2"><i data-lucide="files" class="w-4 h-4"></i> Data Staging</h2>
                    <div class="border-2 border-dashed border-slate-300 rounded-lg p-6 text-center hover:bg-emerald-50 cursor-pointer" @click="$refs.f.click()">
                        <input type="file" ref="f" multiple class="hidden" @change="addFiles">
                        <p class="text-sm font-semibold text-slate-600">Add Files</p>
                    </div>
                    <div v-if="stagedFiles.length" class="mt-4 space-y-3 max-h-[60vh] overflow-y-auto pr-1">
                        <div v-for="(item, i) in stagedFiles" :key="i" class="p-3 bg-slate-50 border rounded-lg text-sm">
                            <div class="flex justify-between items-center mb-2">
                                <span class="font-bold truncate w-2/3">{{ item.file.name }}</span>
                                <button @click="removeFile(i)" class="text-red-400 hover:text-red-600"><i data-lucide="trash-2" class="w-4 h-4"></i></button>
                            </div>
                            <div class="grid grid-cols-1 gap-2">
                                <select v-model="item.equipment" class="w-full p-1.5 border rounded text-xs bg-white font-semibold text-emerald-700">
                                    <optgroup label="Spectroscopy (1D)">
                                        <option value="XPS">XPS</option>
                                        <option value="XRD">XRD (1D)</option>
                                        <option value="Raman">Raman</option>
                                    </optgroup>
                                    <optgroup label="Microscopy (2D)">
                                        <option value="SEM">SEM</option>
                                        <option value="TEM">TEM</option>
                                        <option value="AFM">AFM</option>
                                        <option value="Optical">Optical</option>
                                    </optgroup>
                                    <optgroup label="Diffraction (2D)">
                                        <option value="Diffraction">2D Diffraction</option>
                                    </optgroup>
                                    <optgroup label="Literature">
                                        <option value="Document">Document</option>
                                    </optgroup>
                                </select>
                                
                                <select v-if="isImage(item)" v-model="item.dataType" class="w-full p-1.5 border rounded text-xs bg-slate-50 font-bold">
                                    <option v-if="item.equipment==='Diffraction'" value="2D Diffraction">2D Diffraction</option>
                                    <option v-else value="General">General Object</option>
                                    <option v-if="item.equipment!=='Diffraction'" value="Particle">Particle Analysis</option>
                                    <option v-if="item.equipment!=='Diffraction'" value="Thin Film">Thin Film</option>
                                    <option v-if="item.equipment!=='Diffraction'" value="Crystal Structure (FFT)">Crystal Structure (FFT)</option>
                                </select>

                                <div v-if="item.dataType === 'Crystal Structure (FFT)'" class="flex gap-1">
                                    <input v-model="item.patchSize" type="number" placeholder="Patch (128)" class="w-1/3 p-1 border rounded text-[10px]">
                                    <input v-model="item.step" type="number" placeholder="Step (16)" class="w-1/3 p-1 border rounded text-[10px]">
                                    <input v-model="item.nComp" type="number" placeholder="N_Comp (10)" class="w-1/3 p-1 border rounded text-[10px]">
                                </div>

                                <div v-if="item.dataType === '2D Diffraction'" class="flex gap-2">
                                    <input v-model="item.centerX" type="number" placeholder="CX" class="w-1/2 p-1.5 border rounded text-xs">
                                    <input v-model="item.centerY" type="number" placeholder="CY" class="w-1/2 p-1.5 border rounded text-xs">
                                </div>

                                <select v-if="!isDoc(item)" v-model="item.mode" class="w-full p-1.5 border rounded text-xs bg-white">
                                    <option value="Auto">✨ Auto Process</option>
                                    <option value="None">🚫 Raw Data</option>
                                    <option value="AI-Adaptive">🧠 Agent Mode</option>
                                </select>
                            </div>
                        </div>
                    </div>
                    <button @click="startAnalysis" :disabled="isAnalyzing || stagedFiles.length===0" class="w-full mt-4 py-3 bg-emerald-600 text-white rounded-lg font-bold flex justify-center gap-2 shadow hover:bg-emerald-700 disabled:opacity-50">
                        <span>Generate Report</span>
                    </button>
                </div>
            </div>

            <div class="lg:col-span-8">
                <div v-if="isAnalyzing" class="bg-white p-6 rounded-xl shadow border border-slate-200 text-center py-12">
                    <div class="animate-spin rounded-full h-12 w-12 border-b-2 border-emerald-600 mx-auto mb-4"></div>
                    <h3 class="text-lg font-bold text-slate-700">Analyzing...</h3>
                    <p class="text-sm text-slate-500 mb-4">{{ currentStep }}</p>
                    <div class="w-full bg-slate-200 rounded-full h-2.5 mb-2">
                        <div class="bg-emerald-600 h-2.5 rounded-full transition-all duration-500" :style="{width: progress + '%'}"></div>
                    </div>
                    <p class="text-xs text-right text-slate-400">{{ progress }}%</p>
                </div>

                <div v-if="result && !isAnalyzing" class="space-y-6 animate-fade-in">
                    <div class="flex justify-between items-center bg-white p-2 rounded-xl shadow-sm border border-slate-200">
                        <div class="flex gap-2">
                            <button @click="setLang('ko')" :class="lang==='ko'?'bg-emerald-100 text-emerald-700 font-bold':'text-slate-500'" class="px-4 py-2 rounded text-sm">KR</button>
                            <button @click="setLang('en')" :class="lang==='en'?'bg-blue-100 text-blue-700 font-bold':'text-slate-500'" class="px-4 py-2 rounded text-sm">EN</button>
                        </div>
                        <button @click="dlPDF" class="px-3 py-1.5 bg-slate-800 text-white text-xs rounded hover:bg-slate-700"><i class="lucide-file-down mr-1"></i> Save PDF</button>
                    </div>

                    <div id="report-view" class="bg-white p-8 rounded-xl shadow-sm border border-slate-200">
                        <section class="mb-10">
                            <h2 class="text-lg font-bold text-emerald-700 mb-3 border-b pb-1">1. Synthesis</h2>
                            <div class="prose prose-sm text-slate-700 max-w-none" v-html="md(displayReport)"></div>
                        </section>
                        <section>
                            <h2 class="text-lg font-bold text-slate-800 mb-5 border-b pb-1">2. Data Evidence</h2>
                            <div v-for="(item, idx) in result.results" :key="idx" class="mb-8 p-4 bg-slate-50 rounded-lg border">
                                <div class="flex items-center justify-between mb-3">
                                    <div class="flex items-center gap-2">
                                        <span class="px-2 py-0.5 bg-white border rounded text-[10px] font-bold uppercase text-slate-500">{{ item.equipment }}</span>
                                        <h3 class="font-bold text-sm">{{ item.filename }}</h3>
                                    </div>
                                    <button @click="item.showDocs = !item.showDocs" class="text-xs text-emerald-600 font-bold hover:underline flex items-center gap-1">Info</button>
                                </div>

                                <div v-if="item.chart_data" class="bg-white p-2 rounded border mb-3">
                                    <div class="flex gap-2 justify-end mb-1">
                                        <button @click="toggleTrace(idx, 0)" class="text-[10px] px-2 py-0.5 border rounded">Raw</button>
                                        <button @click="toggleTrace(idx, 1)" class="text-[10px] px-2 py-0.5 border rounded bg-amber-50">Base</button>
                                        <button @click="toggleTrace(idx, 2)" class="text-[10px] px-2 py-0.5 border rounded font-bold text-emerald-600">Proc</button>
                                    </div>
                                    <div :id="'chart-'+idx" class="w-full h-60"></div>
                                    <div v-if="item.interpretation && item.showDocs" class="mt-3 p-3 bg-emerald-50 border border-emerald-100 rounded text-xs text-emerald-800">
                                        <div class="prose prose-xs" v-html="md(item.interpretation)"></div>
                                    </div>
                                </div>

                                <div v-if="item.raw_b64" class="bg-white p-2 rounded border mb-3">
                                    <div class="flex gap-2 mb-2 text-xs justify-center">
                                        <button @click="item.view='proc'" :class="item.view!=='raw'?'font-bold text-emerald-600':''">Processed</button>
                                        <span class="text-slate-300">|</span>
                                        <button @click="item.view='raw'" :class="item.view==='raw'?'font-bold text-emerald-600':''">Original</button>
                                    </div>
                                    <img :src="'data:image/jpeg;base64,' + (item.view==='raw' ? item.raw_b64 : item.proc_b64)" class="max-h-96 mx-auto object-contain bg-black">
                                    
                                    <div v-if="item.footer_b64" class="mt-3 border-t pt-2">
                                        <p class="text-xs font-bold text-slate-500 mb-1">Basis Patterns (W Matrix) & Color Legend</p>
                                        <img :src="'data:image/jpeg;base64,' + item.footer_b64" class="w-full object-contain border">
                                    </div>

                                    <div v-if="item.stats" class="mt-2 p-2 bg-emerald-50 border border-emerald-100 rounded text-xs grid grid-cols-2 gap-2">
                                        <div v-for="(v, k) in item.stats" :key="k">
                                            <span class="font-bold text-emerald-700 capitalize">{{ k }}:</span> {{ v }}
                                        </div>
                                    </div>
                                    <div v-if="item.summary" class="mt-2 text-xs text-slate-600 prose bg-slate-50 p-2" v-html="md(item.summary)"></div>
                                </div>

                                <div v-if="item.log && item.showDocs" class="mt-2 p-2 bg-gray-100 rounded border text-[10px] font-mono text-gray-600">
                                    <ul class="list-disc pl-4 space-y-0.5"><li v-for="l in item.log">{{ l }}</li></ul>
                                </div>
                            </div>
                        </section>
                    </div>
                </div>
            </div>
        </main>
    </div>
    <script>
        const { createApp, ref, computed, nextTick } = Vue;
        createApp({
            setup() {
                const stagedFiles = ref([]);
                const isAnalyzing = ref(false);
                const isTranslating = ref(false);
                const progress = ref(0);
                const currentStep = ref("");
                const result = ref(null);
                const lang = ref('ko');
                const translatedReport = ref('');
                const md = (t) => marked.parse(t||'');
                const displayReport = computed(() => lang.value === 'en' && translatedReport.value ? translatedReport.value : (result.value ? result.value.final_report : ''));

                const addFiles = (e) => {
                    Array.from(e.target.files).forEach(f => {
                        let eq = 'SEM'; let dt = 'General';
                        if(f.name.match(/\.(pdf|ppt|pptx)$/i)) eq = 'Document';
                        else if(f.name.match(/\.(xlsx|csv|txt)$/i)) eq = 'XPS';
                        stagedFiles.value.push({ 
                            file: f, equipment: eq, dataType: dt, goal: '', mode: 'Auto', view: 'proc', showDocs: true, 
                            centerX: null, centerY: null,
                            patchSize: 128, step: 16, nComp: 10
                        });
                    });
                    e.target.value = ''; 
                };
                const removeFile = (i) => stagedFiles.value.splice(i, 1);
                const isDoc = (item) => item.equipment === 'Document';
                const isImage = (item) => ['SEM','TEM','AFM','Optical','Diffraction','2D Spectrum'].includes(item.equipment);

                const startAnalysis = async () => {
                    isAnalyzing.value = true; progress.value = 0; currentStep.value = "Init...";
                    const fd = new FormData();
                    const configs = {};
                    stagedFiles.value.forEach(item => {
                        fd.append('files', item.file);
                        configs[item.file.name] = { 
                            equipment: item.equipment, 
                            data_type: item.equipment === 'Diffraction' ? '2D Diffraction' : item.dataType,
                            goal: item.goal, mode: item.mode,
                            center_coords: (item.centerX && item.centerY) ? `${item.centerX},${item.centerY}` : null,
                            patch_size: item.patchSize, step: item.step, n_components: item.nComp
                        };
                    });
                    fd.append('file_configs', JSON.stringify(configs));
                    try {
                        const initRes = await fetch('/api/analyze', { method: 'POST', body: fd });
                        const { job_id } = await initRes.json();
                        const poll = setInterval(async () => {
                            const statusRes = await fetch(`/api/status/${job_id}`);
                            const status = await statusRes.json();
                            progress.value = status.progress || 0;
                            currentStep.value = status.step || "Processing...";
                            if (status.status === "Completed") {
                                clearInterval(poll);
                                result.value = status.results;
                                if(result.value.results) result.value.results.forEach(r => r.showDocs = true);
                                isAnalyzing.value = false;
                                await nextTick();
                                renderCharts(result.value.results);
                            } else if (status.status === "Failed") {
                                clearInterval(poll); alert("Error: " + status.error); isAnalyzing.value = false;
                            }
                        }, 1000);
                    } catch(e) { alert(e); isAnalyzing.value = false; }
                };

                const renderCharts = (items) => {
                    items.forEach((item, i) => {
                        if(item.chart_data) {
                            const raw = { x: item.chart_data.map(d=>d.x), y: item.chart_data.map(d=>d.y_raw), mode:'lines', name:'Raw', line:{color:'#cbd5e1', width:1}, visible: true };
                            const base = { x: item.chart_data.map(d=>d.x), y: item.chart_data.map(d=>d.y_base), mode:'lines', name:'Base', line:{color:'#f59e0b', width:1, dash:'dot'}, visible: true };
                            const proc = { x: item.chart_data.map(d=>d.x), y: item.chart_data.map(d=>d.y_proc), mode:'lines', name:'Proc', line:{color:'#059669', width:2}, visible: true };
                            Plotly.newPlot('chart-'+i, [raw, base, proc], {margin:{t:10,b:30,l:40,r:10}, showlegend:false}, {displayModeBar:false});
                        }
                    });
                    lucide.createIcons();
                };

                const toggleTrace = (idx, traceIdx) => {
                    const el = document.getElementById('chart-'+idx);
                    if(el) {
                        const current = el.data[traceIdx].visible;
                        const next = current === true ? 'legendonly' : true;
                        Plotly.restyle(el, {'visible': next}, [traceIdx]);
                    }
                };

                const dlPDF = () => {
                    const now = new Date();
                    const ts = now.toISOString().slice(2,10).replace(/-/g,'') + '_' + now.toTimeString().slice(0,4).replace(':','');
                    const eqList = [...new Set(stagedFiles.value.map(f=>f.equipment))].join('-');
                    html2pdf().set({ margin:10, filename:`${ts}_${eqList}_Report.pdf`, image:{type:'jpeg',quality:0.98}, html2canvas:{scale:2}, jsPDF:{unit:'mm',format:'a4'} }).from(document.getElementById('report-view')).save();
                };

                const setLang = async (target) => {
                    if (target === 'ko') { lang.value = 'ko'; return; }
                    if (target === 'en') {
                        if (translatedReport.value) { lang.value = 'en'; return; }
                        isTranslating.value = true;
                        try {
                            const res = await fetch('/api/translate', {
                                method: 'POST', headers: {'Content-Type': 'application/json'},
                                body: JSON.stringify({ text: result.value.final_report })
                            });
                            const data = await res.json();
                            translatedReport.value = data.translated;
                            lang.value = 'en';
                        } catch(e) { alert("Trans Error: " + e); }
                        finally { isTranslating.value = false; }
                    }
                };

                setTimeout(()=>lucide.createIcons(), 100);
                return { stagedFiles, isAnalyzing, progress, currentStep, result, addFiles, removeFile, startAnalysis, md, toggleTrace, isDoc, isImage, dlPDF, setLang, lang, displayReport, isTranslating };
            }
        }).mount('#app');
    </script>
</body>
</html>




엔브이14 코드

import os
import io
# [0] OMP Error Fix
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"
import torch

import asyncio
import base64
import json
import re
import math
import uuid
import time
from typing import List, Dict, Any, Optional
from pydantic import BaseModel
from contextlib import asynccontextmanager
from collections import Counter

# [1] Security
os.environ['NO_PROXY'] = '127.0.0.1,localhost,0.0.0.0,::1'
if 'HTTP_PROXY' in os.environ: del os.environ['HTTP_PROXY']
if 'HTTPS_PROXY' in os.environ: del os.environ['HTTPS_PROXY']

from fastapi import FastAPI, UploadFile, File, Form, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import HTMLResponse, FileResponse
import pandas as pd
import numpy as np

# SciPy & CV & Sklearn
from sklearn.decomposition import NMF
from scipy.signal import find_peaks, savgol_filter
from scipy.optimize import curve_fit
from scipy.ndimage import center_of_mass
from skimage.feature import peak_local_max
import cv2
import matplotlib
import matplotlib.pyplot as plt 

# Models
from segment_anything import sam_model_registry, SamAutomaticMaskGenerator
from ollama import Client 
from pdf2image import convert_from_bytes
from pptx import Presentation
from pptx.enum.shapes import MSO_SHAPE_TYPE

# [2] Config
OLLAMA_HOST = "http://127.0.0.1:11434"
ollama_client = Client(host=OLLAMA_HOST)
VISION_MODEL = "qwen3-vl:30b-a3b-instruct" 
TEXT_MODEL = "llama3:70b"

SAM_CHECKPOINT = "sam_vit_b_01ec64.pth"
SAM_MODEL_TYPE = "vit_b"
device = "cuda" if torch.cuda.is_available() else "cpu"

mask_generator = None 
JOBS = {} 

@asynccontextmanager
async def lifespan(app: FastAPI):
    global mask_generator
    if os.path.exists(SAM_CHECKPOINT):
        print(f"Loading SAM ({SAM_MODEL_TYPE}) on {device}...")
        try:
            sam = sam_model_registry[SAM_MODEL_TYPE](checkpoint=SAM_CHECKPOINT)
            sam.to(device=device)
            mask_generator = SamAutomaticMaskGenerator(
                model=sam, points_per_side=32, pred_iou_thresh=0.86,
                stability_score_thresh=0.90, min_mask_region_area=50
            )
            print("SAM Loaded.")
        except: print("SAM Load Failed.")
    yield
    JOBS.clear()

def sanitize_json(obj):
    if isinstance(obj, float):
        if np.isnan(obj) or np.isinf(obj): return 0.0
        return obj
    elif isinstance(obj, dict): return {k: sanitize_json(v) for k, v in obj.items()}
    elif isinstance(obj, list): return [sanitize_json(v) for v in obj]
    elif isinstance(obj, (np.int64, np.int32)): return int(obj)
    elif isinstance(obj, (np.float64, np.float32)):
        if np.isnan(obj) or np.isinf(obj): return 0.0
        return float(obj)
    return obj

# ==========================================
# [3] NV14 Interactive Engine
# ==========================================
class ScienceProcessorNV14:
    
    # --- Universal Loader ---
    @staticmethod
    def read_universal_dataframe(content: bytes, filename: str) -> List[pd.DataFrame]:
        dfs = []
        fname = filename.lower()
        if fname.endswith(('.xlsx', '.xls')):
            try:
                xls = pd.ExcelFile(io.BytesIO(content))
                for s in xls.sheet_names:
                    df = pd.read_excel(xls, sheet_name=s, header=None)
                    num = df.apply(pd.to_numeric, errors='coerce').notna().sum().sum()
                    if num < 5: continue
                    if not df.empty: dfs.append(df)
                if dfs: return dfs
            except: pass
        text_content = ""
        for enc in ['utf-8', 'cp949', 'latin1']:
            try: text_content = content.decode(enc); break
            except: continue
        if text_content:
            lines = text_content.splitlines()
            data_start = -1; delimiter = None
            separators = [',', '\t', ';', '\s+']
            for i, line in enumerate(lines[:100]):
                for sep in separators:
                    parts = line.split() if sep=='\s+' else line.split(sep)
                    nums = sum(1 for p in parts if re.match(r'^-?\d+(\.\d+)?(?:[eE][+-]?\d+)?$', p.strip()))
                    if nums >= 2: data_start = i; delimiter = sep; break
                if data_start != -1: break
            if data_start != -1:
                try:
                    data_str = "\n".join(lines[data_start:])
                    sep_arg = '\s+' if delimiter=='\s+' else delimiter
                    df = pd.read_csv(io.StringIO(data_str), sep=sep_arg, header=None, engine='python')
                    dfs.append(df)
                except: pass
        return dfs

    # --- Block Parser ---
    @staticmethod
    def detect_structured_blocks(df: pd.DataFrame) -> List[pd.DataFrame]:
        blocks = []
        if df.empty: return []
        df_num = df.apply(pd.to_numeric, errors='coerce')
        if df.shape[1] > 0: is_data_row = df_num.iloc[:, 0].notna()
        else: is_data_row = df_num.notna().any(axis=1)
        groups = (is_data_row != is_data_row.shift()).cumsum()
        for _, group in df.groupby(groups):
            if is_data_row.loc[group.index[0]]:
                block = df_num.loc[group.index].dropna(how='all', axis=1)
                if block.shape[0] >= 10: blocks.append(block.reset_index(drop=True))
        return blocks

    @staticmethod
    def split_horizontal_blocks(df: pd.DataFrame) -> List[pd.DataFrame]:
        sub_blocks = []
        if df.empty: return []
        col_counts = df.apply(pd.to_numeric, errors='coerce').notna().sum()
        valid = np.where(col_counts >= 2)[0]
        if len(valid) == 0: return []
        splits = np.where(np.diff(valid) > 1)[0] + 1
        groups = np.split(valid, splits)
        for g in groups:
            if len(g) > 0:
                sub = df.iloc[:, g].reset_index(drop=True)
                if sub.shape[1] >= 1 and sub.shape[0] >= 2: sub_blocks.append(sub)
        return sub_blocks

    @staticmethod
    def extract_series_from_df(df: pd.DataFrame) -> List[Dict]:
        series_list = []
        try:
            vals = df.values; rows, cols = vals.shape
            if rows < 5: return []
            if cols == 1:
                y = vals[:, 0]; mask = ~np.isnan(y)
                if np.sum(mask) > 5: series_list.append({"x": np.arange(len(y))[mask], "y": y[mask], "name": "Single"})
                return series_list
            def is_axis(arr):
                arr = arr[~np.isnan(arr)]
                if len(arr) < 5: return False
                diff = np.diff(arr)
                if np.all(diff >= 0) or np.all(diff <= 0): return True
                return (np.sum(diff>0)/len(diff)>0.9) or (np.sum(diff<0)/len(diff)>0.9)
            current_x = vals[:, 0]
            if not is_axis(current_x): current_x = np.arange(rows)
            for i in range(1, cols):
                col_data = vals[:, i]; mask_valid = ~np.isnan(col_data)
                if np.sum(mask_valid) < 5: continue
                if is_axis(col_data): current_x = col_data; continue 
                mask = ~np.isnan(current_x) & ~np.isnan(col_data)
                if np.sum(mask) > 5:
                    series_list.append({"x": current_x[mask], "y": col_data[mask], "name": f"Col-{i}"})
        except: pass
        return series_list

    # --- Spectrum Logic ---
    @staticmethod
    def simple_baseline(y):
        try:
            w = max(5, len(y)//10)
            s = pd.Series(y).rolling(window=w, center=True).min()
            b = s.bfill().ffill().values 
            if len(b) > 51: b = savgol_filter(b, 51, 3)
            return b, w
        except: return np.zeros_like(y), 0

    @staticmethod
    def process_spectrum(x, y, mode, goal):
        log = []; y_raw = y.copy(); base = np.zeros_like(y); y_proc = y.copy()
        peaks = []; fits = []; stats_txt = "Error"
        try:
            if mode == "None":
                peaks, _ = find_peaks(y, height=np.max(y)*0.05)
                log.append("Mode: Raw")
                stats_txt = f"Raw Peaks: {len(peaks)}"
            else:
                win = 15; do_fit = False
                if mode == "AI-Adaptive":
                    log.append("Mode: AI-Adaptive")
                    if "noise" in goal.lower(): win = 31
                    if "fit" in goal.lower(): do_fit = True
                else: log.append("Mode: Auto")
                base, bw = ScienceProcessorNV14.simple_baseline(y)
                y_proc = np.maximum(y - base, 0)
                log.append(f"Base: Rolling Min (w={bw})")
                if len(y_proc) > win:
                    y_proc = savgol_filter(y_proc, win, 3)
                    log.append(f"Smooth: SavGol (w={win})")
                peaks, _ = find_peaks(y_proc, height=np.max(y_proc)*0.05, distance=10)
                log.append(f"Peaks: {len(peaks)}")
                stats_txt = f"Range: {np.min(x):.1f}~{np.max(x):.1f}, Peaks: {len(peaks)}"
            return {"x": x, "y_raw": y_raw, "y_proc": y_proc, "y_base": base, "peaks": [{"x": float(x[p]), "y": float(y_proc[p])} for p in peaks], "log": log, "fits": fits, "stats": stats_txt}
        except Exception as e: return {"x": x, "y_raw": y_raw, "y_proc": y_proc, "y_base": base, "peaks": [], "log": [str(e)], "fits": [], "stats": "Error"}

    # --- Footer ---
    @staticmethod
    def separate_footer(img_bgr):
        h, w = img_bgr.shape[:2]
        roi_h = int(h * 0.82)
        roi = img_bgr[roi_h:, :]
        gray = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)
        sobel_y = np.abs(cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=3))
        row_sums = np.sum(sobel_y, axis=1)
        split_idx = np.argmax(row_sums)
        if row_sums[split_idx] > w * 30:
            real_split = roi_h + split_idx
            return img_bgr[:real_split, :], img_bgr[real_split:, :]
        return img_bgr, None

    # --- [E] NV14 Customized FFT-NMF Engine ---
    @staticmethod
    def analyze_crystal_fft_nmf(img_bgr, patch_size=128, step=16, n_components=10):
        log = []
        gray = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2GRAY)
        h, w = gray.shape
        
        log.append(f"Config: Patch={patch_size}, Step={step}, K={n_components}")
        
        # 1. Custom Colormap Generation
        # 0: Black, 1~N-2: Jet, N-1: Gray
        cmap_jet = matplotlib.colormaps['jet']
        custom_colors = []
        
        # Index 0: Black
        custom_colors.append((0, 0, 0))
        
        # Middle indices: Jet
        num_jet = n_components - 2
        if num_jet < 1: num_jet = 1
        for i in range(num_jet):
            # Normalize 0~1 for jet
            val = i / (num_jet - 1) if num_jet > 1 else 0.5
            rgba = cmap_jet(val)
            # RGB -> BGR
            bgr = (int(rgba[2]*255), int(rgba[1]*255), int(rgba[0]*255))
            custom_colors.append(bgr)
            
        # Last index: Gray
        if len(custom_colors) < n_components:
            custom_colors.append((128, 128, 128))
            
        # Fill remaining if any mismatch (safety)
        while len(custom_colors) < n_components:
            custom_colors.append((255, 255, 255))
            
        
        # 2. Collect Data
        cols = [] 
        
        y_steps = list(range(0, h - patch_size + 1, step))
        x_steps = list(range(0, w - patch_size + 1, step))
        grid_h = len(y_steps)
        grid_w = len(x_steps)
        
        for y in y_steps:
            for x in x_steps:
                roi = gray[y:y+patch_size, x:x+patch_size]
                f = np.fft.fft2(roi)
                fshift = np.fft.fftshift(f)
                mag = np.abs(fshift) 
                
                # DC Removal (Center Mask)
                cy, cx = patch_size//2, patch_size//2
                mag[cy-2:cy+3, cx-2:cx+3] = 0
                
                cols.append(mag.flatten())
        
        if not cols: return {"type":"error", "msg":"Img too small"}
        
        try:
            # Transpose for NMF (Pixels, Patches)
            V = np.array(cols).T.astype(np.float32)
            
            # Global Scaling
            V = V - V.min()
            if V.max() > 0: V = V / V.max()
            
            # NMF
            nmf = NMF(n_components=n_components, init='nndsvda', random_state=42, max_iter=1000)
            W = nmf.fit_transform(V) # (Pixels, K) -> Basis Patterns
            H = nmf.components_      # (K, Patches) -> Weights
            
            # Reconstruct Map
            if H.shape[1] != grid_h * grid_w: raise ValueError("Shape Mismatch")
            
            idx_H = np.argmax(H, axis=0) # (N_patches,)
            map_low = idx_H.reshape(grid_h, grid_w).astype(np.uint8)
            map_high = cv2.resize(map_low, (w, h), interpolation=cv2.INTER_NEAREST)
            
            # Overlay
            overlay = img_bgr.copy()
            mask_layer = np.zeros_like(img_bgr)
            
            # Count Stats
            counts = Counter(idx_H)
            total_patches = len(idx_H)
            stats = {}
            
            for i in range(n_components):
                mask = (map_high == i)
                mask_layer[mask] = custom_colors[i]
                
                cnt = counts.get(i, 0)
                ratio = (cnt / total_patches) * 100
                stats[f"Pattern {i}"] = f"{cnt} ({ratio:.1f}%)"
                
            alpha = 0.4
            mask_idx = np.any(mask_layer > 0, axis=-1)
            overlay[mask_idx] = (overlay[mask_idx]*(1-alpha) + mask_layer[mask_idx]*alpha).astype(np.uint8)
            
            # Basis Pattern Strip Generation
            basis_imgs = []
            for k in range(n_components):
                pat = W[:, k].reshape(patch_size, patch_size)
                # Log scale for visualization
                pat_log = np.log(pat + 1e-9)
                pat_norm = cv2.normalize(pat_log, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)
                pat_col = cv2.cvtColor(pat_norm, cv2.COLOR_GRAY2BGR)
                
                # Apply Custom Border Color
                cv2.rectangle(pat_col, (0,0), (patch_size-1, patch_size-1), custom_colors[k], 4)
                cv2.putText(pat_col, f"{k}", (5, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255,255,255), 2)
                
                basis_imgs.append(pat_col)
            
            # Legend Strip (Color Bar)
            legend_imgs = []
            for k in range(n_components):
                # Create a solid color block
                block = np.zeros((30, patch_size, 3), dtype=np.uint8)
                block[:] = custom_colors[k]
                cv2.putText(block, f"P{k}", (5, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255,255,255), 1)
                legend_imgs.append(block)
                
            basis_strip = np.hstack(basis_imgs)
            legend_strip = np.hstack(legend_imgs)
            
            # Combine Legend (Top) and Basis (Bottom)
            # Resize legend to match width if needed (here logic ensures same width)
            full_strip = np.vstack([legend_strip, basis_strip])
            
            # Resize for visibility (2x)
            final_strip = cv2.resize(full_strip, None, fx=2, fy=2, interpolation=cv2.INTER_NEAREST)
            
            def to_b64(im): return base64.b64encode(cv2.imencode('.jpg', im)[1]).decode('utf-8')
            
            return {
                "type": "image", 
                "raw_b64": to_b64(img_bgr), 
                "proc_b64": to_b64(overlay),
                "footer_b64": to_b64(final_strip), 
                "stats": stats, 
                "log": log
            }

        except Exception as e:
            print(f"NMF Error: {e}")
            def to_b64(im): return base64.b64encode(cv2.imencode('.jpg', im)[1]).decode('utf-8')
            return {"type": "image", "raw_b64": to_b64(img_bgr), "proc_b64": to_b64(img_bgr), "stats": {"Error": str(e)}, "log": log}

    # --- Router (Updated with Params) ---
    @staticmethod
    def process_image_nv14(img_bytes, equipment, data_type, mode, goal, user_center=None, 
                           patch_size=128, step=16, n_components=10):
        nparr = np.frombuffer(img_bytes, np.uint8)
        img_raw = cv2.imdecode(nparr, cv2.IMREAD_COLOR)
        if img_raw is None: return {}
        
        body, footer = ScienceProcessorNV14.separate_footer(img_raw)
        
        footer_b64 = None
        if footer is not None:
            footer_b64 = base64.b64encode(cv2.imencode('.jpg', footer)[1]).decode('utf-8')

        # [NV14] Interactive FFT-NMF
        if data_type == "Crystal Structure (FFT)":
            return {**ScienceProcessorNV14.analyze_crystal_fft_nmf(body, patch_size, step, n_components), "footer_b64": footer_b64}
            
        if data_type == "2D Diffraction":
            return {**ScienceProcessorNV14.analyze_diffraction_nv7(body, user_center), "footer_b64": footer_b64}
        
        # General SAM
        if mode == "None":
             def to_b64(im): return base64.b64encode(cv2.imencode('.jpg', im)[1]).decode('utf-8')
             return {"type":"image", "raw_b64":to_b64(body), "proc_b64":to_b64(body), "stats":{}, "log":["Mode: Raw"], "footer_b64": footer_b64}

        img_proc = body.copy(); log = []
        if mode == "Auto" or mode == "AI-Adaptive":
            img_proc = cv2.bilateralFilter(img_proc, 9, 75, 75)
            log.append("Preproc: Denoise")

        img_rgb = cv2.cvtColor(img_proc, cv2.COLOR_BGR2RGB)
        
        if mask_generator:
            try:
                masks = mask_generator.generate(img_rgb)
                log.append(f"SAM: {len(masks)} Segments")
                overlay, stats, sam_log = ScienceProcessorNV14.analyze_sam_results_nv7(img_rgb, masks, data_type)
                log.extend(sam_log)
                overlay = cv2.cvtColor(overlay, cv2.COLOR_RGB2BGR)
            except Exception as e:
                log.append(f"SAM Error: {str(e)}")
                overlay = body; stats = {}
        else:
            log.append("SAM Not Loaded")
            overlay = body; stats = {}

        def to_b64(im): return base64.b64encode(cv2.imencode('.jpg', im)[1]).decode('utf-8')
        return {"type": "image", "raw_b64": to_b64(body), "proc_b64": to_b64(overlay), "stats": stats, "log": log, "footer_b64": footer_b64}

# [4] Helper
async def analyze_vision_ollama(image_bytes: bytes, prompt: str) -> str:
    try:
        b64 = base64.b64encode(image_bytes).decode('utf-8')
        res = await asyncio.to_thread(ollama_client.chat, model=VISION_MODEL, messages=[{'role':'user','content':prompt,'images':[b64]}])
        return res['message']['content']
    except: return "Vision Error"

async def interpret_spectrum_data(raw_context: str, equipment: str) -> str:
    try:
        prompt = f"Analyze spectrum ({equipment}). Brief interpretation in Korean. Data: {raw_context}"
        res = await asyncio.to_thread(ollama_client.chat, model=TEXT_MODEL, messages=[{'role':'user','content':prompt}])
        return res['message']['content']
    except: return ""

# [5] App
app = FastAPI(title="Analyst NV14 Interactive", lifespan=lifespan)
app.add_middleware(CORSMiddleware, allow_origins=["*"], allow_methods=["*"], allow_headers=["*"])

class TranslateReq(BaseModel): text: str

@app.post("/api/translate")
async def translate_text(req: TranslateReq):
    try:
        res = await asyncio.to_thread(ollama_client.chat, model=TEXT_MODEL, messages=[{'role':'system','content':'Translate to English.'}, {'role':'user','content':req.text}])
        return {"translated": res['message']['content']}
    except Exception as e: return {"translated": str(e)}

async def process_analysis_job(job_id: str, files_map: Dict, configs: Dict):
    try:
        JOBS[job_id]["status"] = "Processing"
        JOBS[job_id]["progress"] = 10
        final_results = []
        total_files = len(configs)
        processed_count = 0

        for filename, config in configs.items():
            if filename not in files_map: continue
            content = files_map[filename]
            eq = config.get("equipment", "General")
            data_type = config.get("data_type", "General")
            goal = config.get("goal", "")
            mode = config.get("mode", "Auto")
            
            # [NV14] Params
            user_center = None
            if config.get("center_coords"):
                try: user_center = [int(v) for v in config["center_coords"].split(",")]
                except: pass
            
            # Default values if not provided
            p_size = int(config.get("patch_size", 128))
            p_step = int(config.get("step", 16))
            n_comp = int(config.get("n_components", 10))

            fname_lower = filename.lower()
            JOBS[job_id]["step"] = f"Analyzing {filename}..."
            
            if eq == "Document" or fname_lower.endswith(('.pdf', '.ppt', '.pptx')):
                # (Logic omitted)
                pass 

            elif fname_lower.endswith(('.xlsx', '.xls', '.csv', '.txt')):
                # (Logic omitted, same as NV13)
                try:
                    dfs = ScienceProcessorNV14.read_universal_dataframe(content, filename)
                    for i, df in enumerate(dfs):
                        v_blocks = ScienceProcessorNV14.detect_structured_blocks(df)
                        for v_block in v_blocks:
                            h_blocks = ScienceProcessorNV14.split_horizontal_blocks(v_block)
                            for h_block in h_blocks:
                                series_list = ScienceProcessorNV14.extract_series_from_df(h_block)
                                for s in series_list:
                                    res = ScienceProcessorNV14.process_spectrum(s['x'], s['y'], mode, goal)
                                    step = max(1, len(s['x'])//100)
                                    chart = [{"x": float(res["x"][k]), "y_proc": float(res["y_proc"][k]), "y_raw": float(res["y_raw"][k]), "y_base": float(res["y_base"][k])} for k in range(0, len(s['x']), step)]
                                    ctx = f"Data: {s['name']}\nStats: {res['stats']}"
                                    interp = await interpret_spectrum_data(ctx, eq)
                                    final_results.append({
                                        "type": "spectrum", "filename": f"{filename} ({s['name']})", "equipment": eq,
                                        "raw_context": f"{ctx}\nInterp: {interp}", 
                                        "chart_data": chart, "log": res["log"], "interpretation": interp
                                    })
                except Exception as ex: final_results.append({"type": "error", "filename": filename, "msg": str(ex)})

            elif fname_lower.endswith(('.png', '.jpg', '.jpeg', '.tif', '.bmp')):
                try:
                    JOBS[job_id]["step"] = f"Vision Analysis: {filename}..."
                    vis_res = await asyncio.to_thread(ScienceProcessorNV14.process_image_nv14, content, eq, data_type, mode, goal, user_center, p_size, p_step, n_comp)
                    if vis_res:
                        scale_info = "Unknown"
                        # Footer processing...
                        
                        # Use footer_b64 from FFT result if available (it contains the strip)
                        # If normal SAM, check regular footer
                        final_footer = vis_res.get("footer_b64")

                        # If FFT, footer is the strip, so we don't OCR it for scale
                        if data_type != "Crystal Structure (FFT)" and final_footer:
                             # Logic to OCR scale...
                             pass

                        stats_str = json.dumps(vis_res.get('stats',{}), indent=2)
                        desc_prompt = f"Analyze {eq} ({data_type}). Stats: {stats_str}. Goal: {goal}. Summary in Korean."
                        desc = await analyze_vision_ollama(base64.b64decode(vis_res["proc_b64"]), desc_prompt)
                        
                        final_results.append({
                            "type": "image", "filename": filename, "equipment": eq, "summary": desc,
                            "raw_context": f"Img {eq}: {desc}", "raw_b64": vis_res["raw_b64"], "proc_b64": vis_res["proc_b64"], "footer_b64": final_footer,
                            "stats": vis_res.get("stats",{}), "log": vis_res.get("log", [])
                        })
                except Exception as ex: final_results.append({"type": "error", "filename": filename, "msg": str(ex)})
            
            processed_count += 1
            JOBS[job_id]["progress"] = 10 + int((processed_count / total_files) * 80)

        JOBS[job_id]["step"] = "Writing Report..."
        final_results = sanitize_json(final_results)
        
        data_ctx = ""
        for r in final_results:
            if r.get("type") != "error":
                raw = r.get("raw_context") or ""
                data_ctx += f"\n=== {r['filename']} ===\n{raw[:1000]}\n"

        final_report = "Fail"
        if data_ctx:
            try:
                system_prompt = "You are a research assistant. Write a report in Korean. Summarize data and physics."
                res = await asyncio.to_thread(ollama_client.chat, model=TEXT_MODEL, messages=[{'role':'system','content':system_prompt}, {'role':'user','content':f"Data:\n{data_ctx}"}])
                final_report = res['message']['content']
            except Exception as e: final_report = str(e)

        JOBS[job_id]["results"] = {"results": final_results, "final_report": final_report}
        JOBS[job_id]["status"] = "Completed"
        JOBS[job_id]["progress"] = 100
    except Exception as e:
        JOBS[job_id]["status"] = "Failed"; JOBS[job_id]["error"] = str(e)

@app.post("/api/analyze")
async def start_analysis(background_tasks: BackgroundTasks, files: List[UploadFile] = File(...), file_configs: str = Form(...)):
    job_id = str(uuid.uuid4()); configs = json.loads(file_configs)
    files_map = {f.filename: await f.read() for f in files}
    JOBS[job_id] = {"status": "Pending", "progress": 0, "step": "Init", "results": None}
    background_tasks.add_task(process_analysis_job, job_id, files_map, configs)
    return {"job_id": job_id}

@app.get("/api/status/{job_id}")
async def get_status(job_id: str): return JOBS.get(job_id, {"status": "Not Found"})

@app.get("/", response_class=HTMLResponse)
async def serve_index():
    if os.path.exists("index.html"): return FileResponse("index.html")
    return "<h1>NV14 Interactive Backend</h1>"

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)



    # --- [E] NV13 Transposed NMF Engine (Hotfix) ---
    @staticmethod
    def analyze_crystal_fft_nmf(img_bgr):
        log = []
        gray = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2GRAY)
        h, w = gray.shape
        
        # 1. Config
        patch_size = 64
        step = 32
        n_components = 10 
        
        log.append(f"FFT-NMF: Patch={patch_size}, Step={step}, K={n_components}")
        
        # Grid Info
        y_steps = list(range(0, h - patch_size + 1, step))
        x_steps = list(range(0, w - patch_size + 1, step))
        grid_h = len(y_steps)
        grid_w = len(x_steps)
        
        # 2. Collect Data
        cols = [] 
        coords = []
        
        for y in y_steps:
            for x in x_steps:
                roi = gray[y:y+patch_size, x:x+patch_size]
                f = np.fft.fft2(roi)
                fshift = np.fft.fftshift(f)
                mag = np.abs(fshift) 
                
                # [Optimization] Center Masking (DC Removal)
                cy, cx = patch_size//2, patch_size//2
                mag[cy-2:cy+3, cx-2:cx+3] = 0
                
                cols.append(mag.flatten())
                coords.append((x, y))
        
        if not cols: 
            return {"type":"error", "msg":"Img too small", "proc_b64": "", "raw_b64": ""}
        
        try:
            # Transpose: (Pixels, Patches) -> (4096, N)
            V = np.array(cols).T.astype(np.float32)
            
            # Global Scaling
            V = V - V.min()
            if V.max() > 0: V = V / V.max()
            
            # DC Row Removal (User Logic)
            dc_row_idx = np.argmax(np.mean(V, axis=1)) 
            V[dc_row_idx, :] = 0 
            
            # 3. NMF
            # V ~ W * H
            # W: (4096, 10) -> Basis Images (Patterns)
            # H: (10, N)    -> Coefficients (Spatial Weights)
            nmf = NMF(n_components=n_components, init='nndsvda', random_state=42, max_iter=1000)
            W = nmf.fit_transform(V) 
            H = nmf.components_ 
            
            # 4. Visualization 1: Spatial Map (Using H)
            # H shape is (K, N_patches). We need N_patches dimension.
            if H.shape[1] != grid_h * grid_w:
                 raise ValueError(f"Shape Mismatch: H_cols={H.shape[1]} vs Grid={grid_h*grid_w}")
            
            # Find dominant pattern index for each patch
            idx_H = np.argmax(H, axis=0) # (N_patches,)
            
            # Reshape to grid
            map_low = idx_H.reshape(grid_h, grid_w).astype(np.uint8)
            map_high = cv2.resize(map_low, (w, h), interpolation=cv2.INTER_NEAREST)
            
            # Overlay
            overlay = img_bgr.copy()
            mask_layer = np.zeros_like(img_bgr)
            
            cmap = matplotlib.colormaps['tab10']
            colors = []
            for i in range(n_components):
                rgba = cmap(i)
                bgr = (int(rgba[2]*255), int(rgba[1]*255), int(rgba[0]*255))
                colors.append(bgr)
                
            stats = {}
            for i in range(n_components):
                mask = (map_high == i)
                count = int(np.sum(mask))
                if count > 0: stats[f"Pattern {i}"] = count
                mask_layer[mask] = colors[i]
                
            alpha = 0.4
            mask_idx = np.any(mask_layer > 0, axis=-1)
            overlay[mask_idx] = (overlay[mask_idx]*(1-alpha) + mask_layer[mask_idx]*alpha).astype(np.uint8)
            
            # 5. Visualization 2: Basis Patterns (Using W)
            basis_imgs = []
            for k in range(n_components):
                # W columns are the flattened images
                pat_flat = W[:, k]
                pat_img = pat_flat.reshape(patch_size, patch_size)
                
                # Log scale for visibility (FFT patterns usually need log)
                pat_log = np.log(pat_img + 1e-9)
                pat_norm = cv2.normalize(pat_log, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)
                pat_col = cv2.cvtColor(pat_norm, cv2.COLOR_GRAY2BGR)
                
                # Add colored border
                cv2.rectangle(pat_col, (0,0), (patch_size-1, patch_size-1), colors[k], 4)
                cv2.putText(pat_col, f"P{k}", (2, 15), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255,255,255), 1)
                basis_imgs.append(pat_col)
                
            basis_strip = np.hstack(basis_imgs)
            basis_strip_large = cv2.resize(basis_strip, None, fx=2, fy=2, interpolation=cv2.INTER_NEAREST)
            
            # Encode
            def to_b64(im): return base64.b64encode(cv2.imencode('.jpg', im)[1]).decode('utf-8')
            
            return {
                "type": "image", 
                "raw_b64": to_b64(img_bgr), 
                "proc_b64": to_b64(overlay),
                "footer_b64": to_b64(basis_strip_large), 
                "stats": stats, 
                "log": log
            }

        except Exception as e:
            print(f"NMF Error: {e}")
            def to_b64(im): return base64.b64encode(cv2.imencode('.jpg', im)[1]).decode('utf-8')
            # Return raw image on error to avoid 'Fail' in frontend
            return {
                "type": "image", 
                "raw_b64": to_b64(img_bgr), 
                "proc_b64": to_b64(img_bgr),
                "stats": {"Error": str(e)}, 
                "log": log + [f"Critical NMF Error: {str(e)}"]
            }


엔브이13

import os
import io
# [0] OMP Error Fix
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"
import torch

import asyncio
import base64
import json
import re
import math
import uuid
import time
from typing import List, Dict, Any, Optional
from pydantic import BaseModel
from contextlib import asynccontextmanager
from collections import Counter

# [1] Security
os.environ['NO_PROXY'] = '127.0.0.1,localhost,0.0.0.0,::1'
if 'HTTP_PROXY' in os.environ: del os.environ['HTTP_PROXY']
if 'HTTPS_PROXY' in os.environ: del os.environ['HTTPS_PROXY']

from fastapi import FastAPI, UploadFile, File, Form, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import HTMLResponse, FileResponse
import pandas as pd
import numpy as np

# SciPy & CV & Sklearn
from sklearn.decomposition import NMF
from scipy.signal import find_peaks, savgol_filter
from scipy.optimize import curve_fit
from scipy.ndimage import center_of_mass
from skimage.feature import peak_local_max
import cv2
import matplotlib
import matplotlib.cm as cm 

# Models
from segment_anything import sam_model_registry, SamAutomaticMaskGenerator
from ollama import Client 
from pdf2image import convert_from_bytes
from pptx import Presentation
from pptx.enum.shapes import MSO_SHAPE_TYPE

# [2] Config
OLLAMA_HOST = "http://127.0.0.1:11434"
ollama_client = Client(host=OLLAMA_HOST)
VISION_MODEL = "qwen3-vl:30b-a3b-instruct" 
TEXT_MODEL = "llama3:70b"

SAM_CHECKPOINT = "sam_vit_b_01ec64.pth"
SAM_MODEL_TYPE = "vit_b"
device = "cuda" if torch.cuda.is_available() else "cpu"

mask_generator = None 
JOBS = {} 

@asynccontextmanager
async def lifespan(app: FastAPI):
    global mask_generator
    if os.path.exists(SAM_CHECKPOINT):
        print(f"Loading SAM ({SAM_MODEL_TYPE}) on {device}...")
        try:
            sam = sam_model_registry[SAM_MODEL_TYPE](checkpoint=SAM_CHECKPOINT)
            sam.to(device=device)
            mask_generator = SamAutomaticMaskGenerator(
                model=sam, points_per_side=32, pred_iou_thresh=0.86,
                stability_score_thresh=0.90, min_mask_region_area=50
            )
            print("SAM Loaded.")
        except: print("SAM Load Failed.")
    yield
    JOBS.clear()

def sanitize_json(obj):
    if isinstance(obj, float):
        if np.isnan(obj) or np.isinf(obj): return 0.0
        return obj
    elif isinstance(obj, dict): return {k: sanitize_json(v) for k, v in obj.items()}
    elif isinstance(obj, list): return [sanitize_json(v) for v in obj]
    elif isinstance(obj, (np.int64, np.int32)): return int(obj)
    elif isinstance(obj, (np.float64, np.float32)):
        if np.isnan(obj) or np.isinf(obj): return 0.0
        return float(obj)
    return obj

# ==========================================
# [3] NV13 Transposed NMF Engine
# ==========================================
class ScienceProcessorNV13:
    
    # --- Universal Loader ---
    @staticmethod
    def read_universal_dataframe(content: bytes, filename: str) -> List[pd.DataFrame]:
        dfs = []
        fname = filename.lower()
        if fname.endswith(('.xlsx', '.xls')):
            try:
                xls = pd.ExcelFile(io.BytesIO(content))
                for s in xls.sheet_names:
                    df = pd.read_excel(xls, sheet_name=s, header=None)
                    num = df.apply(pd.to_numeric, errors='coerce').notna().sum().sum()
                    if num < 5: continue
                    if not df.empty: dfs.append(df)
                if dfs: return dfs
            except: pass
        text_content = ""
        for enc in ['utf-8', 'cp949', 'latin1']:
            try: text_content = content.decode(enc); break
            except: continue
        if text_content:
            lines = text_content.splitlines()
            data_start = -1; delimiter = None
            separators = [',', '\t', ';', '\s+']
            for i, line in enumerate(lines[:100]):
                for sep in separators:
                    parts = line.split() if sep=='\s+' else line.split(sep)
                    nums = sum(1 for p in parts if re.match(r'^-?\d+(\.\d+)?(?:[eE][+-]?\d+)?$', p.strip()))
                    if nums >= 2: data_start = i; delimiter = sep; break
                if data_start != -1: break
            if data_start != -1:
                try:
                    data_str = "\n".join(lines[data_start:])
                    sep_arg = '\s+' if delimiter=='\s+' else delimiter
                    df = pd.read_csv(io.StringIO(data_str), sep=sep_arg, header=None, engine='python')
                    dfs.append(df)
                except: pass
        return dfs

    # --- Block Parser ---
    @staticmethod
    def detect_structured_blocks(df: pd.DataFrame) -> List[pd.DataFrame]:
        blocks = []
        if df.empty: return []
        df_num = df.apply(pd.to_numeric, errors='coerce')
        if df.shape[1] > 0: is_data_row = df_num.iloc[:, 0].notna()
        else: is_data_row = df_num.notna().any(axis=1)
        groups = (is_data_row != is_data_row.shift()).cumsum()
        for _, group in df.groupby(groups):
            if is_data_row.loc[group.index[0]]:
                block = df_num.loc[group.index].dropna(how='all', axis=1)
                if block.shape[0] >= 10: blocks.append(block.reset_index(drop=True))
        return blocks

    @staticmethod
    def split_horizontal_blocks(df: pd.DataFrame) -> List[pd.DataFrame]:
        sub_blocks = []
        if df.empty: return []
        col_counts = df.apply(pd.to_numeric, errors='coerce').notna().sum()
        valid = np.where(col_counts >= 2)[0]
        if len(valid) == 0: return []
        splits = np.where(np.diff(valid) > 1)[0] + 1
        groups = np.split(valid, splits)
        for g in groups:
            if len(g) > 0:
                sub = df.iloc[:, g].reset_index(drop=True)
                if sub.shape[1] >= 1 and sub.shape[0] >= 2: sub_blocks.append(sub)
        return sub_blocks

    @staticmethod
    def extract_series_from_df(df: pd.DataFrame) -> List[Dict]:
        series_list = []
        try:
            vals = df.values; rows, cols = vals.shape
            if rows < 5: return []
            if cols == 1:
                y = vals[:, 0]; mask = ~np.isnan(y)
                if np.sum(mask) > 5: series_list.append({"x": np.arange(len(y))[mask], "y": y[mask], "name": "Single"})
                return series_list
            def is_axis(arr):
                arr = arr[~np.isnan(arr)]
                if len(arr) < 5: return False
                diff = np.diff(arr)
                if np.all(diff >= 0) or np.all(diff <= 0): return True
                return (np.sum(diff>0)/len(diff)>0.9) or (np.sum(diff<0)/len(diff)>0.9)
            current_x = vals[:, 0]
            if not is_axis(current_x): current_x = np.arange(rows)
            for i in range(1, cols):
                col_data = vals[:, i]; mask_valid = ~np.isnan(col_data)
                if np.sum(mask_valid) < 5: continue
                if is_axis(col_data): current_x = col_data; continue 
                mask = ~np.isnan(current_x) & ~np.isnan(col_data)
                if np.sum(mask) > 5:
                    series_list.append({"x": current_x[mask], "y": col_data[mask], "name": f"Col-{i}"})
        except: pass
        return series_list

    # --- Spectrum Logic ---
    @staticmethod
    def simple_baseline(y):
        try:
            w = max(5, len(y)//10)
            s = pd.Series(y).rolling(window=w, center=True).min()
            b = s.bfill().ffill().values 
            if len(b) > 51: b = savgol_filter(b, 51, 3)
            return b, w
        except: return np.zeros_like(y), 0

    @staticmethod
    def process_spectrum(x, y, mode, goal):
        log = []; y_raw = y.copy(); base = np.zeros_like(y); y_proc = y.copy()
        peaks = []; fits = []; stats_txt = "Error"
        try:
            if mode == "None":
                peaks, _ = find_peaks(y, height=np.max(y)*0.05)
                log.append("Mode: Raw")
                stats_txt = f"Raw Peaks: {len(peaks)}"
            else:
                win = 15; do_fit = False
                if mode == "AI-Adaptive":
                    log.append("Mode: AI-Adaptive")
                    if "noise" in goal.lower(): win = 31
                    if "fit" in goal.lower(): do_fit = True
                else: log.append("Mode: Auto")
                base, bw = ScienceProcessorNV13.simple_baseline(y)
                y_proc = np.maximum(y - base, 0)
                log.append(f"Base: Rolling Min (w={bw})")
                if len(y_proc) > win:
                    y_proc = savgol_filter(y_proc, win, 3)
                    log.append(f"Smooth: SavGol (w={win})")
                peaks, _ = find_peaks(y_proc, height=np.max(y_proc)*0.05, distance=10)
                log.append(f"Peaks: {len(peaks)}")
                stats_txt = f"Range: {np.min(x):.1f}~{np.max(x):.1f}, Peaks: {len(peaks)}"
            return {"x": x, "y_raw": y_raw, "y_proc": y_proc, "y_base": base, "peaks": [{"x": float(x[p]), "y": float(y_proc[p])} for p in peaks], "log": log, "fits": fits, "stats": stats_txt}
        except Exception as e: return {"x": x, "y_raw": y_raw, "y_proc": y_proc, "y_base": base, "peaks": [], "log": [str(e)], "fits": [], "stats": "Error"}

    # --- Footer ---
    @staticmethod
    def separate_footer(img_bgr):
        h, w = img_bgr.shape[:2]
        roi_h = int(h * 0.82)
        roi = img_bgr[roi_h:, :]
        gray = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)
        sobel_y = np.abs(cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=3))
        row_sums = np.sum(sobel_y, axis=1)
        split_idx = np.argmax(row_sums)
        if row_sums[split_idx] > w * 30:
            real_split = roi_h + split_idx
            return img_bgr[:real_split, :], img_bgr[real_split:, :]
        return img_bgr, None

    # --- [E] NV13 Transposed NMF Engine ---
    @staticmethod
    def analyze_crystal_fft_nmf(img_bgr):
        log = []
        gray = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2GRAY)
        h, w = gray.shape
        
        # 1. Config
        patch_size = 64
        step = 32
        n_components = 10 
        log.append(f"NMF (Transposed): Patch={patch_size}, Step={step}, K={n_components}")
        
        # Grid Info
        y_steps = list(range(0, h - patch_size + 1, step))
        x_steps = list(range(0, w - patch_size + 1, step))
        grid_h = len(y_steps)
        grid_w = len(x_steps)
        
        # 2. Collect Data (Flattened Patches)
        # reshaped[:, k] means: (Pixels, k-th Patch)
        # So we collect patches and then stack them as columns
        
        cols = [] # Will hold patches
        
        for y in y_steps:
            for x in x_steps:
                roi = gray[y:y+patch_size, x:x+patch_size]
                # FFT
                f = np.fft.fft2(roi)
                fshift = np.fft.fftshift(f)
                mag = np.abs(fshift) # No Log yet, keep linear for NMF or apply Log? Usually Abs is better for structure.
                # User's pseudo code said: np.abs(fftshift(fft2))
                cols.append(mag.flatten())
        
        if not cols: return {"type":"error", "msg":"Img too small"}
        
        try:
            # Create Matrix V: (Pixels, Patches)
            # cols is list of (Pixels,), so np.array(cols) is (Patches, Pixels)
            # We transpose it to match user's logic: (Pixels, Patches)
            V = np.array(cols).T 
            
            # [Step 2: DC Removal - User's Way]
            # "max_index = np.argmax(reshaped[:,0])" 
            # We find the row index that corresponds to the DC component (max val)
            # Using mean across all patches is safer to find the DC pixel
            dc_row_idx = np.argmax(np.mean(V, axis=1)) 
            V[dc_row_idx, :] = 0 # Kill DC for all patches
            
            # [Step 3: NMF]
            # W (Pixels, K) -> Basis Patterns (Images)
            # H (K, Patches) -> Weights (Coefficients)
            nmf = NMF(n_components=n_components, init='nndsvda', random_state=42, max_iter=1000)
            W = nmf.fit_transform(V) # Input is (Pixels, Patches)
            H = nmf.components_ 
            
            # [Step 4: Mapping - Primary Dominant]
            # H shape is (10, N_Patches)
            # idx_H: For each patch (column), which component (row) is max?
            idx_H = np.argmax(H, axis=0) # Shape: (N_Patches,)
            
            # Primary Counts
            count_IDX = Counter(idx_H)
            log.append(f"Dominant Patterns: {dict(count_IDX)}")
            
            # [Secondary Dominant - User Request]
            # Partition to find second largest
            # H is (10, N) -> Partition along axis 0
            if n_components >= 2:
                # np.partition puts the k-th element in position. -2 means 2nd largest.
                # But argpartition is easier to get indices.
                sorted_indices = np.argsort(H, axis=0) # (10, N), ascending
                idx_H2 = sorted_indices[-2, :] # Second to last row is 2nd largest
                count_IDX2 = Counter(idx_H2)
                log.append(f"Secondary Patterns: {dict(count_IDX2)}")
            
            # [Step 5: Visualization - Index Map]
            # Reshape idx_H to grid
            if idx_H.shape[0] != grid_h * grid_w:
                 raise ValueError("Shape mismatch")
                 
            map_low = idx_H.reshape(grid_h, grid_w).astype(np.uint8)
            # Resize to original image size (Nearest Neighbor to keep integers 0-9)
            map_high = cv2.resize(map_low, (w, h), interpolation=cv2.INTER_NEAREST)
            
            # Create Color Overlay
            overlay = img_bgr.copy()
            mask_layer = np.zeros_like(img_bgr)
            
            # Colormap (0-9)
            cmap = matplotlib.colormaps['tab10']
            colors = []
            for i in range(n_components):
                rgba = cmap(i)
                bgr = (int(rgba[2]*255), int(rgba[1]*255), int(rgba[0]*255))
                colors.append(bgr)
                
            for i in range(n_components):
                mask = (map_high == i)
                mask_layer[mask] = colors[i]
                
            alpha = 0.4
            mask_idx = np.any(mask_layer > 0, axis=-1)
            overlay[mask_idx] = (overlay[mask_idx]*(1-alpha) + mask_layer[mask_idx]*alpha).astype(np.uint8)
            
            # [Step 6: Basis Patterns Strip]
            # W is (Pixels, K). Each column is a pattern.
            basis_imgs = []
            for k in range(n_components):
                # Extract k-th column
                pat_flat = W[:, k]
                pat_img = pat_flat.reshape(patch_size, patch_size)
                
                # Log scale for display if needed (FFT patterns usually need log)
                # But NMF output W is in linear scale of input V (which was abs).
                # Let's verify: V was abs(fft). So W is abs(fft). Log makes it visible.
                pat_log = np.log(pat_img + 1e-9)
                pat_norm = cv2.normalize(pat_log, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)
                
                pat_col = cv2.cvtColor(pat_norm, cv2.COLOR_GRAY2BGR)
                
                # Border color matches map
                cv2.rectangle(pat_col, (0,0), (patch_size-1, patch_size-1), colors[k], 4)
                cv2.putText(pat_col, f"P{k}", (2, 15), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255,255,255), 1)
                
                basis_imgs.append(pat_col)
                
            basis_strip = np.hstack(basis_imgs)
            basis_strip_large = cv2.resize(basis_strip, None, fx=2, fy=2, interpolation=cv2.INTER_NEAREST)
            
            def to_b64(im): return base64.b64encode(cv2.imencode('.jpg', im)[1]).decode('utf-8')
            
            return {
                "type": "image", 
                "raw_b64": to_b64(img_bgr), 
                "proc_b64": to_b64(overlay),
                "footer_b64": to_b64(basis_strip_large), 
                "stats": {"Dominant": dict(count_IDX)}, 
                "log": log
            }

        except Exception as e:
            print(f"NMF Error: {e}")
            def to_b64(im): return base64.b64encode(cv2.imencode('.jpg', im)[1]).decode('utf-8')
            return {
                "type": "image", "raw_b64": to_b64(img_bgr), "proc_b64": to_b64(img_bgr),
                "stats": {"error": str(e)}, "log": log + [f"NMF Error: {str(e)}"]
            }

    # --- [F] Diffraction ---
    @staticmethod
    def analyze_diffraction_nv7(img_bgr, user_center=None):
        log = []
        stats = {}
        gray = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2GRAY)
        
        if user_center and len(user_center)==2 and user_center[0]>0:
            cx, cy = user_center
            log.append(f"Center: Manual [{cx}, {cy}]")
        else:
            _, th = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY+cv2.THRESH_OTSU)
            cyf, cxf = center_of_mass(th)
            if np.isnan(cyf): cyf, cxf = gray.shape[0]//2, gray.shape[1]//2
            cx, cy = int(cxf), int(cyf)
            log.append(f"Center: Auto [{cx}, {cy}]")

        g1 = cv2.GaussianBlur(gray, (3,3), 0)
        g2 = cv2.GaussianBlur(gray, (51,51), 0)
        diff = cv2.subtract(g1, g2)
        
        coords = peak_local_max(diff, min_distance=10, threshold_abs=20)
        
        enhanced = np.log1p(diff.astype(np.float32))
        enhanced = cv2.normalize(enhanced, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)
        vis = cv2.cvtColor(enhanced, cv2.COLOR_GRAY2BGR)
        
        cv2.drawMarker(vis, (cx, cy), (0,0,255), cv2.MARKER_CROSS, 20, 2)
        peak_list = []
        for p in coords:
            py, px = p
            if abs(py-cy)<5 and abs(px-cx)<5: continue
            cv2.circle(vis, (px, py), 3, (0,255,0), 1)
            peak_list.append(f"[{px},{py}]")

        log.append(f"Spots: {len(peak_list)}")
        def to_b64(im): return base64.b64encode(cv2.imencode('.jpg', im)[1]).decode('utf-8')
        return {"type": "image", "raw_b64": to_b64(img_bgr), "proc_b64": to_b64(vis), "stats": {"center": [cx, cy], "spots": len(peak_list)}, "log": log}

    # --- [G] SAM ---
    @staticmethod
    def analyze_sam_results_nv7(img_rgb, masks, data_type):
        overlay = img_rgb.copy()
        mask_overlay = np.zeros_like(img_rgb)
        h_img, w_img = img_rgb.shape[:2]
        stats = {"count": 0}
        log = []
        
        valid_masks = []
        for ann in masks:
            area = ann['area']; bbox = ann['bbox']; x, y, w, h = [int(v) for v in bbox]
            if area > h_img * w_img * 0.95 or area < 20: continue 
            if data_type == "Particle":
                if x<=1 or y<=1 or (x+w)>=w_img-1 or (y+h)>=h_img-1: continue
                mask_uint8 = (ann['segmentation'] * 255).astype(np.uint8)
                cnts, _ = cv2.findContours(mask_uint8, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
                if cnts:
                    perim = cv2.arcLength(cnts[0], True)
                    circ = 4*np.pi*area/(perim**2) if perim>0 else 0
                    if circ < 0.25: continue
            valid_masks.append(ann)

        if data_type == "Thin Film":
            sorted_masks = sorted(valid_masks, key=lambda x: x['bbox'][1])
        else:
            sorted_masks = sorted(valid_masks, key=lambda x: x['area'], reverse=True)

        thickness_vals = []; diameters = []

        for i, ann in enumerate(sorted_masks):
            m = ann['segmentation']
            bbox = ann['bbox']; x, y, w, h = [int(v) for v in bbox]
            cx, cy = int(x + w/2), int(y + h/2)
            color = (0, 255, 0) 

            if data_type == "Thin Film":
                is_bound = (i == 0 or i == len(sorted_masks)-1)
                cv2.putText(overlay, f"#{i+1}", (cx, cy), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (255, 255, 255), 3)
                if not is_bound:
                    thk = ann['area'] / w
                    thickness_vals.append(thk)
                    color = (255, 0, 0) 
                else: color = (100, 100, 100)
                if i < len(sorted_masks) - 1:
                    ys, xs = np.where(m)
                    if len(xs) > 10:
                        roughness = np.std(ys)
                        cv2.line(overlay, (x, y+h), (x+w, y+h), (0, 255, 255), 2)
                        cv2.putText(overlay, f"Int#{i+1} R:{roughness:.2f}", (x+10, y+h-5), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)
            elif data_type == "Particle":
                equivalent_diameter = 2 * np.sqrt(ann['area'] / np.pi)
                diameters.append(equivalent_diameter)

            mask_overlay[m] = color
            mask_uint8 = (m * 255).astype(np.uint8)
            cnts, _ = cv2.findContours(mask_uint8, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
            if cnts: cv2.drawContours(overlay, [cnts[0]], -1, color, 1)

        alpha = 0.35
        mask_idx = np.any(mask_overlay > 0, axis=-1)
        overlay[mask_idx] = (overlay[mask_idx]*(1-alpha) + mask_overlay[mask_idx]*alpha).astype(np.uint8)

        stats["count"] = len(sorted_masks)
        if data_type == "Thin Film":
            avg = round(np.mean(thickness_vals), 2) if thickness_vals else 0
            stats["thickness_avg"] = avg
            log.append(f"Layers: {len(sorted_masks)}, Avg Thk: {avg}")
        elif data_type == "Particle":
            avg = round(np.mean(diameters), 2) if diameters else 0
            stats["diameter_avg"] = avg
            log.append(f"Particles: {len(sorted_masks)}")
        return overlay, stats, log

    # --- Router ---
    @staticmethod
    def process_image_nv13(img_bytes, equipment, data_type, mode, goal, user_center=None):
        nparr = np.frombuffer(img_bytes, np.uint8)
        img_raw = cv2.imdecode(nparr, cv2.IMREAD_COLOR)
        if img_raw is None: return {}
        
        body, footer = ScienceProcessorNV13.separate_footer(img_raw)
        
        footer_b64 = None
        if footer is not None:
            footer_b64 = base64.b64encode(cv2.imencode('.jpg', footer)[1]).decode('utf-8')

        # [NV13] Transposed NMF
        if data_type == "Crystal Structure (FFT)":
            return {**ScienceProcessorNV13.analyze_crystal_fft_nmf(body), "footer_b64": footer_b64}
            
        if data_type == "2D Diffraction":
            return {**ScienceProcessorNV13.analyze_diffraction_nv7(body, user_center), "footer_b64": footer_b64}
        
        if mode == "None":
             def to_b64(im): return base64.b64encode(cv2.imencode('.jpg', im)[1]).decode('utf-8')
             return {"type":"image", "raw_b64":to_b64(body), "proc_b64":to_b64(body), "stats":{}, "log":["Mode: Raw"], "footer_b64": footer_b64}

        img_proc = body.copy(); log = []
        if mode == "Auto" or mode == "AI-Adaptive":
            img_proc = cv2.bilateralFilter(img_proc, 9, 75, 75)
            log.append("Preproc: Denoise")

        img_rgb = cv2.cvtColor(img_proc, cv2.COLOR_BGR2RGB)
        
        if mask_generator:
            try:
                masks = mask_generator.generate(img_rgb)
                log.append(f"SAM: {len(masks)} Segments")
                overlay, stats, sam_log = ScienceProcessorNV13.analyze_sam_results_nv7(img_rgb, masks, data_type)
                log.extend(sam_log)
                overlay = cv2.cvtColor(overlay, cv2.COLOR_RGB2BGR)
            except Exception as e:
                log.append(f"SAM Error: {str(e)}")
                overlay = body; stats = {}
        else:
            log.append("SAM Not Loaded")
            overlay = body; stats = {}

        def to_b64(im): return base64.b64encode(cv2.imencode('.jpg', im)[1]).decode('utf-8')
        return {"type": "image", "raw_b64": to_b64(body), "proc_b64": to_b64(overlay), "stats": stats, "log": log, "footer_b64": footer_b64}

# [4] Helper
async def analyze_vision_ollama(image_bytes: bytes, prompt: str) -> str:
    try:
        b64 = base64.b64encode(image_bytes).decode('utf-8')
        res = await asyncio.to_thread(ollama_client.chat, model=VISION_MODEL, messages=[{'role':'user','content':prompt,'images':[b64]}])
        return res['message']['content']
    except: return "Vision Error"

async def interpret_spectrum_data(raw_context: str, equipment: str) -> str:
    try:
        prompt = f"Analyze spectrum ({equipment}). Brief interpretation in Korean. Data: {raw_context}"
        res = await asyncio.to_thread(ollama_client.chat, model=TEXT_MODEL, messages=[{'role':'user','content':prompt}])
        return res['message']['content']
    except: return ""

# [5] App
app = FastAPI(title="Analyst NV13 Transposed", lifespan=lifespan)
app.add_middleware(CORSMiddleware, allow_origins=["*"], allow_methods=["*"], allow_headers=["*"])

class TranslateReq(BaseModel): text: str

@app.post("/api/translate")
async def translate_text(req: TranslateReq):
    try:
        res = await asyncio.to_thread(ollama_client.chat, model=TEXT_MODEL, messages=[{'role':'system','content':'Translate to English.'}, {'role':'user','content':req.text}])
        return {"translated": res['message']['content']}
    except Exception as e: return {"translated": str(e)}

async def process_analysis_job(job_id: str, files_map: Dict, configs: Dict):
    try:
        JOBS[job_id]["status"] = "Processing"
        JOBS[job_id]["progress"] = 10
        final_results = []
        total_files = len(configs)
        processed_count = 0

        for filename, config in configs.items():
            if filename not in files_map: continue
            content = files_map[filename]
            eq = config.get("equipment", "General")
            data_type = config.get("data_type", "General")
            goal = config.get("goal", "")
            mode = config.get("mode", "Auto")
            
            user_center = None
            if config.get("center_coords"):
                try: user_center = [int(v) for v in config["center_coords"].split(",")]
                except: pass

            fname_lower = filename.lower()
            JOBS[job_id]["step"] = f"Analyzing {filename}..."
            
            if eq == "Document" or fname_lower.endswith(('.pdf', '.ppt', '.pptx')):
                try:
                    if fname_lower.endswith('.pdf'):
                        images = convert_from_bytes(content, dpi=150, fmt='jpeg')[:10]
                        full_txt = ""; pages = []
                        for idx, img in enumerate(images):
                            buf = io.BytesIO(); img.save(buf, format="JPEG")
                            b64 = base64.b64encode(buf.getvalue()).decode()
                            desc = await analyze_vision_ollama(buf.getvalue(), "한글 요약.")
                            full_txt += f"\nPage {idx+1}: {desc}"
                            pages.append({"page_num": idx+1, "image_b64": b64, "desc": desc})
                        final_results.append({"type": "pdf", "filename": filename, "equipment": "Lit", "raw_context": full_txt, "pages": pages})
                    elif fname_lower.endswith(('.ppt', '.pptx')):
                        prs = Presentation(io.BytesIO(content)); slides = []; full_txt = ""
                        for i, slide in enumerate(prs.slides):
                            txt = ""; imgs = []
                            for s in slide.shapes:
                                if hasattr(s, "text"): txt += s.text + "\n"
                                if s.shape_type == MSO_SHAPE_TYPE.PICTURE:
                                    try:
                                        ib64 = base64.b64encode(s.image.blob).decode('utf-8')
                                        desc = await analyze_vision_ollama(s.image.blob, "한글 설명.")
                                        imgs.append({"b64": ib64, "desc": desc})
                                    except: pass
                            slides.append({"slide_num": i+1, "text": txt, "images": imgs})
                        final_results.append({"type": "ppt", "filename": filename, "equipment": "Lit", "raw_context": full_txt, "slides": slides})
                except Exception as e: final_results.append({"type": "error", "filename": filename, "msg": str(e)})

            elif fname_lower.endswith(('.xlsx', '.xls', '.csv', '.txt')):
                try:
                    dfs = ScienceProcessorNV13.read_universal_dataframe(content, filename)
                    for i, df in enumerate(dfs):
                        v_blocks = ScienceProcessorNV13.detect_structured_blocks(df)
                        for v_block in v_blocks:
                            h_blocks = ScienceProcessorNV13.split_horizontal_blocks(v_block)
                            for h_block in h_blocks:
                                series_list = ScienceProcessorNV13.extract_series_from_df(h_block)
                                for s in series_list:
                                    res = ScienceProcessorNV13.process_spectrum(s['x'], s['y'], mode, goal)
                                    step = max(1, len(s['x'])//100)
                                    chart = [{"x": float(res["x"][k]), "y_proc": float(res["y_proc"][k]), "y_raw": float(res["y_raw"][k]), "y_base": float(res["y_base"][k])} for k in range(0, len(s['x']), step)]
                                    ctx = f"Data: {s['name']}\nStats: {res['stats']}"
                                    interp = await interpret_spectrum_data(ctx, eq)
                                    final_results.append({
                                        "type": "spectrum", "filename": f"{filename} ({s['name']})", "equipment": eq,
                                        "raw_context": f"{ctx}\nInterp: {interp}", 
                                        "chart_data": chart, "log": res["log"], "interpretation": interp
                                    })
                except Exception as ex: final_results.append({"type": "error", "filename": filename, "msg": str(ex)})

            elif fname_lower.endswith(('.png', '.jpg', '.jpeg', '.tif', '.bmp')):
                try:
                    JOBS[job_id]["step"] = f"Vision Analysis: {filename}..."
                    vis_res = await asyncio.to_thread(ScienceProcessorNV13.process_image_nv13, content, eq, data_type, mode, goal, user_center)
                    if vis_res:
                        scale_info = "Unknown"
                        if vis_res.get("footer_b64") and data_type != "Crystal Structure (FFT)":
                            JOBS[job_id]["step"] = f"Scale Reading..."
                            scale_text = await analyze_vision_ollama(base64.b64decode(vis_res["footer_b64"]), "Read scale bar text only.")
                            scale_info = scale_text
                            vis_res["log"].append(f"Scale: {scale_text}")

                        stats_str = json.dumps(vis_res.get('stats',{}), indent=2)
                        desc_prompt = f"""
                        Analyze this {eq} image ({data_type}).
                        Quantitative Analysis: {stats_str}
                        Scale Bar: {scale_info}
                        Goal: {goal}
                        Summary in Korean.
                        """
                        desc = await analyze_vision_ollama(base64.b64decode(vis_res["proc_b64"]), desc_prompt)
                        
                        final_results.append({
                            "type": "image", "filename": filename, "equipment": eq, "summary": desc,
                            "raw_context": f"Img {eq}: {desc}", "raw_b64": vis_res["raw_b64"], "proc_b64": vis_res["proc_b64"], "footer_b64": vis_res.get("footer_b64"),
                            "stats": vis_res.get("stats",{}), "log": vis_res.get("log", [])
                        })
                except Exception as ex: final_results.append({"type": "error", "filename": filename, "msg": str(ex)})
            
            processed_count += 1
            JOBS[job_id]["progress"] = 10 + int((processed_count / total_files) * 80)

        JOBS[job_id]["step"] = "Writing Report..."
        final_results = sanitize_json(final_results)
        
        data_ctx = ""
        for r in final_results:
            if r.get("type") != "error":
                raw = r.get("raw_context") or ""
                data_ctx += f"\n=== {r['filename']} ===\n{raw[:1000]}\n"

        final_report = "Fail"
        if data_ctx:
            try:
                system_prompt = "You are a research assistant. Write a report in Korean. Summarize data and physics."
                res = await asyncio.to_thread(ollama_client.chat, model=TEXT_MODEL, messages=[{'role':'system','content':system_prompt}, {'role':'user','content':f"Data:\n{data_ctx}"}])
                final_report = res['message']['content']
            except Exception as e: final_report = str(e)

        JOBS[job_id]["results"] = {"results": final_results, "final_report": final_report}
        JOBS[job_id]["status"] = "Completed"
        JOBS[job_id]["progress"] = 100
    except Exception as e:
        JOBS[job_id]["status"] = "Failed"; JOBS[job_id]["error"] = str(e)

@app.post("/api/analyze")
async def start_analysis(background_tasks: BackgroundTasks, files: List[UploadFile] = File(...), file_configs: str = Form(...)):
    job_id = str(uuid.uuid4()); configs = json.loads(file_configs)
    files_map = {f.filename: await f.read() for f in files}
    JOBS[job_id] = {"status": "Pending", "progress": 0, "step": "Init", "results": None}
    background_tasks.add_task(process_analysis_job, job_id, files_map, configs)
    return {"job_id": job_id}

@app.get("/api/status/{job_id}")
async def get_status(job_id: str): return JOBS.get(job_id, {"status": "Not Found"})

@app.get("/", response_class=HTMLResponse)
async def serve_index():
    if os.path.exists("index.html"): return FileResponse("index.html")
    return "<h1>NV13 Transposed Backend</h1>"

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)





엔브이12

import os
import io
# [0] OMP & Torch
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"
import torch

import asyncio
import base64
import json
import re
import math
import uuid
import time
from typing import List, Dict, Any, Optional
from pydantic import BaseModel
from contextlib import asynccontextmanager

# [1] Security
os.environ['NO_PROXY'] = '127.0.0.1,localhost,0.0.0.0,::1'
if 'HTTP_PROXY' in os.environ: del os.environ['HTTP_PROXY']
if 'HTTPS_PROXY' in os.environ: del os.environ['HTTPS_PROXY']

from fastapi import FastAPI, UploadFile, File, Form, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import HTMLResponse, FileResponse
import pandas as pd
import numpy as np

# SciPy & CV & Sklearn
from sklearn.decomposition import NMF
# Note: No MinMaxScaler per patch! We do global scaling manually.
from scipy.signal import find_peaks, savgol_filter
from scipy.optimize import curve_fit
from scipy.ndimage import center_of_mass
from skimage.feature import peak_local_max
import cv2
import matplotlib
import matplotlib.cm as cm 

# Models
from segment_anything import sam_model_registry, SamAutomaticMaskGenerator
from ollama import Client 
from pdf2image import convert_from_bytes
from pptx import Presentation
from pptx.enum.shapes import MSO_SHAPE_TYPE

# [2] Config
OLLAMA_HOST = "http://127.0.0.1:11434"
ollama_client = Client(host=OLLAMA_HOST)
VISION_MODEL = "qwen3-vl:30b-a3b-instruct" 
TEXT_MODEL = "llama3:70b"

SAM_CHECKPOINT = "sam_vit_b_01ec64.pth"
SAM_MODEL_TYPE = "vit_b"
device = "cuda" if torch.cuda.is_available() else "cpu"

mask_generator = None 
JOBS = {} 

@asynccontextmanager
async def lifespan(app: FastAPI):
    global mask_generator
    if os.path.exists(SAM_CHECKPOINT):
        print(f"Loading SAM ({SAM_MODEL_TYPE}) on {device}...")
        try:
            sam = sam_model_registry[SAM_MODEL_TYPE](checkpoint=SAM_CHECKPOINT)
            sam.to(device=device)
            mask_generator = SamAutomaticMaskGenerator(
                model=sam, points_per_side=32, pred_iou_thresh=0.86,
                stability_score_thresh=0.90, min_mask_region_area=50
            )
            print("SAM Loaded.")
        except: print("SAM Load Failed.")
    yield
    JOBS.clear()

def sanitize_json(obj):
    if isinstance(obj, float):
        if np.isnan(obj) or np.isinf(obj): return 0.0
        return obj
    elif isinstance(obj, dict): return {k: sanitize_json(v) for k, v in obj.items()}
    elif isinstance(obj, list): return [sanitize_json(v) for v in obj]
    elif isinstance(obj, (np.int64, np.int32)): return int(obj)
    elif isinstance(obj, (np.float64, np.float32)):
        if np.isnan(obj) or np.isinf(obj): return 0.0
        return float(obj)
    return obj

# ==========================================
# [3] NV12 DC-Filtered NMF Engine
# ==========================================
class ScienceProcessorNV12:
    
    # --- Universal Loader ---
    @staticmethod
    def read_universal_dataframe(content: bytes, filename: str) -> List[pd.DataFrame]:
        dfs = []
        fname = filename.lower()
        if fname.endswith(('.xlsx', '.xls')):
            try:
                xls = pd.ExcelFile(io.BytesIO(content))
                for s in xls.sheet_names:
                    df = pd.read_excel(xls, sheet_name=s, header=None)
                    num = df.apply(pd.to_numeric, errors='coerce').notna().sum().sum()
                    if num < 5: continue
                    if not df.empty: dfs.append(df)
                if dfs: return dfs
            except: pass
        text_content = ""
        for enc in ['utf-8', 'cp949', 'latin1']:
            try: text_content = content.decode(enc); break
            except: continue
        if text_content:
            lines = text_content.splitlines()
            data_start = -1; delimiter = None
            separators = [',', '\t', ';', '\s+']
            for i, line in enumerate(lines[:100]):
                for sep in separators:
                    parts = line.split() if sep=='\s+' else line.split(sep)
                    nums = sum(1 for p in parts if re.match(r'^-?\d+(\.\d+)?(?:[eE][+-]?\d+)?$', p.strip()))
                    if nums >= 2: data_start = i; delimiter = sep; break
                if data_start != -1: break
            if data_start != -1:
                try:
                    data_str = "\n".join(lines[data_start:])
                    sep_arg = '\s+' if delimiter=='\s+' else delimiter
                    df = pd.read_csv(io.StringIO(data_str), sep=sep_arg, header=None, engine='python')
                    dfs.append(df)
                except: pass
        return dfs

    # --- Block Parser ---
    @staticmethod
    def detect_structured_blocks(df: pd.DataFrame) -> List[pd.DataFrame]:
        blocks = []
        if df.empty: return []
        df_num = df.apply(pd.to_numeric, errors='coerce')
        if df.shape[1] > 0: is_data_row = df_num.iloc[:, 0].notna()
        else: is_data_row = df_num.notna().any(axis=1)
        groups = (is_data_row != is_data_row.shift()).cumsum()
        for _, group in df.groupby(groups):
            if is_data_row.loc[group.index[0]]:
                block = df_num.loc[group.index].dropna(how='all', axis=1)
                if block.shape[0] >= 10: blocks.append(block.reset_index(drop=True))
        return blocks

    @staticmethod
    def split_horizontal_blocks(df: pd.DataFrame) -> List[pd.DataFrame]:
        sub_blocks = []
        if df.empty: return []
        col_counts = df.apply(pd.to_numeric, errors='coerce').notna().sum()
        valid = np.where(col_counts >= 2)[0]
        if len(valid) == 0: return []
        splits = np.where(np.diff(valid) > 1)[0] + 1
        groups = np.split(valid, splits)
        for g in groups:
            if len(g) > 0:
                sub = df.iloc[:, g].reset_index(drop=True)
                if sub.shape[1] >= 1 and sub.shape[0] >= 2: sub_blocks.append(sub)
        return sub_blocks

    @staticmethod
    def extract_series_from_df(df: pd.DataFrame) -> List[Dict]:
        series_list = []
        try:
            vals = df.values; rows, cols = vals.shape
            if rows < 5: return []
            if cols == 1:
                y = vals[:, 0]; mask = ~np.isnan(y)
                if np.sum(mask) > 5: series_list.append({"x": np.arange(len(y))[mask], "y": y[mask], "name": "Single"})
                return series_list
            def is_axis(arr):
                arr = arr[~np.isnan(arr)]
                if len(arr) < 5: return False
                diff = np.diff(arr)
                if np.all(diff >= 0) or np.all(diff <= 0): return True
                return (np.sum(diff>0)/len(diff)>0.9) or (np.sum(diff<0)/len(diff)>0.9)
            current_x = vals[:, 0]
            if not is_axis(current_x): current_x = np.arange(rows)
            for i in range(1, cols):
                col_data = vals[:, i]; mask_valid = ~np.isnan(col_data)
                if np.sum(mask_valid) < 5: continue
                if is_axis(col_data): current_x = col_data; continue 
                mask = ~np.isnan(current_x) & ~np.isnan(col_data)
                if np.sum(mask) > 5:
                    series_list.append({"x": current_x[mask], "y": col_data[mask], "name": f"Col-{i}"})
        except: pass
        return series_list

    # --- Spectrum Logic ---
    @staticmethod
    def simple_baseline(y):
        try:
            w = max(5, len(y)//10)
            s = pd.Series(y).rolling(window=w, center=True).min()
            b = s.bfill().ffill().values 
            if len(b) > 51: b = savgol_filter(b, 51, 3)
            return b, w
        except: return np.zeros_like(y), 0

    @staticmethod
    def process_spectrum(x, y, mode, goal):
        log = []; y_raw = y.copy(); base = np.zeros_like(y); y_proc = y.copy()
        peaks = []; fits = []; stats_txt = "Error"
        try:
            if mode == "None":
                peaks, _ = find_peaks(y, height=np.max(y)*0.05)
                log.append("Mode: Raw")
                stats_txt = f"Raw Peaks: {len(peaks)}"
            else:
                win = 15; do_fit = False
                if mode == "AI-Adaptive":
                    log.append("Mode: AI-Adaptive")
                    if "noise" in goal.lower(): win = 31
                    if "fit" in goal.lower(): do_fit = True
                else: log.append("Mode: Auto")
                base, bw = ScienceProcessorNV12.simple_baseline(y)
                y_proc = np.maximum(y - base, 0)
                log.append(f"Base: Rolling Min (w={bw})")
                if len(y_proc) > win:
                    y_proc = savgol_filter(y_proc, win, 3)
                    log.append(f"Smooth: SavGol (w={win})")
                peaks, _ = find_peaks(y_proc, height=np.max(y_proc)*0.05, distance=10)
                log.append(f"Peaks: {len(peaks)}")
                stats_txt = f"Range: {np.min(x):.1f}~{np.max(x):.1f}, Peaks: {len(peaks)}"
            return {"x": x, "y_raw": y_raw, "y_proc": y_proc, "y_base": base, "peaks": [{"x": float(x[p]), "y": float(y_proc[p])} for p in peaks], "log": log, "fits": fits, "stats": stats_txt}
        except Exception as e: return {"x": x, "y_raw": y_raw, "y_proc": y_proc, "y_base": base, "peaks": [], "log": [str(e)], "fits": [], "stats": "Error"}

    # --- Footer ---
    @staticmethod
    def separate_footer(img_bgr):
        h, w = img_bgr.shape[:2]
        roi_h = int(h * 0.82)
        roi = img_bgr[roi_h:, :]
        gray = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)
        sobel_y = np.abs(cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=3))
        row_sums = np.sum(sobel_y, axis=1)
        split_idx = np.argmax(row_sums)
        if row_sums[split_idx] > w * 30:
            real_split = roi_h + split_idx
            return img_bgr[:real_split, :], img_bgr[real_split:, :]
        return img_bgr, None

    # --- [E] NV12 FFT-NMF Crystal Engine (DC Removal Fix) ---
    @staticmethod
    def analyze_crystal_fft_nmf(img_bgr):
        log = []
        gray = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2GRAY)
        h, w = gray.shape
        
        # 1. Config
        patch_size = 64
        step = 32
        n_components = 10 
        
        log.append(f"FFT-NMF: Patch={patch_size}, Step={step}, K={n_components}")
        
        # 2. Collect FFT Patterns
        features = []
        
        y_steps = range(0, h - patch_size + 1, step)
        x_steps = range(0, w - patch_size + 1, step)
        grid_h = len(y_steps)
        grid_w = len(x_steps)
        
        # [NV12 Fix] Prepare Circular Mask for DC Removal
        # Center of the patch (64x64) -> (32, 32)
        cy, cx = patch_size // 2, patch_size // 2
        y_indices, x_indices = np.ogrid[:patch_size, :patch_size]
        # Radius 4 ~ 5 px to cover DC and immediate halo
        dc_mask = (x_indices - cx)**2 + (y_indices - cy)**2 <= 5**2

        for y in y_steps:
            for x in x_steps:
                roi = gray[y:y+patch_size, x:x+patch_size]
                
                # FFT
                f = np.fft.fft2(roi)
                fshift = np.fft.fftshift(f)
                mag = 20 * np.log(np.abs(fshift) + 1e-9)
                
                # [NV12 Fix] Apply Circular DC Removal
                mag[dc_mask] = 0
                
                features.append(mag.flatten())
        
        if not features: return {"type":"error", "msg":"Img too small"}
        
        try:
            # 3. NMF Decomposition with Global Scaling
            X = np.array(features, dtype=np.float32)
            
            # [Fix] Global Scaling (Relative Intensity Preservation)
            global_min = X.min()
            X_shifted = X - global_min 
            if X_shifted.min() < 0: X_shifted -= X_shifted.min()
            
            global_max = X_shifted.max()
            if global_max > 0: X_norm = X_shifted / global_max
            else: X_norm = X_shifted

            nmf = NMF(n_components=n_components, init='nndsvda', random_state=42, max_iter=2000)
            W = nmf.fit_transform(X_norm) 
            H = nmf.components_ 
            
            # 4. Reconstruct Maps
            if W.shape[0] != grid_h * grid_w:
                raise ValueError(f"Shape Mismatch: W={W.shape[0]} vs Grid={grid_h*grid_w}")

            weight_map_lowres = W.reshape(grid_h, grid_w, n_components)
            weight_map_highres = cv2.resize(weight_map_lowres, (w, h), interpolation=cv2.INTER_CUBIC)
            dominant_labels = np.argmax(weight_map_highres, axis=2) 
            
            # 5. Visualization (Overlay)
            overlay = img_bgr.copy()
            mask_layer = np.zeros_like(img_bgr)
            
            cmap = matplotlib.colormaps['tab10']
            colors = []
            for i in range(n_components):
                rgba = cmap(i)
                bgr = (int(rgba[2]*255), int(rgba[1]*255), int(rgba[0]*255))
                colors.append(bgr)
            
            stats = {f"Pattern {i}": 0 for i in range(n_components)}
            
            for i in range(n_components):
                mask = (dominant_labels == i)
                stats[f"Pattern {i}"] = int(np.sum(mask))
                mask_layer[mask] = colors[i]
                
            alpha = 0.4
            mask_indices = np.any(mask_layer > 0, axis=-1)
            overlay[mask_indices] = (overlay[mask_indices]*(1-alpha) + mask_layer[mask_indices]*alpha).astype(np.uint8)
            
            # 6. Basis Patterns Visualization (Strip)
            basis_imgs = []
            for k in range(n_components):
                pat = H[k].reshape(patch_size, patch_size)
                # Normalize for display
                pat_norm = cv2.normalize(pat, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)
                pat_col = cv2.cvtColor(pat_norm, cv2.COLOR_GRAY2BGR)
                
                # Border
                cv2.rectangle(pat_col, (0,0), (patch_size-1, patch_size-1), colors[k], 4)
                # Text
                cv2.putText(pat_col, f"P{k}", (2, 15), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255,255,255), 1)
                
                basis_imgs.append(pat_col)
                
            basis_strip = np.hstack(basis_imgs)
            basis_strip_large = cv2.resize(basis_strip, None, fx=2, fy=2, interpolation=cv2.INTER_NEAREST)
            
            def to_b64(im): return base64.b64encode(cv2.imencode('.jpg', im)[1]).decode('utf-8')
            
            return {
                "type": "image", 
                "raw_b64": to_b64(img_bgr), 
                "proc_b64": to_b64(overlay),
                "footer_b64": to_b64(basis_strip_large), # Basis Patterns
                "stats": stats, 
                "log": log
            }

        except Exception as e:
            print(f"NMF Error: {e}")
            def to_b64(im): return base64.b64encode(cv2.imencode('.jpg', im)[1]).decode('utf-8')
            return {
                "type": "image", "raw_b64": to_b64(img_bgr), "proc_b64": to_b64(img_bgr),
                "stats": {"error": str(e)}, "log": log + [f"NMF Error: {str(e)}"]
            }

    # --- Router ---
    @staticmethod
    def process_image_nv12(img_bytes, equipment, data_type, mode, goal, user_center=None):
        nparr = np.frombuffer(img_bytes, np.uint8)
        img_raw = cv2.imdecode(nparr, cv2.IMREAD_COLOR)
        if img_raw is None: return {}
        
        body, footer = ScienceProcessorNV12.separate_footer(img_raw)
        
        footer_b64 = None
        if footer is not None:
            footer_b64 = base64.b64encode(cv2.imencode('.jpg', footer)[1]).decode('utf-8')

        if data_type == "Crystal Structure (FFT)":
            return {**ScienceProcessorNV12.analyze_crystal_fft_nmf(body), "footer_b64": footer_b64}
            
        if data_type == "2D Diffraction":
            return {**ScienceProcessorNV12.analyze_diffraction_nv7(body, user_center), "footer_b64": footer_b64}
        
        if mode == "None":
             def to_b64(im): return base64.b64encode(cv2.imencode('.jpg', im)[1]).decode('utf-8')
             return {"type":"image", "raw_b64":to_b64(body), "proc_b64":to_b64(body), "stats":{}, "log":["Mode: Raw"], "footer_b64": footer_b64}

        img_proc = body.copy(); log = []
        if mode == "Auto" or mode == "AI-Adaptive":
            img_proc = cv2.bilateralFilter(img_proc, 9, 75, 75)
            log.append("Preproc: Denoise")

        img_rgb = cv2.cvtColor(img_proc, cv2.COLOR_BGR2RGB)
        
        if mask_generator:
            try:
                masks = mask_generator.generate(img_rgb)
                log.append(f"SAM: {len(masks)} Segments")
                overlay, stats, sam_log = ScienceProcessorNV12.analyze_sam_results_nv7(img_rgb, masks, data_type)
                log.extend(sam_log)
                overlay = cv2.cvtColor(overlay, cv2.COLOR_RGB2BGR)
            except Exception as e:
                log.append(f"SAM Error: {str(e)}")
                overlay = body; stats = {}
        else:
            log.append("SAM Not Loaded")
            overlay = body; stats = {}

        def to_b64(im): return base64.b64encode(cv2.imencode('.jpg', im)[1]).decode('utf-8')
        return {"type": "image", "raw_b64": to_b64(body), "proc_b64": to_b64(overlay), "stats": stats, "log": log, "footer_b64": footer_b64}

# [4] Helper
async def analyze_vision_ollama(image_bytes: bytes, prompt: str) -> str:
    try:
        b64 = base64.b64encode(image_bytes).decode('utf-8')
        res = await asyncio.to_thread(ollama_client.chat, model=VISION_MODEL, messages=[{'role':'user','content':prompt,'images':[b64]}])
        return res['message']['content']
    except: return "Vision Error"

async def interpret_spectrum_data(raw_context: str, equipment: str) -> str:
    try:
        prompt = f"Analyze spectrum ({equipment}). Brief interpretation in Korean. Data: {raw_context}"
        res = await asyncio.to_thread(ollama_client.chat, model=TEXT_MODEL, messages=[{'role':'user','content':prompt}])
        return res['message']['content']
    except: return ""

# [5] App
app = FastAPI(title="Analyst NV12 DC-Filtered", lifespan=lifespan)
app.add_middleware(CORSMiddleware, allow_origins=["*"], allow_methods=["*"], allow_headers=["*"])

class TranslateReq(BaseModel): text: str

@app.post("/api/translate")
async def translate_text(req: TranslateReq):
    try:
        res = await asyncio.to_thread(ollama_client.chat, model=TEXT_MODEL, messages=[{'role':'system','content':'Translate to English.'}, {'role':'user','content':req.text}])
        return {"translated": res['message']['content']}
    except Exception as e: return {"translated": str(e)}

async def process_analysis_job(job_id: str, files_map: Dict, configs: Dict):
    try:
        JOBS[job_id]["status"] = "Processing"
        JOBS[job_id]["progress"] = 10
        final_results = []
        total_files = len(configs)
        processed_count = 0

        for filename, config in configs.items():
            if filename not in files_map: continue
            content = files_map[filename]
            eq = config.get("equipment", "General")
            data_type = config.get("data_type", "General")
            goal = config.get("goal", "")
            mode = config.get("mode", "Auto")
            
            user_center = None
            if config.get("center_coords"):
                try: user_center = [int(v) for v in config["center_coords"].split(",")]
                except: pass

            fname_lower = filename.lower()
            JOBS[job_id]["step"] = f"Analyzing {filename}..."
            
            if eq == "Document" or fname_lower.endswith(('.pdf', '.ppt', '.pptx')):
                # (Existing logic omitted for brevity)
                pass 

            elif fname_lower.endswith(('.xlsx', '.xls', '.csv', '.txt')):
                try:
                    dfs = ScienceProcessorNV12.read_universal_dataframe(content, filename)
                    for i, df in enumerate(dfs):
                        v_blocks = ScienceProcessorNV12.detect_structured_blocks(df)
                        for v_block in v_blocks:
                            h_blocks = ScienceProcessorNV12.split_horizontal_blocks(v_block)
                            for h_block in h_blocks:
                                series_list = ScienceProcessorNV12.extract_series_from_df(h_block)
                                for s in series_list:
                                    res = ScienceProcessorNV12.process_spectrum(s['x'], s['y'], mode, goal)
                                    step = max(1, len(s['x'])//100)
                                    chart = [{"x": float(res["x"][k]), "y_proc": float(res["y_proc"][k]), "y_raw": float(res["y_raw"][k]), "y_base": float(res["y_base"][k])} for k in range(0, len(s['x']), step)]
                                    ctx = f"Data: {s['name']}\nStats: {res['stats']}"
                                    interp = await interpret_spectrum_data(ctx, eq)
                                    final_results.append({
                                        "type": "spectrum", "filename": f"{filename} ({s['name']})", "equipment": eq,
                                        "raw_context": f"{ctx}\nInterp: {interp}", 
                                        "chart_data": chart, "log": res["log"], "interpretation": interp
                                    })
                except Exception as ex: final_results.append({"type": "error", "filename": filename, "msg": str(ex)})

            elif fname_lower.endswith(('.png', '.jpg', '.jpeg', '.tif', '.bmp')):
                try:
                    JOBS[job_id]["step"] = f"Vision Analysis: {filename}..."
                    vis_res = await asyncio.to_thread(ScienceProcessorNV12.process_image_nv12, content, eq, data_type, mode, goal, user_center)
                    if vis_res:
                        scale_info = "Unknown"
                        if vis_res.get("footer_b64") and data_type != "Crystal Structure (FFT)":
                            JOBS[job_id]["step"] = f"Scale Reading..."
                            scale_text = await analyze_vision_ollama(base64.b64decode(vis_res["footer_b64"]), "Read scale bar text only.")
                            scale_info = scale_text
                            vis_res["log"].append(f"Scale: {scale_text}")

                        # Description
                        stats_str = json.dumps(vis_res.get('stats',{}), indent=2)
                        desc_prompt = f"""
                        Analyze this {eq} image ({data_type}).
                        Quantitative Analysis (NMF Patterns): {stats_str}
                        Scale Bar: {scale_info}
                        Goal: {goal}
                        Summary in Korean.
                        """
                        desc = await analyze_vision_ollama(base64.b64decode(vis_res["proc_b64"]), desc_prompt)
                        
                        final_results.append({
                            "type": "image", "filename": filename, "equipment": eq, "summary": desc,
                            "raw_context": f"Img {eq}: {desc}", "raw_b64": vis_res["raw_b64"], "proc_b64": vis_res["proc_b64"], "footer_b64": vis_res.get("footer_b64"),
                            "stats": vis_res.get("stats",{}), "log": vis_res.get("log", [])
                        })
                except Exception as ex: final_results.append({"type": "error", "filename": filename, "msg": str(ex)})
            
            processed_count += 1
            JOBS[job_id]["progress"] = 10 + int((processed_count / total_files) * 80)

        JOBS[job_id]["step"] = "Writing Report..."
        final_results = sanitize_json(final_results)
        
        data_ctx = ""
        for r in final_results:
            if r.get("type") != "error":
                raw = r.get("raw_context") or ""
                data_ctx += f"\n=== {r['filename']} ===\n{raw[:1000]}\n"

        final_report = "Fail"
        if data_ctx:
            try:
                system_prompt = "You are a research assistant. Write a report in Korean. Summarize data and physics."
                res = await asyncio.to_thread(ollama_client.chat, model=TEXT_MODEL, messages=[{'role':'system','content':system_prompt}, {'role':'user','content':f"Data:\n{data_ctx}"}])
                final_report = res['message']['content']
            except Exception as e: final_report = str(e)

        JOBS[job_id]["results"] = {"results": final_results, "final_report": final_report}
        JOBS[job_id]["status"] = "Completed"
        JOBS[job_id]["progress"] = 100
    except Exception as e:
        JOBS[job_id]["status"] = "Failed"; JOBS[job_id]["error"] = str(e)

@app.post("/api/analyze")
async def start_analysis(background_tasks: BackgroundTasks, files: List[UploadFile] = File(...), file_configs: str = Form(...)):
    job_id = str(uuid.uuid4()); configs = json.loads(file_configs)
    files_map = {f.filename: await f.read() for f in files}
    JOBS[job_id] = {"status": "Pending", "progress": 0, "step": "Init", "results": None}
    background_tasks.add_task(process_analysis_job, job_id, files_map, configs)
    return {"job_id": job_id}

@app.get("/api/status/{job_id}")
async def get_status(job_id: str): return JOBS.get(job_id, {"status": "Not Found"})

@app.get("/", response_class=HTMLResponse)
async def serve_index():
    if os.path.exists("index.html"): return FileResponse("index.html")
    return "<h1>NV12 DC-Filtered Backend</h1>"

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)



엔브이10

import os
import io
# [0] OMP Error Fix
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"
import torch

import asyncio
import base64
import json
import re
import math
import uuid
import time
from typing import List, Dict, Any, Optional
from pydantic import BaseModel
from contextlib import asynccontextmanager

# [1] Security
os.environ['NO_PROXY'] = '127.0.0.1,localhost,0.0.0.0,::1'
if 'HTTP_PROXY' in os.environ: del os.environ['HTTP_PROXY']
if 'HTTPS_PROXY' in os.environ: del os.environ['HTTPS_PROXY']

from fastapi import FastAPI, UploadFile, File, Form, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import HTMLResponse, FileResponse
import pandas as pd
import numpy as np

# SciPy & CV & Sklearn
from sklearn.decomposition import NMF
from sklearn.preprocessing import MinMaxScaler
from scipy.signal import find_peaks, savgol_filter
from scipy.optimize import curve_fit
from scipy.ndimage import center_of_mass
from skimage.feature import peak_local_max
import cv2
import matplotlib
import matplotlib.cm as cm 

# Models
from segment_anything import sam_model_registry, SamAutomaticMaskGenerator
from ollama import Client 
from pdf2image import convert_from_bytes
from pptx import Presentation
from pptx.enum.shapes import MSO_SHAPE_TYPE

# [2] Config
OLLAMA_HOST = "http://127.0.0.1:11434"
ollama_client = Client(host=OLLAMA_HOST)
VISION_MODEL = "qwen3-vl:30b-a3b-instruct" 
TEXT_MODEL = "llama3:70b"

SAM_CHECKPOINT = "sam_vit_b_01ec64.pth"
SAM_MODEL_TYPE = "vit_b"
device = "cuda" if torch.cuda.is_available() else "cpu"

mask_generator = None 
JOBS = {} 

@asynccontextmanager
async def lifespan(app: FastAPI):
    global mask_generator
    if os.path.exists(SAM_CHECKPOINT):
        print(f"Loading SAM ({SAM_MODEL_TYPE}) on {device}...")
        try:
            sam = sam_model_registry[SAM_MODEL_TYPE](checkpoint=SAM_CHECKPOINT)
            sam.to(device=device)
            mask_generator = SamAutomaticMaskGenerator(
                model=sam, points_per_side=32, pred_iou_thresh=0.86,
                stability_score_thresh=0.90, min_mask_region_area=50
            )
            print("SAM Loaded.")
        except: print("SAM Load Failed.")
    yield
    JOBS.clear()

def sanitize_json(obj):
    if isinstance(obj, float):
        if np.isnan(obj) or np.isinf(obj): return 0.0
        return obj
    elif isinstance(obj, dict): return {k: sanitize_json(v) for k, v in obj.items()}
    elif isinstance(obj, list): return [sanitize_json(v) for v in obj]
    elif isinstance(obj, (np.int64, np.int32)): return int(obj)
    elif isinstance(obj, (np.float64, np.float32)):
        if np.isnan(obj) or np.isinf(obj): return 0.0
        return float(obj)
    return obj

# ==========================================
# [3] NV10 FFT-NMF (K=10) Engine
# ==========================================
class ScienceProcessorNV10:
    
    # --- Universal Loader ---
    @staticmethod
    def read_universal_dataframe(content: bytes, filename: str) -> List[pd.DataFrame]:
        dfs = []
        fname = filename.lower()
        if fname.endswith(('.xlsx', '.xls')):
            try:
                xls = pd.ExcelFile(io.BytesIO(content))
                for s in xls.sheet_names:
                    df = pd.read_excel(xls, sheet_name=s, header=None)
                    num = df.apply(pd.to_numeric, errors='coerce').notna().sum().sum()
                    if num < 5: continue
                    if not df.empty: dfs.append(df)
                if dfs: return dfs
            except: pass
        text_content = ""
        for enc in ['utf-8', 'cp949', 'latin1']:
            try: text_content = content.decode(enc); break
            except: continue
        if text_content:
            lines = text_content.splitlines()
            data_start = -1; delimiter = None
            separators = [',', '\t', ';', '\s+']
            for i, line in enumerate(lines[:100]):
                for sep in separators:
                    parts = line.split() if sep=='\s+' else line.split(sep)
                    nums = sum(1 for p in parts if re.match(r'^-?\d+(\.\d+)?(?:[eE][+-]?\d+)?$', p.strip()))
                    if nums >= 2: data_start = i; delimiter = sep; break
                if data_start != -1: break
            if data_start != -1:
                try:
                    data_str = "\n".join(lines[data_start:])
                    sep_arg = '\s+' if delimiter=='\s+' else delimiter
                    df = pd.read_csv(io.StringIO(data_str), sep=sep_arg, header=None, engine='python')
                    dfs.append(df)
                except: pass
        return dfs

    # --- Block Parser ---
    @staticmethod
    def detect_structured_blocks(df: pd.DataFrame) -> List[pd.DataFrame]:
        blocks = []
        if df.empty: return []
        df_num = df.apply(pd.to_numeric, errors='coerce')
        if df.shape[1] > 0: is_data_row = df_num.iloc[:, 0].notna()
        else: is_data_row = df_num.notna().any(axis=1)
        groups = (is_data_row != is_data_row.shift()).cumsum()
        for _, group in df.groupby(groups):
            if is_data_row.loc[group.index[0]]:
                block = df_num.loc[group.index].dropna(how='all', axis=1)
                if block.shape[0] >= 10: blocks.append(block.reset_index(drop=True))
        return blocks

    @staticmethod
    def split_horizontal_blocks(df: pd.DataFrame) -> List[pd.DataFrame]:
        sub_blocks = []
        if df.empty: return []
        col_counts = df.apply(pd.to_numeric, errors='coerce').notna().sum()
        valid = np.where(col_counts >= 2)[0]
        if len(valid) == 0: return []
        splits = np.where(np.diff(valid) > 1)[0] + 1
        groups = np.split(valid, splits)
        for g in groups:
            if len(g) > 0:
                sub = df.iloc[:, g].reset_index(drop=True)
                if sub.shape[1] >= 1 and sub.shape[0] >= 2: sub_blocks.append(sub)
        return sub_blocks

    @staticmethod
    def extract_series_from_df(df: pd.DataFrame) -> List[Dict]:
        series_list = []
        try:
            vals = df.values; rows, cols = vals.shape
            if rows < 5: return []
            if cols == 1:
                y = vals[:, 0]; mask = ~np.isnan(y)
                if np.sum(mask) > 5: series_list.append({"x": np.arange(len(y))[mask], "y": y[mask], "name": "Single"})
                return series_list
            def is_axis(arr):
                arr = arr[~np.isnan(arr)]
                if len(arr) < 5: return False
                diff = np.diff(arr)
                if np.all(diff >= 0) or np.all(diff <= 0): return True
                return (np.sum(diff>0)/len(diff)>0.9) or (np.sum(diff<0)/len(diff)>0.9)
            current_x = vals[:, 0]
            if not is_axis(current_x): current_x = np.arange(rows)
            for i in range(1, cols):
                col_data = vals[:, i]; mask_valid = ~np.isnan(col_data)
                if np.sum(mask_valid) < 5: continue
                if is_axis(col_data): current_x = col_data; continue 
                mask = ~np.isnan(current_x) & ~np.isnan(col_data)
                if np.sum(mask) > 5:
                    series_list.append({"x": current_x[mask], "y": col_data[mask], "name": f"Col-{i}"})
        except: pass
        return series_list

    # --- Spectrum Logic ---
    @staticmethod
    def simple_baseline(y):
        try:
            w = max(5, len(y)//10)
            s = pd.Series(y).rolling(window=w, center=True).min()
            b = s.bfill().ffill().values 
            if len(b) > 51: b = savgol_filter(b, 51, 3)
            return b, w
        except: return np.zeros_like(y), 0

    @staticmethod
    def process_spectrum(x, y, mode, goal):
        log = []; y_raw = y.copy(); base = np.zeros_like(y); y_proc = y.copy()
        peaks = []; fits = []; stats_txt = "Error"
        try:
            if mode == "None":
                peaks, _ = find_peaks(y, height=np.max(y)*0.05)
                log.append("Mode: Raw")
                stats_txt = f"Raw Peaks: {len(peaks)}"
            else:
                win = 15; do_fit = False
                if mode == "AI-Adaptive":
                    log.append("Mode: AI-Adaptive")
                    if "noise" in goal.lower(): win = 31
                    if "fit" in goal.lower(): do_fit = True
                else: log.append("Mode: Auto")
                
                base, bw = ScienceProcessorNV10.simple_baseline(y)
                y_proc = np.maximum(y - base, 0)
                log.append(f"Base: Rolling Min (w={bw})")
                
                if len(y_proc) > win:
                    y_proc = savgol_filter(y_proc, win, 3)
                    log.append(f"Smooth: SavGol (w={win})")
                
                peaks, _ = find_peaks(y_proc, height=np.max(y_proc)*0.05, distance=10)
                log.append(f"Peaks: {len(peaks)}")
                stats_txt = f"Range: {np.min(x):.1f}~{np.max(x):.1f}, Peaks: {len(peaks)}"
            return {"x": x, "y_raw": y_raw, "y_proc": y_proc, "y_base": base, "peaks": [{"x": float(x[p]), "y": float(y_proc[p])} for p in peaks], "log": log, "fits": fits, "stats": stats_txt}
        except Exception as e: return {"x": x, "y_raw": y_raw, "y_proc": y_proc, "y_base": base, "peaks": [], "log": [str(e)], "fits": [], "stats": "Error"}

    # --- Footer ---
    @staticmethod
    def separate_footer(img_bgr):
        h, w = img_bgr.shape[:2]
        roi_h = int(h * 0.82)
        roi = img_bgr[roi_h:, :]
        gray = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)
        sobel_y = np.abs(cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=3))
        row_sums = np.sum(sobel_y, axis=1)
        split_idx = np.argmax(row_sums)
        if row_sums[split_idx] > w * 30:
            real_split = roi_h + split_idx
            return img_bgr[:real_split, :], img_bgr[real_split:, :]
        return img_bgr, None

    # --- [E] NV10 FFT-NMF Crystal Engine (K=10) ---
    @staticmethod
    def analyze_crystal_fft_nmf(img_bgr):
        log = []
        gray = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2GRAY)
        h, w = gray.shape
        
        # 1. Config: K=10 for detailed pattern separation
        patch_size = 64
        step = 32
        n_components = 10 
        
        log.append(f"FFT-NMF: Patch={patch_size}, Step={step}, K={n_components}")
        
        # 2. Collect FFT Patterns
        features = []
        
        y_steps = range(0, h - patch_size + 1, step)
        x_steps = range(0, w - patch_size + 1, step)
        grid_h = len(y_steps)
        grid_w = len(x_steps)
        
        for y in y_steps:
            for x in x_steps:
                roi = gray[y:y+patch_size, x:x+patch_size]
                f = np.fft.fft2(roi)
                fshift = np.fft.fftshift(f)
                mag = 20 * np.log(np.abs(fshift) + 1e-9)
                cy, cx = patch_size//2, patch_size//2
                mag[cy-2:cy+3, cx-2:cx+3] = 0
                features.append(mag.flatten())
        
        if not features: return {"type":"error", "msg":"Img too small"}
        
        try:
            # 3. NMF Decomposition
            X = np.array(features)
            scaler = MinMaxScaler()
            X_norm = scaler.fit_transform(X.T).T 
            
            nmf = NMF(n_components=n_components, init='nndsvda', random_state=42, max_iter=2000)
            W = nmf.fit_transform(X_norm) 
            H = nmf.components_ 
            
            # 4. Reconstruct Maps
            if W.shape[0] != grid_h * grid_w:
                raise ValueError(f"Shape Mismatch: W={W.shape[0]} vs Grid={grid_h*grid_w}")

            weight_map_lowres = W.reshape(grid_h, grid_w, n_components)
            weight_map_highres = cv2.resize(weight_map_lowres, (w, h), interpolation=cv2.INTER_CUBIC)
            dominant_labels = np.argmax(weight_map_highres, axis=2) 
            
            # 5. Visualization (Overlay)
            overlay = img_bgr.copy()
            mask_layer = np.zeros_like(img_bgr)
            
            cmap = matplotlib.colormaps['tab10'] # 10 distinct colors
            colors = []
            for i in range(n_components):
                rgba = cmap(i)
                bgr = (int(rgba[2]*255), int(rgba[1]*255), int(rgba[0]*255))
                colors.append(bgr)
            
            stats = {f"Pattern {i}": 0 for i in range(n_components)}
            
            for i in range(n_components):
                mask = (dominant_labels == i)
                stats[f"Pattern {i}"] = int(np.sum(mask))
                mask_layer[mask] = colors[i]
                
            alpha = 0.4
            mask_indices = np.any(mask_layer > 0, axis=-1)
            overlay[mask_indices] = (overlay[mask_indices]*(1-alpha) + mask_layer[mask_indices]*alpha).astype(np.uint8)
            
            # 6. Basis Patterns Visualization (H Matrix)
            basis_imgs = []
            for k in range(n_components):
                pat = H[k].reshape(patch_size, patch_size)
                pat_norm = cv2.normalize(pat, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)
                # Apply Color Map to Pattern itself for better visibility
                pat_color = cv2.applyColorMap(pat_norm, cv2.COLORMAP_VIRIDIS)
                
                # Add colored border matching the overlay color
                cv2.rectangle(pat_color, (0,0), (patch_size-1, patch_size-1), colors[k], 4)
                
                # Add Pattern Number
                cv2.putText(pat_color, f"P{k}", (2, 15), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255,255,255), 1)
                
                basis_imgs.append(pat_color)
                
            basis_strip = np.hstack(basis_imgs)
            # Resize for better visibility in UI (2x scale)
            basis_strip_large = cv2.resize(basis_strip, None, fx=2, fy=2, interpolation=cv2.INTER_NEAREST)
            
            def to_b64(im): return base64.b64encode(cv2.imencode('.jpg', im)[1]).decode('utf-8')
            
            return {
                "type": "image", 
                "raw_b64": to_b64(img_bgr), 
                "proc_b64": to_b64(overlay),
                "footer_b64": to_b64(basis_strip_large), # Basis Patterns Strip
                "stats": stats, 
                "log": log
            }

        except Exception as e:
            print(f"NMF Error: {e}")
            def to_b64(im): return base64.b64encode(cv2.imencode('.jpg', im)[1]).decode('utf-8')
            return {
                "type": "image", "raw_b64": to_b64(img_bgr), "proc_b64": to_b64(img_bgr),
                "stats": {"error": str(e)}, "log": log + [f"NMF Error: {str(e)}"]
            }

    # --- [F] Diffraction ---
    @staticmethod
    def analyze_diffraction_nv7(img_bgr, user_center=None):
        log = []
        stats = {}
        gray = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2GRAY)
        
        if user_center and len(user_center)==2 and user_center[0]>0:
            cx, cy = user_center
            log.append(f"Center: Manual [{cx}, {cy}]")
        else:
            _, th = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY+cv2.THRESH_OTSU)
            cyf, cxf = center_of_mass(th)
            if np.isnan(cyf): cyf, cxf = gray.shape[0]//2, gray.shape[1]//2
            cx, cy = int(cxf), int(cyf)
            log.append(f"Center: Auto [{cx}, {cy}]")

        g1 = cv2.GaussianBlur(gray, (3,3), 0)
        g2 = cv2.GaussianBlur(gray, (51,51), 0)
        diff = cv2.subtract(g1, g2)
        
        coords = peak_local_max(diff, min_distance=10, threshold_abs=20)
        
        enhanced = np.log1p(diff.astype(np.float32))
        enhanced = cv2.normalize(enhanced, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)
        vis = cv2.cvtColor(enhanced, cv2.COLOR_GRAY2BGR)
        
        cv2.drawMarker(vis, (cx, cy), (0,0,255), cv2.MARKER_CROSS, 20, 2)
        peak_list = []
        for p in coords:
            py, px = p
            if abs(py-cy)<5 and abs(px-cx)<5: continue
            cv2.circle(vis, (px, py), 3, (0,255,0), 1)
            peak_list.append(f"[{px},{py}]")

        log.append(f"Spots: {len(peak_list)}")
        def to_b64(im): return base64.b64encode(cv2.imencode('.jpg', im)[1]).decode('utf-8')
        return {"type": "image", "raw_b64": to_b64(img_bgr), "proc_b64": to_b64(vis), "stats": {"center": [cx, cy], "spots": len(peak_list)}, "log": log}

    # --- [G] SAM ---
    @staticmethod
    def analyze_sam_results_nv7(img_rgb, masks, data_type):
        overlay = img_rgb.copy()
        mask_overlay = np.zeros_like(img_rgb)
        h_img, w_img = img_rgb.shape[:2]
        stats = {"count": 0}
        log = []
        
        valid_masks = []
        for ann in masks:
            area = ann['area']; bbox = ann['bbox']; x, y, w, h = [int(v) for v in bbox]
            if area > h_img * w_img * 0.95 or area < 20: continue 
            if data_type == "Particle":
                if x<=1 or y<=1 or (x+w)>=w_img-1 or (y+h)>=h_img-1: continue
                mask_uint8 = (ann['segmentation'] * 255).astype(np.uint8)
                cnts, _ = cv2.findContours(mask_uint8, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
                if cnts:
                    perim = cv2.arcLength(cnts[0], True)
                    circ = 4*np.pi*area/(perim**2) if perim>0 else 0
                    if circ < 0.25: continue
            valid_masks.append(ann)

        if data_type == "Thin Film":
            sorted_masks = sorted(valid_masks, key=lambda x: x['bbox'][1])
        else:
            sorted_masks = sorted(valid_masks, key=lambda x: x['area'], reverse=True)

        thickness_vals = []; diameters = []

        for i, ann in enumerate(sorted_masks):
            m = ann['segmentation']
            bbox = ann['bbox']; x, y, w, h = [int(v) for v in bbox]
            cx, cy = int(x + w/2), int(y + h/2)
            color = (0, 255, 0) 

            if data_type == "Thin Film":
                is_bound = (i == 0 or i == len(sorted_masks)-1)
                cv2.putText(overlay, f"#{i+1}", (cx, cy), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (255, 255, 255), 3)
                if not is_bound:
                    thk = ann['area'] / w
                    thickness_vals.append(thk)
                    color = (255, 0, 0) 
                else: color = (100, 100, 100)
                if i < len(sorted_masks) - 1:
                    ys, xs = np.where(m)
                    if len(xs) > 10:
                        roughness = np.std(ys)
                        cv2.line(overlay, (x, y+h), (x+w, y+h), (0, 255, 255), 2)
                        cv2.putText(overlay, f"Int#{i+1} R:{roughness:.2f}", (x+10, y+h-5), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)
            elif data_type == "Particle":
                equivalent_diameter = 2 * np.sqrt(ann['area'] / np.pi)
                diameters.append(equivalent_diameter)

            mask_overlay[m] = color
            mask_uint8 = (m * 255).astype(np.uint8)
            cnts, _ = cv2.findContours(mask_uint8, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
            if cnts: cv2.drawContours(overlay, [cnts[0]], -1, color, 1)

        alpha = 0.35
        mask_idx = np.any(mask_overlay > 0, axis=-1)
        overlay[mask_idx] = (overlay[mask_idx]*(1-alpha) + mask_overlay[mask_idx]*alpha).astype(np.uint8)

        stats["count"] = len(sorted_masks)
        if data_type == "Thin Film":
            avg = round(np.mean(thickness_vals), 2) if thickness_vals else 0
            stats["thickness_avg"] = avg
            log.append(f"Layers: {len(sorted_masks)}, Avg Thk: {avg}")
        elif data_type == "Particle":
            avg = round(np.mean(diameters), 2) if diameters else 0
            stats["diameter_avg"] = avg
            log.append(f"Particles: {len(sorted_masks)}")
        return overlay, stats, log

    # --- Router ---
    @staticmethod
    def process_image_nv10(img_bytes, equipment, data_type, mode, goal, user_center=None):
        nparr = np.frombuffer(img_bytes, np.uint8)
        img_raw = cv2.imdecode(nparr, cv2.IMREAD_COLOR)
        if img_raw is None: return {}
        
        body, footer = ScienceProcessorNV10.separate_footer(img_raw)
        
        footer_b64 = None
        if footer is not None:
            footer_b64 = base64.b64encode(cv2.imencode('.jpg', footer)[1]).decode('utf-8')

        # [NV10] FFT-NMF (K=10)
        if data_type == "Crystal Structure (FFT)":
            return ScienceProcessorNV10.analyze_crystal_fft_nmf(body)
            
        if data_type == "2D Diffraction":
            return {**ScienceProcessorNV10.analyze_diffraction_nv7(body, user_center), "footer_b64": footer_b64}
        
        if mode == "None":
             def to_b64(im): return base64.b64encode(cv2.imencode('.jpg', im)[1]).decode('utf-8')
             return {"type":"image", "raw_b64":to_b64(body), "proc_b64":to_b64(body), "stats":{}, "log":["Mode: Raw"], "footer_b64": footer_b64}

        img_proc = body.copy(); log = []
        if mode == "Auto" or mode == "AI-Adaptive":
            img_proc = cv2.bilateralFilter(img_proc, 9, 75, 75)
            log.append("Preproc: Denoise")

        img_rgb = cv2.cvtColor(img_proc, cv2.COLOR_BGR2RGB)
        
        if mask_generator:
            try:
                masks = mask_generator.generate(img_rgb)
                log.append(f"SAM: {len(masks)} Segments")
                overlay, stats, sam_log = ScienceProcessorNV10.analyze_sam_results_nv7(img_rgb, masks, data_type)
                log.extend(sam_log)
                overlay = cv2.cvtColor(overlay, cv2.COLOR_RGB2BGR)
            except Exception as e:
                log.append(f"SAM Error: {str(e)}")
                overlay = body; stats = {}
        else:
            log.append("SAM Not Loaded")
            overlay = body; stats = {}

        def to_b64(im): return base64.b64encode(cv2.imencode('.jpg', im)[1]).decode('utf-8')
        return {"type": "image", "raw_b64": to_b64(body), "proc_b64": to_b64(overlay), "stats": stats, "log": log, "footer_b64": footer_b64}

# [4] Helper
async def analyze_vision_ollama(image_bytes: bytes, prompt: str) -> str:
    try:
        b64 = base64.b64encode(image_bytes).decode('utf-8')
        res = await asyncio.to_thread(ollama_client.chat, model=VISION_MODEL, messages=[{'role':'user','content':prompt,'images':[b64]}])
        return res['message']['content']
    except: return "Vision Error"

async def interpret_spectrum_data(raw_context: str, equipment: str) -> str:
    try:
        prompt = f"Analyze spectrum ({equipment}). Brief interpretation in Korean. Data: {raw_context}"
        res = await asyncio.to_thread(ollama_client.chat, model=TEXT_MODEL, messages=[{'role':'user','content':prompt}])
        return res['message']['content']
    except: return ""

# [5] App
app = FastAPI(title="Analyst NV10 NMF-K10", lifespan=lifespan)
app.add_middleware(CORSMiddleware, allow_origins=["*"], allow_methods=["*"], allow_headers=["*"])

class TranslateReq(BaseModel): text: str

@app.post("/api/translate")
async def translate_text(req: TranslateReq):
    try:
        res = await asyncio.to_thread(ollama_client.chat, model=TEXT_MODEL, messages=[{'role':'system','content':'Translate to English.'}, {'role':'user','content':req.text}])
        return {"translated": res['message']['content']}
    except Exception as e: return {"translated": str(e)}

async def process_analysis_job(job_id: str, files_map: Dict, configs: Dict):
    try:
        JOBS[job_id]["status"] = "Processing"
        JOBS[job_id]["progress"] = 10
        final_results = []
        total_files = len(configs)
        processed_count = 0

        for filename, config in configs.items():
            if filename not in files_map: continue
            content = files_map[filename]
            eq = config.get("equipment", "General")
            data_type = config.get("data_type", "General")
            goal = config.get("goal", "")
            mode = config.get("mode", "Auto")
            
            user_center = None
            if config.get("center_coords"):
                try: user_center = [int(v) for v in config["center_coords"].split(",")]
                except: pass

            fname_lower = filename.lower()
            JOBS[job_id]["step"] = f"Analyzing {filename}..."
            
            if eq == "Document" or fname_lower.endswith(('.pdf', '.ppt', '.pptx')):
                # (Same logic as NV9)
                try:
                    if fname_lower.endswith('.pdf'):
                        images = convert_from_bytes(content, dpi=150, fmt='jpeg')[:10]
                        full_txt = ""; pages = []
                        for idx, img in enumerate(images):
                            buf = io.BytesIO(); img.save(buf, format="JPEG")
                            b64 = base64.b64encode(buf.getvalue()).decode()
                            desc = await analyze_vision_ollama(buf.getvalue(), "한글 요약.")
                            full_txt += f"\nPage {idx+1}: {desc}"
                            pages.append({"page_num": idx+1, "image_b64": b64, "desc": desc})
                        final_results.append({"type": "pdf", "filename": filename, "equipment": "Lit", "raw_context": full_txt, "pages": pages})
                    elif fname_lower.endswith(('.ppt', '.pptx')):
                        prs = Presentation(io.BytesIO(content)); slides = []; full_txt = ""
                        for i, slide in enumerate(prs.slides):
                            txt = ""; imgs = []
                            for s in slide.shapes:
                                if hasattr(s, "text"): txt += s.text + "\n"
                                if s.shape_type == MSO_SHAPE_TYPE.PICTURE:
                                    try:
                                        ib64 = base64.b64encode(s.image.blob).decode('utf-8')
                                        desc = await analyze_vision_ollama(s.image.blob, "한글 설명.")
                                        imgs.append({"b64": ib64, "desc": desc})
                                    except: pass
                            slides.append({"slide_num": i+1, "text": txt, "images": imgs})
                        final_results.append({"type": "ppt", "filename": filename, "equipment": "Lit", "raw_context": full_txt, "slides": slides})
                except Exception as e: final_results.append({"type": "error", "filename": filename, "msg": str(e)})

            elif fname_lower.endswith(('.xlsx', '.xls', '.csv', '.txt')):
                try:
                    dfs = ScienceProcessorNV10.read_universal_dataframe(content, filename)
                    for i, df in enumerate(dfs):
                        v_blocks = ScienceProcessorNV10.detect_structured_blocks(df)
                        for v_block in v_blocks:
                            h_blocks = ScienceProcessorNV10.split_horizontal_blocks(v_block)
                            for h_block in h_blocks:
                                series_list = ScienceProcessorNV10.extract_series_from_df(h_block)
                                for s in series_list:
                                    res = ScienceProcessorNV10.process_spectrum(s['x'], s['y'], mode, goal)
                                    step = max(1, len(s['x'])//100)
                                    chart = [{"x": float(res["x"][k]), "y_proc": float(res["y_proc"][k]), "y_raw": float(res["y_raw"][k]), "y_base": float(res["y_base"][k])} for k in range(0, len(s['x']), step)]
                                    ctx = f"Data: {s['name']}\nStats: {res['stats']}"
                                    interp = await interpret_spectrum_data(ctx, eq)
                                    final_results.append({
                                        "type": "spectrum", "filename": f"{filename} ({s['name']})", "equipment": eq,
                                        "raw_context": f"{ctx}\nInterp: {interp}", 
                                        "chart_data": chart, "log": res["log"], "interpretation": interp
                                    })
                except Exception as ex: final_results.append({"type": "error", "filename": filename, "msg": str(ex)})

            elif fname_lower.endswith(('.png', '.jpg', '.jpeg', '.tif', '.bmp')):
                try:
                    JOBS[job_id]["step"] = f"Vision Analysis: {filename}..."
                    vis_res = await asyncio.to_thread(ScienceProcessorNV10.process_image_nv10, content, eq, data_type, mode, goal, user_center)
                    if vis_res:
                        scale_info = "Unknown"
                        if vis_res.get("footer_b64") and data_type != "Crystal Structure (FFT)":
                            JOBS[job_id]["step"] = f"Scale Reading..."
                            scale_text = await analyze_vision_ollama(base64.b64decode(vis_res["footer_b64"]), "Read scale bar text only.")
                            scale_info = scale_text
                            vis_res["log"].append(f"Scale: {scale_text}")

                        # Description
                        stats_str = json.dumps(vis_res.get('stats',{}), indent=2)
                        desc_prompt = f"""
                        Analyze this {eq} image ({data_type}).
                        Stats: {stats_str}
                        Scale: {scale_info}
                        Goal: {goal}
                        Summary in Korean.
                        """
                        desc = await analyze_vision_ollama(base64.b64decode(vis_res["proc_b64"]), desc_prompt)
                        
                        final_results.append({
                            "type": "image", "filename": filename, "equipment": eq, "summary": desc,
                            "raw_context": f"Img {eq}: {desc}", "raw_b64": vis_res["raw_b64"], "proc_b64": vis_res["proc_b64"], "footer_b64": vis_res.get("footer_b64"),
                            "stats": vis_res.get("stats",{}), "log": vis_res.get("log", [])
                        })
                except Exception as ex: final_results.append({"type": "error", "filename": filename, "msg": str(ex)})
            
            processed_count += 1
            JOBS[job_id]["progress"] = 10 + int((processed_count / total_files) * 80)

        JOBS[job_id]["step"] = "Writing Report..."
        final_results = sanitize_json(final_results)
        
        data_ctx = ""
        for r in final_results:
            if r.get("type") != "error":
                raw = r.get("raw_context") or ""
                data_ctx += f"\n=== {r['filename']} ===\n{raw[:1000]}\n"

        final_report = "Fail"
        if data_ctx:
            try:
                system_prompt = "You are a research assistant. Write a report in Korean. Summarize data and physics."
                res = await asyncio.to_thread(ollama_client.chat, model=TEXT_MODEL, messages=[{'role':'system','content':system_prompt}, {'role':'user','content':f"Data:\n{data_ctx}"}])
                final_report = res['message']['content']
            except Exception as e: final_report = str(e)

        JOBS[job_id]["results"] = {"results": final_results, "final_report": final_report}
        JOBS[job_id]["status"] = "Completed"
        JOBS[job_id]["progress"] = 100
    except Exception as e:
        JOBS[job_id]["status"] = "Failed"; JOBS[job_id]["error"] = str(e)

@app.post("/api/analyze")
async def start_analysis(background_tasks: BackgroundTasks, files: List[UploadFile] = File(...), file_configs: str = Form(...)):
    job_id = str(uuid.uuid4()); configs = json.loads(file_configs)
    files_map = {f.filename: await f.read() for f in files}
    JOBS[job_id] = {"status": "Pending", "progress": 0, "step": "Init", "results": None}
    background_tasks.add_task(process_analysis_job, job_id, files_map, configs)
    return {"job_id": job_id}

@app.get("/api/status/{job_id}")
async def get_status(job_id: str): return JOBS.get(job_id, {"status": "Not Found"})

@app.get("/", response_class=HTMLResponse)
async def serve_index():
    if os.path.exists("index.html"): return FileResponse("index.html")
    return "<h1>NV10 Backend Running</h1>"

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)



새로운 함수

    # --- [E] NV9 FFT-NMF Crystal Engine (Robust Fix) ---
    @staticmethod
    def analyze_crystal_fft_nmf(img_bgr):
        log = []
        gray = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2GRAY)
        h, w = gray.shape
        
        # 1. Config
        patch_size = 64
        step = 32
        n_components = 4 # Amorphous, Crystal A, Crystal B, Boundary
        
        log.append(f"FFT-NMF: Patch={patch_size}, Step={step}, K={n_components}")
        
        # 2. Collect FFT Patterns
        features = []
        
        # Grid 계산
        y_steps = range(0, h - patch_size + 1, step)
        x_steps = range(0, w - patch_size + 1, step)
        grid_h = len(y_steps)
        grid_w = len(x_steps)
        
        for y in y_steps:
            for x in x_steps:
                roi = gray[y:y+patch_size, x:x+patch_size]
                
                # FFT
                f = np.fft.fft2(roi)
                fshift = np.fft.fftshift(f)
                mag = 20 * np.log(np.abs(fshift) + 1e-9)
                
                # Center Mask (Remove DC)
                cy, cx = patch_size//2, patch_size//2
                mag[cy-2:cy+3, cx-2:cx+3] = 0
                
                features.append(mag.flatten())
        
        # 예외 처리: 이미지가 너무 작거나 패치가 없는 경우
        if not features: 
            return {"type":"error", "msg":"Img too small or No features"}
        
        try:
            # 3. NMF Decomposition
            X = np.array(features)
            scaler = MinMaxScaler()
            X_norm = scaler.fit_transform(X.T).T 
            
            # [Fix] Max Iter Increased (500 -> 2000) to prevent convergence warning
            nmf = NMF(n_components=n_components, init='nndsvda', random_state=42, max_iter=2000)
            W = nmf.fit_transform(X_norm) 
            H = nmf.components_ 
            
            # 4. Reconstruct Maps
            # W shape check logic
            if W.shape[0] != grid_h * grid_w:
                raise ValueError(f"Shape Mismatch: W={W.shape[0]} vs Grid={grid_h*grid_w}")

            weight_map_lowres = W.reshape(grid_h, grid_w, n_components)
            weight_map_highres = cv2.resize(weight_map_lowres, (w, h), interpolation=cv2.INTER_CUBIC)
            dominant_labels = np.argmax(weight_map_highres, axis=2) 
            
            # 5. Visualization (Overlay)
            overlay = img_bgr.copy()
            mask_layer = np.zeros_like(img_bgr)
            
            cmap = matplotlib.colormaps['tab10']
            colors = []
            for i in range(n_components):
                rgba = cmap(i)
                bgr = (int(rgba[2]*255), int(rgba[1]*255), int(rgba[0]*255))
                colors.append(bgr)
            
            stats = {f"Pattern {i}": 0 for i in range(n_components)}
            
            for i in range(n_components):
                mask = (dominant_labels == i)
                stats[f"Pattern {i}"] = int(np.sum(mask))
                mask_layer[mask] = colors[i]
                
            alpha = 0.4
            mask_indices = np.any(mask_layer > 0, axis=-1)
            overlay[mask_indices] = (overlay[mask_indices]*(1-alpha) + mask_layer[mask_indices]*alpha).astype(np.uint8)
            
            # 6. Basis Patterns (H) Visual
            basis_imgs = []
            for k in range(n_components):
                pat = H[k].reshape(patch_size, patch_size)
                pat_norm = cv2.normalize(pat, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)
                pat_col = cv2.cvtColor(pat_norm, cv2.COLOR_GRAY2BGR)
                cv2.rectangle(pat_col, (0,0), (patch_size-1, patch_size-1), colors[k], 2)
                basis_imgs.append(pat_col)
                
            basis_strip = np.hstack(basis_imgs)
            basis_strip_large = cv2.resize(basis_strip, None, fx=2, fy=2, interpolation=cv2.INTER_NEAREST)
            
            def to_b64(im): return base64.b64encode(cv2.imencode('.jpg', im)[1]).decode('utf-8')
            
            return {
                "type": "image", 
                "raw_b64": to_b64(img_bgr), 
                "proc_b64": to_b64(overlay),
                "footer_b64": to_b64(basis_strip_large), 
                "stats": stats, 
                "log": log
            }

        except Exception as e:
            # [Fix] Fallback mechanism: NMF 실패 시 원본 이미지라도 반환 (Fail 방지)
            print(f"NMF Failed: {e}")
            def to_b64(im): return base64.b64encode(cv2.imencode('.jpg', im)[1]).decode('utf-8')
            return {
                "type": "image",
                "raw_b64": to_b64(img_bgr),
                "proc_b64": to_b64(img_bgr), # Overlay 대신 원본 반환
                "stats": {"error": str(e)},
                "log": log + [f"NMF Error: {str(e)}"]
            }

    # --- Router (Safe Handling) ---
    @staticmethod
    def process_image_nv9(img_bytes, equipment, data_type, mode, goal, user_center=None):
        nparr = np.frombuffer(img_bytes, np.uint8)
        img_raw = cv2.imdecode(nparr, cv2.IMREAD_COLOR)
        if img_raw is None: return {}
        
        body, footer = ScienceProcessorNV9.separate_footer(img_raw)
        
        footer_b64 = None
        if footer is not None:
            footer_b64 = base64.b64encode(cv2.imencode('.jpg', footer)[1]).decode('utf-8')

        # [NV9] FFT-NMF
        if data_type == "Crystal Structure (FFT)":
            # 결과가 올바른 딕셔너리인지 확인
            res = ScienceProcessorNV9.analyze_crystal_fft_nmf(body)
            if "proc_b64" not in res: # 만약 Error dict가 왔다면
                 res["proc_b64"] = base64.b64encode(cv2.imencode('.jpg', body)[1]).decode('utf-8')
            return {**res} # footer_b64는 analyze 내부에서 basis image로 덮어씌워짐
            
        if data_type == "2D Diffraction":
            return {**ScienceProcessorNV9.analyze_diffraction_nv7(body, user_center), "footer_b64": footer_b64}
        
        # ... (이하 기존 SAM 로직 동일)
        if mode == "None":
             def to_b64(im): return base64.b64encode(cv2.imencode('.jpg', im)[1]).decode('utf-8')
             return {"type":"image", "raw_b64":to_b64(body), "proc_b64":to_b64(body), "stats":{}, "log":["Mode: Raw"], "footer_b64": footer_b64}

        img_proc = body.copy(); log = []
        if mode == "Auto" or mode == "AI-Adaptive":
            img_proc = cv2.bilateralFilter(img_proc, 9, 75, 75)
            log.append("Preproc: Denoise")

        img_rgb = cv2.cvtColor(img_proc, cv2.COLOR_BGR2RGB)
        
        if mask_generator:
            try:
                masks = mask_generator.generate(img_rgb)
                log.append(f"SAM: {len(masks)} Segments")
                overlay, stats, sam_log = ScienceProcessorNV9.analyze_sam_results_nv7(img_rgb, masks, data_type)
                log.extend(sam_log)
                overlay = cv2.cvtColor(overlay, cv2.COLOR_RGB2BGR)
            except Exception as e:
                log.append(f"SAM Error: {str(e)}")
                overlay = body; stats = {}
        else:
            log.append("SAM Not Loaded")
            overlay = body; stats = {}

        def to_b64(im): return base64.b64encode(cv2.imencode('.jpg', im)[1]).decode('utf-8')
        return {"type": "image", "raw_b64": to_b64(body), "proc_b64": to_b64(overlay), "stats": stats, "log": log, "footer_b64": footer_b64}


엔브이9

import os
import io
# [0] OMP Error Fix
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"
import torch

import asyncio
import base64
import json
import re
import math
import uuid
import time
from typing import List, Dict, Any, Optional
from pydantic import BaseModel
from contextlib import asynccontextmanager

# [1] Security
os.environ['NO_PROXY'] = '127.0.0.1,localhost,0.0.0.0,::1'
if 'HTTP_PROXY' in os.environ: del os.environ['HTTP_PROXY']
if 'HTTPS_PROXY' in os.environ: del os.environ['HTTPS_PROXY']

from fastapi import FastAPI, UploadFile, File, Form, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import HTMLResponse, FileResponse
import pandas as pd
import numpy as np

# SciPy & CV & Sklearn
from sklearn.decomposition import NMF
from sklearn.preprocessing import MinMaxScaler
from scipy.signal import find_peaks, savgol_filter
from scipy.optimize import curve_fit
from scipy.ndimage import center_of_mass
from skimage.feature import peak_local_max
import cv2
import matplotlib.cm as cm 

# Models
from segment_anything import sam_model_registry, SamAutomaticMaskGenerator
from ollama import Client 
from pdf2image import convert_from_bytes
from pptx import Presentation
from pptx.enum.shapes import MSO_SHAPE_TYPE

# [2] Config
OLLAMA_HOST = "http://127.0.0.1:11434"
ollama_client = Client(host=OLLAMA_HOST)
VISION_MODEL = "qwen3-vl:30b-a3b-instruct" 
TEXT_MODEL = "llama3:70b"

SAM_CHECKPOINT = "sam_vit_b_01ec64.pth"
SAM_MODEL_TYPE = "vit_b"
device = "cuda" if torch.cuda.is_available() else "cpu"

mask_generator = None 
JOBS = {} 

@asynccontextmanager
async def lifespan(app: FastAPI):
    global mask_generator
    if os.path.exists(SAM_CHECKPOINT):
        print(f"Loading SAM ({SAM_MODEL_TYPE}) on {device}...")
        try:
            sam = sam_model_registry[SAM_MODEL_TYPE](checkpoint=SAM_CHECKPOINT)
            sam.to(device=device)
            mask_generator = SamAutomaticMaskGenerator(
                model=sam, points_per_side=32, pred_iou_thresh=0.86,
                stability_score_thresh=0.90, min_mask_region_area=50
            )
            print("SAM Loaded.")
        except: print("SAM Load Failed.")
    yield
    JOBS.clear()

def sanitize_json(obj):
    if isinstance(obj, float):
        if np.isnan(obj) or np.isinf(obj): return 0.0
        return obj
    elif isinstance(obj, dict): return {k: sanitize_json(v) for k, v in obj.items()}
    elif isinstance(obj, list): return [sanitize_json(v) for v in obj]
    elif isinstance(obj, (np.int64, np.int32)): return int(obj)
    elif isinstance(obj, (np.float64, np.float32)):
        if np.isnan(obj) or np.isinf(obj): return 0.0
        return float(obj)
    return obj

# ==========================================
# [3] NV9 FFT-NMF Engine
# ==========================================
class ScienceProcessorNV9:
    
    # --- Universal Loader ---
    @staticmethod
    def read_universal_dataframe(content: bytes, filename: str) -> List[pd.DataFrame]:
        dfs = []
        fname = filename.lower()
        if fname.endswith(('.xlsx', '.xls')):
            try:
                xls = pd.ExcelFile(io.BytesIO(content))
                for s in xls.sheet_names:
                    df = pd.read_excel(xls, sheet_name=s, header=None)
                    num = df.apply(pd.to_numeric, errors='coerce').notna().sum().sum()
                    if num < 5: continue
                    if not df.empty: dfs.append(df)
                if dfs: return dfs
            except: pass
        text_content = ""
        for enc in ['utf-8', 'cp949', 'latin1']:
            try: text_content = content.decode(enc); break
            except: continue
        if text_content:
            lines = text_content.splitlines()
            data_start = -1; delimiter = None
            separators = [',', '\t', ';', '\s+']
            for i, line in enumerate(lines[:100]):
                for sep in separators:
                    parts = line.split() if sep=='\s+' else line.split(sep)
                    nums = sum(1 for p in parts if re.match(r'^-?\d+(\.\d+)?(?:[eE][+-]?\d+)?$', p.strip()))
                    if nums >= 2: data_start = i; delimiter = sep; break
                if data_start != -1: break
            if data_start != -1:
                try:
                    data_str = "\n".join(lines[data_start:])
                    sep_arg = '\s+' if delimiter=='\s+' else delimiter
                    df = pd.read_csv(io.StringIO(data_str), sep=sep_arg, header=None, engine='python')
                    dfs.append(df)
                except: pass
        return dfs

    # --- Block Parser ---
    @staticmethod
    def detect_structured_blocks(df: pd.DataFrame) -> List[pd.DataFrame]:
        blocks = []
        if df.empty: return []
        df_num = df.apply(pd.to_numeric, errors='coerce')
        if df.shape[1] > 0: is_data_row = df_num.iloc[:, 0].notna()
        else: is_data_row = df_num.notna().any(axis=1)
        groups = (is_data_row != is_data_row.shift()).cumsum()
        for _, group in df.groupby(groups):
            if is_data_row.loc[group.index[0]]:
                block = df_num.loc[group.index].dropna(how='all', axis=1)
                if block.shape[0] >= 10: blocks.append(block.reset_index(drop=True))
        return blocks

    @staticmethod
    def split_horizontal_blocks(df: pd.DataFrame) -> List[pd.DataFrame]:
        sub_blocks = []
        if df.empty: return []
        col_counts = df.apply(pd.to_numeric, errors='coerce').notna().sum()
        valid = np.where(col_counts >= 2)[0]
        if len(valid) == 0: return []
        splits = np.where(np.diff(valid) > 1)[0] + 1
        groups = np.split(valid, splits)
        for g in groups:
            if len(g) > 0:
                sub = df.iloc[:, g].reset_index(drop=True)
                if sub.shape[1] >= 1 and sub.shape[0] >= 2: sub_blocks.append(sub)
        return sub_blocks

    @staticmethod
    def extract_series_from_df(df: pd.DataFrame) -> List[Dict]:
        series_list = []
        try:
            vals = df.values; rows, cols = vals.shape
            if rows < 5: return []
            if cols == 1:
                y = vals[:, 0]; mask = ~np.isnan(y)
                if np.sum(mask) > 5: series_list.append({"x": np.arange(len(y))[mask], "y": y[mask], "name": "Single"})
                return series_list
            def is_axis(arr):
                arr = arr[~np.isnan(arr)]
                if len(arr) < 5: return False
                diff = np.diff(arr)
                if np.all(diff >= 0) or np.all(diff <= 0): return True
                return (np.sum(diff>0)/len(diff)>0.9) or (np.sum(diff<0)/len(diff)>0.9)
            current_x = vals[:, 0]
            if not is_axis(current_x): current_x = np.arange(rows)
            for i in range(1, cols):
                col_data = vals[:, i]; mask_valid = ~np.isnan(col_data)
                if np.sum(mask_valid) < 5: continue
                if is_axis(col_data): current_x = col_data; continue 
                mask = ~np.isnan(current_x) & ~np.isnan(col_data)
                if np.sum(mask) > 5:
                    series_list.append({"x": current_x[mask], "y": col_data[mask], "name": f"Col-{i}"})
        except: pass
        return series_list

    # --- Spectrum Logic ---
    @staticmethod
    def simple_baseline(y):
        try:
            w = max(5, len(y)//10)
            s = pd.Series(y).rolling(window=w, center=True).min()
            b = s.bfill().ffill().values 
            if len(b) > 51: b = savgol_filter(b, 51, 3)
            return b, w
        except: return np.zeros_like(y), 0

    @staticmethod
    def process_spectrum(x, y, mode, goal):
        log = []; y_raw = y.copy(); base = np.zeros_like(y); y_proc = y.copy()
        peaks = []; fits = []; stats_txt = "Error"
        try:
            if mode == "None":
                peaks, _ = find_peaks(y, height=np.max(y)*0.05)
                log.append("Mode: Raw")
                stats_txt = f"Raw Peaks: {len(peaks)}"
            else:
                win = 15; do_fit = False
                if mode == "AI-Adaptive":
                    log.append("Mode: AI-Adaptive")
                    if "noise" in goal.lower(): win = 31
                    if "fit" in goal.lower(): do_fit = True
                
                base, bw = ScienceProcessorNV9.simple_baseline(y)
                y_proc = np.maximum(y - base, 0)
                log.append(f"Base: Rolling Min (w={bw})")
                
                if len(y_proc) > win:
                    y_proc = savgol_filter(y_proc, win, 3)
                    log.append(f"Smooth: SavGol (w={win})")
                
                peaks, _ = find_peaks(y_proc, height=np.max(y_proc)*0.05, distance=10)
                log.append(f"Peaks: {len(peaks)}")
                stats_txt = f"Range: {np.min(x):.1f}~{np.max(x):.1f}, Peaks: {len(peaks)}"
            return {"x": x, "y_raw": y_raw, "y_proc": y_proc, "y_base": base, "peaks": [{"x": float(x[p]), "y": float(y_proc[p])} for p in peaks], "log": log, "fits": fits, "stats": stats_txt}
        except Exception as e: return {"x": x, "y_raw": y_raw, "y_proc": y_proc, "y_base": base, "peaks": [], "log": [str(e)], "fits": [], "stats": "Error"}

    # --- Footer ---
    @staticmethod
    def separate_footer(img_bgr):
        h, w = img_bgr.shape[:2]
        roi_h = int(h * 0.82)
        roi = img_bgr[roi_h:, :]
        gray = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)
        sobel_y = np.abs(cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=3))
        row_sums = np.sum(sobel_y, axis=1)
        split_idx = np.argmax(row_sums)
        if row_sums[split_idx] > w * 30:
            real_split = roi_h + split_idx
            return img_bgr[:real_split, :], img_bgr[real_split:, :]
        return img_bgr, None

    # --- [E] NV9 FFT-NMF Crystal Engine ---
    @staticmethod
    def analyze_crystal_fft_nmf(img_bgr):
        log = []
        gray = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2GRAY)
        h, w = gray.shape
        
        # 1. Config
        patch_size = 64
        step = 32
        n_components = 4 # Amorphous, Crystal A, Crystal B, Boundary
        
        log.append(f"FFT-NMF: Patch={patch_size}, Step={step}, K={n_components}")
        
        # 2. Collect FFT Patterns
        features = []
        
        # Calculate Grid Dimensions
        y_steps = range(0, h - patch_size + 1, step)
        x_steps = range(0, w - patch_size + 1, step)
        grid_h = len(y_steps)
        grid_w = len(x_steps)
        
        for y in y_steps:
            for x in x_steps:
                roi = gray[y:y+patch_size, x:x+patch_size]
                
                # Apply FFT
                f = np.fft.fft2(roi)
                fshift = np.fft.fftshift(f)
                mag = 20 * np.log(np.abs(fshift) + 1e-9)
                
                # Center Mask (Remove DC)
                cy, cx = patch_size//2, patch_size//2
                mag[cy-2:cy+3, cx-2:cx+3] = 0
                
                features.append(mag.flatten())
        
        if not features: return {"type":"error", "msg":"Img too small"}
        
        # 3. NMF Decomposition
        X = np.array(features) # (N_patches, 64*64)
        scaler = MinMaxScaler()
        X_norm = scaler.fit_transform(X.T).T # Normalize per patch pattern
        
        nmf = NMF(n_components=n_components, init='nndsvda', random_state=42, max_iter=500)
        W = nmf.fit_transform(X_norm) # (N_patches, K) -> Weights per patch
        H = nmf.components_         # (K, 64*64) -> Basis Patterns
        
        # 4. Reconstruct Weight Maps (Dominant Pattern Map)
        # W shape is (N_patches, K). We reshape to (Grid_H, Grid_W, K)
        try:
            weight_map_lowres = W.reshape(grid_h, grid_w, n_components)
        except:
            return {"type":"error", "msg":f"Reshape fail: {W.shape} -> {grid_h}x{grid_w}"}
            
        # Resize to original image size
        weight_map_highres = cv2.resize(weight_map_lowres, (w, h), interpolation=cv2.INTER_CUBIC)
        
        # Find Dominant Component per pixel
        dominant_labels = np.argmax(weight_map_highres, axis=2) # (H, W)
        
        # 5. Visualization (Overlay)
        overlay = img_bgr.copy()
        mask_layer = np.zeros_like(img_bgr)
        
        # Colors: K distinct colors
        cmap = cm.get_cmap('tab10', n_components)
        colors = [tuple(int(c*255) for c in cmap(i)[:3][::-1]) for i in range(n_components)]
        
        stats = {f"Pattern {i}": 0 for i in range(n_components)}
        
        for i in range(n_components):
            mask = (dominant_labels == i)
            pixel_count = np.sum(mask)
            stats[f"Pattern {i}"] = pixel_count
            
            # Apply color to mask_layer
            mask_layer[mask] = colors[i]
            
        # Alpha Blend
        alpha = 0.4
        mask_indices = np.any(mask_layer > 0, axis=-1)
        overlay[mask_indices] = (overlay[mask_indices]*(1-alpha) + mask_layer[mask_indices]*alpha).astype(np.uint8)
        
        # 6. Save Basis Patterns (H) as an Image Strip for User
        basis_imgs = []
        for k in range(n_components):
            pat = H[k].reshape(patch_size, patch_size)
            pat_norm = cv2.normalize(pat, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)
            # Add colored border matching the overlay
            pat_col = cv2.cvtColor(pat_norm, cv2.COLOR_GRAY2BGR)
            cv2.rectangle(pat_col, (0,0), (patch_size-1, patch_size-1), colors[k], 2)
            basis_imgs.append(pat_col)
            
        # Concatenate basis images horizontally
        basis_strip = np.hstack(basis_imgs)
        # Scale up for visibility
        basis_strip_large = cv2.resize(basis_strip, None, fx=2, fy=2, interpolation=cv2.INTER_NEAREST)
        
        def to_b64(im): return base64.b64encode(cv2.imencode('.jpg', im)[1]).decode('utf-8')
        
        return {
            "type": "image", 
            "raw_b64": to_b64(img_bgr), 
            "proc_b64": to_b64(overlay),
            "footer_b64": to_b64(basis_strip_large), # Re-using footer slot to show Basis Patterns
            "stats": stats, 
            "log": log
        }

    # --- [F] Diffraction ---
    @staticmethod
    def analyze_diffraction_nv7(img_bgr, user_center=None):
        log = []
        stats = {}
        gray = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2GRAY)
        
        if user_center and len(user_center)==2 and user_center[0]>0:
            cx, cy = user_center
            log.append(f"Center: Manual [{cx}, {cy}]")
        else:
            _, th = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY+cv2.THRESH_OTSU)
            cyf, cxf = center_of_mass(th)
            if np.isnan(cyf): cyf, cxf = gray.shape[0]//2, gray.shape[1]//2
            cx, cy = int(cxf), int(cyf)
            log.append(f"Center: Auto [{cx}, {cy}]")

        g1 = cv2.GaussianBlur(gray, (3,3), 0)
        g2 = cv2.GaussianBlur(gray, (51,51), 0)
        diff = cv2.subtract(g1, g2)
        
        coords = peak_local_max(diff, min_distance=10, threshold_abs=20)
        
        enhanced = np.log1p(diff.astype(np.float32))
        enhanced = cv2.normalize(enhanced, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)
        vis = cv2.cvtColor(enhanced, cv2.COLOR_GRAY2BGR)
        
        cv2.drawMarker(vis, (cx, cy), (0,0,255), cv2.MARKER_CROSS, 20, 2)
        peak_list = []
        for p in coords:
            py, px = p
            if abs(py-cy)<5 and abs(px-cx)<5: continue
            cv2.circle(vis, (px, py), 3, (0,255,0), 1)
            peak_list.append(f"[{px},{py}]")

        log.append(f"Spots: {len(peak_list)}")
        def to_b64(im): return base64.b64encode(cv2.imencode('.jpg', im)[1]).decode('utf-8')
        return {"type": "image", "raw_b64": to_b64(img_bgr), "proc_b64": to_b64(vis), "stats": {"center": [cx, cy], "spots": len(peak_list)}, "log": log}

    # --- [G] SAM ---
    @staticmethod
    def analyze_sam_results_nv7(img_rgb, masks, data_type):
        overlay = img_rgb.copy()
        mask_overlay = np.zeros_like(img_rgb)
        h_img, w_img = img_rgb.shape[:2]
        stats = {"count": 0}
        log = []
        
        valid_masks = []
        for ann in masks:
            area = ann['area']; bbox = ann['bbox']; x, y, w, h = [int(v) for v in bbox]
            if area > h_img * w_img * 0.95 or area < 20: continue 
            if data_type == "Particle":
                if x<=1 or y<=1 or (x+w)>=w_img-1 or (y+h)>=h_img-1: continue
                mask_uint8 = (ann['segmentation'] * 255).astype(np.uint8)
                cnts, _ = cv2.findContours(mask_uint8, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
                if cnts:
                    perim = cv2.arcLength(cnts[0], True)
                    circ = 4*np.pi*area/(perim**2) if perim>0 else 0
                    if circ < 0.25: continue
            valid_masks.append(ann)

        if data_type == "Thin Film":
            sorted_masks = sorted(valid_masks, key=lambda x: x['bbox'][1])
        else:
            sorted_masks = sorted(valid_masks, key=lambda x: x['area'], reverse=True)

        thickness_vals = []; diameters = []

        for i, ann in enumerate(sorted_masks):
            m = ann['segmentation']
            bbox = ann['bbox']; x, y, w, h = [int(v) for v in bbox]
            cx, cy = int(x + w/2), int(y + h/2)
            color = (0, 255, 0) 

            if data_type == "Thin Film":
                is_bound = (i == 0 or i == len(sorted_masks)-1)
                cv2.putText(overlay, f"#{i+1}", (cx, cy), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (255, 255, 255), 3)
                if not is_bound:
                    thk = ann['area'] / w
                    thickness_vals.append(thk)
                    color = (255, 0, 0) 
                else: color = (100, 100, 100)
                if i < len(sorted_masks) - 1:
                    ys, xs = np.where(m)
                    if len(xs) > 10:
                        roughness = np.std(ys)
                        cv2.line(overlay, (x, y+h), (x+w, y+h), (0, 255, 255), 2)
                        cv2.putText(overlay, f"Int#{i+1} R:{roughness:.2f}", (x+10, y+h-5), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)
            elif data_type == "Particle":
                equivalent_diameter = 2 * np.sqrt(ann['area'] / np.pi)
                diameters.append(equivalent_diameter)

            mask_overlay[m] = color
            mask_uint8 = (m * 255).astype(np.uint8)
            cnts, _ = cv2.findContours(mask_uint8, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
            if cnts: cv2.drawContours(overlay, [cnts[0]], -1, color, 1)

        alpha = 0.35
        mask_idx = np.any(mask_overlay > 0, axis=-1)
        overlay[mask_idx] = (overlay[mask_idx]*(1-alpha) + mask_overlay[mask_idx]*alpha).astype(np.uint8)

        stats["count"] = len(sorted_masks)
        if data_type == "Thin Film":
            avg = round(np.mean(thickness_vals), 2) if thickness_vals else 0
            stats["thickness_avg"] = avg
            log.append(f"Layers: {len(sorted_masks)}, Avg Thk: {avg}")
        elif data_type == "Particle":
            avg = round(np.mean(diameters), 2) if diameters else 0
            stats["diameter_avg"] = avg
            log.append(f"Particles: {len(sorted_masks)}")
        return overlay, stats, log

    # --- Router ---
    @staticmethod
    def process_image_nv9(img_bytes, equipment, data_type, mode, goal, user_center=None):
        nparr = np.frombuffer(img_bytes, np.uint8)
        img_raw = cv2.imdecode(nparr, cv2.IMREAD_COLOR)
        if img_raw is None: return {}
        
        body, footer = ScienceProcessorNV9.separate_footer(img_raw)
        
        # [NV9] FFT-NMF
        if data_type == "Crystal Structure (FFT)":
            return ScienceProcessorNV9.analyze_crystal_fft_nmf(body)
            
        # Footer Image for other modes
        footer_b64 = None
        if footer is not None:
            footer_b64 = base64.b64encode(cv2.imencode('.jpg', footer)[1]).decode('utf-8')

        if data_type == "2D Diffraction":
            return {**ScienceProcessorNV9.analyze_diffraction_nv7(body, user_center), "footer_b64": footer_b64}
        
        if mode == "None":
             def to_b64(im): return base64.b64encode(cv2.imencode('.jpg', im)[1]).decode('utf-8')
             return {"type":"image", "raw_b64":to_b64(body), "proc_b64":to_b64(body), "stats":{}, "log":["Mode: Raw"], "footer_b64": footer_b64}

        img_proc = body.copy(); log = []
        if mode == "Auto" or mode == "AI-Adaptive":
            img_proc = cv2.bilateralFilter(img_proc, 9, 75, 75)
            log.append("Preproc: Denoise")

        img_rgb = cv2.cvtColor(img_proc, cv2.COLOR_BGR2RGB)
        
        if mask_generator:
            try:
                masks = mask_generator.generate(img_rgb)
                log.append(f"SAM: {len(masks)} Segments")
                overlay, stats, sam_log = ScienceProcessorNV9.analyze_sam_results_nv7(img_rgb, masks, data_type)
                log.extend(sam_log)
                overlay = cv2.cvtColor(overlay, cv2.COLOR_RGB2BGR)
            except Exception as e:
                log.append(f"SAM Error: {str(e)}")
                overlay = body; stats = {}
        else:
            log.append("SAM Not Loaded")
            overlay = body; stats = {}

        def to_b64(im): return base64.b64encode(cv2.imencode('.jpg', im)[1]).decode('utf-8')
        return {"type": "image", "raw_b64": to_b64(body), "proc_b64": to_b64(overlay), "stats": stats, "log": log, "footer_b64": footer_b64}

# [4] Helper
async def analyze_vision_ollama(image_bytes: bytes, prompt: str) -> str:
    try:
        b64 = base64.b64encode(image_bytes).decode('utf-8')
        res = await asyncio.to_thread(ollama_client.chat, model=VISION_MODEL, messages=[{'role':'user','content':prompt,'images':[b64]}])
        return res['message']['content']
    except: return "Vision Error"

async def interpret_spectrum_data(raw_context: str, equipment: str) -> str:
    try:
        prompt = f"Analyze spectrum ({equipment}). Brief interpretation in Korean. Data: {raw_context}"
        res = await asyncio.to_thread(ollama_client.chat, model=TEXT_MODEL, messages=[{'role':'user','content':prompt}])
        return res['message']['content']
    except: return ""

# [5] App
app = FastAPI(title="Analyst NV9 FFT-NMF", lifespan=lifespan)
app.add_middleware(CORSMiddleware, allow_origins=["*"], allow_methods=["*"], allow_headers=["*"])

class TranslateReq(BaseModel): text: str

@app.post("/api/translate")
async def translate_text(req: TranslateReq):
    try:
        res = await asyncio.to_thread(ollama_client.chat, model=TEXT_MODEL, messages=[{'role':'system','content':'Translate to English.'}, {'role':'user','content':req.text}])
        return {"translated": res['message']['content']}
    except Exception as e: return {"translated": str(e)}

async def process_analysis_job(job_id: str, files_map: Dict, configs: Dict):
    try:
        JOBS[job_id]["status"] = "Processing"
        JOBS[job_id]["progress"] = 10
        final_results = []
        total_files = len(configs)
        processed_count = 0

        for filename, config in configs.items():
            if filename not in files_map: continue
            content = files_map[filename]
            eq = config.get("equipment", "General")
            data_type = config.get("data_type", "General")
            goal = config.get("goal", "")
            mode = config.get("mode", "Auto")
            
            user_center = None
            if config.get("center_coords"):
                try: user_center = [int(v) for v in config["center_coords"].split(",")]
                except: pass

            fname_lower = filename.lower()
            JOBS[job_id]["step"] = f"Analyzing {filename}..."
            
            if eq == "Document" or fname_lower.endswith(('.pdf', '.ppt', '.pptx')):
                # (Existing logic omitted for brevity, assume same as NV7)
                pass # ... Copy document logic from previous version

            elif fname_lower.endswith(('.xlsx', '.xls', '.csv', '.txt')):
                try:
                    dfs = ScienceProcessorNV9.read_universal_dataframe(content, filename)
                    for i, df in enumerate(dfs):
                        v_blocks = ScienceProcessorNV9.detect_structured_blocks(df)
                        for v_block in v_blocks:
                            h_blocks = ScienceProcessorNV9.split_horizontal_blocks(v_block)
                            for h_block in h_blocks:
                                series_list = ScienceProcessorNV9.extract_series_from_df(h_block)
                                for s in series_list:
                                    res = ScienceProcessorNV9.process_spectrum(s['x'], s['y'], mode, goal)
                                    step = max(1, len(s['x'])//100)
                                    chart = [{"x": float(res["x"][k]), "y_proc": float(res["y_proc"][k]), "y_raw": float(res["y_raw"][k]), "y_base": float(res["y_base"][k])} for k in range(0, len(s['x']), step)]
                                    ctx = f"Data: {s['name']}\nStats: {res['stats']}"
                                    interp = await interpret_spectrum_data(ctx, eq)
                                    final_results.append({
                                        "type": "spectrum", "filename": f"{filename} ({s['name']})", "equipment": eq,
                                        "raw_context": f"{ctx}\nInterp: {interp}", 
                                        "chart_data": chart, "log": res["log"], "interpretation": interp
                                    })
                except Exception as ex: final_results.append({"type": "error", "filename": filename, "msg": str(ex)})

            elif fname_lower.endswith(('.png', '.jpg', '.jpeg', '.tif', '.bmp')):
                try:
                    JOBS[job_id]["step"] = f"Vision Analysis: {filename}..."
                    vis_res = await asyncio.to_thread(ScienceProcessorNV9.process_image_nv9, content, eq, data_type, mode, goal, user_center)
                    if vis_res:
                        scale_info = "Unknown"
                        # If footer exists (or basis pattern strip exists in footer_b64 slot)
                        # We only OCR footer if it's NOT the Crystal FFT mode (because FFT mode overwrites footer with basis patterns)
                        if vis_res.get("footer_b64") and data_type != "Crystal Structure (FFT)":
                            JOBS[job_id]["step"] = f"Scale Reading..."
                            scale_text = await analyze_vision_ollama(base64.b64decode(vis_res["footer_b64"]), "Read scale bar text only.")
                            scale_info = scale_text
                            vis_res["log"].append(f"Scale: {scale_text}")

                        # Description
                        stats_str = json.dumps(vis_res.get('stats',{}), indent=2)
                        desc_prompt = f"""
                        Analyze this {eq} image ({data_type}).
                        Stats: {stats_str}
                        Scale: {scale_info}
                        Goal: {goal}
                        Summary in Korean.
                        """
                        desc = await analyze_vision_ollama(base64.b64decode(vis_res["proc_b64"]), desc_prompt)
                        
                        final_results.append({
                            "type": "image", "filename": filename, "equipment": eq, "summary": desc,
                            "raw_context": f"Img {eq}: {desc}", "raw_b64": vis_res["raw_b64"], "proc_b64": vis_res["proc_b64"], "footer_b64": vis_res.get("footer_b64"),
                            "stats": vis_res.get("stats",{}), "log": vis_res.get("log", [])
                        })
                except Exception as ex: final_results.append({"type": "error", "filename": filename, "msg": str(ex)})
            
            processed_count += 1
            JOBS[job_id]["progress"] = 10 + int((processed_count / total_files) * 80)

        JOBS[job_id]["step"] = "Writing Report..."
        final_results = sanitize_json(final_results)
        
        data_ctx = ""
        for r in final_results:
            if r.get("type") != "error":
                raw = r.get("raw_context") or ""
                data_ctx += f"\n=== {r['filename']} ===\n{raw[:1000]}\n"

        final_report = "Fail"
        if data_ctx:
            try:
                system_prompt = "You are a research assistant. Write a report in Korean. Summarize data and physics."
                res = await asyncio.to_thread(ollama_client.chat, model=TEXT_MODEL, messages=[{'role':'system','content':system_prompt}, {'role':'user','content':f"Data:\n{data_ctx}"}])
                final_report = res['message']['content']
            except Exception as e: final_report = str(e)

        JOBS[job_id]["results"] = {"results": final_results, "final_report": final_report}
        JOBS[job_id]["status"] = "Completed"
        JOBS[job_id]["progress"] = 100
    except Exception as e:
        JOBS[job_id]["status"] = "Failed"; JOBS[job_id]["error"] = str(e)

@app.post("/api/analyze")
async def start_analysis(background_tasks: BackgroundTasks, files: List[UploadFile] = File(...), file_configs: str = Form(...)):
    job_id = str(uuid.uuid4()); configs = json.loads(file_configs)
    files_map = {f.filename: await f.read() for f in files}
    JOBS[job_id] = {"status": "Pending", "progress": 0, "step": "Init", "results": None}
    background_tasks.add_task(process_analysis_job, job_id, files_map, configs)
    return {"job_id": job_id}

@app.get("/api/status/{job_id}")
async def get_status(job_id: str): return JOBS.get(job_id, {"status": "Not Found"})

@app.get("/", response_class=HTMLResponse)
async def serve_index():
    if os.path.exists("index.html"): return FileResponse("index.html")
    return "<h1>NV9 FFT-NMF Backend</h1>"

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)




엔브이7 엔엠에프2

import os
import io
# [0] OMP Error Fix & Torch First
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"
import torch

import asyncio
import base64
import json
import re
import math
import uuid
import time
from typing import List, Dict, Any, Optional
from pydantic import BaseModel
from contextlib import asynccontextmanager

# [1] Security
os.environ['NO_PROXY'] = '127.0.0.1,localhost,0.0.0.0,::1'
if 'HTTP_PROXY' in os.environ: del os.environ['HTTP_PROXY']
if 'HTTPS_PROXY' in os.environ: del os.environ['HTTPS_PROXY']

from fastapi import FastAPI, UploadFile, File, Form, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import HTMLResponse, FileResponse
import pandas as pd
import numpy as np

# SciPy & CV & Sklearn (NMF Raw Patch)
from sklearn.decomposition import NMF
from sklearn.preprocessing import MinMaxScaler
from scipy.signal import find_peaks, savgol_filter
from scipy.optimize import curve_fit
from scipy.ndimage import center_of_mass
from skimage.feature import peak_local_max
import cv2
import matplotlib.cm as cm # For colormap

# Models
from segment_anything import sam_model_registry, SamAutomaticMaskGenerator
from ollama import Client 
from pdf2image import convert_from_bytes
from pptx import Presentation
from pptx.enum.shapes import MSO_SHAPE_TYPE

# [2] Config
OLLAMA_HOST = "http://127.0.0.1:11434"
ollama_client = Client(host=OLLAMA_HOST)
VISION_MODEL = "qwen3-vl:30b-a3b-instruct" 
TEXT_MODEL = "llama3:70b"

SAM_CHECKPOINT = "sam_vit_b_01ec64.pth"
SAM_MODEL_TYPE = "vit_b"
device = "cuda" if torch.cuda.is_available() else "cpu"

mask_generator = None 
JOBS = {} 

@asynccontextmanager
async def lifespan(app: FastAPI):
    global mask_generator
    if os.path.exists(SAM_CHECKPOINT):
        print(f"Loading SAM ({SAM_MODEL_TYPE}) on {device}...")
        try:
            sam = sam_model_registry[SAM_MODEL_TYPE](checkpoint=SAM_CHECKPOINT)
            sam.to(device=device)
            mask_generator = SamAutomaticMaskGenerator(
                model=sam, points_per_side=32, pred_iou_thresh=0.86,
                stability_score_thresh=0.90, min_mask_region_area=50
            )
            print("SAM Loaded.")
        except: print("SAM Load Failed.")
    yield
    JOBS.clear()

def sanitize_json(obj):
    if isinstance(obj, float):
        if np.isnan(obj) or np.isinf(obj): return 0.0
        return obj
    elif isinstance(obj, dict): return {k: sanitize_json(v) for k, v in obj.items()}
    elif isinstance(obj, list): return [sanitize_json(v) for v in obj]
    elif isinstance(obj, (np.int64, np.int32)): return int(obj)
    elif isinstance(obj, (np.float64, np.float32)):
        if np.isnan(obj) or np.isinf(obj): return 0.0
        return float(obj)
    return obj

# ==========================================
# [3] NV8 Raw NMF Engine
# ==========================================
class ScienceProcessorNV8:
    
    # --- Universal Loader ---
    @staticmethod
    def read_universal_dataframe(content: bytes, filename: str) -> List[pd.DataFrame]:
        dfs = []
        fname = filename.lower()
        if fname.endswith(('.xlsx', '.xls')):
            try:
                xls = pd.ExcelFile(io.BytesIO(content))
                for s in xls.sheet_names:
                    df = pd.read_excel(xls, sheet_name=s, header=None)
                    num = df.apply(pd.to_numeric, errors='coerce').notna().sum().sum()
                    if num < 5: continue
                    if not df.empty: dfs.append(df)
                if dfs: return dfs
            except: pass
        text_content = ""
        for enc in ['utf-8', 'cp949', 'latin1']:
            try: text_content = content.decode(enc); break
            except: continue
        if text_content:
            lines = text_content.splitlines()
            data_start = -1; delimiter = None
            separators = [',', '\t', ';', '\s+']
            for i, line in enumerate(lines[:100]):
                for sep in separators:
                    parts = line.split() if sep=='\s+' else line.split(sep)
                    nums = sum(1 for p in parts if re.match(r'^-?\d+(\.\d+)?(?:[eE][+-]?\d+)?$', p.strip()))
                    if nums >= 2: data_start = i; delimiter = sep; break
                if data_start != -1: break
            if data_start != -1:
                try:
                    data_str = "\n".join(lines[data_start:])
                    sep_arg = '\s+' if delimiter=='\s+' else delimiter
                    df = pd.read_csv(io.StringIO(data_str), sep=sep_arg, header=None, engine='python')
                    dfs.append(df)
                except: pass
        return dfs

    # --- Block Parser ---
    @staticmethod
    def detect_structured_blocks(df: pd.DataFrame) -> List[pd.DataFrame]:
        blocks = []
        if df.empty: return []
        df_num = df.apply(pd.to_numeric, errors='coerce')
        if df.shape[1] > 0: is_data_row = df_num.iloc[:, 0].notna()
        else: is_data_row = df_num.notna().any(axis=1)
        groups = (is_data_row != is_data_row.shift()).cumsum()
        for _, group in df.groupby(groups):
            if is_data_row.loc[group.index[0]]:
                block = df_num.loc[group.index].dropna(how='all', axis=1)
                if block.shape[0] >= 10: blocks.append(block.reset_index(drop=True))
        return blocks

    @staticmethod
    def split_horizontal_blocks(df: pd.DataFrame) -> List[pd.DataFrame]:
        sub_blocks = []
        if df.empty: return []
        col_counts = df.apply(pd.to_numeric, errors='coerce').notna().sum()
        valid = np.where(col_counts >= 2)[0]
        if len(valid) == 0: return []
        splits = np.where(np.diff(valid) > 1)[0] + 1
        groups = np.split(valid, splits)
        for g in groups:
            if len(g) > 0:
                sub = df.iloc[:, g].reset_index(drop=True)
                if sub.shape[1] >= 1 and sub.shape[0] >= 2: sub_blocks.append(sub)
        return sub_blocks

    @staticmethod
    def extract_series_from_df(df: pd.DataFrame) -> List[Dict]:
        series_list = []
        try:
            vals = df.values; rows, cols = vals.shape
            if rows < 5: return []
            if cols == 1:
                y = vals[:, 0]; mask = ~np.isnan(y)
                if np.sum(mask) > 5: series_list.append({"x": np.arange(len(y))[mask], "y": y[mask], "name": "Single"})
                return series_list
            def is_axis(arr):
                arr = arr[~np.isnan(arr)]
                if len(arr) < 5: return False
                diff = np.diff(arr)
                if np.all(diff >= 0) or np.all(diff <= 0): return True
                return (np.sum(diff>0)/len(diff)>0.9) or (np.sum(diff<0)/len(diff)>0.9)
            current_x = vals[:, 0]
            if not is_axis(current_x): current_x = np.arange(rows)
            for i in range(1, cols):
                col_data = vals[:, i]; mask_valid = ~np.isnan(col_data)
                if np.sum(mask_valid) < 5: continue
                if is_axis(col_data): current_x = col_data; continue 
                mask = ~np.isnan(current_x) & ~np.isnan(col_data)
                if np.sum(mask) > 5:
                    series_list.append({"x": current_x[mask], "y": col_data[mask], "name": f"Col-{i}"})
        except: pass
        return series_list

    # --- Spectrum Logic ---
    @staticmethod
    def simple_baseline(y):
        try:
            w = max(5, len(y)//10)
            s = pd.Series(y).rolling(window=w, center=True).min()
            b = s.bfill().ffill().values 
            if len(b) > 51: b = savgol_filter(b, 51, 3)
            return b, w
        except: return np.zeros_like(y), 0

    @staticmethod
    def process_spectrum(x, y, mode, goal):
        log = []; y_raw = y.copy(); base = np.zeros_like(y); y_proc = y.copy()
        peaks = []; fits = []; stats_txt = "Error"
        try:
            if mode == "None":
                peaks, _ = find_peaks(y, height=np.max(y)*0.05)
                log.append("Mode: Raw")
                stats_txt = f"Raw Peaks: {len(peaks)}"
            else:
                win = 15; do_fit = False
                if mode == "AI-Adaptive":
                    log.append("Mode: AI-Adaptive")
                    if "noise" in goal.lower(): win = 31
                    if "fit" in goal.lower(): do_fit = True
                
                base, bw = ScienceProcessorNV8.simple_baseline(y)
                y_proc = np.maximum(y - base, 0)
                log.append(f"Base: Rolling Min (w={bw})")
                
                if len(y_proc) > win:
                    y_proc = savgol_filter(y_proc, win, 3)
                    log.append(f"Smooth: SavGol (w={win})")
                
                peaks, _ = find_peaks(y_proc, height=np.max(y_proc)*0.05, distance=10)
                log.append(f"Peaks: {len(peaks)}")
                stats_txt = f"Range: {np.min(x):.1f}~{np.max(x):.1f}, Peaks: {len(peaks)}"
            return {"x": x, "y_raw": y_raw, "y_proc": y_proc, "y_base": base, "peaks": [{"x": float(x[p]), "y": float(y_proc[p])} for p in peaks], "log": log, "fits": fits, "stats": stats_txt}
        except Exception as e: return {"x": x, "y_raw": y_raw, "y_proc": y_proc, "y_base": base, "peaks": [], "log": [str(e)], "fits": [], "stats": "Error"}

    # --- Footer ---
    @staticmethod
    def separate_footer(img_bgr):
        h, w = img_bgr.shape[:2]
        roi_h = int(h * 0.82)
        roi = img_bgr[roi_h:, :]
        gray = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)
        sobel_y = np.abs(cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=3))
        row_sums = np.sum(sobel_y, axis=1)
        split_idx = np.argmax(row_sums)
        if row_sums[split_idx] > w * 30:
            real_split = roi_h + split_idx
            return img_bgr[:real_split, :], img_bgr[real_split:, :]
        return img_bgr, None

    # --- [E] NV8 Raw NMF Engine (Patch=128, k=10) ---
    @staticmethod
    def analyze_crystal_nmf(img_bgr):
        log = []
        gray = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2GRAY)
        h, w = gray.shape
        
        # [Update] Patch Size 128
        patch_size = 128
        step = 64 # Overlap for smoother result
        patches = []
        coords = []
        
        log.append(f"Raw NMF: Patch={patch_size}, Step={step}, k=10")
        
        # 1. Collect Raw Patches
        for y in range(0, h - patch_size + 1, step):
            for x in range(0, w - patch_size + 1, step):
                roi = gray[y:y+patch_size, x:x+patch_size]
                patches.append(roi.flatten())
                coords.append((x, y))
        
        if not patches: return {"type":"error", "msg":"Img too small for 128px patch"}
        
        # 2. NMF on Raw Pixel Data
        X = np.array(patches).astype(np.float32) # Shape: (N_patches, 128*128)
        
        # Normalize (NMF requires non-negative)
        scaler = MinMaxScaler()
        X_norm = scaler.fit_transform(X.T).T # Normalize per patch
        
        # NMF with 10 components
        n_components = 10
        nmf = NMF(n_components=n_components, init='nndsvda', random_state=42, max_iter=500)
        W = nmf.fit_transform(X_norm) # (N_patches, 10) - Weights
        H = nmf.components_         # (10, 128*128) - Basis patterns
        
        # 3. Visualization (Segmentation Map by Dominant Component)
        overlay = img_bgr.copy()
        mask_layer = np.zeros_like(img_bgr)
        
        # Get 10 distinct colors from matplotlib colormap
        cmap = cm.get_cmap('tab10', n_components)
        colors = [tuple(int(c*255) for c in cmap(i)[:3][::-1]) for i in range(n_components)] # RGB -> BGR
        
        stats = {f"Pattern {i}": 0 for i in range(n_components)}
        
        # Assign dominant component index to each patch area
        dominant_labels = np.argmax(W, axis=1)
        
        for idx, label in enumerate(dominant_labels):
            x, y = coords[idx]
            stats[f"Pattern {label}"] += 1
            color = colors[label]
            
            # Draw colored patch (Simple overwrite for now, overlap handled by order)
            cv2.rectangle(mask_layer, (x, y), (x+patch_size, y+patch_size), color, -1)
            
        # Blend
        alpha = 0.5
        mask_idx = np.any(mask_layer > 0, axis=-1)
        overlay[mask_idx] = (overlay[mask_idx]*(1-alpha) + mask_layer[mask_idx]*alpha).astype(np.uint8)
        
        # Simplified Stats for LLM
        log.append(f"Patterns found: {n_components}")
        
        def to_b64(im): return base64.b64encode(cv2.imencode('.jpg', im)[1]).decode('utf-8')
        return {"type": "image", "raw_b64": to_b64(img_bgr), "proc_b64": to_b64(overlay), "stats": stats, "log": log}

    # --- [F] Diffraction ---
    @staticmethod
    def analyze_diffraction_nv7(img_bgr, user_center=None):
        log = []
        gray = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2GRAY)
        
        if user_center and len(user_center)==2 and user_center[0]>0:
            cx, cy = user_center
            log.append(f"Center: Manual [{cx}, {cy}]")
        else:
            _, th = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY+cv2.THRESH_OTSU)
            cyf, cxf = center_of_mass(th)
            if np.isnan(cyf): cyf, cxf = gray.shape[0]//2, gray.shape[1]//2
            cx, cy = int(cxf), int(cyf)
            log.append(f"Center: Auto [{cx}, {cy}]")

        g1 = cv2.GaussianBlur(gray, (3,3), 0)
        g2 = cv2.GaussianBlur(gray, (51,51), 0)
        diff = cv2.subtract(g1, g2)
        
        coords = peak_local_max(diff, min_distance=10, threshold_abs=20)
        
        enhanced = np.log1p(diff.astype(np.float32))
        enhanced = cv2.normalize(enhanced, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)
        vis = cv2.cvtColor(enhanced, cv2.COLOR_GRAY2BGR)
        
        cv2.drawMarker(vis, (cx, cy), (0,0,255), cv2.MARKER_CROSS, 20, 2)
        peak_list = []
        for p in coords:
            py, px = p
            if abs(py-cy)<5 and abs(px-cx)<5: continue
            cv2.circle(vis, (px, py), 3, (0,255,0), 1)
            peak_list.append(f"[{px},{py}]")

        log.append(f"Spots: {len(peak_list)}")
        def to_b64(im): return base64.b64encode(cv2.imencode('.jpg', im)[1]).decode('utf-8')
        return {"type": "image", "raw_b64": to_b64(img_bgr), "proc_b64": to_b64(vis), "stats": {"center": [cx, cy], "spots": len(peak_list)}, "log": log}

    # --- [G] SAM ---
    @staticmethod
    def analyze_sam_results_nv7(img_rgb, masks, data_type):
        overlay = img_rgb.copy()
        mask_overlay = np.zeros_like(img_rgb)
        h_img, w_img = img_rgb.shape[:2]
        stats = {"count": 0}
        log = []
        
        valid_masks = []
        for ann in masks:
            area = ann['area']; bbox = ann['bbox']; x, y, w, h = [int(v) for v in bbox]
            if area > h_img * w_img * 0.95 or area < 20: continue 
            if data_type == "Particle":
                if x<=1 or y<=1 or (x+w)>=w_img-1 or (y+h)>=h_img-1: continue
                mask_uint8 = (ann['segmentation'] * 255).astype(np.uint8)
                cnts, _ = cv2.findContours(mask_uint8, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
                if cnts:
                    perim = cv2.arcLength(cnts[0], True)
                    circ = 4*np.pi*area/(perim**2) if perim>0 else 0
                    if circ < 0.25: continue
            valid_masks.append(ann)

        if data_type == "Thin Film":
            sorted_masks = sorted(valid_masks, key=lambda x: x['bbox'][1])
        else:
            sorted_masks = sorted(valid_masks, key=lambda x: x['area'], reverse=True)

        thickness_vals = []; diameters = []

        for i, ann in enumerate(sorted_masks):
            m = ann['segmentation']
            bbox = ann['bbox']; x, y, w, h = [int(v) for v in bbox]
            cx, cy = int(x + w/2), int(y + h/2)
            color = (0, 255, 0) 

            if data_type == "Thin Film":
                is_bound = (i == 0 or i == len(sorted_masks)-1)
                cv2.putText(overlay, f"#{i+1}", (cx, cy), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (255, 255, 255), 3)
                if not is_bound:
                    thk = ann['area'] / w
                    thickness_vals.append(thk)
                    color = (255, 0, 0) 
                else: color = (100, 100, 100)
                if i < len(sorted_masks) - 1:
                    ys, xs = np.where(m)
                    if len(xs) > 10:
                        roughness = np.std(ys)
                        cv2.line(overlay, (x, y+h), (x+w, y+h), (0, 255, 255), 2)
                        cv2.putText(overlay, f"Int#{i+1} R:{roughness:.2f}", (x+10, y+h-5), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)
            elif data_type == "Particle":
                equivalent_diameter = 2 * np.sqrt(ann['area'] / np.pi)
                diameters.append(equivalent_diameter)

            mask_overlay[m] = color
            mask_uint8 = (m * 255).astype(np.uint8)
            cnts, _ = cv2.findContours(mask_uint8, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
            if cnts: cv2.drawContours(overlay, [cnts[0]], -1, color, 1)

        alpha = 0.35
        mask_idx = np.any(mask_overlay > 0, axis=-1)
        overlay[mask_idx] = (overlay[mask_idx]*(1-alpha) + mask_overlay[mask_idx]*alpha).astype(np.uint8)

        stats["count"] = len(sorted_masks)
        if data_type == "Thin Film":
            avg = round(np.mean(thickness_vals), 2) if thickness_vals else 0
            stats["thickness_avg"] = avg
            log.append(f"Layers: {len(sorted_masks)}, Avg Thk: {avg}")
        elif data_type == "Particle":
            avg = round(np.mean(diameters), 2) if diameters else 0
            stats["diameter_avg"] = avg
            log.append(f"Particles: {len(sorted_masks)}")
        return overlay, stats, log

    # --- Router ---
    @staticmethod
    def process_image_nv8(img_bytes, equipment, data_type, mode, goal, user_center=None):
        nparr = np.frombuffer(img_bytes, np.uint8)
        img_raw = cv2.imdecode(nparr, cv2.IMREAD_COLOR)
        if img_raw is None: return {}
        
        body, footer = ScienceProcessorNV8.separate_footer(img_raw)
        footer_b64 = None
        if footer is not None:
            footer_b64 = base64.b64encode(cv2.imencode('.jpg', footer)[1]).decode('utf-8')

        # [NV8] Raw NMF Clustering
        if data_type == "Crystal Structure (FFT)":
            return {**ScienceProcessorNV8.analyze_crystal_nmf(body), "footer_b64": footer_b64}
            
        if data_type == "2D Diffraction":
            return {**ScienceProcessorNV8.analyze_diffraction_nv7(body, user_center), "footer_b64": footer_b64}
        
        if mode == "None":
             def to_b64(im): return base64.b64encode(cv2.imencode('.jpg', im)[1]).decode('utf-8')
             return {"type":"image", "raw_b64":to_b64(body), "proc_b64":to_b64(body), "stats":{}, "log":["Mode: Raw"], "footer_b64": footer_b64}

        img_proc = body.copy(); log = []
        if mode == "Auto" or mode == "AI-Adaptive":
            img_proc = cv2.bilateralFilter(img_proc, 9, 75, 75)
            log.append("Preproc: Denoise")

        img_rgb = cv2.cvtColor(img_proc, cv2.COLOR_BGR2RGB)
        
        if mask_generator:
            try:
                masks = mask_generator.generate(img_rgb)
                log.append(f"SAM: {len(masks)} Segments")
                overlay, stats, sam_log = ScienceProcessorNV8.analyze_sam_results_nv7(img_rgb, masks, data_type)
                log.extend(sam_log)
                overlay = cv2.cvtColor(overlay, cv2.COLOR_RGB2BGR)
            except Exception as e:
                log.append(f"SAM Error: {str(e)}")
                overlay = body; stats = {}
        else:
            log.append("SAM Not Loaded")
            overlay = body; stats = {}

        def to_b64(im): return base64.b64encode(cv2.imencode('.jpg', im)[1]).decode('utf-8')
        return {"type": "image", "raw_b64": to_b64(body), "proc_b64": to_b64(overlay), "stats": stats, "log": log, "footer_b64": footer_b64}

# [4] Helper
async def analyze_vision_ollama(image_bytes: bytes, prompt: str) -> str:
    try:
        b64 = base64.b64encode(image_bytes).decode('utf-8')
        res = await asyncio.to_thread(ollama_client.chat, model=VISION_MODEL, messages=[{'role':'user','content':prompt,'images':[b64]}])
        return res['message']['content']
    except: return "Vision Error"

async def interpret_spectrum_data(raw_context: str, equipment: str) -> str:
    try:
        prompt = f"Analyze spectrum ({equipment}). Brief interpretation in Korean. Data: {raw_context}"
        res = await asyncio.to_thread(ollama_client.chat, model=TEXT_MODEL, messages=[{'role':'user','content':prompt}])
        return res['message']['content']
    except: return ""

# [5] App
app = FastAPI(title="Analyst NV8 Raw NMF", lifespan=lifespan)
app.add_middleware(CORSMiddleware, allow_origins=["*"], allow_methods=["*"], allow_headers=["*"])

class TranslateReq(BaseModel): text: str

@app.post("/api/translate")
async def translate_text(req: TranslateReq):
    try:
        res = await asyncio.to_thread(ollama_client.chat, model=TEXT_MODEL, messages=[{'role':'system','content':'Translate to English.'}, {'role':'user','content':req.text}])
        return {"translated": res['message']['content']}
    except Exception as e: return {"translated": str(e)}

async def process_analysis_job(job_id: str, files_map: Dict, configs: Dict):
    try:
        JOBS[job_id]["status"] = "Processing"
        JOBS[job_id]["progress"] = 10
        final_results = []
        total_files = len(configs)
        processed_count = 0

        for filename, config in configs.items():
            if filename not in files_map: continue
            content = files_map[filename]
            eq = config.get("equipment", "General")
            data_type = config.get("data_type", "General")
            goal = config.get("goal", "")
            mode = config.get("mode", "Auto")
            
            user_center = None
            if config.get("center_coords"):
                try: user_center = [int(v) for v in config["center_coords"].split(",")]
                except: pass

            fname_lower = filename.lower()
            JOBS[job_id]["step"] = f"Analyzing {filename}..."
            
            if eq == "Document" or fname_lower.endswith(('.pdf', '.ppt', '.pptx')):
                try:
                    if fname_lower.endswith('.pdf'):
                        images = convert_from_bytes(content, dpi=150, fmt='jpeg')[:10]
                        full_txt = ""; pages = []
                        for idx, img in enumerate(images):
                            buf = io.BytesIO(); img.save(buf, format="JPEG")
                            b64 = base64.b64encode(buf.getvalue()).decode()
                            desc = await analyze_vision_ollama(buf.getvalue(), "한글 요약.")
                            full_txt += f"\nPage {idx+1}: {desc}"
                            pages.append({"page_num": idx+1, "image_b64": b64, "desc": desc})
                        final_results.append({"type": "pdf", "filename": filename, "equipment": "Lit", "raw_context": full_txt, "pages": pages})
                    elif fname_lower.endswith(('.ppt', '.pptx')):
                        prs = Presentation(io.BytesIO(content)); slides = []; full_txt = ""
                        for i, slide in enumerate(prs.slides):
                            txt = ""; imgs = []
                            for s in slide.shapes:
                                if hasattr(s, "text"): txt += s.text + "\n"
                                if s.shape_type == MSO_SHAPE_TYPE.PICTURE:
                                    try:
                                        ib64 = base64.b64encode(s.image.blob).decode('utf-8')
                                        desc = await analyze_vision_ollama(s.image.blob, "한글 설명.")
                                        imgs.append({"b64": ib64, "desc": desc})
                                    except: pass
                            slides.append({"slide_num": i+1, "text": txt, "images": imgs})
                        final_results.append({"type": "ppt", "filename": filename, "equipment": "Lit", "raw_context": full_txt, "slides": slides})
                except Exception as e: final_results.append({"type": "error", "filename": filename, "msg": str(e)})

            elif fname_lower.endswith(('.xlsx', '.xls', '.csv', '.txt')):
                try:
                    dfs = ScienceProcessorNV8.read_universal_dataframe(content, filename)
                    for i, df in enumerate(dfs):
                        v_blocks = ScienceProcessorNV8.detect_structured_blocks(df)
                        for v_block in v_blocks:
                            h_blocks = ScienceProcessorNV8.split_horizontal_blocks(v_block)
                            for h_block in h_blocks:
                                series_list = ScienceProcessorNV8.extract_series_from_df(h_block)
                                for s in series_list:
                                    res = ScienceProcessorNV8.process_spectrum(s['x'], s['y'], mode, goal)
                                    step = max(1, len(s['x'])//100)
                                    chart = [{"x": float(res["x"][k]), "y_proc": float(res["y_proc"][k]), "y_raw": float(res["y_raw"][k]), "y_base": float(res["y_base"][k])} for k in range(0, len(s['x']), step)]
                                    ctx = f"Data: {s['name']}\nStats: {res['stats']}"
                                    interp = await interpret_spectrum_data(ctx, eq)
                                    final_results.append({
                                        "type": "spectrum", "filename": f"{filename} ({s['name']})", "equipment": eq,
                                        "raw_context": f"{ctx}\nInterp: {interp}", 
                                        "chart_data": chart, "log": res["log"], "interpretation": interp
                                    })
                except Exception as ex: final_results.append({"type": "error", "filename": filename, "msg": str(ex)})

            elif fname_lower.endswith(('.png', '.jpg', '.jpeg', '.tif', '.bmp')):
                try:
                    JOBS[job_id]["step"] = f"Vision Analysis: {filename}..."
                    vis_res = await asyncio.to_thread(ScienceProcessorNV8.process_image_nv8, content, eq, data_type, mode, goal, user_center)
                    if vis_res:
                        scale_info = "Unknown"
                        if vis_res.get("footer_b64"):
                            JOBS[job_id]["step"] = f"Scale Reading..."
                            scale_text = await analyze_vision_ollama(base64.b64decode(vis_res["footer_b64"]), "Read scale bar text only.")
                            scale_info = scale_text
                            vis_res["log"].append(f"Scale: {scale_text}")

                        # [Fix] Richer Prompt for LLM using NMF Stats
                        stats_str = json.dumps(vis_res.get('stats',{}), indent=2)
                        desc_prompt = f"""
                        Analyze this {eq} image ({data_type}).
                        Quantitative Analysis (NMF Patterns): {stats_str}
                        Scale Bar: {scale_info}
                        Goal: {goal}
                        Please provide a detailed scientific interpretation in Korean, focusing on the distribution of the 10 identified patterns and what they might represent structurally.
                        """
                        desc = await analyze_vision_ollama(base64.b64decode(vis_res["proc_b64"]), desc_prompt)
                        
                        final_results.append({
                            "type": "image", "filename": filename, "equipment": eq, "summary": desc,
                            "raw_context": f"Img {eq}: {desc}", "raw_b64": vis_res["raw_b64"], "proc_b64": vis_res["proc_b64"], "footer_b64": vis_res.get("footer_b64"),
                            "stats": vis_res.get("stats",{}), "log": vis_res.get("log", [])
                        })
                except Exception as ex: final_results.append({"type": "error", "filename": filename, "msg": str(ex)})
            
            processed_count += 1
            JOBS[job_id]["progress"] = 10 + int((processed_count / total_files) * 80)

        JOBS[job_id]["step"] = "Writing Report..."
        final_results = sanitize_json(final_results)
        
        data_ctx = ""
        for r in final_results:
            if r.get("type") != "error":
                raw = r.get("raw_context") or ""
                data_ctx += f"\n=== {r['filename']} ===\n{raw[:1000]}\n"

        final_report = "Fail"
        if data_ctx:
            try:
                system_prompt = "You are a research assistant. Write a report in Korean. Summarize data and physics."
                res = await asyncio.to_thread(ollama_client.chat, model=TEXT_MODEL, messages=[{'role':'system','content':system_prompt}, {'role':'user','content':f"Data:\n{data_ctx}"}])
                final_report = res['message']['content']
            except Exception as e: final_report = str(e)

        JOBS[job_id]["results"] = {"results": final_results, "final_report": final_report}
        JOBS[job_id]["status"] = "Completed"
        JOBS[job_id]["progress"] = 100
    except Exception as e:
        JOBS[job_id]["status"] = "Failed"; JOBS[job_id]["error"] = str(e)

@app.post("/api/analyze")
async def start_analysis(background_tasks: BackgroundTasks, files: List[UploadFile] = File(...), file_configs: str = Form(...)):
    job_id = str(uuid.uuid4()); configs = json.loads(file_configs)
    files_map = {f.filename: await f.read() for f in files}
    JOBS[job_id] = {"status": "Pending", "progress": 0, "step": "Init", "results": None}
    background_tasks.add_task(process_analysis_job, job_id, files_map, configs)
    return {"job_id": job_id}

@app.get("/api/status/{job_id}")
async def get_status(job_id: str): return JOBS.get(job_id, {"status": "Not Found"})

@app.get("/", response_class=HTMLResponse)
async def serve_index():
    if os.path.exists("index.html"): return FileResponse("index.html")
    return "<h1>NV8 Raw NMF Backend</h1>"

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)




엔브이7 엔엠에프

import os
import io
# [0] OMP Error Fix & Torch First
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"
import torch

import asyncio
import base64
import json
import re
import math
import uuid
import time
from typing import List, Dict, Any, Optional
from pydantic import BaseModel
from contextlib import asynccontextmanager

# [1] Security
os.environ['NO_PROXY'] = '127.0.0.1,localhost,0.0.0.0,::1'
if 'HTTP_PROXY' in os.environ: del os.environ['HTTP_PROXY']
if 'HTTPS_PROXY' in os.environ: del os.environ['HTTPS_PROXY']

from fastapi import FastAPI, UploadFile, File, Form, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import HTMLResponse, FileResponse
import pandas as pd
import numpy as np

# SciPy & CV & Sklearn (NMF Added)
from sklearn.decomposition import NMF
from sklearn.preprocessing import MinMaxScaler
from scipy.signal import find_peaks, savgol_filter
from scipy.optimize import curve_fit
from scipy.ndimage import center_of_mass
from skimage.feature import peak_local_max
import cv2

# Models
from segment_anything import sam_model_registry, SamAutomaticMaskGenerator
from ollama import Client 
from pdf2image import convert_from_bytes
from pptx import Presentation
from pptx.enum.shapes import MSO_SHAPE_TYPE

# [2] Config
OLLAMA_HOST = "http://127.0.0.1:11434"
ollama_client = Client(host=OLLAMA_HOST)
VISION_MODEL = "qwen3-vl:30b-a3b-instruct" 
TEXT_MODEL = "llama3:70b"

SAM_CHECKPOINT = "sam_vit_b_01ec64.pth"
SAM_MODEL_TYPE = "vit_b"
device = "cuda" if torch.cuda.is_available() else "cpu"

mask_generator = None 
JOBS = {} 

@asynccontextmanager
async def lifespan(app: FastAPI):
    global mask_generator
    if os.path.exists(SAM_CHECKPOINT):
        print(f"Loading SAM ({SAM_MODEL_TYPE}) on {device}...")
        try:
            sam = sam_model_registry[SAM_MODEL_TYPE](checkpoint=SAM_CHECKPOINT)
            sam.to(device=device)
            mask_generator = SamAutomaticMaskGenerator(
                model=sam, points_per_side=32, pred_iou_thresh=0.86,
                stability_score_thresh=0.90, min_mask_region_area=50
            )
            print("SAM Loaded.")
        except: print("SAM Load Failed.")
    yield
    JOBS.clear()

def sanitize_json(obj):
    if isinstance(obj, float):
        if np.isnan(obj) or np.isinf(obj): return 0.0
        return obj
    elif isinstance(obj, dict): return {k: sanitize_json(v) for k, v in obj.items()}
    elif isinstance(obj, list): return [sanitize_json(v) for v in obj]
    elif isinstance(obj, (np.int64, np.int32)): return int(obj)
    elif isinstance(obj, (np.float64, np.float32)):
        if np.isnan(obj) or np.isinf(obj): return 0.0
        return float(obj)
    return obj

# ==========================================
# [3] NV7 NMF Engine
# ==========================================
class ScienceProcessorNV7:
    
    # --- Universal Loader ---
    @staticmethod
    def read_universal_dataframe(content: bytes, filename: str) -> List[pd.DataFrame]:
        dfs = []
        fname = filename.lower()
        if fname.endswith(('.xlsx', '.xls')):
            try:
                xls = pd.ExcelFile(io.BytesIO(content))
                for s in xls.sheet_names:
                    df = pd.read_excel(xls, sheet_name=s, header=None)
                    num = df.apply(pd.to_numeric, errors='coerce').notna().sum().sum()
                    if num < 5: continue
                    if not df.empty: dfs.append(df)
                if dfs: return dfs
            except: pass
        text_content = ""
        for enc in ['utf-8', 'cp949', 'latin1']:
            try: text_content = content.decode(enc); break
            except: continue
        if text_content:
            lines = text_content.splitlines()
            data_start = -1; delimiter = None
            separators = [',', '\t', ';', '\s+']
            for i, line in enumerate(lines[:100]):
                for sep in separators:
                    parts = line.split() if sep=='\s+' else line.split(sep)
                    nums = sum(1 for p in parts if re.match(r'^-?\d+(\.\d+)?(?:[eE][+-]?\d+)?$', p.strip()))
                    if nums >= 2: data_start = i; delimiter = sep; break
                if data_start != -1: break
            if data_start != -1:
                try:
                    data_str = "\n".join(lines[data_start:])
                    sep_arg = '\s+' if delimiter=='\s+' else delimiter
                    df = pd.read_csv(io.StringIO(data_str), sep=sep_arg, header=None, engine='python')
                    dfs.append(df)
                except: pass
        return dfs

    # --- Block Parser ---
    @staticmethod
    def detect_structured_blocks(df: pd.DataFrame) -> List[pd.DataFrame]:
        blocks = []
        if df.empty: return []
        df_num = df.apply(pd.to_numeric, errors='coerce')
        if df.shape[1] > 0: is_data_row = df_num.iloc[:, 0].notna()
        else: is_data_row = df_num.notna().any(axis=1)
        groups = (is_data_row != is_data_row.shift()).cumsum()
        for _, group in df.groupby(groups):
            if is_data_row.loc[group.index[0]]:
                block = df_num.loc[group.index].dropna(how='all', axis=1)
                if block.shape[0] >= 10: blocks.append(block.reset_index(drop=True))
        return blocks

    @staticmethod
    def split_horizontal_blocks(df: pd.DataFrame) -> List[pd.DataFrame]:
        sub_blocks = []
        if df.empty: return []
        col_counts = df.apply(pd.to_numeric, errors='coerce').notna().sum()
        valid = np.where(col_counts >= 2)[0]
        if len(valid) == 0: return []
        splits = np.where(np.diff(valid) > 1)[0] + 1
        groups = np.split(valid, splits)
        for g in groups:
            if len(g) > 0:
                sub = df.iloc[:, g].reset_index(drop=True)
                if sub.shape[1] >= 1 and sub.shape[0] >= 2: sub_blocks.append(sub)
        return sub_blocks

    @staticmethod
    def extract_series_from_df(df: pd.DataFrame) -> List[Dict]:
        series_list = []
        try:
            vals = df.values; rows, cols = vals.shape
            if rows < 5: return []
            if cols == 1:
                y = vals[:, 0]; mask = ~np.isnan(y)
                if np.sum(mask) > 5: series_list.append({"x": np.arange(len(y))[mask], "y": y[mask], "name": "Single"})
                return series_list
            def is_axis(arr):
                arr = arr[~np.isnan(arr)]
                if len(arr) < 5: return False
                diff = np.diff(arr)
                if np.all(diff >= 0) or np.all(diff <= 0): return True
                return (np.sum(diff>0)/len(diff)>0.9) or (np.sum(diff<0)/len(diff)>0.9)
            current_x = vals[:, 0]
            if not is_axis(current_x): current_x = np.arange(rows)
            for i in range(1, cols):
                col_data = vals[:, i]; mask_valid = ~np.isnan(col_data)
                if np.sum(mask_valid) < 5: continue
                if is_axis(col_data): current_x = col_data; continue 
                mask = ~np.isnan(current_x) & ~np.isnan(col_data)
                if np.sum(mask) > 5:
                    series_list.append({"x": current_x[mask], "y": col_data[mask], "name": f"Col-{i}"})
        except: pass
        return series_list

    # --- Spectrum Logic ---
    @staticmethod
    def simple_baseline(y):
        try:
            w = max(5, len(y)//10)
            s = pd.Series(y).rolling(window=w, center=True).min()
            b = s.bfill().ffill().values 
            if len(b) > 51: b = savgol_filter(b, 51, 3)
            return b, w
        except: return np.zeros_like(y), 0

    @staticmethod
    def process_spectrum(x, y, mode, goal):
        log = []; y_raw = y.copy(); base = np.zeros_like(y); y_proc = y.copy()
        peaks = []; fits = []; stats_txt = "Error"
        try:
            if mode == "None":
                peaks, _ = find_peaks(y, height=np.max(y)*0.05)
                log.append("Mode: Raw")
                stats_txt = f"Raw Peaks: {len(peaks)}"
            else:
                win = 15; do_fit = False
                if mode == "AI-Adaptive":
                    log.append("Mode: AI-Adaptive")
                    if "noise" in goal.lower(): win = 31
                    if "fit" in goal.lower(): do_fit = True
                
                base, bw = ScienceProcessorNV7.simple_baseline(y)
                y_proc = np.maximum(y - base, 0)
                log.append(f"Base: Rolling Min (w={bw})")
                
                if len(y_proc) > win:
                    y_proc = savgol_filter(y_proc, win, 3)
                    log.append(f"Smooth: SavGol (w={win})")
                
                peaks, _ = find_peaks(y_proc, height=np.max(y_proc)*0.05, distance=10)
                log.append(f"Peaks: {len(peaks)}")
                stats_txt = f"Range: {np.min(x):.1f}~{np.max(x):.1f}, Peaks: {len(peaks)}"
            return {"x": x, "y_raw": y_raw, "y_proc": y_proc, "y_base": base, "peaks": [{"x": float(x[p]), "y": float(y_proc[p])} for p in peaks], "log": log, "fits": fits, "stats": stats_txt}
        except Exception as e: return {"x": x, "y_raw": y_raw, "y_proc": y_proc, "y_base": base, "peaks": [], "log": [str(e)], "fits": [], "stats": "Error"}

    # --- Footer ---
    @staticmethod
    def separate_footer(img_bgr):
        h, w = img_bgr.shape[:2]
        roi_h = int(h * 0.82)
        roi = img_bgr[roi_h:, :]
        gray = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)
        sobel_y = np.abs(cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=3))
        row_sums = np.sum(sobel_y, axis=1)
        split_idx = np.argmax(row_sums)
        if row_sums[split_idx] > w * 30:
            real_split = roi_h + split_idx
            return img_bgr[:real_split, :], img_bgr[real_split:, :]
        return img_bgr, None

    # --- [E] NV7 NMF Crystal Engine ---
    @staticmethod
    def analyze_crystal_nmf(img_bgr):
        log = []
        gray = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2GRAY)
        h, w = gray.shape
        
        # 1. Feature Extraction
        patch_size = 32
        step = 16
        features = []
        coords = []
        
        log.append(f"NMF Analysis: Patch={patch_size}, Step={step}")
        
        for y in range(0, h - patch_size, step):
            for x in range(0, w - patch_size, step):
                roi = gray[y:y+patch_size, x:x+patch_size]
                
                # F1: Variance (Normalized)
                var = np.var(roi)
                
                # F2: FFT Peak (Crystallinity)
                f = np.fft.fft2(roi)
                fshift = np.fft.fftshift(f)
                mag = 20 * np.log(np.abs(fshift) + 1e-9)
                cy, cx = patch_size//2, patch_size//2
                mag[cy-2:cy+3, cx-2:cx+3] = 0 
                peak_val = np.max(mag)
                
                # F3: Entropy
                hist, _ = np.histogram(roi.ravel(), bins=256, range=(0, 256), density=True)
                ent = -np.sum(hist * np.log2(hist + 1e-7))
                
                features.append([var, peak_val, ent])
                coords.append((x, y))
        
        if len(features) < 10: return {"type":"error", "msg":"Img too small"}
        
        X = np.array(features)
        # NMF needs non-negative inputs
        scaler = MinMaxScaler()
        X_norm = scaler.fit_transform(X) + 0.001 # Avoid pure zero
        
        # 2. NMF (Components=3: Amor, Part, Cryst)
        nmf = NMF(n_components=3, init='random', random_state=42)
        W = nmf.fit_transform(X_norm) # Basis weights per sample
        H = nmf.components_         # Component characteristics
        
        # 3. Identify Components Automatically
        # H shape: (3, 3) -> (Component, Feature[Var, Peak, Ent])
        # Find which component has highest FFT Peak (Index 1) -> Crystal
        feat_fft_idx = 1
        comp_fft_strength = H[:, feat_fft_idx]
        
        # Sort indices by FFT strength
        sorted_indices = np.argsort(comp_fft_strength)
        
        # Map: Low FFT -> Amorphous, Mid -> Partial, High -> Crystal
        idx_amor = sorted_indices[0]
        idx_part = sorted_indices[1]
        idx_crys = sorted_indices[2]
        
        mapping = {idx_amor: "Amorphous", idx_part: "Partial", idx_crys: "Perfect"}
        colors = {
            "Amorphous": (100, 100, 100), # Gray
            "Partial": (0, 165, 255),     # Orange
            "Perfect": (255, 0, 0)        # Blue
        }
        
        # 4. Visualization & Stats
        overlay = img_bgr.copy()
        mask_layer = np.zeros_like(img_bgr)
        stats = {"Amorphous":0, "Partial":0, "Perfect":0}
        
        # Dominant Component Selection
        labels = np.argmax(W, axis=1)
        
        for idx, label in enumerate(labels):
            x, y = coords[idx]
            cat = mapping[label]
            stats[cat] += 1
            
            # Weighted Alpha based on confidence (W value)
            confidence = W[idx, label]
            
            cv2.rectangle(mask_layer, (x, y), (x+patch_size, y+patch_size), colors[cat], -1)
            
        alpha = 0.4
        mask_idx = np.any(mask_layer > 0, axis=-1)
        overlay[mask_idx] = (overlay[mask_idx]*(1-alpha) + mask_layer[mask_idx]*alpha).astype(np.uint8)
        
        # Rich Stats for LLM
        total = len(labels)
        ratio_crys = round(stats['Perfect']/total * 100, 1)
        ratio_part = round(stats['Partial']/total * 100, 1)
        ratio_amor = round(stats['Amorphous']/total * 100, 1)
        
        log.append(f"NMF Result: Crystal={ratio_crys}%, Partial={ratio_part}%, Amor={ratio_amor}%")
        
        # Detailed Stats Dictionary for LLM Prompt
        rich_stats = {
            "Total Regions": total,
            "Crystallinity": f"{ratio_crys}% (Perfect) + {ratio_part}% (Partial)",
            "Amorphous Ratio": f"{ratio_amor}%",
            "Dominant Phase": max(stats, key=stats.get)
        }
        
        def to_b64(im): return base64.b64encode(cv2.imencode('.jpg', im)[1]).decode('utf-8')
        return {"type": "image", "raw_b64": to_b64(img_bgr), "proc_b64": to_b64(overlay), "stats": rich_stats, "log": log}

    # --- [F] Diffraction ---
    @staticmethod
    def analyze_diffraction_nv7(img_bgr, user_center=None):
        log = []
        gray = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2GRAY)
        
        if user_center and len(user_center)==2 and user_center[0]>0:
            cx, cy = user_center
            log.append(f"Center: Manual [{cx}, {cy}]")
        else:
            _, th = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY+cv2.THRESH_OTSU)
            cyf, cxf = center_of_mass(th)
            if np.isnan(cyf): cyf, cxf = gray.shape[0]//2, gray.shape[1]//2
            cx, cy = int(cxf), int(cyf)
            log.append(f"Center: Auto [{cx}, {cy}]")

        g1 = cv2.GaussianBlur(gray, (3,3), 0)
        g2 = cv2.GaussianBlur(gray, (51,51), 0)
        diff = cv2.subtract(g1, g2)
        
        coords = peak_local_max(diff, min_distance=10, threshold_abs=20)
        
        enhanced = np.log1p(diff.astype(np.float32))
        enhanced = cv2.normalize(enhanced, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)
        vis = cv2.cvtColor(enhanced, cv2.COLOR_GRAY2BGR)
        
        cv2.drawMarker(vis, (cx, cy), (0,0,255), cv2.MARKER_CROSS, 20, 2)
        peak_list = []
        for p in coords:
            py, px = p
            if abs(py-cy)<5 and abs(px-cx)<5: continue
            cv2.circle(vis, (px, py), 3, (0,255,0), 1)
            peak_list.append(f"[{px},{py}]")

        log.append(f"Spots: {len(peak_list)}")
        def to_b64(im): return base64.b64encode(cv2.imencode('.jpg', im)[1]).decode('utf-8')
        return {"type": "image", "raw_b64": to_b64(img_bgr), "proc_b64": to_b64(vis), "stats": {"center": [cx, cy], "spots": len(peak_list)}, "log": log}

    # --- [G] SAM ---
    @staticmethod
    def analyze_sam_results_nv7(img_rgb, masks, data_type):
        overlay = img_rgb.copy()
        mask_overlay = np.zeros_like(img_rgb)
        h_img, w_img = img_rgb.shape[:2]
        stats = {"count": 0}
        log = []
        
        valid_masks = []
        for ann in masks:
            area = ann['area']; bbox = ann['bbox']; x, y, w, h = [int(v) for v in bbox]
            if area > h_img * w_img * 0.95 or area < 20: continue 
            if data_type == "Particle":
                if x<=1 or y<=1 or (x+w)>=w_img-1 or (y+h)>=h_img-1: continue
                mask_uint8 = (ann['segmentation'] * 255).astype(np.uint8)
                cnts, _ = cv2.findContours(mask_uint8, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
                if cnts:
                    perim = cv2.arcLength(cnts[0], True)
                    circ = 4*np.pi*area/(perim**2) if perim>0 else 0
                    if circ < 0.25: continue
            valid_masks.append(ann)

        if data_type == "Thin Film":
            sorted_masks = sorted(valid_masks, key=lambda x: x['bbox'][1])
        else:
            sorted_masks = sorted(valid_masks, key=lambda x: x['area'], reverse=True)

        thickness_vals = []; diameters = []

        for i, ann in enumerate(sorted_masks):
            m = ann['segmentation']
            bbox = ann['bbox']; x, y, w, h = [int(v) for v in bbox]
            cx, cy = int(x + w/2), int(y + h/2)
            color = (0, 255, 0) 

            if data_type == "Thin Film":
                is_bound = (i == 0 or i == len(sorted_masks)-1)
                cv2.putText(overlay, f"#{i+1}", (cx, cy), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (255, 255, 255), 3)
                if not is_bound:
                    thk = ann['area'] / w
                    thickness_vals.append(thk)
                    color = (255, 0, 0) 
                else: color = (100, 100, 100)
                if i < len(sorted_masks) - 1:
                    ys, xs = np.where(m)
                    if len(xs) > 10:
                        roughness = np.std(ys)
                        cv2.line(overlay, (x, y+h), (x+w, y+h), (0, 255, 255), 2)
                        cv2.putText(overlay, f"Int#{i+1} R:{roughness:.2f}", (x+10, y+h-5), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)
            elif data_type == "Particle":
                equivalent_diameter = 2 * np.sqrt(ann['area'] / np.pi)
                diameters.append(equivalent_diameter)

            mask_overlay[m] = color
            mask_uint8 = (m * 255).astype(np.uint8)
            cnts, _ = cv2.findContours(mask_uint8, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
            if cnts: cv2.drawContours(overlay, [cnts[0]], -1, color, 1)

        alpha = 0.35
        mask_idx = np.any(mask_overlay > 0, axis=-1)
        overlay[mask_idx] = (overlay[mask_idx]*(1-alpha) + mask_overlay[mask_idx]*alpha).astype(np.uint8)

        stats["count"] = len(sorted_masks)
        if data_type == "Thin Film":
            avg = round(np.mean(thickness_vals), 2) if thickness_vals else 0
            stats["thickness_avg"] = avg
            log.append(f"Layers: {len(sorted_masks)}, Avg Thk: {avg}")
        elif data_type == "Particle":
            avg = round(np.mean(diameters), 2) if diameters else 0
            stats["diameter_avg"] = avg
            log.append(f"Particles: {len(sorted_masks)}")
        return overlay, stats, log

    # --- Router ---
    @staticmethod
    def process_image_nv7(img_bytes, equipment, data_type, mode, goal, user_center=None):
        nparr = np.frombuffer(img_bytes, np.uint8)
        img_raw = cv2.imdecode(nparr, cv2.IMREAD_COLOR)
        if img_raw is None: return {}
        
        body, footer = ScienceProcessorNV7.separate_footer(img_raw)
        footer_b64 = None
        if footer is not None:
            footer_b64 = base64.b64encode(cv2.imencode('.jpg', footer)[1]).decode('utf-8')

        # [NV7] NMF Clustering
        if data_type == "Crystal Structure (FFT)":
            return {**ScienceProcessorNV7.analyze_crystal_nmf(body), "footer_b64": footer_b64}
            
        if data_type == "2D Diffraction":
            return {**ScienceProcessorNV7.analyze_diffraction_nv7(body, user_center), "footer_b64": footer_b64}
        
        if mode == "None":
             def to_b64(im): return base64.b64encode(cv2.imencode('.jpg', im)[1]).decode('utf-8')
             return {"type":"image", "raw_b64":to_b64(body), "proc_b64":to_b64(body), "stats":{}, "log":["Mode: Raw"], "footer_b64": footer_b64}

        img_proc = body.copy(); log = []
        if mode == "Auto" or mode == "AI-Adaptive":
            img_proc = cv2.bilateralFilter(img_proc, 9, 75, 75)
            log.append("Preproc: Denoise")

        img_rgb = cv2.cvtColor(img_proc, cv2.COLOR_BGR2RGB)
        
        if mask_generator:
            try:
                masks = mask_generator.generate(img_rgb)
                log.append(f"SAM: {len(masks)} Segments")
                overlay, stats, sam_log = ScienceProcessorNV7.analyze_sam_results_nv7(img_rgb, masks, data_type)
                log.extend(sam_log)
                overlay = cv2.cvtColor(overlay, cv2.COLOR_RGB2BGR)
            except Exception as e:
                log.append(f"SAM Error: {str(e)}")
                overlay = body; stats = {}
        else:
            log.append("SAM Not Loaded")
            overlay = body; stats = {}

        def to_b64(im): return base64.b64encode(cv2.imencode('.jpg', im)[1]).decode('utf-8')
        return {"type": "image", "raw_b64": to_b64(body), "proc_b64": to_b64(overlay), "stats": stats, "log": log, "footer_b64": footer_b64}

# [4] Helper
async def analyze_vision_ollama(image_bytes: bytes, prompt: str) -> str:
    try:
        b64 = base64.b64encode(image_bytes).decode('utf-8')
        res = await asyncio.to_thread(ollama_client.chat, model=VISION_MODEL, messages=[{'role':'user','content':prompt,'images':[b64]}])
        return res['message']['content']
    except: return "Vision Error"

async def interpret_spectrum_data(raw_context: str, equipment: str) -> str:
    try:
        prompt = f"Analyze spectrum ({equipment}). Brief interpretation in Korean. Data: {raw_context}"
        res = await asyncio.to_thread(ollama_client.chat, model=TEXT_MODEL, messages=[{'role':'user','content':prompt}])
        return res['message']['content']
    except: return ""

# [5] App
app = FastAPI(title="Analyst NV7 NMF", lifespan=lifespan)
app.add_middleware(CORSMiddleware, allow_origins=["*"], allow_methods=["*"], allow_headers=["*"])

class TranslateReq(BaseModel): text: str

@app.post("/api/translate")
async def translate_text(req: TranslateReq):
    try:
        res = await asyncio.to_thread(ollama_client.chat, model=TEXT_MODEL, messages=[{'role':'system','content':'Translate to English.'}, {'role':'user','content':req.text}])
        return {"translated": res['message']['content']}
    except Exception as e: return {"translated": str(e)}

async def process_analysis_job(job_id: str, files_map: Dict, configs: Dict):
    try:
        JOBS[job_id]["status"] = "Processing"
        JOBS[job_id]["progress"] = 10
        final_results = []
        total_files = len(configs)
        processed_count = 0

        for filename, config in configs.items():
            if filename not in files_map: continue
            content = files_map[filename]
            eq = config.get("equipment", "General")
            data_type = config.get("data_type", "General")
            goal = config.get("goal", "")
            mode = config.get("mode", "Auto")
            
            user_center = None
            if config.get("center_coords"):
                try: user_center = [int(v) for v in config["center_coords"].split(",")]
                except: pass

            fname_lower = filename.lower()
            JOBS[job_id]["step"] = f"Analyzing {filename}..."
            
            if eq == "Document" or fname_lower.endswith(('.pdf', '.ppt', '.pptx')):
                try:
                    if fname_lower.endswith('.pdf'):
                        images = convert_from_bytes(content, dpi=150, fmt='jpeg')[:10]
                        full_txt = ""; pages = []
                        for idx, img in enumerate(images):
                            buf = io.BytesIO(); img.save(buf, format="JPEG")
                            b64 = base64.b64encode(buf.getvalue()).decode()
                            desc = await analyze_vision_ollama(buf.getvalue(), "한글 요약.")
                            full_txt += f"\nPage {idx+1}: {desc}"
                            pages.append({"page_num": idx+1, "image_b64": b64, "desc": desc})
                        final_results.append({"type": "pdf", "filename": filename, "equipment": "Lit", "raw_context": full_txt, "pages": pages})
                    elif fname_lower.endswith(('.ppt', '.pptx')):
                        prs = Presentation(io.BytesIO(content)); slides = []; full_txt = ""
                        for i, slide in enumerate(prs.slides):
                            txt = ""; imgs = []
                            for s in slide.shapes:
                                if hasattr(s, "text"): txt += s.text + "\n"
                                if s.shape_type == MSO_SHAPE_TYPE.PICTURE:
                                    try:
                                        ib64 = base64.b64encode(s.image.blob).decode('utf-8')
                                        desc = await analyze_vision_ollama(s.image.blob, "한글 설명.")
                                        imgs.append({"b64": ib64, "desc": desc})
                                    except: pass
                            slides.append({"slide_num": i+1, "text": txt, "images": imgs})
                        final_results.append({"type": "ppt", "filename": filename, "equipment": "Lit", "raw_context": full_txt, "slides": slides})
                except Exception as e: final_results.append({"type": "error", "filename": filename, "msg": str(e)})

            elif fname_lower.endswith(('.xlsx', '.xls', '.csv', '.txt')):
                try:
                    dfs = ScienceProcessorNV7.read_universal_dataframe(content, filename)
                    for i, df in enumerate(dfs):
                        v_blocks = ScienceProcessorNV7.detect_structured_blocks(df)
                        for v_block in v_blocks:
                            h_blocks = ScienceProcessorNV7.split_horizontal_blocks(v_block)
                            for h_block in h_blocks:
                                series_list = ScienceProcessorNV7.extract_series_from_df(h_block)
                                for s in series_list:
                                    res = ScienceProcessorNV7.process_spectrum(s['x'], s['y'], mode, goal)
                                    step = max(1, len(s['x'])//100)
                                    chart = [{"x": float(res["x"][k]), "y_proc": float(res["y_proc"][k]), "y_raw": float(res["y_raw"][k]), "y_base": float(res["y_base"][k])} for k in range(0, len(s['x']), step)]
                                    ctx = f"Data: {s['name']}\nStats: {res['stats']}"
                                    interp = await interpret_spectrum_data(ctx, eq)
                                    final_results.append({
                                        "type": "spectrum", "filename": f"{filename} ({s['name']})", "equipment": eq,
                                        "raw_context": f"{ctx}\nInterp: {interp}", 
                                        "chart_data": chart, "log": res["log"], "interpretation": interp
                                    })
                except Exception as ex: final_results.append({"type": "error", "filename": filename, "msg": str(ex)})

            elif fname_lower.endswith(('.png', '.jpg', '.jpeg', '.tif', '.bmp')):
                try:
                    JOBS[job_id]["step"] = f"Vision Analysis: {filename}..."
                    vis_res = await asyncio.to_thread(ScienceProcessorNV7.process_image_nv7, content, eq, data_type, mode, goal, user_center)
                    if vis_res:
                        scale_info = "Unknown"
                        if vis_res.get("footer_b64"):
                            JOBS[job_id]["step"] = f"Scale Reading..."
                            scale_text = await analyze_vision_ollama(base64.b64decode(vis_res["footer_b64"]), "Read scale bar text only.")
                            scale_info = scale_text
                            vis_res["log"].append(f"Scale: {scale_text}")

                        # [Fix] Richer Prompt for LLM using NMF Stats
                        stats_str = json.dumps(vis_res.get('stats',{}), indent=2)
                        desc_prompt = f"""
                        Analyze this {eq} image ({data_type}).
                        Quantitative Analysis: {stats_str}
                        Scale Bar: {scale_info}
                        Goal: {goal}
                        Please provide a detailed scientific interpretation in Korean, focusing on the crystallinity ratio, dominant phases, and structural features based on the NMF statistics provided above.
                        """
                        desc = await analyze_vision_ollama(base64.b64decode(vis_res["proc_b64"]), desc_prompt)
                        
                        final_results.append({
                            "type": "image", "filename": filename, "equipment": eq, "summary": desc,
                            "raw_context": f"Img {eq}: {desc}", "raw_b64": vis_res["raw_b64"], "proc_b64": vis_res["proc_b64"], "footer_b64": vis_res.get("footer_b64"),
                            "stats": vis_res.get("stats",{}), "log": vis_res.get("log", [])
                        })
                except Exception as ex: final_results.append({"type": "error", "filename": filename, "msg": str(ex)})
            
            processed_count += 1
            JOBS[job_id]["progress"] = 10 + int((processed_count / total_files) * 80)

        JOBS[job_id]["step"] = "Writing Report..."
        final_results = sanitize_json(final_results)
        
        data_ctx = ""
        for r in final_results:
            if r.get("type") != "error":
                raw = r.get("raw_context") or ""
                data_ctx += f"\n=== {r['filename']} ===\n{raw[:1000]}\n"

        final_report = "Fail"
        if data_ctx:
            try:
                system_prompt = "You are a research assistant. Write a report in Korean. Summarize data and physics."
                res = await asyncio.to_thread(ollama_client.chat, model=TEXT_MODEL, messages=[{'role':'system','content':system_prompt}, {'role':'user','content':f"Data:\n{data_ctx}"}])
                final_report = res['message']['content']
            except Exception as e: final_report = str(e)

        JOBS[job_id]["results"] = {"results": final_results, "final_report": final_report}
        JOBS[job_id]["status"] = "Completed"
        JOBS[job_id]["progress"] = 100
    except Exception as e:
        JOBS[job_id]["status"] = "Failed"; JOBS[job_id]["error"] = str(e)

@app.post("/api/analyze")
async def start_analysis(background_tasks: BackgroundTasks, files: List[UploadFile] = File(...), file_configs: str = Form(...)):
    job_id = str(uuid.uuid4()); configs = json.loads(file_configs)
    files_map = {f.filename: await f.read() for f in files}
    JOBS[job_id] = {"status": "Pending", "progress": 0, "step": "Init", "results": None}
    background_tasks.add_task(process_analysis_job, job_id, files_map, configs)
    return {"job_id": job_id}

@app.get("/api/status/{job_id}")
async def get_status(job_id: str): return JOBS.get(job_id, {"status": "Not Found"})

@app.get("/", response_class=HTMLResponse)
async def serve_index():
    if os.path.exists("index.html"): return FileResponse("index.html")
    return "<h1>NV7 NMF Backend</h1>"

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)


엔브이6

import os
import io
import asyncio
import base64
import json
import re
import math
import uuid
import time
from typing import List, Dict, Any, Optional
from pydantic import BaseModel
from contextlib import asynccontextmanager

# [0] OMP Error Fix
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"

# [1] Security
os.environ['NO_PROXY'] = '127.0.0.1,localhost,0.0.0.0,::1'
if 'HTTP_PROXY' in os.environ: del os.environ['HTTP_PROXY']
if 'HTTPS_PROXY' in os.environ: del os.environ['HTTPS_PROXY']

from fastapi import FastAPI, UploadFile, File, Form, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import HTMLResponse, FileResponse
import pandas as pd
import numpy as np

# SciPy & CV
from scipy.signal import find_peaks, savgol_filter
from scipy.optimize import curve_fit
from scipy.ndimage import center_of_mass
from scipy.stats import entropy
from skimage.feature import peak_local_max, graycomatrix, graycoprops
from sklearn.cluster import KMeans
from sklearn.preprocessing import MinMaxScaler
import cv2
import torch

# SAM & LLM
from segment_anything import sam_model_registry, SamAutomaticMaskGenerator
from ollama import Client 
from pdf2image import convert_from_bytes
from pptx import Presentation
from pptx.enum.shapes import MSO_SHAPE_TYPE

# [2] Config
OLLAMA_HOST = "http://127.0.0.1:11434"
ollama_client = Client(host=OLLAMA_HOST)
VISION_MODEL = "qwen3-vl:30b-a3b-instruct" 
TEXT_MODEL = "llama3:70b"

SAM_CHECKPOINT = "sam_vit_b_01ec64.pth"
SAM_MODEL_TYPE = "vit_b"
device = "cuda" if torch.cuda.is_available() else "cpu"

mask_generator = None 
JOBS = {} 

@asynccontextmanager
async def lifespan(app: FastAPI):
    global mask_generator
    if os.path.exists(SAM_CHECKPOINT):
        print(f"Loading SAM...")
        try:
            sam = sam_model_registry[SAM_MODEL_TYPE](checkpoint=SAM_CHECKPOINT)
            sam.to(device=device)
            mask_generator = SamAutomaticMaskGenerator(
                model=sam, points_per_side=32, pred_iou_thresh=0.86,
                stability_score_thresh=0.90, min_mask_region_area=50
            )
            print("SAM Loaded.")
        except: print("SAM Load Failed.")
    yield
    JOBS.clear()

def sanitize_json(obj):
    if isinstance(obj, float):
        if np.isnan(obj) or np.isinf(obj): return 0.0
        return obj
    elif isinstance(obj, dict): return {k: sanitize_json(v) for k, v in obj.items()}
    elif isinstance(obj, list): return [sanitize_json(v) for v in obj]
    elif isinstance(obj, (np.int64, np.int32)): return int(obj)
    elif isinstance(obj, (np.float64, np.float32)):
        if np.isnan(obj) or np.isinf(obj): return 0.0
        return float(obj)
    return obj

# ==========================================
# [3] NV6: Texture Clustering Engine
# ==========================================
class ScienceProcessorNV6:
    
    # --- [A] Universal Loader ---
    @staticmethod
    def read_universal_dataframe(content: bytes, filename: str) -> List[pd.DataFrame]:
        dfs = []
        fname = filename.lower()
        if fname.endswith(('.xlsx', '.xls')):
            try:
                xls = pd.ExcelFile(io.BytesIO(content))
                for s in xls.sheet_names:
                    df = pd.read_excel(xls, sheet_name=s, header=None)
                    if df.apply(pd.to_numeric, errors='coerce').notna().sum().sum() < 5: continue
                    if not df.empty: dfs.append(df)
                if dfs: return dfs
            except: pass
        text_content = ""
        for enc in ['utf-8', 'cp949', 'latin1']:
            try: text_content = content.decode(enc); break
            except: continue
        if text_content:
            lines = text_content.splitlines()
            data_start = -1; delimiter = None
            separators = [',', '\t', ';', '\s+']
            for i, line in enumerate(lines[:100]):
                for sep in separators:
                    parts = line.split() if sep=='\s+' else line.split(sep)
                    nums = sum(1 for p in parts if re.match(r'^-?\d+(\.\d+)?(?:[eE][+-]?\d+)?$', p.strip()))
                    if nums >= 2: data_start = i; delimiter = sep; break
                if data_start != -1: break
            if data_start != -1:
                try:
                    data_str = "\n".join(lines[data_start:])
                    sep_arg = '\s+' if delimiter=='\s+' else delimiter
                    df = pd.read_csv(io.StringIO(data_str), sep=sep_arg, header=None, engine='python')
                    dfs.append(df)
                except: pass
        return dfs

    # --- [B] Block Parser ---
    @staticmethod
    def detect_structured_blocks(df: pd.DataFrame) -> List[pd.DataFrame]:
        blocks = []
        if df.empty: return []
        df_num = df.apply(pd.to_numeric, errors='coerce')
        if df.shape[1] > 0: is_data_row = df_num.iloc[:, 0].notna()
        else: is_data_row = df_num.notna().any(axis=1)
        groups = (is_data_row != is_data_row.shift()).cumsum()
        for _, group in df.groupby(groups):
            if is_data_row.loc[group.index[0]]:
                block = df_num.loc[group.index].dropna(how='all', axis=1)
                if block.shape[0] >= 10: blocks.append(block.reset_index(drop=True))
        return blocks

    @staticmethod
    def split_horizontal_blocks(df: pd.DataFrame) -> List[pd.DataFrame]:
        sub_blocks = []
        if df.empty: return []
        col_counts = df.apply(pd.to_numeric, errors='coerce').notna().sum()
        valid = np.where(col_counts >= 2)[0]
        if len(valid) == 0: return []
        splits = np.where(np.diff(valid) > 1)[0] + 1
        groups = np.split(valid, splits)
        for g in groups:
            if len(g) > 0:
                sub = df.iloc[:, g].reset_index(drop=True)
                if sub.shape[1] >= 1 and sub.shape[0] >= 2: sub_blocks.append(sub)
        return sub_blocks

    @staticmethod
    def extract_series_from_df(df: pd.DataFrame) -> List[Dict]:
        series_list = []
        try:
            vals = df.values; rows, cols = vals.shape
            if rows < 5: return []
            if cols == 1:
                y = vals[:, 0]; mask = ~np.isnan(y)
                if np.sum(mask) > 5: series_list.append({"x": np.arange(len(y))[mask], "y": y[mask], "name": "Single"})
                return series_list
            def is_axis(arr):
                arr = arr[~np.isnan(arr)]
                if len(arr) < 5: return False
                diff = np.diff(arr)
                if np.all(diff >= 0) or np.all(diff <= 0): return True
                return (np.sum(diff>0)/len(diff)>0.9) or (np.sum(diff<0)/len(diff)>0.9)
            current_x = vals[:, 0]
            if not is_axis(current_x): current_x = np.arange(rows)
            for i in range(1, cols):
                col_data = vals[:, i]; mask_valid = ~np.isnan(col_data)
                if np.sum(mask_valid) < 5: continue
                if is_axis(col_data): current_x = col_data; continue 
                mask = ~np.isnan(current_x) & ~np.isnan(col_data)
                if np.sum(mask) > 5:
                    series_list.append({"x": current_x[mask], "y": col_data[mask], "name": f"Col-{i}"})
        except: pass
        return series_list

    # --- [C] Spectrum ---
    @staticmethod
    def simple_baseline(y):
        try:
            w = max(5, len(y)//10)
            s = pd.Series(y).rolling(window=w, center=True).min()
            b = s.bfill().ffill().values 
            if len(b) > 51: b = savgol_filter(b, 51, 3)
            return b, w
        except: return np.zeros_like(y), 0

    @staticmethod
    def process_spectrum(x, y, mode, goal):
        log = []; y_raw = y.copy(); base = np.zeros_like(y); y_proc = y.copy()
        peaks = []; fits = []; stats_txt = "Error"
        try:
            if mode == "None":
                peaks, _ = find_peaks(y, height=np.max(y)*0.05)
                log.append("Mode: Raw")
                stats_txt = f"Raw Peaks: {len(peaks)}"
            else:
                win = 15; do_fit = False
                if mode == "AI-Adaptive":
                    log.append("Mode: AI-Adaptive")
                    if "noise" in goal.lower(): win = 31
                    if "fit" in goal.lower(): do_fit = True
                else: log.append("Mode: Auto")
                base, bw = ScienceProcessorNV6.simple_baseline(y)
                y_proc = np.maximum(y - base, 0)
                log.append(f"Base: Rolling Min (w={bw})")
                if len(y_proc) > win:
                    y_proc = savgol_filter(y_proc, win, 3)
                    log.append(f"Smooth: SavGol (w={win})")
                peaks, _ = find_peaks(y_proc, height=np.max(y_proc)*0.05, distance=10)
                log.append(f"Peaks: {len(peaks)}")
                stats_txt = f"Range: {np.min(x):.1f}~{np.max(x):.1f}, Peaks: {len(peaks)}"
            return {"x": x, "y_raw": y_raw, "y_proc": y_proc, "y_base": base, "peaks": [{"x": float(x[p]), "y": float(y_proc[p])} for p in peaks], "log": log, "fits": fits, "stats": stats_txt}
        except Exception as e: return {"x": x, "y_raw": y_raw, "y_proc": y_proc, "y_base": base, "peaks": [], "log": [str(e)], "fits": [], "stats": "Error"}

    # --- [D] Footer ---
    @staticmethod
    def separate_footer(img_bgr):
        h, w = img_bgr.shape[:2]
        roi_h = int(h * 0.82)
        roi = img_bgr[roi_h:, :]
        gray = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)
        sobel_y = np.abs(cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=3))
        row_sums = np.sum(sobel_y, axis=1)
        split_idx = np.argmax(row_sums)
        if row_sums[split_idx] > w * 30:
            real_split = roi_h + split_idx
            return img_bgr[:real_split, :], img_bgr[real_split:, :]
        return img_bgr, None

    # --- [E] NV6 Crystal Clustering (K-Means) ---
    @staticmethod
    def analyze_crystal_clustering(img_bgr):
        log = []
        gray = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2GRAY)
        h, w = gray.shape
        
        # 1. Feature Extraction (Patch-based)
        patch_size = 32
        step = 16
        features = []
        coords = []
        
        log.append(f"Clustering: Patch={patch_size}, Step={step}")
        
        for y in range(0, h - patch_size, step):
            for x in range(0, w - patch_size, step):
                roi = gray[y:y+patch_size, x:x+patch_size]
                
                # F1: Variance (Texture roughness)
                var = np.var(roi)
                
                # F2: FFT Peak Strength (Crystallinity)
                f = np.fft.fft2(roi)
                fshift = np.fft.fftshift(f)
                mag = 20 * np.log(np.abs(fshift) + 1e-9)
                cy, cx = patch_size//2, patch_size//2
                mag[cy-2:cy+3, cx-2:cx+3] = 0 # Block DC
                peak_val = np.max(mag)
                
                # F3: Entropy (Complexity)
                # Simple histogram entropy
                hist, _ = np.histogram(roi.ravel(), bins=256, range=(0, 256), density=True)
                ent = -np.sum(hist * np.log2(hist + 1e-7))
                
                features.append([var, peak_val, ent])
                coords.append((x, y))
        
        # 2. K-Means Clustering (k=4)
        if len(features) < 10: return {"type":"error", "msg":"Img too small"}
        
        X = np.array(features)
        scaler = MinMaxScaler()
        X_norm = scaler.fit_transform(X)
        
        kmeans = KMeans(n_clusters=4, random_state=42, n_init=10)
        labels = kmeans.fit_predict(X_norm)
        centers = kmeans.cluster_centers_
        
        # 3. Auto-Labeling Logic
        # Cluster with Max FFT Peak = Perfect Crystal
        # Cluster with Max Variance/Entropy (but not Crystal) = Partial
        # Lowest Variance = Amorphous? Or Boundary?
        
        # Sort clusters by FFT Peak strength
        sorted_indices = np.argsort(centers[:, 1]) # Index 1 is FFT Peak
        
        # Mapping based on sorted Feature Strength (Heuristic)
        # 0 (Lowest FFT) -> Boundary/Amorphous
        # 3 (Highest FFT) -> Perfect Crystal
        
        label_map = {
            sorted_indices[3]: "Perfect Crystal",
            sorted_indices[2]: "Partial Crystal",
            sorted_indices[1]: "Amorphous",
            sorted_indices[0]: "Boundary"
        }
        
        color_map = {
            "Perfect Crystal": (255, 0, 0),    # Blue
            "Partial Crystal": (0, 165, 255),  # Orange
            "Amorphous": (128, 128, 128),      # Gray
            "Boundary": (0, 255, 255)          # Yellow
        }
        
        # 4. Visualization
        overlay = img_bgr.copy()
        mask_layer = np.zeros_like(img_bgr)
        
        stats = {"Perfect Crystal":0, "Partial Crystal":0, "Amorphous":0, "Boundary":0}
        
        for idx, label in enumerate(labels):
            x, y = coords[idx]
            category = label_map[label]
            stats[category] += 1
            
            color = color_map[category]
            # Draw semi-transparent box
            cv2.rectangle(mask_layer, (x, y), (x+patch_size, y+patch_size), color, -1)
            
        # Blend
        alpha = 0.4
        mask_idx = np.any(mask_layer > 0, axis=-1)
        overlay[mask_idx] = (overlay[mask_idx]*(1-alpha) + mask_layer[mask_idx]*alpha).astype(np.uint8)
        
        # Add Legend text
        log.append(f"Found {stats['Perfect Crystal']} Crystal Blocks (Blue)")
        log.append(f"Found {stats['Partial Crystal']} Partial Blocks (Orange)")
        log.append(f"Found {stats['Amorphous']} Amorphous Blocks (Gray)")
        log.append(f"Found {stats['Boundary']} Boundary Blocks (Yellow)")
        
        def to_b64(im): return base64.b64encode(cv2.imencode('.jpg', im)[1]).decode('utf-8')
        return {"type": "image", "raw_b64": to_b64(img_bgr), "proc_b64": to_b64(overlay), "stats": stats, "log": log}

    # --- [F] Diffraction ---
    @staticmethod
    def analyze_diffraction_nv6(img_bgr, user_center=None):
        log = []
        stats = {}
        gray = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2GRAY)
        
        if user_center and len(user_center)==2 and user_center[0]>0:
            cx, cy = user_center
            log.append(f"Center: Manual [{cx}, {cy}]")
        else:
            _, th = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY+cv2.THRESH_OTSU)
            cyf, cxf = center_of_mass(th)
            if np.isnan(cyf): cyf, cxf = gray.shape[0]//2, gray.shape[1]//2
            cx, cy = int(cxf), int(cyf)
            log.append(f"Center: Auto [{cx}, {cy}]")

        g1 = cv2.GaussianBlur(gray, (3,3), 0)
        g2 = cv2.GaussianBlur(gray, (51,51), 0)
        diff = cv2.subtract(g1, g2)
        
        coords = peak_local_max(diff, min_distance=10, threshold_abs=20)
        
        enhanced = np.log1p(diff.astype(np.float32))
        enhanced = cv2.normalize(enhanced, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)
        vis = cv2.cvtColor(enhanced, cv2.COLOR_GRAY2BGR)
        
        cv2.drawMarker(vis, (cx, cy), (0,0,255), cv2.MARKER_CROSS, 20, 2)
        peak_list = []
        for p in coords:
            py, px = p
            if abs(py-cy)<5 and abs(px-cx)<5: continue
            cv2.circle(vis, (px, py), 3, (0,255,0), 1)
            peak_list.append(f"[{px},{py}]")

        log.append(f"Spots: {len(peak_list)}")
        def to_b64(im): return base64.b64encode(cv2.imencode('.jpg', im)[1]).decode('utf-8')
        return {"type": "image", "raw_b64": to_b64(img_bgr), "proc_b64": to_b64(vis), "stats": {"center": [cx, cy], "spots": len(peak_list)}, "log": log}

    # --- [G] SAM ---
    @staticmethod
    def analyze_sam_results_nv6(img_rgb, masks, data_type):
        overlay = img_rgb.copy()
        mask_overlay = np.zeros_like(img_rgb)
        h_img, w_img = img_rgb.shape[:2]
        stats = {"count": 0}
        log = []
        
        valid_masks = []
        for ann in masks:
            area = ann['area']; bbox = ann['bbox']; x, y, w, h = [int(v) for v in bbox]
            if area > h_img * w_img * 0.95 or area < 20: continue 
            if data_type == "Particle":
                # Strict Border Kill
                if x<=1 or y<=1 or (x+w)>=w_img-1 or (y+h)>=h_img-1: continue
                # Circularity Check (0.25+)
                mask_uint8 = (ann['segmentation'] * 255).astype(np.uint8)
                cnts, _ = cv2.findContours(mask_uint8, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
                if cnts:
                    perim = cv2.arcLength(cnts[0], True)
                    circ = 4*np.pi*area/(perim**2) if perim>0 else 0
                    if circ < 0.25: continue
            valid_masks.append(ann)

        if data_type == "Thin Film":
            sorted_masks = sorted(valid_masks, key=lambda x: x['bbox'][1])
        else:
            sorted_masks = sorted(valid_masks, key=lambda x: x['area'], reverse=True)

        thickness_vals = []; diameters = []

        for i, ann in enumerate(sorted_masks):
            m = ann['segmentation']
            bbox = ann['bbox']; x, y, w, h = [int(v) for v in bbox]
            cx, cy = int(x + w/2), int(y + h/2)
            color = (0, 255, 0) 

            if data_type == "Thin Film":
                is_bound = (i == 0 or i == len(sorted_masks)-1)
                cv2.putText(overlay, f"#{i+1}", (cx, cy), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (255, 255, 255), 3)
                
                if not is_bound:
                    thk = ann['area'] / w
                    thickness_vals.append(thk)
                    color = (255, 0, 0) 
                else: color = (100, 100, 100)

                if i < len(sorted_masks) - 1:
                    ys, xs = np.where(m)
                    if len(xs) > 10:
                        roughness = np.std(ys)
                        cv2.line(overlay, (x, y+h), (x+w, y+h), (0, 255, 255), 2)
                        cv2.putText(overlay, f"Int#{i+1} R:{roughness:.2f}", (x+10, y+h-5), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)

            elif data_type == "Particle":
                equivalent_diameter = 2 * np.sqrt(ann['area'] / np.pi)
                diameters.append(equivalent_diameter)

            mask_overlay[m] = color
            mask_uint8 = (m * 255).astype(np.uint8)
            cnts, _ = cv2.findContours(mask_uint8, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
            if cnts: cv2.drawContours(overlay, [cnts[0]], -1, color, 1)

        alpha = 0.35
        mask_idx = np.any(mask_overlay > 0, axis=-1)
        overlay[mask_idx] = (overlay[mask_idx]*(1-alpha) + mask_overlay[mask_idx]*alpha).astype(np.uint8)

        stats["count"] = len(sorted_masks)
        if data_type == "Thin Film":
            avg = round(np.mean(thickness_vals), 2) if thickness_vals else 0
            stats["thickness_avg"] = avg
            log.append(f"Layers: {len(sorted_masks)}, Avg Thk: {avg}")
        elif data_type == "Particle":
            avg = round(np.mean(diameters), 2) if diameters else 0
            stats["diameter_avg"] = avg
            log.append(f"Particles: {len(sorted_masks)}")
        
        return overlay, stats, log

    @staticmethod
    def process_image_nv6(img_bytes, equipment, data_type, mode, goal, user_center=None):
        nparr = np.frombuffer(img_bytes, np.uint8)
        img_raw = cv2.imdecode(nparr, cv2.IMREAD_COLOR)
        if img_raw is None: return {}
        
        body, footer = ScienceProcessorNV6.separate_footer(img_raw)
        footer_b64 = None
        if footer is not None:
            footer_b64 = base64.b64encode(cv2.imencode('.jpg', footer)[1]).decode('utf-8')

        # [NV6] K-Means Clustering
        if data_type == "Crystal Structure (FFT)":
            return {**ScienceProcessorNV6.analyze_crystal_clustering(body), "footer_b64": footer_b64}
            
        if data_type == "2D Diffraction":
            return {**ScienceProcessorNV6.analyze_diffraction_nv6(body, user_center), "footer_b64": footer_b64}
        
        if mode == "None":
             def to_b64(im): return base64.b64encode(cv2.imencode('.jpg', im)[1]).decode('utf-8')
             return {"type":"image", "raw_b64":to_b64(body), "proc_b64":to_b64(body), "stats":{}, "log":["Mode: Raw"], "footer_b64": footer_b64}

        img_proc = body.copy(); log = []
        if mode == "Auto" or mode == "AI-Adaptive":
            img_proc = cv2.bilateralFilter(img_proc, 9, 75, 75)
            log.append("Preproc: Denoise")

        img_rgb = cv2.cvtColor(img_proc, cv2.COLOR_BGR2RGB)
        
        if mask_generator:
            try:
                masks = mask_generator.generate(img_rgb)
                log.append(f"SAM: {len(masks)} Segments")
                overlay, stats, sam_log = ScienceProcessorNV6.analyze_sam_results_nv6(img_rgb, masks, data_type)
                log.extend(sam_log)
                overlay = cv2.cvtColor(overlay, cv2.COLOR_RGB2BGR)
            except Exception as e:
                log.append(f"SAM Error: {str(e)}")
                overlay = body; stats = {}
        else:
            log.append("SAM Not Loaded")
            overlay = body; stats = {}

        def to_b64(im): return base64.b64encode(cv2.imencode('.jpg', im)[1]).decode('utf-8')
        return {"type": "image", "raw_b64": to_b64(body), "proc_b64": to_b64(overlay), "stats": stats, "log": log, "footer_b64": footer_b64}

# [4] Helper
async def analyze_vision_ollama(image_bytes: bytes, prompt: str) -> str:
    try:
        b64 = base64.b64encode(image_bytes).decode('utf-8')
        res = await asyncio.to_thread(ollama_client.chat, model=VISION_MODEL, messages=[{'role':'user','content':prompt,'images':[b64]}])
        return res['message']['content']
    except: return "Vision Error"

async def interpret_spectrum_data(raw_context: str, equipment: str) -> str:
    try:
        prompt = f"Analyze spectrum ({equipment}). Brief interpretation in Korean. Data: {raw_context}"
        res = await asyncio.to_thread(ollama_client.chat, model=TEXT_MODEL, messages=[{'role':'user','content':prompt}])
        return res['message']['content']
    except: return ""

# [5] App
app = FastAPI(title="Analyst NV6 Clustering", lifespan=lifespan)
app.add_middleware(CORSMiddleware, allow_origins=["*"], allow_methods=["*"], allow_headers=["*"])

class TranslateReq(BaseModel): text: str

@app.post("/api/translate")
async def translate_text(req: TranslateReq):
    try:
        res = await asyncio.to_thread(ollama_client.chat, model=TEXT_MODEL, messages=[{'role':'system','content':'Translate to English.'}, {'role':'user','content':req.text}])
        return {"translated": res['message']['content']}
    except Exception as e: return {"translated": str(e)}

async def process_analysis_job(job_id: str, files_map: Dict, configs: Dict):
    try:
        JOBS[job_id]["status"] = "Processing"
        JOBS[job_id]["progress"] = 10
        final_results = []
        total_files = len(configs)
        processed_count = 0

        for filename, config in configs.items():
            if filename not in files_map: continue
            content = files_map[filename]
            eq = config.get("equipment", "General")
            data_type = config.get("data_type", "General")
            goal = config.get("goal", "")
            mode = config.get("mode", "Auto")
            
            user_center = None
            if config.get("center_coords"):
                try: user_center = [int(v) for v in config["center_coords"].split(",")]
                except: pass

            fname_lower = filename.lower()
            JOBS[job_id]["step"] = f"Analyzing {filename}..."
            
            if eq == "Document" or fname_lower.endswith(('.pdf', '.ppt', '.pptx')):
                try:
                    if fname_lower.endswith('.pdf'):
                        images = convert_from_bytes(content, dpi=150, fmt='jpeg')[:10]
                        full_txt = ""; pages = []
                        for idx, img in enumerate(images):
                            buf = io.BytesIO(); img.save(buf, format="JPEG")
                            b64 = base64.b64encode(buf.getvalue()).decode()
                            desc = await analyze_vision_ollama(buf.getvalue(), "한글 요약.")
                            full_txt += f"\nPage {idx+1}: {desc}"
                            pages.append({"page_num": idx+1, "image_b64": b64, "desc": desc})
                        final_results.append({"type": "pdf", "filename": filename, "equipment": "Lit", "raw_context": full_txt, "pages": pages})
                    elif fname_lower.endswith(('.ppt', '.pptx')):
                        prs = Presentation(io.BytesIO(content)); slides = []; full_txt = ""
                        for i, slide in enumerate(prs.slides):
                            txt = ""; imgs = []
                            for s in slide.shapes:
                                if hasattr(s, "text"): txt += s.text + "\n"
                                if s.shape_type == MSO_SHAPE_TYPE.PICTURE:
                                    try:
                                        ib64 = base64.b64encode(s.image.blob).decode('utf-8')
                                        desc = await analyze_vision_ollama(s.image.blob, "한글 설명.")
                                        imgs.append({"b64": ib64, "desc": desc})
                                    except: pass
                            slides.append({"slide_num": i+1, "text": txt, "images": imgs})
                        final_results.append({"type": "ppt", "filename": filename, "equipment": "Lit", "raw_context": full_txt, "slides": slides})
                except Exception as e: final_results.append({"type": "error", "filename": filename, "msg": str(e)})

            elif fname_lower.endswith(('.xlsx', '.xls', '.csv', '.txt')):
                try:
                    dfs = ScienceProcessorNV6.read_universal_dataframe(content, filename)
                    for i, df in enumerate(dfs):
                        v_blocks = ScienceProcessorNV6.detect_structured_blocks(df)
                        for v_block in v_blocks:
                            h_blocks = ScienceProcessorNV6.split_horizontal_blocks(v_block)
                            for h_block in h_blocks:
                                series_list = ScienceProcessorNV6.extract_series_from_df(h_block)
                                for s in series_list:
                                    res = ScienceProcessorNV6.process_spectrum(s['x'], s['y'], mode, goal)
                                    step = max(1, len(s['x'])//100)
                                    chart = [{"x": float(res["x"][k]), "y_proc": float(res["y_proc"][k]), "y_raw": float(res["y_raw"][k]), "y_base": float(res["y_base"][k])} for k in range(0, len(s['x']), step)]
                                    ctx = f"Data: {s['name']}\nStats: {res['stats']}"
                                    interp = await interpret_spectrum_data(ctx, eq)
                                    final_results.append({
                                        "type": "spectrum", "filename": f"{filename} ({s['name']})", "equipment": eq,
                                        "raw_context": f"{ctx}\nInterp: {interp}", 
                                        "chart_data": chart, "log": res["log"], "interpretation": interp
                                    })
                except Exception as ex: final_results.append({"type": "error", "filename": filename, "msg": str(ex)})

            elif fname_lower.endswith(('.png', '.jpg', '.jpeg', '.tif', '.bmp')):
                try:
                    JOBS[job_id]["step"] = f"Vision Analysis: {filename}..."
                    vis_res = await asyncio.to_thread(ScienceProcessorNV6.process_image_nv6, content, eq, data_type, mode, goal, user_center)
                    if vis_res:
                        scale_info = "Unknown"
                        if vis_res.get("footer_b64"):
                            JOBS[job_id]["step"] = f"Scale Reading..."
                            scale_text = await analyze_vision_ollama(base64.b64decode(vis_res["footer_b64"]), "Read scale bar text only.")
                            scale_info = scale_text
                            vis_res["log"].append(f"Scale: {scale_text}")

                        desc = await analyze_vision_ollama(base64.b64decode(vis_res["proc_b64"]), f"Summary in Korean. {eq}. {data_type}. Stats: {vis_res.get('stats',{})}")
                        final_results.append({
                            "type": "image", "filename": filename, "equipment": eq, "summary": desc,
                            "raw_context": f"Img {eq}: {desc}", "raw_b64": vis_res["raw_b64"], "proc_b64": vis_res["proc_b64"], "footer_b64": vis_res.get("footer_b64"),
                            "stats": vis_res.get("stats",{}), "log": vis_res.get("log", [])
                        })
                except Exception as ex: final_results.append({"type": "error", "filename": filename, "msg": str(ex)})
            
            processed_count += 1
            JOBS[job_id]["progress"] = 10 + int((processed_count / total_files) * 80)

        JOBS[job_id]["step"] = "Writing Report..."
        final_results = sanitize_json(final_results)
        
        data_ctx = ""
        for r in final_results:
            if r.get("type") != "error":
                raw = r.get("raw_context") or ""
                data_ctx += f"\n=== {r['filename']} ===\n{raw[:1000]}\n"

        final_report = "Fail"
        if data_ctx:
            try:
                system_prompt = "You are a research assistant. Write a report in Korean. Summarize data and physics."
                res = await asyncio.to_thread(ollama_client.chat, model=TEXT_MODEL, messages=[{'role':'system','content':system_prompt}, {'role':'user','content':f"Data:\n{data_ctx}"}])
                final_report = res['message']['content']
            except Exception as e: final_report = str(e)

        JOBS[job_id]["results"] = {"results": final_results, "final_report": final_report}
        JOBS[job_id]["status"] = "Completed"
        JOBS[job_id]["progress"] = 100
    except Exception as e:
        JOBS[job_id]["status"] = "Failed"; JOBS[job_id]["error"] = str(e)

@app.post("/api/analyze")
async def start_analysis(background_tasks: BackgroundTasks, files: List[UploadFile] = File(...), file_configs: str = Form(...)):
    job_id = str(uuid.uuid4()); configs = json.loads(file_configs)
    files_map = {f.filename: await f.read() for f in files}
    JOBS[job_id] = {"status": "Pending", "progress": 0, "step": "Init", "results": None}
    background_tasks.add_task(process_analysis_job, job_id, files_map, configs)
    return {"job_id": job_id}

@app.get("/api/status/{job_id}")
async def get_status(job_id: str): return JOBS.get(job_id, {"status": "Not Found"})

@app.get("/", response_class=HTMLResponse)
async def serve_index():
    if os.path.exists("index.html"): return FileResponse("index.html")
    return "<h1>NV6 Clustering Backend</h1>"

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)



엔브이5 파이널

import os
import io
import asyncio
import base64
import json
import re
import math
import uuid
import time
from typing import List, Dict, Any, Optional
from pydantic import BaseModel
from contextlib import asynccontextmanager

# [0] OMP & Lib Error Prevention
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"

# [1] Security
os.environ['NO_PROXY'] = '127.0.0.1,localhost,0.0.0.0,::1'
if 'HTTP_PROXY' in os.environ: del os.environ['HTTP_PROXY']
if 'HTTPS_PROXY' in os.environ: del os.environ['HTTPS_PROXY']

from fastapi import FastAPI, UploadFile, File, Form, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import HTMLResponse, FileResponse
import pandas as pd
import numpy as np

# SciPy & CV
from scipy.signal import find_peaks, savgol_filter
from scipy.optimize import curve_fit
from scipy.ndimage import center_of_mass
from skimage.feature import peak_local_max
import cv2
import torch

# Models & Utils
from segment_anything import sam_model_registry, SamAutomaticMaskGenerator
from ollama import Client 
from pdf2image import convert_from_bytes
from pptx import Presentation
from pptx.enum.shapes import MSO_SHAPE_TYPE

# [2] Config
OLLAMA_HOST = "http://127.0.0.1:11434"
ollama_client = Client(host=OLLAMA_HOST)
VISION_MODEL = "qwen3-vl:30b-a3b-instruct" 
TEXT_MODEL = "llama3:70b"

SAM_CHECKPOINT = "sam_vit_b_01ec64.pth"
SAM_MODEL_TYPE = "vit_b"
device = "cuda" if torch.cuda.is_available() else "cpu"

mask_generator = None 
JOBS = {} 

@asynccontextmanager
async def lifespan(app: FastAPI):
    global mask_generator
    if os.path.exists(SAM_CHECKPOINT):
        print(f"Loading SAM ({SAM_MODEL_TYPE}) on {device}...")
        try:
            sam = sam_model_registry[SAM_MODEL_TYPE](checkpoint=SAM_CHECKPOINT)
            sam.to(device=device)
            mask_generator = SamAutomaticMaskGenerator(
                model=sam, points_per_side=32, pred_iou_thresh=0.86,
                stability_score_thresh=0.90, min_mask_region_area=50
            )
            print("SAM Loaded.")
        except: print("SAM Load Failed.")
    yield
    JOBS.clear()

def sanitize_json(obj):
    if isinstance(obj, float):
        if np.isnan(obj) or np.isinf(obj): return 0.0
        return obj
    elif isinstance(obj, dict): return {k: sanitize_json(v) for k, v in obj.items()}
    elif isinstance(obj, list): return [sanitize_json(v) for v in obj]
    elif isinstance(obj, (np.int64, np.int32)): return int(obj)
    elif isinstance(obj, (np.float64, np.float32)):
        if np.isnan(obj) or np.isinf(obj): return 0.0
        return float(obj)
    return obj

# ==========================================
# [3] NV5 Final Engine
# ==========================================
class ScienceProcessorNV5:
    
    # --- [A] Universal Loader ---
    @staticmethod
    def read_universal_dataframe(content: bytes, filename: str) -> List[pd.DataFrame]:
        dfs = []
        fname = filename.lower()
        if fname.endswith(('.xlsx', '.xls')):
            try:
                xls = pd.ExcelFile(io.BytesIO(content))
                for s in xls.sheet_names:
                    df = pd.read_excel(xls, sheet_name=s, header=None)
                    num = df.apply(pd.to_numeric, errors='coerce').notna().sum().sum()
                    if num < 5: continue
                    if not df.empty: dfs.append(df)
                if dfs: return dfs
            except: pass
        
        text_content = ""
        for enc in ['utf-8', 'cp949', 'latin1']:
            try: text_content = content.decode(enc); break
            except: continue
        if text_content:
            lines = text_content.splitlines()
            data_start = -1; delimiter = None
            separators = [',', '\t', ';', '\s+']
            for i, line in enumerate(lines[:100]):
                for sep in separators:
                    parts = line.split() if sep=='\s+' else line.split(sep)
                    nums = sum(1 for p in parts if re.match(r'^-?\d+(\.\d+)?(?:[eE][+-]?\d+)?$', p.strip()))
                    if nums >= 2: data_start = i; delimiter = sep; break
                if data_start != -1: break
            if data_start != -1:
                try:
                    data_str = "\n".join(lines[data_start:])
                    sep_arg = '\s+' if delimiter=='\s+' else delimiter
                    df = pd.read_csv(io.StringIO(data_str), sep=sep_arg, header=None, engine='python')
                    dfs.append(df)
                except: pass
        return dfs

    # --- [B] Block Parser ---
    @staticmethod
    def detect_structured_blocks(df: pd.DataFrame) -> List[pd.DataFrame]:
        blocks = []
        if df.empty: return []
        df_num = df.apply(pd.to_numeric, errors='coerce')
        if df.shape[1] > 0: is_data_row = df_num.iloc[:, 0].notna()
        else: is_data_row = df_num.notna().any(axis=1)
        groups = (is_data_row != is_data_row.shift()).cumsum()
        for _, group in df.groupby(groups):
            if is_data_row.loc[group.index[0]]:
                block = df_num.loc[group.index].dropna(how='all', axis=1)
                if block.shape[0] >= 10: blocks.append(block.reset_index(drop=True))
        return blocks

    @staticmethod
    def split_horizontal_blocks(df: pd.DataFrame) -> List[pd.DataFrame]:
        sub_blocks = []
        if df.empty: return []
        col_counts = df.apply(pd.to_numeric, errors='coerce').notna().sum()
        valid = np.where(col_counts >= 2)[0]
        if len(valid) == 0: return []
        splits = np.where(np.diff(valid) > 1)[0] + 1
        groups = np.split(valid, splits)
        for g in groups:
            if len(g) > 0:
                sub = df.iloc[:, g].reset_index(drop=True)
                if sub.shape[1] >= 1 and sub.shape[0] >= 2: sub_blocks.append(sub)
        return sub_blocks

    @staticmethod
    def extract_series_from_df(df: pd.DataFrame) -> List[Dict]:
        series_list = []
        try:
            vals = df.values; rows, cols = vals.shape
            if rows < 5: return []
            if cols == 1:
                y = vals[:, 0]; mask = ~np.isnan(y)
                if np.sum(mask) > 5: series_list.append({"x": np.arange(len(y))[mask], "y": y[mask], "name": "Single"})
                return series_list
            
            def is_axis(arr):
                arr = arr[~np.isnan(arr)]
                if len(arr) < 5: return False
                diff = np.diff(arr)
                if np.all(diff >= 0) or np.all(diff <= 0): return True
                return (np.sum(diff>0)/len(diff)>0.9) or (np.sum(diff<0)/len(diff)>0.9)

            current_x = vals[:, 0]
            if not is_axis(current_x): current_x = np.arange(rows)
            for i in range(1, cols):
                col_data = vals[:, i]; mask_valid = ~np.isnan(col_data)
                if np.sum(mask_valid) < 5: continue
                if is_axis(col_data): current_x = col_data; continue 
                mask = ~np.isnan(current_x) & ~np.isnan(col_data)
                if np.sum(mask) > 5:
                    series_list.append({"x": current_x[mask], "y": col_data[mask], "name": f"Col-{i}"})
        except: pass
        return series_list

    # --- [C] Spectrum Logic ---
    @staticmethod
    def simple_baseline(y):
        try:
            w = max(5, len(y)//10)
            s = pd.Series(y).rolling(window=w, center=True).min()
            b = s.bfill().ffill().values 
            if len(b) > 51: b = savgol_filter(b, 51, 3)
            return b, w
        except: return np.zeros_like(y), 0

    @staticmethod
    def process_spectrum(x, y, mode, goal):
        log = []; y_raw = y.copy(); base = np.zeros_like(y); y_proc = y.copy()
        peaks = []; fits = []; stats_txt = "Error"
        try:
            if mode == "None":
                peaks, _ = find_peaks(y, height=np.max(y)*0.05)
                log.append("Mode: Raw")
                stats_txt = f"Raw Peaks: {len(peaks)}"
            else:
                win = 15; do_fit = False
                if mode == "AI-Adaptive":
                    log.append("Mode: AI-Adaptive")
                    if "noise" in goal.lower(): win = 31
                    if "fit" in goal.lower(): do_fit = True
                
                base, bw = ScienceProcessorNV5.simple_baseline(y)
                y_proc = np.maximum(y - base, 0)
                log.append(f"Base: Rolling Min (w={bw})")
                
                if len(y_proc) > win:
                    y_proc = savgol_filter(y_proc, win, 3)
                    log.append(f"Smooth: SavGol (w={win})")
                
                peaks, _ = find_peaks(y_proc, height=np.max(y_proc)*0.05, distance=10)
                log.append(f"Peaks: {len(peaks)}")
                stats_txt = f"Range: {np.min(x):.1f}~{np.max(x):.1f}, Peaks: {len(peaks)}"
            return {"x": x, "y_raw": y_raw, "y_proc": y_proc, "y_base": base, "peaks": [{"x": float(x[p]), "y": float(y_proc[p])} for p in peaks], "log": log, "fits": fits, "stats": stats_txt}
        except Exception as e: return {"x": x, "y_raw": y_raw, "y_proc": y_proc, "y_base": base, "peaks": [], "log": [str(e)], "fits": [], "stats": "Error"}

    # --- [D] Footer & Scale ---
    @staticmethod
    def separate_footer(img_bgr):
        h, w = img_bgr.shape[:2]
        roi_h = int(h * 0.82)
        roi = img_bgr[roi_h:, :]
        gray = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)
        sobel_y = np.abs(cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=3))
        row_sums = np.sum(sobel_y, axis=1)
        split_idx = np.argmax(row_sums)
        if row_sums[split_idx] > w * 30:
            real_split = roi_h + split_idx
            return img_bgr[:real_split, :], img_bgr[real_split:, :]
        return img_bgr, None

    # --- [E] FFT Crystal (20x20 + Classification) ---
    @staticmethod
    def analyze_crystal_structure(img_bgr):
        log = []
        gray = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2GRAY)
        h, w = gray.shape
        tile_size = w // 20
        if tile_size < 16: tile_size = 16
        
        overlay = img_bgr.copy()
        mask_overlay = np.zeros_like(img_bgr)
        
        stats = {"Amorphous": 0, "Partial": 0, "Crystalline": 0}
        
        # Draw Grid (Visual Guide)
        for x in range(0, w, tile_size): cv2.line(overlay, (x, 0), (x, h), (50, 50, 50), 1)
        for y in range(0, h, tile_size): cv2.line(overlay, (0, y), (w, y), (50, 50, 50), 1)

        for y in range(0, h, tile_size):
            for x in range(0, w, tile_size):
                roi = gray[y:y+tile_size, x:x+tile_size]
                if roi.shape[0] != tile_size or roi.shape[1] != tile_size: continue
                
                # FFT
                f = np.fft.fft2(roi)
                fshift = np.fft.fftshift(f)
                mag = 20 * np.log(np.abs(fshift) + 1e-9)
                
                # Center Mask (DC)
                cy, cx = tile_size//2, tile_size//2
                mag[cy-2:cy+3, cx-2:cx+3] = 0
                
                # Detect Peaks (Spots)
                local_max = peak_local_max(mag, min_distance=3, threshold_rel=0.5)
                num_spots = len(local_max)
                
                # Detect Rings (Radial Profile)
                y_grid, x_grid = np.ogrid[:tile_size, :tile_size]
                r_grid = np.sqrt((x_grid - cx)**2 + (y_grid - cy)**2).astype(int)
                tbin = np.bincount(r_grid.ravel(), mag.ravel())
                nr = np.bincount(r_grid.ravel())
                radial_profile = tbin / (nr + 1e-9)
                # Check sharpness of ring
                ring_score = (np.max(radial_profile) - np.mean(radial_profile)) / (np.std(radial_profile) + 1e-9)

                color = None
                label = ""

                # 1. Crystalline: Distinct Spots
                if num_spots > 4 and ring_score < 3.0: 
                    stats["Crystalline"] += 1
                    color = (255, 0, 0) # Blue
                
                # 2. Partial: Sharp Ring/Arc
                elif ring_score > 3.5:
                    stats["Partial"] += 1
                    color = (0, 165, 255) # Orange
                    # Angle Logic (Simplistic)
                    _, th = cv2.threshold(mag, np.max(mag)*0.7, 255, cv2.THRESH_BINARY)
                    M = cv2.moments(th)
                    if M["m00"] != 0:
                        acx = int(M["m10"] / M["m00"])
                        acy = int(M["m01"] / M["m00"])
                        deg = math.degrees(math.atan2(acy-cy, acx-cx))
                        if deg < 0: deg += 180
                        label = f"{int(deg)}"

                # 3. Amorphous: Diffuse
                else:
                    stats["Amorphous"] += 1
                    # No Color (Gray/Transparent)
                
                if color:
                    cv2.rectangle(mask_overlay, (x, y), (x+tile_size, y+tile_size), color, -1)
                    if label:
                        cv2.putText(overlay, label, (x+2, y+10), cv2.FONT_HERSHEY_SIMPLEX, 0.3, (255,255,255), 1)

        alpha = 0.35
        mask_indices = np.any(mask_overlay > 0, axis=-1)
        overlay[mask_indices] = (overlay[mask_indices] * (1 - alpha) + mask_overlay[mask_indices] * alpha).astype(np.uint8)
        
        log.append(f"Grid: {tile_size}px")
        log.append(f"Analysis: Cryst={stats['Crystalline']}, Part={stats['Partial']}, Amor={stats['Amorphous']}")
        
        def to_b64(im): return base64.b64encode(cv2.imencode('.jpg', im)[1]).decode('utf-8')
        return {"type": "image", "raw_b64": to_b64(img_bgr), "proc_b64": to_b64(overlay), "stats": stats, "log": log}

    # --- [F] Diffraction ---
    @staticmethod
    def analyze_diffraction_nv5(img_bgr, user_center=None):
        log = []
        stats = {}
        gray = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2GRAY)
        
        if user_center and len(user_center)==2 and user_center[0]>0:
            cx, cy = user_center
            log.append(f"Center: Manual [{cx}, {cy}]")
        else:
            _, th = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY+cv2.THRESH_OTSU)
            cyf, cxf = center_of_mass(th)
            if np.isnan(cyf): cyf, cxf = gray.shape[0]//2, gray.shape[1]//2
            cx, cy = int(cxf), int(cyf)
            log.append(f"Center: Auto [{cx}, {cy}]")

        g1 = cv2.GaussianBlur(gray, (3,3), 0)
        g2 = cv2.GaussianBlur(gray, (51,51), 0)
        diff = cv2.subtract(g1, g2)
        
        coords = peak_local_max(diff, min_distance=10, threshold_abs=20)
        
        enhanced = np.log1p(diff.astype(np.float32))
        enhanced = cv2.normalize(enhanced, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)
        vis = cv2.cvtColor(enhanced, cv2.COLOR_GRAY2BGR)
        
        cv2.drawMarker(vis, (cx, cy), (0,0,255), cv2.MARKER_CROSS, 20, 2)
        peak_list = []
        for p in coords:
            py, px = p
            if abs(py-cy)<5 and abs(px-cx)<5: continue
            cv2.circle(vis, (px, py), 3, (0,255,0), 1)
            peak_list.append(f"[{px},{py}]")

        log.append(f"Spots: {len(peak_list)}")
        def to_b64(im): return base64.b64encode(cv2.imencode('.jpg', im)[1]).decode('utf-8')
        return {"type": "image", "raw_b64": to_b64(img_bgr), "proc_b64": to_b64(vis), "stats": {"center": [cx, cy], "spots": len(peak_list)}, "log": log}

    # --- [G] SAM (Updated Particle & Film) ---
    @staticmethod
    def analyze_sam_results_nv5(img_rgb, masks, data_type):
        overlay = img_rgb.copy()
        mask_overlay = np.zeros_like(img_rgb)
        h_img, w_img = img_rgb.shape[:2]
        stats = {"count": 0}
        log = []
        
        valid_masks = []
        for ann in masks:
            area = ann['area']; bbox = ann['bbox']; x, y, w, h = [int(v) for v in bbox]
            if area > h_img * w_img * 0.95 or area < 20: continue # [Fix] Min area relaxed to 20
            
            # Particle: Relaxed Circularity
            if data_type == "Particle":
                if x<=1 or y<=1 or (x+w)>=w_img-1 or (y+h)>=h_img-1: continue # Border kill
                
                # [Fix] Circularity Check Relaxed
                mask_uint8 = (ann['segmentation'] * 255).astype(np.uint8)
                cnts, _ = cv2.findContours(mask_uint8, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
                if not cnts: continue
                perimeter = cv2.arcLength(cnts[0], True)
                circularity = 4 * np.pi * area / (perimeter**2) if perimeter > 0 else 0
                
                # Allow more irregular shapes (0.25+)
                if circularity < 0.25: continue

            valid_masks.append(ann)

        if data_type == "Thin Film":
            sorted_masks = sorted(valid_masks, key=lambda x: x['bbox'][1])
        else:
            sorted_masks = sorted(valid_masks, key=lambda x: x['area'], reverse=True)

        thickness_vals = []; diameters = []

        for i, ann in enumerate(sorted_masks):
            m = ann['segmentation']
            bbox = ann['bbox']; x, y, w, h = [int(v) for v in bbox]
            cx, cy = int(x + w/2), int(y + h/2)
            color = (0, 255, 0) # Green

            if data_type == "Thin Film":
                is_bound = (i == 0 or i == len(sorted_masks)-1)
                
                # [Fix] Bigger Font for Layer #
                cv2.putText(overlay, f"#{i+1}", (cx, cy), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (255, 255, 255), 3)
                
                if not is_bound:
                    thk = ann['area'] / w
                    thickness_vals.append(thk)
                    color = (255, 0, 0) # Red
                else: color = (100, 100, 100)

                # [Fix] Interface Number & Roughness
                if i < len(sorted_masks) - 1:
                    ys, xs = np.where(m)
                    if len(xs) > 10:
                        roughness = np.std(ys)
                        # Draw Yellow Interface Line
                        cv2.line(overlay, (x, y+h), (x+w, y+h), (0, 255, 255), 2)
                        # Interface Label
                        label_txt = f"Int#{i+1} R:{roughness:.2f}"
                        cv2.putText(overlay, label_txt, (x+10, y+h-5), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)

            elif data_type == "Particle":
                equivalent_diameter = 2 * np.sqrt(ann['area'] / np.pi)
                diameters.append(equivalent_diameter)

            mask_overlay[m] = color
            # Outline
            mask_uint8 = (m * 255).astype(np.uint8)
            cnts, _ = cv2.findContours(mask_uint8, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
            if cnts: cv2.drawContours(overlay, [cnts[0]], -1, color, 1)

        alpha = 0.35
        mask_indices = np.any(mask_overlay > 0, axis=-1)
        overlay[mask_indices] = (overlay[mask_indices] * (1 - alpha) + mask_overlay[mask_indices] * alpha).astype(np.uint8)

        stats["count"] = len(sorted_masks)
        if data_type == "Thin Film":
            avg_thk = round(np.mean(thickness_vals), 2) if thickness_vals else 0
            stats["thickness_avg"] = avg_thk
            log.append(f"Layers: {len(sorted_masks)}, Avg Thk: {avg_thk}px")
        elif data_type == "Particle":
            avg_dia = round(np.mean(diameters), 2) if diameters else 0
            stats["diameter_avg"] = avg_dia
            log.append(f"Valid Particles: {len(sorted_masks)}, Avg Dia: {avg_dia}px")
        else:
            log.append(f"Detected: {len(sorted_masks)}")

        return overlay, stats, log

    # --- Router ---
    @staticmethod
    def process_image_nv5(img_bytes, equipment, data_type, mode, goal, user_center=None):
        nparr = np.frombuffer(img_bytes, np.uint8)
        img_raw = cv2.imdecode(nparr, cv2.IMREAD_COLOR)
        if img_raw is None: return {}
        
        # [Update 1] Split Footer
        body, footer = ScienceProcessorNV5.separate_footer(img_raw)
        
        footer_b64 = None
        if footer is not None:
            footer_b64 = base64.b64encode(cv2.imencode('.jpg', footer)[1]).decode('utf-8')

        if data_type == "Crystal Structure (FFT)":
            return {**ScienceProcessorNV5.analyze_crystal_structure(body), "footer_b64": footer_b64}
        if data_type == "2D Diffraction":
            return {**ScienceProcessorNV5.analyze_diffraction_nv5(body, user_center), "footer_b64": footer_b64}
        
        # General SAM
        if mode == "None":
             def to_b64(im): return base64.b64encode(cv2.imencode('.jpg', im)[1]).decode('utf-8')
             return {"type":"image", "raw_b64":to_b64(body), "proc_b64":to_b64(body), "stats":{}, "log":["Mode: Raw"], "footer_b64": footer_b64}

        img_proc = body.copy(); log = []
        if mode == "Auto" or mode == "AI-Adaptive":
            img_proc = cv2.bilateralFilter(img_proc, 9, 75, 75)
            log.append("Preproc: Denoise")

        img_rgb = cv2.cvtColor(img_proc, cv2.COLOR_BGR2RGB)
        
        if mask_generator:
            try:
                masks = mask_generator.generate(img_rgb)
                log.append(f"SAM: {len(masks)} Segments")
                overlay, stats, sam_log = ScienceProcessorNV5.analyze_sam_results_nv5(img_rgb, masks, data_type)
                log.extend(sam_log)
                overlay = cv2.cvtColor(overlay, cv2.COLOR_RGB2BGR)
            except Exception as e:
                log.append(f"SAM Error: {str(e)}")
                overlay = body; stats = {}
        else:
            log.append("SAM Error: Not Loaded")
            overlay = body; stats = {}

        def to_b64(im): return base64.b64encode(cv2.imencode('.jpg', im)[1]).decode('utf-8')
        return {"type": "image", "raw_b64": to_b64(body), "proc_b64": to_b64(overlay), "stats": stats, "log": log, "footer_b64": footer_b64}

# [4] Helper Async Functions
async def analyze_vision_ollama(image_bytes: bytes, prompt: str) -> str:
    try:
        b64 = base64.b64encode(image_bytes).decode('utf-8')
        res = await asyncio.to_thread(ollama_client.chat, model=VISION_MODEL, messages=[{'role':'user','content':prompt,'images':[b64]}])
        return res['message']['content']
    except: return "Vision Error"

async def interpret_spectrum_data(raw_context: str, equipment: str) -> str:
    try:
        prompt = f"Analyze spectrum ({equipment}). Brief interpretation in Korean. Data: {raw_context}"
        res = await asyncio.to_thread(ollama_client.chat, model=TEXT_MODEL, messages=[{'role':'user','content':prompt}])
        return res['message']['content']
    except: return ""

# [5] App
app = FastAPI(title="Analyst NV5 Final", lifespan=lifespan)
app.add_middleware(CORSMiddleware, allow_origins=["*"], allow_methods=["*"], allow_headers=["*"])

class TranslateReq(BaseModel): text: str

@app.post("/api/translate")
async def translate_text(req: TranslateReq):
    try:
        res = await asyncio.to_thread(ollama_client.chat, model=TEXT_MODEL, messages=[{'role':'system','content':'Translate to English.'}, {'role':'user','content':req.text}])
        return {"translated": res['message']['content']}
    except Exception as e: return {"translated": str(e)}

async def process_analysis_job(job_id: str, files_map: Dict, configs: Dict):
    try:
        JOBS[job_id]["status"] = "Processing"
        JOBS[job_id]["progress"] = 10
        final_results = []
        total_files = len(configs)
        processed_count = 0

        for filename, config in configs.items():
            if filename not in files_map: continue
            content = files_map[filename]
            eq = config.get("equipment", "General")
            data_type = config.get("data_type", "General")
            goal = config.get("goal", "")
            mode = config.get("mode", "Auto")
            
            user_center = None
            if config.get("center_coords"):
                try: user_center = [int(v) for v in config["center_coords"].split(",")]
                except: pass

            fname_lower = filename.lower()
            JOBS[job_id]["step"] = f"Analyzing {filename}..."
            
            # [A] Docs
            if eq == "Document" or fname_lower.endswith(('.pdf', '.ppt', '.pptx')):
                try:
                    if fname_lower.endswith('.pdf'):
                        images = convert_from_bytes(content, dpi=150, fmt='jpeg')[:10]
                        full_txt = ""; pages = []
                        for idx, img in enumerate(images):
                            buf = io.BytesIO(); img.save(buf, format="JPEG")
                            b64 = base64.b64encode(buf.getvalue()).decode()
                            desc = await analyze_vision_ollama(buf.getvalue(), "한글 요약.")
                            full_txt += f"\nPage {idx+1}: {desc}"
                            pages.append({"page_num": idx+1, "image_b64": b64, "desc": desc})
                        final_results.append({"type": "pdf", "filename": filename, "equipment": "Lit", "raw_context": full_txt, "pages": pages})
                    elif fname_lower.endswith(('.ppt', '.pptx')):
                        prs = Presentation(io.BytesIO(content)); slides = []; full_txt = ""
                        for i, slide in enumerate(prs.slides):
                            txt = ""; imgs = []
                            for s in slide.shapes:
                                if hasattr(s, "text"): txt += s.text + "\n"
                                if s.shape_type == MSO_SHAPE_TYPE.PICTURE:
                                    try:
                                        ib64 = base64.b64encode(s.image.blob).decode('utf-8')
                                        desc = await analyze_vision_ollama(s.image.blob, "한글 설명.")
                                        imgs.append({"b64": ib64, "desc": desc})
                                    except: pass
                            slides.append({"slide_num": i+1, "text": txt, "images": imgs})
                        final_results.append({"type": "ppt", "filename": filename, "equipment": "Lit", "raw_context": full_txt, "slides": slides})
                except Exception as e: final_results.append({"type": "error", "filename": filename, "msg": str(e)})

            # [B] Spectrum
            elif fname_lower.endswith(('.xlsx', '.xls', '.csv', '.txt')):
                try:
                    dfs = ScienceProcessorNV5.read_universal_dataframe(content, filename)
                    for i, df in enumerate(dfs):
                        v_blocks = ScienceProcessorNV5.detect_structured_blocks(df)
                        for v_block in v_blocks:
                            h_blocks = ScienceProcessorNV5.split_horizontal_blocks(v_block)
                            for h_block in h_blocks:
                                series_list = ScienceProcessorNV5.extract_series_from_df(h_block)
                                for s in series_list:
                                    res = ScienceProcessorNV5.process_spectrum(s['x'], s['y'], mode, goal)
                                    step = max(1, len(s['x'])//100)
                                    chart = [{"x": float(res["x"][k]), "y_proc": float(res["y_proc"][k]), "y_raw": float(res["y_raw"][k]), "y_base": float(res["y_base"][k])} for k in range(0, len(s['x']), step)]
                                    ctx = f"Data: {s['name']}\nStats: {res['stats']}"
                                    interp = await interpret_spectrum_data(ctx, eq)
                                    final_results.append({
                                        "type": "spectrum", "filename": f"{filename} ({s['name']})", "equipment": eq,
                                        "raw_context": f"{ctx}\nInterp: {interp}", 
                                        "chart_data": chart, "log": res["log"], "interpretation": interp
                                    })
                except Exception as ex: final_results.append({"type": "error", "filename": filename, "msg": str(ex)})

            # [C] Image (NV5)
            elif fname_lower.endswith(('.png', '.jpg', '.jpeg', '.tif', '.bmp')):
                try:
                    JOBS[job_id]["step"] = f"Vision Analysis: {filename}..."
                    vis_res = await asyncio.to_thread(ScienceProcessorNV5.process_image_nv5, content, eq, data_type, mode, goal, user_center)
                    if vis_res:
                        scale_info = "Unknown"
                        if vis_res.get("footer_b64"):
                            JOBS[job_id]["step"] = f"Reading Scale: {filename}..."
                            scale_text = await analyze_vision_ollama(base64.b64decode(vis_res["footer_b64"]), "Read scale bar text only.")
                            scale_info = scale_text
                            vis_res["log"].append(f"Scale: {scale_text}")

                        desc = await analyze_vision_ollama(base64.b64decode(vis_res["proc_b64"]), f"Summary in Korean. {eq}. {data_type}. Stats: {vis_res.get('stats',{})}")
                        final_results.append({
                            "type": "image", "filename": filename, "equipment": eq, "summary": desc,
                            "raw_context": f"Img {eq}: {desc}", "raw_b64": vis_res["raw_b64"], "proc_b64": vis_res["proc_b64"], "footer_b64": vis_res.get("footer_b64"),
                            "stats": vis_res.get("stats",{}), "log": vis_res.get("log", [])
                        })
                except Exception as ex: final_results.append({"type": "error", "filename": filename, "msg": str(ex)})
            
            processed_count += 1
            JOBS[job_id]["progress"] = 10 + int((processed_count / total_files) * 80)

        JOBS[job_id]["step"] = "Writing Report..."
        final_results = sanitize_json(final_results)
        
        data_ctx = ""
        for r in final_results:
            if r.get("type") != "error":
                raw = r.get("raw_context") or ""
                data_ctx += f"\n=== {r['filename']} ===\n{raw[:1000]}\n"

        final_report = "Fail"
        if data_ctx:
            try:
                system_prompt = "You are a research assistant. Write a report in Korean. Summarize data and physics."
                res = await asyncio.to_thread(ollama_client.chat, model=TEXT_MODEL, messages=[{'role':'system','content':system_prompt}, {'role':'user','content':f"Data:\n{data_ctx}"}])
                final_report = res['message']['content']
            except Exception as e: final_report = str(e)

        JOBS[job_id]["results"] = {"results": final_results, "final_report": final_report}
        JOBS[job_id]["status"] = "Completed"
        JOBS[job_id]["progress"] = 100
    except Exception as e:
        JOBS[job_id]["status"] = "Failed"; JOBS[job_id]["error"] = str(e)

@app.post("/api/analyze")
async def start_analysis(background_tasks: BackgroundTasks, files: List[UploadFile] = File(...), file_configs: str = Form(...)):
    job_id = str(uuid.uuid4()); configs = json.loads(file_configs)
    files_map = {f.filename: await f.read() for f in files}
    JOBS[job_id] = {"status": "Pending", "progress": 0, "step": "Init", "results": None}
    background_tasks.add_task(process_analysis_job, job_id, files_map, configs)
    return {"job_id": job_id}

@app.get("/api/status/{job_id}")
async def get_status(job_id: str): return JOBS.get(job_id, {"status": "Not Found"})

@app.get("/", response_class=HTMLResponse)
async def serve_index():
    if os.path.exists("index.html"): return FileResponse("index.html")
    return "<h1>NV5 Final Backend</h1>"

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)





엔브이5리파인 코드

import os
import io
import asyncio
import base64
import json
import re
import math
import uuid
import time
from typing import List, Dict, Any, Optional
from pydantic import BaseModel
from contextlib import asynccontextmanager

# [0] OMP 에러 방지
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"

# [1] 보안 설정
os.environ['NO_PROXY'] = '127.0.0.1,localhost,0.0.0.0,::1'
if 'HTTP_PROXY' in os.environ: del os.environ['HTTP_PROXY']
if 'HTTPS_PROXY' in os.environ: del os.environ['HTTPS_PROXY']

from fastapi import FastAPI, UploadFile, File, Form, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import HTMLResponse, FileResponse
import pandas as pd
import numpy as np

# 과학/영상 연산
from scipy.signal import find_peaks, savgol_filter
from scipy.optimize import curve_fit
from scipy.ndimage import center_of_mass
from skimage.feature import peak_local_max
import cv2
import torch

# SAM & LLM
from segment_anything import sam_model_registry, SamAutomaticMaskGenerator
from ollama import Client 
from pdf2image import convert_from_bytes
from pptx import Presentation
from pptx.enum.shapes import MSO_SHAPE_TYPE

# [2] 설정
OLLAMA_HOST = "http://127.0.0.1:11434"
ollama_client = Client(host=OLLAMA_HOST)
VISION_MODEL = "qwen3-vl:30b-a3b-instruct" 
TEXT_MODEL = "llama3:70b"

SAM_CHECKPOINT = "sam_vit_b_01ec64.pth"
SAM_MODEL_TYPE = "vit_b"
device = "cuda" if torch.cuda.is_available() else "cpu"

mask_generator = None 
JOBS = {} 

@asynccontextmanager
async def lifespan(app: FastAPI):
    global mask_generator
    if os.path.exists(SAM_CHECKPOINT):
        print(f"Loading SAM ({SAM_MODEL_TYPE}) on {device}...")
        try:
            sam = sam_model_registry[SAM_MODEL_TYPE](checkpoint=SAM_CHECKPOINT)
            sam.to(device=device)
            mask_generator = SamAutomaticMaskGenerator(
                model=sam, points_per_side=32, pred_iou_thresh=0.88,
                stability_score_thresh=0.92, min_mask_region_area=100
            )
            print("SAM Loaded.")
        except: print("SAM Load Failed.")
    yield
    JOBS.clear()

def sanitize_json(obj):
    if isinstance(obj, float):
        if np.isnan(obj) or np.isinf(obj): return 0.0
        return obj
    elif isinstance(obj, dict): return {k: sanitize_json(v) for k, v in obj.items()}
    elif isinstance(obj, list): return [sanitize_json(v) for v in obj]
    elif isinstance(obj, (np.int64, np.int32)): return int(obj)
    elif isinstance(obj, (np.float64, np.float32)):
        if np.isnan(obj) or np.isinf(obj): return 0.0
        return float(obj)
    return obj

# ==========================================
# [3] NV5 Refined 과학 엔진
# ==========================================
class ScienceProcessorNV5:
    
    # --- Universal Loader (유지) ---
    @staticmethod
    def read_universal_dataframe(content: bytes, filename: str) -> List[pd.DataFrame]:
        dfs = []
        fname = filename.lower()
        if fname.endswith(('.xlsx', '.xls')):
            try:
                xls = pd.ExcelFile(io.BytesIO(content))
                for s in xls.sheet_names:
                    df = pd.read_excel(xls, sheet_name=s, header=None)
                    num_count = df.apply(pd.to_numeric, errors='coerce').notna().sum().sum()
                    if num_count < 5: raise Exception("Not enough numbers")
                    if not df.empty: dfs.append(df)
                if dfs: return dfs
            except: pass
        text_content = ""
        for enc in ['utf-8', 'cp949', 'latin1', 'utf-16']:
            try: text_content = content.decode(enc); break
            except: continue
        if text_content:
            lines = text_content.splitlines()
            data_start = -1; delimiter = None
            separators = [',', '\t', ';', '\s+']
            for i, line in enumerate(lines[:100]):
                for sep in separators:
                    parts = line.split() if sep=='\s+' else line.split(sep)
                    nums = sum(1 for p in parts if re.match(r'^-?\d+(\.\d+)?(?:[eE][+-]?\d+)?$', p.strip()))
                    if nums >= 2: data_start = i; delimiter = sep; break
                if data_start != -1: break
            if data_start != -1:
                try:
                    data_str = "\n".join(lines[data_start:])
                    sep_arg = '\s+' if delimiter=='\s+' else delimiter
                    df = pd.read_csv(io.StringIO(data_str), sep=sep_arg, header=None, engine='python')
                    dfs.append(df)
                except: pass
        return dfs

    # --- Block Parser (NV2 Robust 유지) ---
    @staticmethod
    def detect_structured_blocks(df: pd.DataFrame) -> List[pd.DataFrame]:
        blocks = []
        if df.empty: return []
        df_num = df.apply(pd.to_numeric, errors='coerce')
        if df.shape[1] > 0: is_data_row = df_num.iloc[:, 0].notna()
        else: is_data_row = df_num.notna().any(axis=1)
        groups = (is_data_row != is_data_row.shift()).cumsum()
        for _, group in df.groupby(groups):
            if is_data_row.loc[group.index[0]]:
                block = df_num.loc[group.index].dropna(how='all', axis=1)
                if block.shape[0] >= 10: blocks.append(block.reset_index(drop=True))
        return blocks

    @staticmethod
    def split_horizontal_blocks(df: pd.DataFrame) -> List[pd.DataFrame]:
        sub_blocks = []
        if df.empty: return []
        col_counts = df.apply(pd.to_numeric, errors='coerce').notna().sum()
        valid_col_indices = np.where(col_counts >= 2)[0]
        if len(valid_col_indices) == 0: return []
        split_points = np.where(np.diff(valid_col_indices) > 1)[0] + 1
        groups = np.split(valid_col_indices, split_points)
        for g in groups:
            if len(g) > 0:
                sub_df = df.iloc[:, g].reset_index(drop=True)
                if sub_df.shape[1] >= 1 and sub_df.shape[0] >= 2: sub_blocks.append(sub_df)
        return sub_blocks

    @staticmethod
    def extract_series_from_df(df: pd.DataFrame) -> List[Dict]:
        series_list = []
        try:
            vals = df.values; rows, cols = vals.shape
            if rows < 5: return []
            if cols == 1:
                y = vals[:, 0]; mask = ~np.isnan(y)
                if np.sum(mask) > 5: series_list.append({"x": np.arange(len(y))[mask], "y": y[mask], "name": "Single"})
                return series_list
            
            def is_axis(arr):
                arr = arr[~np.isnan(arr)]
                if len(arr) < 5: return False
                diff = np.diff(arr)
                if np.all(diff >= 0) or np.all(diff <= 0): return True
                return (np.sum(diff>0)/len(diff)>0.9) or (np.sum(diff<0)/len(diff)>0.9)

            current_x = vals[:, 0]
            if not is_axis(current_x): current_x = np.arange(rows)
            for i in range(1, cols):
                col_data = vals[:, i]
                mask_valid = ~np.isnan(col_data)
                if np.sum(mask_valid) < 5: continue
                if is_axis(col_data): current_x = col_data; continue 
                mask = ~np.isnan(current_x) & ~np.isnan(col_data)
                if np.sum(mask) > 5:
                    series_list.append({"x": current_x[mask], "y": col_data[mask], "name": f"Col-{i}"})
        except: pass
        return series_list

    # --- Spectrum Logic ---
    @staticmethod
    def simple_baseline(y):
        try:
            w = max(5, len(y)//10)
            s = pd.Series(y).rolling(window=w, center=True).min()
            b = s.bfill().ffill().values 
            if len(b) > 51: b = savgol_filter(b, 51, 3)
            return b, w
        except: return np.zeros_like(y), 0

    @staticmethod
    def process_spectrum(x, y, mode, goal):
        log = []
        y_raw = y.copy(); base = np.zeros_like(y); y_proc = y.copy()
        peaks = []; fits = []; stats_txt = "Error"
        try:
            if mode == "None":
                peaks, _ = find_peaks(y, height=np.max(y)*0.05)
                log.append("Mode: Raw")
                stats_txt = f"Raw Peaks: {len(peaks)}"
            else:
                win = 15; do_fit = False
                if mode == "AI-Adaptive":
                    log.append("Mode: AI-Adaptive")
                    if "noise" in goal.lower(): win = 31; log.append("AI: Strong Smooth")
                    if "fit" in goal.lower(): do_fit = True; log.append("AI: Fit On")
                else: log.append("Mode: Auto")
                
                base, bw = ScienceProcessorNV5.simple_baseline(y)
                y_proc = np.maximum(y - base, 0)
                log.append(f"Base: Rolling Min (w={bw})")
                
                if len(y_proc) > win:
                    y_proc = savgol_filter(y_proc, win, 3)
                    log.append(f"Smooth: SavGol (w={win})")
                
                peaks, _ = find_peaks(y_proc, height=np.max(y_proc)*0.05, distance=10)
                log.append(f"Peaks: {len(peaks)}")
                
                stats_txt = f"Range: {np.min(x):.1f}~{np.max(x):.1f}, Peaks: {len(peaks)}"
            return {"x": x, "y_raw": y_raw, "y_proc": y_proc, "y_base": base, "peaks": [{"x": float(x[p]), "y": float(y_proc[p])} for p in peaks], "log": log, "fits": fits, "stats": stats_txt}
        except Exception as e: return {"x": x, "y_raw": y_raw, "y_proc": y_proc, "y_base": base, "peaks": [], "log": [str(e)], "fits": [], "stats": "Error"}

    # --- [D] Footer & Scale Bar Extraction (New) ---
    @staticmethod
    def separate_footer(img_bgr):
        """ 
        [Update 1] Split Image into Body and Footer. 
        Returns: body_img, footer_img (or None if no footer detected)
        """
        h, w = img_bgr.shape[:2]
        roi_h = int(h * 0.85)
        roi = img_bgr[roi_h:, :]
        gray = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)
        
        # Sobel to find horizontal split line
        sobel_y = np.abs(cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=3))
        row_sums = np.sum(sobel_y, axis=1)
        split_idx = np.argmax(row_sums)
        
        if row_sums[split_idx] > w * 50:
            real_split = roi_h + split_idx
            # Margin adjustment
            return img_bgr[:real_split, :], img_bgr[real_split:, :]
        return img_bgr, None

    # --- [E] FFT Crystal Analysis (20x20 Grid & Angle) ---
    @staticmethod
    def analyze_crystal_structure(img_bgr):
        log = []
        gray = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2GRAY)
        h, w = gray.shape
        
        # [Update 3] 20x20 Grid
        tile_size = w // 20
        if tile_size < 16: tile_size = 16 # Safety limit
        
        overlay = img_bgr.copy()
        mask_overlay = np.zeros_like(img_bgr)
        
        stats = {"Amorphous": 0, "Partial": 0, "Crystalline": 0}
        
        for y in range(0, h, tile_size):
            for x in range(0, w, tile_size):
                roi = gray[y:y+tile_size, x:x+tile_size]
                if roi.shape[0] != tile_size or roi.shape[1] != tile_size: continue
                
                # 2D FFT
                f = np.fft.fft2(roi)
                fshift = np.fft.fftshift(f)
                magnitude = 20 * np.log(np.abs(fshift) + 1e-9)
                
                # Center masking (DC removal)
                cy, cx = tile_size // 2, tile_size // 2
                magnitude[cy-2:cy+3, cx-2:cx+3] = 0
                
                # Metrics
                max_mag = np.max(magnitude)
                mean_mag = np.mean(magnitude)
                std_mag = np.std(magnitude)
                
                # Classification Logic (Heuristic)
                peak_ratio = max_mag / (mean_mag + 1e-9)
                
                color = None
                angle_text = ""
                
                # 1. Crystalline: Very sharp peaks
                if peak_ratio > 3.5: # Threshold TBD
                    stats["Crystalline"] += 1
                    color = (255, 0, 0) # Blue (BGR)
                
                # 2. Partial: Anisotropic (Directional)
                elif peak_ratio > 2.0:
                    stats["Partial"] += 1
                    color = (0, 165, 255) # Orange (BGR)
                    
                    # Calculate dominant angle using moments of FFT magnitude
                    # (Find centroid of the brightest spots excluding center)
                    _, th = cv2.threshold(magnitude, max_mag*0.7, 255, cv2.THRESH_BINARY)
                    M = cv2.moments(th)
                    if M["m00"] != 0:
                        cX = int(M["m10"] / M["m00"])
                        cY = int(M["m01"] / M["m00"])
                        # Angle relative to center
                        angle = math.degrees(math.atan2(cY - cy, cX - cx))
                        if angle < 0: angle += 180
                        angle_text = f"{int(angle)}"
                
                # 3. Amorphous: Diffuse
                else:
                    stats["Amorphous"] += 1
                    # No color (Transparent)
                
                if color:
                    cv2.rectangle(mask_overlay, (x, y), (x+tile_size, y+tile_size), color, -1)
                    if angle_text:
                        cv2.putText(overlay, angle_text, (x+2, y+tile_size-2), cv2.FONT_HERSHEY_SIMPLEX, 0.3, (255, 255, 255), 1)

        # Blend
        alpha = 0.4
        mask_indices = np.any(mask_overlay > 0, axis=-1)
        overlay[mask_indices] = (overlay[mask_indices] * (1 - alpha) + mask_overlay[mask_indices] * alpha).astype(np.uint8)
        
        log.append(f"Grid: {tile_size}px (20 divs)")
        log.append(f"Stats: Amor={stats['Amorphous']}, Part={stats['Partial']}, Cryst={stats['Crystalline']}")
        
        def to_b64(im): return base64.b64encode(cv2.imencode('.jpg', im)[1]).decode('utf-8')
        return {"type": "image", "raw_b64": to_b64(img_bgr), "proc_b64": to_b64(overlay), "stats": stats, "log": log}

    # --- [F] Diffraction (Center User Input) ---
    @staticmethod
    def analyze_diffraction_nv5(img_bgr, user_center=None):
        log = []
        gray = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2GRAY)
        
        if user_center and len(user_center) == 2 and user_center[0] > 0:
            cx, cy = user_center
            log.append(f"Center: Manual [{cx}, {cy}]")
        else:
            _, th = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)
            cy_f, cx_f = center_of_mass(th)
            if np.isnan(cy_f): cy_f, cx_f = gray.shape[0]//2, gray.shape[1]//2
            cx, cy = int(cx_f), int(cy_f)
            log.append(f"Center: Auto [{cx}, {cy}]")

        g1 = cv2.GaussianBlur(gray, (3, 3), 0)
        g2 = cv2.GaussianBlur(gray, (51, 51), 0)
        diff = cv2.subtract(g1, g2)
        
        coordinates = peak_local_max(diff, min_distance=10, threshold_abs=20)
        
        enhanced = np.log1p(diff.astype(np.float32))
        enhanced = cv2.normalize(enhanced, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)
        result_vis = cv2.cvtColor(enhanced, cv2.COLOR_GRAY2BGR)
        
        cv2.drawMarker(result_vis, (cx, cy), (0, 0, 255), markerType=cv2.MARKER_CROSS, markerSize=20, thickness=2)
        
        peak_list = []
        for p in coordinates:
            py, px = p
            if abs(py-cy) < 5 and abs(px-cx) < 5: continue 
            cv2.circle(result_vis, (px, py), 3, (0, 255, 0), 1)
            peak_list.append(f"[{px},{py}]")

        log.append(f"Spots Found: {len(peak_list)}")
        
        def to_b64(im): return base64.b64encode(cv2.imencode('.jpg', im)[1]).decode('utf-8')
        return {
            "type": "image", 
            "raw_b64": to_b64(img_bgr), 
            "proc_b64": to_b64(result_vis), 
            "stats": {"center": [cx, cy], "spots": len(peak_list)}, 
            "log": log
        }

    # --- [G] SAM (Numbers & Roughness) ---
    @staticmethod
    def analyze_sam_results_nv5(img_rgb, masks, data_type):
        overlay = img_rgb.copy()
        mask_overlay = np.zeros_like(img_rgb)
        h_img, w_img = img_rgb.shape[:2]
        stats = {"count": 0}
        log = []
        
        valid_masks = []
        for ann in masks:
            area = ann['area']; bbox = ann['bbox']; x, y, w, h = [int(v) for v in bbox]
            if area > h_img * w_img * 0.95 or area < 50: continue
            if data_type == "Particle":
                if x <= 1 or y <= 1 or (x + w) >= w_img - 1 or (y + h) >= h_img - 1: continue 
            valid_masks.append(ann)

        if data_type == "Thin Film":
            sorted_masks = sorted(valid_masks, key=lambda x: x['bbox'][1]) # Sort Top-Down
        else:
            sorted_masks = sorted(valid_masks, key=lambda x: x['area'], reverse=True)

        thickness_vals = []; roughness_vals = []; diameters = []

        for i, ann in enumerate(sorted_masks):
            m = ann['segmentation']
            bbox = ann['bbox']; x, y, w, h = [int(v) for v in bbox]
            cx, cy = int(x + w/2), int(y + h/2)
            
            color = (0, 255, 0)
            
            # [Update 2] Thin Film Numbering & Roughness
            if data_type == "Thin Film":
                is_boundary = (i == 0 or i == len(sorted_masks)-1)
                
                # Numbering
                cv2.putText(overlay, f"#{i+1}", (cx, cy), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
                
                if not is_boundary:
                    thk = ann['area'] / w
                    thickness_vals.append(thk)
                    color = (255, 0, 0) 
                else:
                    color = (100, 100, 100) 

                # Interface Roughness (Bottom edge of current layer)
                if i < len(sorted_masks) - 1:
                    ys, xs = np.where(m)
                    if len(xs) > 10:
                        roughness = np.std(ys) 
                        roughness_vals.append(roughness)
                        # Draw Line & Roughness Value
                        cv2.line(overlay, (x, y+h), (x+w, y+h), (0, 255, 255), 2)
                        cv2.putText(overlay, f"R:{roughness:.1f}", (x+5, y+h-5), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (0, 255, 255), 1)

            elif data_type == "Particle":
                equivalent_diameter = 2 * np.sqrt(ann['area'] / np.pi)
                diameters.append(equivalent_diameter)

            mask_overlay[m] = color
            mask_uint8 = (m * 255).astype(np.uint8)
            cnts, _ = cv2.findContours(mask_uint8, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
            if cnts: cv2.drawContours(overlay, [cnts[0]], -1, color, 2)

        alpha = 0.35
        mask_indices = np.any(mask_overlay > 0, axis=-1)
        overlay[mask_indices] = (overlay[mask_indices] * (1 - alpha) + mask_overlay[mask_indices] * alpha).astype(np.uint8)

        if data_type == "Thin Film":
            avg_thk = round(np.mean(thickness_vals), 2) if thickness_vals else 0
            stats["thickness_avg"] = avg_thk
            log.append(f"Layers: {len(sorted_masks)} (Excl. Top/Bot)")
            log.append(f"Avg Thk: {avg_thk}px")
        elif data_type == "Particle":
            avg_dia = round(np.mean(diameters), 2) if diameters else 0
            stats["diameter_avg"] = avg_dia
            log.append(f"Particles: {len(sorted_masks)}")
        else:
            log.append(f"Detected: {len(sorted_masks)}")

        return overlay, stats, log

    @staticmethod
    def process_image_nv5(img_bytes, equipment, data_type, mode, goal, user_center=None):
        nparr = np.frombuffer(img_bytes, np.uint8)
        img_raw = cv2.imdecode(nparr, cv2.IMREAD_COLOR)
        if img_raw is None: return {}
        
        # [Update 1] Split Footer First
        body, footer = ScienceProcessorNV5.separate_footer(img_raw)
        
        footer_b64 = None
        if footer is not None:
            footer_b64 = base64.b64encode(cv2.imencode('.jpg', footer)[1]).decode('utf-8')

        if data_type == "Crystal Structure (FFT)":
            return {**ScienceProcessorNV5.analyze_crystal_structure(body), "footer_b64": footer_b64}
            
        if data_type == "2D Diffraction":
            return {**ScienceProcessorNV5.analyze_diffraction_nv5(body, user_center), "footer_b64": footer_b64}
        
        # General SAM
        if mode == "None":
             def to_b64(im): return base64.b64encode(cv2.imencode('.jpg', im)[1]).decode('utf-8')
             return {"type":"image", "raw_b64":to_b64(body), "proc_b64":to_b64(body), "stats":{}, "log":["Mode: Raw"], "footer_b64": footer_b64}

        img_proc = body.copy()
        log = []
        if mode == "Auto" or mode == "AI-Adaptive":
            img_proc = cv2.bilateralFilter(img_proc, 9, 75, 75)
            log.append("Denoise: Bilateral")

        img_rgb = cv2.cvtColor(img_proc, cv2.COLOR_BGR2RGB)
        
        if mask_generator:
            try:
                masks = mask_generator.generate(img_rgb)
                log.append(f"SAM: {len(masks)} Segments")
                overlay, stats, sam_log = ScienceProcessorNV5.analyze_sam_results_nv5(img_rgb, masks, data_type)
                log.extend(sam_log)
                overlay = cv2.cvtColor(overlay, cv2.COLOR_RGB2BGR)
            except Exception as e:
                log.append(f"SAM Error: {str(e)}")
                overlay = body
                stats = {}
        else:
            log.append("SAM Error: Not Loaded")
            overlay = body
            stats = {}

        def to_b64(im): return base64.b64encode(cv2.imencode('.jpg', im)[1]).decode('utf-8')
        return {
            "type": "image", "raw_b64": to_b64(body), "proc_b64": to_b64(overlay), 
            "stats": stats, "log": log, "footer_b64": footer_b64
        }

# [4] Helper Async Functions
async def analyze_vision_ollama(image_bytes: bytes, prompt: str) -> str:
    try:
        b64 = base64.b64encode(image_bytes).decode('utf-8')
        res = await asyncio.to_thread(ollama_client.chat, model=VISION_MODEL, messages=[{'role':'user','content':prompt,'images':[b64]}])
        return res['message']['content']
    except: return "Vision Error"

async def interpret_spectrum_data(raw_context: str, equipment: str) -> str:
    try:
        prompt = f"Analyze spectrum ({equipment}). Brief interpretation in Korean. Data: {raw_context}"
        res = await asyncio.to_thread(ollama_client.chat, model=TEXT_MODEL, messages=[{'role':'user','content':prompt}])
        return res['message']['content']
    except: return ""

# [5] App & Job System
app = FastAPI(title="Analyst NV5 Refined", lifespan=lifespan)
app.add_middleware(CORSMiddleware, allow_origins=["*"], allow_methods=["*"], allow_headers=["*"])

class TranslateReq(BaseModel): text: str

@app.post("/api/translate")
async def translate_text(req: TranslateReq):
    try:
        res = await asyncio.to_thread(ollama_client.chat, model=TEXT_MODEL, messages=[{'role':'system','content':'Translate to English.'}, {'role':'user','content':req.text}])
        return {"translated": res['message']['content']}
    except Exception as e: return {"translated": str(e)}

async def process_analysis_job(job_id: str, files_map: Dict, configs: Dict):
    try:
        JOBS[job_id]["status"] = "Processing"
        JOBS[job_id]["progress"] = 10
        final_results = []
        total_files = len(configs)
        processed_count = 0

        for filename, config in configs.items():
            if filename not in files_map: continue
            content = files_map[filename]
            eq = config.get("equipment", "General")
            data_type = config.get("data_type", "General")
            goal = config.get("goal", "")
            mode = config.get("mode", "Auto")
            
            user_center = None
            if config.get("center_coords"):
                try: user_center = [int(v) for v in config["center_coords"].split(",")]
                except: pass

            fname_lower = filename.lower()
            JOBS[job_id]["step"] = f"Analyzing {filename}..."
            
            # [A] Document
            if eq == "Document" or fname_lower.endswith(('.pdf', '.ppt', '.pptx')):
                try:
                    if fname_lower.endswith('.pdf'):
                        images = convert_from_bytes(content, dpi=150, fmt='jpeg')[:10]
                        full_txt = ""; pages = []
                        for idx, img in enumerate(images):
                            buf = io.BytesIO(); img.save(buf, format="JPEG")
                            b64 = base64.b64encode(buf.getvalue()).decode()
                            desc = await analyze_vision_ollama(buf.getvalue(), "한글 요약.")
                            full_txt += f"\nPage {idx+1}: {desc}"
                            pages.append({"page_num": idx+1, "image_b64": b64, "desc": desc})
                        final_results.append({"type": "pdf", "filename": filename, "equipment": "Lit", "raw_context": full_txt, "pages": pages})
                    elif fname_lower.endswith(('.ppt', '.pptx')):
                        prs = Presentation(io.BytesIO(content))
                        slides = []; full_txt = ""
                        for i, slide in enumerate(prs.slides):
                            txt = ""; imgs = []
                            for s in slide.shapes:
                                if hasattr(s, "text"): txt += s.text + "\n"
                                if s.shape_type == MSO_SHAPE_TYPE.PICTURE:
                                    try:
                                        ib64 = base64.b64encode(s.image.blob).decode('utf-8')
                                        desc = await analyze_vision_ollama(s.image.blob, "한글 설명.")
                                        imgs.append({"b64": ib64, "desc": desc})
                                    except: pass
                            slides.append({"slide_num": i+1, "text": txt, "images": imgs})
                        final_results.append({"type": "ppt", "filename": filename, "equipment": "Lit", "raw_context": full_txt, "slides": slides})
                except Exception as e: final_results.append({"type": "error", "filename": filename, "msg": str(e)})

            # [B] Spectrum
            elif fname_lower.endswith(('.xlsx', '.xls', '.csv', '.txt')):
                try:
                    dfs = ScienceProcessorNV5.read_universal_dataframe(content, filename)
                    for i, df in enumerate(dfs):
                        v_blocks = ScienceProcessorNV5.detect_structured_blocks(df)
                        for v_block in v_blocks:
                            h_blocks = ScienceProcessorNV5.split_horizontal_blocks(v_block)
                            for h_block in h_blocks:
                                series_list = ScienceProcessorNV5.extract_series_from_df(h_block)
                                for s in series_list:
                                    res = ScienceProcessorNV5.process_spectrum(s['x'], s['y'], mode, goal)
                                    step = max(1, len(s['x'])//100)
                                    chart = [{"x": float(res["x"][k]), "y_proc": float(res["y_proc"][k]), "y_raw": float(res["y_raw"][k]), "y_base": float(res["y_base"][k])} for k in range(0, len(s['x']), step)]
                                    ctx = f"Data: {s['name']}\nStats: {res['stats']}"
                                    interp = await interpret_spectrum_data(ctx, eq)
                                    final_results.append({
                                        "type": "spectrum", "filename": f"{filename} ({s['name']})", "equipment": eq,
                                        "raw_context": f"{ctx}\nInterp: {interp}", 
                                        "chart_data": chart, "log": res["log"], "interpretation": interp
                                    })
                except Exception as ex: final_results.append({"type": "error", "filename": filename, "msg": str(ex)})

            # [C] Image (NV5 Refined)
            elif fname_lower.endswith(('.png', '.jpg', '.jpeg', '.tif', '.bmp')):
                try:
                    JOBS[job_id]["step"] = f"Vision Analysis: {filename}..."
                    vis_res = await asyncio.to_thread(ScienceProcessorNV5.process_image_nv5, content, eq, data_type, mode, goal, user_center)
                    
                    if vis_res:
                        # [Update 1] Footer to Scale Info
                        scale_info = "Unknown"
                        if vis_res.get("footer_b64"):
                            JOBS[job_id]["step"] = f"Reading Scale: {filename}..."
                            # Vision LLM에게 Footer만 보내서 Scale Bar 읽게 함
                            scale_prompt = "Read the scale bar text (e.g. '100 nm', '2 um') from this image footer. Return ONLY the value."
                            scale_text = await analyze_vision_ollama(base64.b64decode(vis_res["footer_b64"]), scale_prompt)
                            scale_info = scale_text
                            vis_res["log"].append(f"Scale Bar: {scale_text}")

                        # Main Description
                        desc_prompt = f"Analyze this {eq} image ({data_type}). Scale: {scale_info}. Goal: {goal}. Stats: {vis_res.get('stats',{})}. Summary in Korean."
                        desc = await analyze_vision_ollama(base64.b64decode(vis_res["proc_b64"]), desc_prompt)
                        
                        final_results.append({
                            "type": "image", "filename": filename, "equipment": eq, "summary": desc,
                            "raw_context": f"Img {eq}: {desc}\nScale: {scale_info}",
                            "raw_b64": vis_res["raw_b64"], "proc_b64": vis_res["proc_b64"], "footer_b64": vis_res.get("footer_b64"),
                            "stats": vis_res.get("stats",{}), "log": vis_res.get("log", [])
                        })
                except Exception as ex: final_results.append({"type": "error", "filename": filename, "msg": str(ex)})
            
            processed_count += 1
            JOBS[job_id]["progress"] = 10 + int((processed_count / total_files) * 80)

        JOBS[job_id]["step"] = "Writing Report..."
        final_results = sanitize_json(final_results)
        
        data_ctx = ""
        for r in final_results:
            if r.get("type") != "error":
                raw = r.get("raw_context") or ""
                data_ctx += f"\n=== {r['filename']} ===\n{raw[:1000]}\n"

        final_report = "Fail"
        if data_ctx:
            try:
                system_prompt = "You are a research assistant. Write a report in Korean. Summarize data and physics."
                res = await asyncio.to_thread(ollama_client.chat, model=TEXT_MODEL, messages=[{'role':'system','content':system_prompt}, {'role':'user','content':f"Data:\n{data_ctx}"}])
                final_report = res['message']['content']
            except Exception as e: final_report = str(e)

        JOBS[job_id]["results"] = {"results": final_results, "final_report": final_report}
        JOBS[job_id]["status"] = "Completed"
        JOBS[job_id]["progress"] = 100
    except Exception as e:
        JOBS[job_id]["status"] = "Failed"
        JOBS[job_id]["error"] = str(e)

@app.post("/api/analyze")
async def start_analysis(background_tasks: BackgroundTasks, files: List[UploadFile] = File(...), file_configs: str = Form(...)):
    job_id = str(uuid.uuid4())
    configs = json.loads(file_configs)
    files_map = {f.filename: await f.read() for f in files}
    JOBS[job_id] = {"status": "Pending", "progress": 0, "step": "Init", "results": None}
    background_tasks.add_task(process_analysis_job, job_id, files_map, configs)
    return {"job_id": job_id}

@app.get("/api/status/{job_id}")
async def get_status(job_id: str): return JOBS.get(job_id, {"status": "Not Found"})

@app.get("/", response_class=HTMLResponse)
async def serve_index():
    if os.path.exists("index.html"): return FileResponse("index.html")
    return "<h1>NV5 Refined Running</h1>"

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)






엔브이5 웹 코드


<!DOCTYPE html>
<html lang="ko">
<head>
    <meta charset="UTF-8">
    <title>Analyst NV5 Physics</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <script src="https://unpkg.com/vue@3/dist/vue.global.js"></script>
    <script src="https://cdn.plot.ly/plotly-2.27.0.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/marked/marked.min.js"></script>
    <script src="https://unpkg.com/lucide@latest"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/html2pdf.js/0.10.1/html2pdf.bundle.min.js"></script>
    <style>
        @import url('https://fonts.googleapis.com/css2?family=Inter:wght@400;600;700&display=swap');
        body { font-family: 'Inter', sans-serif; }
        .prose table { border-collapse: collapse; width: 100%; margin: 1em 0; }
        .prose th, .prose td { border: 1px solid #cbd5e1; padding: 8px; font-size: 0.9rem; }
        .prose th { background-color: #f1f5f9; }
    </style>
</head>
<body class="bg-slate-100 text-slate-800">
    <div id="app" class="min-h-screen p-6">
        <header class="max-w-7xl mx-auto mb-6 flex items-center gap-3">
            <div class="bg-emerald-600 p-2 rounded-lg shadow"><i data-lucide="atom" class="w-6 h-6 text-white"></i></div>
            <div><h1 class="text-2xl font-bold">Analyst NV5 <span class="text-sm font-normal text-slate-500">Deep Physics & Diffraction</span></h1></div>
        </header>

        <main class="max-w-7xl mx-auto grid grid-cols-1 lg:grid-cols-12 gap-6">
            <div class="lg:col-span-4 space-y-4">
                <div class="bg-white p-5 rounded-xl shadow-sm border border-slate-200">
                    <h2 class="font-bold mb-3 flex items-center gap-2"><i data-lucide="files" class="w-4 h-4"></i> Data Staging</h2>
                    <div class="border-2 border-dashed border-slate-300 rounded-lg p-6 text-center hover:bg-emerald-50 cursor-pointer" @click="$refs.f.click()">
                        <input type="file" ref="f" multiple class="hidden" @change="addFiles">
                        <p class="text-sm font-semibold text-slate-600">Add Files</p>
                    </div>
                    <div v-if="stagedFiles.length" class="mt-4 space-y-3 max-h-[60vh] overflow-y-auto pr-1">
                        <div v-for="(item, i) in stagedFiles" :key="i" class="p-3 bg-slate-50 border rounded-lg text-sm">
                            <div class="flex justify-between items-center mb-2">
                                <span class="font-bold truncate w-2/3">{{ item.file.name }}</span>
                                <button @click="removeFile(i)" class="text-red-400 hover:text-red-600"><i data-lucide="trash-2" class="w-4 h-4"></i></button>
                            </div>
                            <div class="grid grid-cols-1 gap-2">
                                <select v-model="item.equipment" class="w-full p-1.5 border rounded text-xs bg-white font-semibold text-emerald-700">
                                    <optgroup label="Spectroscopy (1D)">
                                        <option value="XPS">XPS</option>
                                        <option value="XRD">XRD (1D)</option>
                                        <option value="EELS">EELS</option>
                                        <option value="EDS">EDS</option>
                                        <option value="FT-IR">FT-IR</option>
                                        <option value="NMR">NMR</option>
                                        <option value="UV-VIS">UV-VIS</option>
                                        <option value="PL">Photo-luminescence</option>
                                        <option value="Time-resolved">Time-resolved</option>
                                    </optgroup>
                                    <optgroup label="Microscopy (2D)">
                                        <option value="SEM">SEM</option>
                                        <option value="TEM">TEM</option>
                                        <option value="AFM">AFM</option>
                                        <option value="Optical">Optical</option>
                                        <option value="2D Spectrum">2D Spectrum</option>
                                    </optgroup>
                                    <optgroup label="Diffraction (2D)">
                                        <option value="Diffraction">Diffraction (2D)</option>
                                    </optgroup>
                                    <optgroup label="Literature">
                                        <option value="Document">Document</option>
                                    </optgroup>
                                </select>
                                
                                <select v-if="isImage(item)" v-model="item.dataType" class="w-full p-1.5 border rounded text-xs bg-slate-50 font-bold">
                                    <option v-if="item.equipment==='Diffraction'" value="2D Diffraction">2D Diffraction Analysis</option>
                                    <option v-else value="General">General Object</option>
                                    
                                    <option v-if="item.equipment!=='Diffraction'" value="Particle">Particle Analysis (Border Excl.)</option>
                                    <option v-if="item.equipment!=='Diffraction'" value="Thin Film">Thin Film (Roughness)</option>
                                    <option v-if="item.equipment!=='Diffraction'" value="Crystal Structure (FFT)">Crystal Structure (FFT)</option>
                                </select>

                                <div v-if="item.dataType === '2D Diffraction'" class="flex gap-2">
                                    <input v-model="item.centerX" type="number" placeholder="Center X" class="w-1/2 p-1.5 border rounded text-xs">
                                    <input v-model="item.centerY" type="number" placeholder="Center Y" class="w-1/2 p-1.5 border rounded text-xs">
                                </div>

                                <select v-if="!isDoc(item)" v-model="item.mode" class="w-full p-1.5 border rounded text-xs bg-white">
                                    <option value="Auto">✨ Auto Process</option>
                                    <option value="None">🚫 Raw Data</option>
                                    <option value="AI-Adaptive">🧠 Agent Mode</option>
                                </select>
                            </div>
                        </div>
                    </div>
                    <button @click="startAnalysis" :disabled="isAnalyzing || stagedFiles.length===0" class="w-full mt-4 py-3 bg-emerald-600 text-white rounded-lg font-bold flex justify-center gap-2 shadow hover:bg-emerald-700 disabled:opacity-50">
                        <span>Generate Report</span>
                    </button>
                </div>
            </div>

            <div class="lg:col-span-8">
                <div v-if="isAnalyzing" class="bg-white p-6 rounded-xl shadow border border-slate-200 text-center py-12">
                    <div class="animate-spin rounded-full h-12 w-12 border-b-2 border-emerald-600 mx-auto mb-4"></div>
                    <h3 class="text-lg font-bold text-slate-700">Analyzing...</h3>
                    <p class="text-sm text-slate-500 mb-4">{{ currentStep }}</p>
                    <div class="w-full bg-slate-200 rounded-full h-2.5 mb-2">
                        <div class="bg-emerald-600 h-2.5 rounded-full transition-all duration-500" :style="{width: progress + '%'}"></div>
                    </div>
                    <p class="text-xs text-right text-slate-400">{{ progress }}%</p>
                </div>

                <div v-if="result && !isAnalyzing" class="space-y-6 animate-fade-in">
                    <div class="flex justify-between items-center bg-white p-2 rounded-xl shadow-sm border border-slate-200">
                        <div class="flex gap-2">
                            <button @click="setLang('ko')" :class="lang==='ko'?'bg-emerald-100 text-emerald-700 font-bold':'text-slate-500'" class="px-4 py-2 rounded text-sm">KR</button>
                            <button @click="setLang('en')" :class="lang==='en'?'bg-blue-100 text-blue-700 font-bold':'text-slate-500'" class="px-4 py-2 rounded text-sm">EN</button>
                        </div>
                        <button @click="dlPDF" class="px-3 py-1.5 bg-slate-800 text-white text-xs rounded hover:bg-slate-700"><i class="lucide-file-down mr-1"></i> Save PDF</button>
                    </div>

                    <div id="report-view" class="bg-white p-8 rounded-xl shadow-sm border border-slate-200">
                        <section class="mb-10">
                            <h2 class="text-lg font-bold text-emerald-700 mb-3 border-b pb-1">1. Synthesis</h2>
                            <div class="prose prose-sm text-slate-700 max-w-none" v-html="md(displayReport)"></div>
                        </section>
                        <section>
                            <h2 class="text-lg font-bold text-slate-800 mb-5 border-b pb-1">2. Data Evidence</h2>
                            <div v-for="(item, idx) in result.results" :key="idx" class="mb-8 p-4 bg-slate-50 rounded-lg border">
                                <div class="flex items-center justify-between mb-3">
                                    <div class="flex items-center gap-2">
                                        <span class="px-2 py-0.5 bg-white border rounded text-[10px] font-bold uppercase text-slate-500">{{ item.equipment }}</span>
                                        <h3 class="font-bold text-sm">{{ item.filename }}</h3>
                                    </div>
                                    <button @click="item.showDocs = !item.showDocs" class="text-xs text-emerald-600 font-bold hover:underline flex items-center gap-1">Info</button>
                                </div>

                                <div v-if="item.chart_data" class="bg-white p-2 rounded border mb-3">
                                    <div class="flex gap-2 justify-end mb-1">
                                        <button @click="toggleTrace(idx, 0)" class="text-[10px] px-2 py-0.5 border rounded">Raw</button>
                                        <button @click="toggleTrace(idx, 1)" class="text-[10px] px-2 py-0.5 border rounded bg-amber-50">Base</button>
                                        <button @click="toggleTrace(idx, 2)" class="text-[10px] px-2 py-0.5 border rounded font-bold text-emerald-600">Proc</button>
                                    </div>
                                    <div :id="'chart-'+idx" class="w-full h-60"></div>
                                    <div v-if="item.interpretation && item.showDocs" class="mt-3 p-3 bg-emerald-50 border border-emerald-100 rounded text-xs text-emerald-800">
                                        <div class="prose prose-xs" v-html="md(item.interpretation)"></div>
                                    </div>
                                </div>

                                <div v-if="item.raw_b64" class="bg-white p-2 rounded border mb-3">
                                    <div class="flex gap-2 mb-2 text-xs justify-center">
                                        <button @click="item.view='proc'" :class="item.view!=='raw'?'font-bold text-emerald-600':''">Processed</button>
                                        <span class="text-slate-300">|</span>
                                        <button @click="item.view='raw'" :class="item.view==='raw'?'font-bold text-emerald-600':''">Original</button>
                                    </div>
                                    <img :src="'data:image/jpeg;base64,' + (item.view==='raw' ? item.raw_b64 : item.proc_b64)" class="max-h-96 mx-auto object-contain bg-black">
                                    <div v-if="item.stats" class="mt-2 p-2 bg-emerald-50 border border-emerald-100 rounded text-xs grid grid-cols-2 gap-2">
                                        <div v-for="(v, k) in item.stats" :key="k">
                                            <span class="font-bold text-emerald-700 capitalize">{{ k }}:</span> {{ v }}
                                        </div>
                                    </div>
                                    <div v-if="item.summary" class="mt-2 text-xs text-slate-600 prose bg-slate-50 p-2" v-html="md(item.summary)"></div>
                                </div>

                                <div v-if="item.log && item.showDocs" class="mt-2 p-2 bg-gray-100 rounded border text-[10px] font-mono text-gray-600">
                                    <ul class="list-disc pl-4 space-y-0.5"><li v-for="l in item.log">{{ l }}</li></ul>
                                </div>
                            </div>
                        </section>
                    </div>
                </div>
            </div>
        </main>
    </div>
    <script>
        const { createApp, ref, computed, nextTick } = Vue;
        createApp({
            setup() {
                const stagedFiles = ref([]);
                const isAnalyzing = ref(false);
                const isTranslating = ref(false);
                const progress = ref(0);
                const currentStep = ref("");
                const result = ref(null);
                const lang = ref('ko');
                const translatedReport = ref('');
                const md = (t) => marked.parse(t||'');
                const displayReport = computed(() => lang.value === 'en' && translatedReport.value ? translatedReport.value : (result.value ? result.value.final_report : ''));

                const addFiles = (e) => {
                    Array.from(e.target.files).forEach(f => {
                        let eq = 'SEM'; let dt = 'General';
                        if(f.name.match(/\.(pdf|ppt|pptx)$/i)) eq = 'Document';
                        else if(f.name.match(/\.(xlsx|csv|txt)$/i)) eq = 'XPS';
                        stagedFiles.value.push({ file: f, equipment: eq, dataType: dt, goal: '', mode: 'Auto', view: 'proc', showDocs: true, centerX: null, centerY: null });
                    });
                    e.target.value = ''; 
                };
                const removeFile = (i) => stagedFiles.value.splice(i, 1);
                const isDoc = (item) => item.equipment === 'Document';
                const isImage = (item) => ['SEM','TEM','AFM','Optical','Diffraction','2D Spectrum'].includes(item.equipment);

                const startAnalysis = async () => {
                    isAnalyzing.value = true; progress.value = 0; currentStep.value = "Init...";
                    const fd = new FormData();
                    const configs = {};
                    stagedFiles.value.forEach(item => {
                        fd.append('files', item.file);
                        configs[item.file.name] = { 
                            equipment: item.equipment, 
                            data_type: item.equipment === 'Diffraction' ? '2D Diffraction' : item.dataType,
                            goal: item.goal, mode: item.mode,
                            center_coords: (item.centerX && item.centerY) ? `${item.centerX},${item.centerY}` : null
                        };
                    });
                    fd.append('file_configs', JSON.stringify(configs));
                    try {
                        const initRes = await fetch('/api/analyze', { method: 'POST', body: fd });
                        const { job_id } = await initRes.json();
                        const poll = setInterval(async () => {
                            const statusRes = await fetch(`/api/status/${job_id}`);
                            const status = await statusRes.json();
                            progress.value = status.progress || 0;
                            currentStep.value = status.step || "Processing...";
                            if (status.status === "Completed") {
                                clearInterval(poll);
                                result.value = status.results;
                                if(result.value.results) result.value.results.forEach(r => r.showDocs = true);
                                isAnalyzing.value = false;
                                await nextTick();
                                renderCharts(result.value.results);
                            } else if (status.status === "Failed") {
                                clearInterval(poll); alert("Error: " + status.error); isAnalyzing.value = false;
                            }
                        }, 1000);
                    } catch(e) { alert(e); isAnalyzing.value = false; }
                };

                const renderCharts = (items) => {
                    items.forEach((item, i) => {
                        if(item.chart_data) {
                            const raw = { x: item.chart_data.map(d=>d.x), y: item.chart_data.map(d=>d.y_raw), mode:'lines', name:'Raw', line:{color:'#cbd5e1', width:1}, visible: true };
                            const base = { x: item.chart_data.map(d=>d.x), y: item.chart_data.map(d=>d.y_base), mode:'lines', name:'Base', line:{color:'#f59e0b', width:1, dash:'dot'}, visible: true };
                            const proc = { x: item.chart_data.map(d=>d.x), y: item.chart_data.map(d=>d.y_proc), mode:'lines', name:'Proc', line:{color:'#059669', width:2}, visible: true };
                            Plotly.newPlot('chart-'+i, [raw, base, proc], {margin:{t:10,b:30,l:40,r:10}, showlegend:false}, {displayModeBar:false});
                        }
                    });
                    lucide.createIcons();
                };

                const toggleTrace = (idx, traceIdx) => {
                    const el = document.getElementById('chart-'+idx);
                    if(el) {
                        const current = el.data[traceIdx].visible;
                        const next = current === true ? 'legendonly' : true;
                        Plotly.restyle(el, {'visible': next}, [traceIdx]);
                    }
                };

                const dlPDF = () => {
                    const now = new Date();
                    const ts = now.toISOString().slice(2,10).replace(/-/g,'') + '_' + now.toTimeString().slice(0,4).replace(':','');
                    const eqList = [...new Set(stagedFiles.value.map(f=>f.equipment))].join('-');
                    html2pdf().set({ margin:10, filename:`${ts}_${eqList}_Report.pdf`, image:{type:'jpeg',quality:0.98}, html2canvas:{scale:2}, jsPDF:{unit:'mm',format:'a4'} }).from(document.getElementById('report-view')).save();
                };

                const setLang = async (target) => {
                    if (target === 'ko') { lang.value = 'ko'; return; }
                    if (target === 'en') {
                        if (translatedReport.value) { lang.value = 'en'; return; }
                        isTranslating.value = true;
                        try {
                            const res = await fetch('/api/translate', {
                                method: 'POST', headers: {'Content-Type': 'application/json'},
                                body: JSON.stringify({ text: result.value.final_report })
                            });
                            const data = await res.json();
                            translatedReport.value = data.translated;
                            lang.value = 'en';
                        } catch(e) { alert("Trans Error: " + e); }
                        finally { isTranslating.value = false; }
                    }
                };

                setTimeout(()=>lucide.createIcons(), 100);
                return { stagedFiles, isAnalyzing, progress, currentStep, result, addFiles, removeFile, startAnalysis, md, toggleTrace, isDoc, isImage, dlPDF, setLang, lang, displayReport, isTranslating };
            }
        }).mount('#app');
    </script>
</body>
</html>






엔브이 5 코드

import os
import io
import asyncio
import base64
import json
import re
import math
import uuid
import time
from typing import List, Dict, Any, Optional
from pydantic import BaseModel
from contextlib import asynccontextmanager

# [0] OMP 에러 방지
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"

# [1] 보안 설정
os.environ['NO_PROXY'] = '127.0.0.1,localhost,0.0.0.0,::1'
if 'HTTP_PROXY' in os.environ: del os.environ['HTTP_PROXY']
if 'HTTPS_PROXY' in os.environ: del os.environ['HTTPS_PROXY']

from fastapi import FastAPI, UploadFile, File, Form, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import HTMLResponse, FileResponse
import pandas as pd
import numpy as np

# 과학/영상 연산
from scipy.signal import find_peaks, savgol_filter
from scipy.optimize import curve_fit
from scipy.ndimage import center_of_mass, map_coordinates
from skimage.feature import peak_local_max
import cv2
import torch

# SAM & LLM & Docs
from segment_anything import sam_model_registry, SamAutomaticMaskGenerator
from ollama import Client 
from pdf2image import convert_from_bytes
from pptx import Presentation
from pptx.enum.shapes import MSO_SHAPE_TYPE

# [2] 설정
OLLAMA_HOST = "http://127.0.0.1:11434"
ollama_client = Client(host=OLLAMA_HOST)
VISION_MODEL = "qwen3-vl:30b-a3b-instruct" 
TEXT_MODEL = "llama3:70b"

SAM_CHECKPOINT = "sam_vit_b_01ec64.pth"
SAM_MODEL_TYPE = "vit_b"
device = "cuda" if torch.cuda.is_available() else "cpu"

mask_generator = None 
JOBS = {} 

@asynccontextmanager
async def lifespan(app: FastAPI):
    global mask_generator
    if os.path.exists(SAM_CHECKPOINT):
        print(f"Loading SAM ({SAM_MODEL_TYPE}) on {device}...")
        try:
            sam = sam_model_registry[SAM_MODEL_TYPE](checkpoint=SAM_CHECKPOINT)
            sam.to(device=device)
            mask_generator = SamAutomaticMaskGenerator(
                model=sam, points_per_side=32, pred_iou_thresh=0.88,
                stability_score_thresh=0.92, min_mask_region_area=100
            )
            print("SAM Loaded.")
        except: print("SAM Load Failed.")
    yield
    JOBS.clear()

def sanitize_json(obj):
    if isinstance(obj, float):
        if np.isnan(obj) or np.isinf(obj): return 0.0
        return obj
    elif isinstance(obj, dict): return {k: sanitize_json(v) for k, v in obj.items()}
    elif isinstance(obj, list): return [sanitize_json(v) for v in obj]
    elif isinstance(obj, (np.int64, np.int32)): return int(obj)
    elif isinstance(obj, (np.float64, np.float32)):
        if np.isnan(obj) or np.isinf(obj): return 0.0
        return float(obj)
    return obj

# ==========================================
# [3] NV5 과학 엔진
# ==========================================
class ScienceProcessorNV5:
    
    # --- [A] Universal Loader (NV2 Robust 유지) ---
    @staticmethod
    def read_universal_dataframe(content: bytes, filename: str) -> List[pd.DataFrame]:
        dfs = []
        fname = filename.lower()
        if fname.endswith(('.xlsx', '.xls')):
            try:
                xls = pd.ExcelFile(io.BytesIO(content))
                for s in xls.sheet_names:
                    df = pd.read_excel(xls, sheet_name=s, header=None)
                    num_count = df.apply(pd.to_numeric, errors='coerce').notna().sum().sum()
                    if num_count < 5: raise Exception("Not enough numbers")
                    if not df.empty: dfs.append(df)
                if dfs: return dfs
            except: pass
        text_content = ""
        for enc in ['utf-8', 'cp949', 'latin1', 'utf-16']:
            try: text_content = content.decode(enc); break
            except: continue
        if text_content:
            lines = text_content.splitlines()
            data_start = -1; delimiter = None
            separators = [',', '\t', ';', '\s+']
            for i, line in enumerate(lines[:100]):
                for sep in separators:
                    parts = line.split() if sep=='\s+' else line.split(sep)
                    nums = sum(1 for p in parts if re.match(r'^-?\d+(\.\d+)?(?:[eE][+-]?\d+)?$', p.strip()))
                    if nums >= 2: data_start = i; delimiter = sep; break
                if data_start != -1: break
            if data_start != -1:
                try:
                    data_str = "\n".join(lines[data_start:])
                    sep_arg = '\s+' if delimiter=='\s+' else delimiter
                    df = pd.read_csv(io.StringIO(data_str), sep=sep_arg, header=None, engine='python')
                    dfs.append(df)
                except: pass
        return dfs

    # --- [B] Block Parser (NV2 Robust) ---
    @staticmethod
    def detect_structured_blocks(df: pd.DataFrame) -> List[pd.DataFrame]:
        blocks = []
        if df.empty: return []
        df_num = df.apply(pd.to_numeric, errors='coerce')
        if df.shape[1] > 0: is_data_row = df_num.iloc[:, 0].notna()
        else: is_data_row = df_num.notna().any(axis=1)
        groups = (is_data_row != is_data_row.shift()).cumsum()
        for _, group in df.groupby(groups):
            if is_data_row.loc[group.index[0]]:
                block = df_num.loc[group.index].dropna(how='all', axis=1)
                if block.shape[0] >= 10: blocks.append(block.reset_index(drop=True))
        return blocks

    @staticmethod
    def split_horizontal_blocks(df: pd.DataFrame) -> List[pd.DataFrame]:
        sub_blocks = []
        if df.empty: return []
        col_counts = df.apply(pd.to_numeric, errors='coerce').notna().sum()
        valid_col_indices = np.where(col_counts >= 2)[0]
        if len(valid_col_indices) == 0: return []
        split_points = np.where(np.diff(valid_col_indices) > 1)[0] + 1
        groups = np.split(valid_col_indices, split_points)
        for g in groups:
            if len(g) > 0:
                sub_df = df.iloc[:, g].reset_index(drop=True)
                if sub_df.shape[1] >= 1 and sub_df.shape[0] >= 2: sub_blocks.append(sub_df)
        return sub_blocks

    @staticmethod
    def extract_series_from_df(df: pd.DataFrame) -> List[Dict]:
        series_list = []
        try:
            vals = df.values; rows, cols = vals.shape
            if rows < 5: return []
            if cols == 1:
                y = vals[:, 0]; mask = ~np.isnan(y)
                if np.sum(mask) > 5: series_list.append({"x": np.arange(len(y))[mask], "y": y[mask], "name": "Single"})
                return series_list
            
            # Simple Axis Detection (NV4 Style)
            def is_axis(arr):
                arr = arr[~np.isnan(arr)]
                if len(arr) < 5: return False
                diff = np.diff(arr)
                if np.all(diff >= 0) or np.all(diff <= 0): return True
                return (np.sum(diff>0)/len(diff)>0.9) or (np.sum(diff<0)/len(diff)>0.9)

            current_x = vals[:, 0]
            if not is_axis(current_x): current_x = np.arange(rows)
            for i in range(1, cols):
                col_data = vals[:, i]
                mask_valid = ~np.isnan(col_data)
                if np.sum(mask_valid) < 5: continue
                if is_axis(col_data): current_x = col_data; continue 
                mask = ~np.isnan(current_x) & ~np.isnan(col_data)
                if np.sum(mask) > 5:
                    series_list.append({"x": current_x[mask], "y": col_data[mask], "name": f"Col-{i}"})
        except: pass
        return series_list

    # --- [C] Spectrum Logic (Updated Log) ---
    @staticmethod
    def simple_baseline(y):
        try:
            w = max(5, len(y)//10)
            s = pd.Series(y).rolling(window=w, center=True).min()
            b = s.bfill().ffill().values 
            if len(b) > 51: b = savgol_filter(b, 51, 3)
            return b, w
        except: return np.zeros_like(y), 0

    @staticmethod
    def process_spectrum(x, y, mode, goal):
        log = []
        y_raw = y.copy(); base = np.zeros_like(y); y_proc = y.copy()
        peaks = []; fits = []; stats_txt = "Error"
        try:
            if mode == "None":
                peaks, _ = find_peaks(y, height=np.max(y)*0.05)
                log.append("Mode: Raw Data Only")
                stats_txt = f"Raw Peaks: {len(peaks)}"
            else:
                win = 15; do_fit = False
                if mode == "AI-Adaptive":
                    log.append("Mode: AI-Adaptive")
                    if "noise" in goal.lower(): win = 31; log.append("AI: Strong Smoothing")
                    if "fit" in goal.lower(): do_fit = True; log.append("AI: Fitting On")
                else: log.append("Mode: Auto")
                
                base, bw = ScienceProcessorNV5.simple_baseline(y)
                y_proc = np.maximum(y - base, 0)
                log.append(f"Baseline Removed (Rolling Min, Win={bw})")
                
                if len(y_proc) > win:
                    y_proc = savgol_filter(y_proc, win, 3)
                    log.append(f"Smoothing (Savitzky-Golay, Win={win}, Poly=3)")
                
                peaks, _ = find_peaks(y_proc, height=np.max(y_proc)*0.05, distance=10)
                log.append(f"Peaks Found: {len(peaks)}")
                
                stats_txt = f"Range: {np.min(x):.1f}~{np.max(x):.1f}, Peaks: {len(peaks)}"
            return {"x": x, "y_raw": y_raw, "y_proc": y_proc, "y_base": base, "peaks": [{"x": float(x[p]), "y": float(y_proc[p])} for p in peaks], "log": log, "fits": fits, "stats": stats_txt}
        except Exception as e: return {"x": x, "y_raw": y_raw, "y_proc": y_proc, "y_base": base, "peaks": [], "log": [str(e)], "fits": [], "stats": "Error"}

    # --- [D] Image Utils (Footer Removal) ---
    @staticmethod
    def remove_footer(img_bgr):
        h, w = img_bgr.shape[:2]
        # Check bottom 15% for footer (usually black box or scale bar)
        roi_h = int(h * 0.85)
        roi = img_bgr[roi_h:, :]
        gray = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)
        
        # Simple heuristic: if row variance is low (solid color) or very high intensity (white text on black)
        # We assume footer is separated by a horizontal line or distinct block
        # Sobel Y to find horizontal edges
        sobel_y = np.abs(cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=3))
        row_sums = np.sum(sobel_y, axis=1)
        
        # Find strongest edge in the footer region
        split_idx = np.argmax(row_sums)
        
        if row_sums[split_idx] > w * 50: # Threshold for line detection
            real_split = roi_h + split_idx
            return img_bgr[:real_split, :], img_bgr[real_split:, :]
        
        return img_bgr, None

    # --- [E] FFT Crystal Analysis (New) ---
    @staticmethod
    def analyze_crystal_structure(img_bgr):
        log = []
        gray = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2GRAY)
        h, w = gray.shape
        tile_size = w // 10
        if tile_size < 16: tile_size = 16
        
        overlay = img_bgr.copy()
        mask_overlay = np.zeros_like(img_bgr)
        
        stats = {"partial_crystal_regions": 0}
        
        # Iterate grids
        for y in range(0, h, tile_size):
            for x in range(0, w, tile_size):
                roi = gray[y:y+tile_size, x:x+tile_size]
                if roi.shape[0] != tile_size or roi.shape[1] != tile_size: continue
                
                # 2D FFT
                f = np.fft.fft2(roi)
                fshift = np.fft.fftshift(f)
                magnitude = 20 * np.log(np.abs(fshift) + 1e-9)
                
                # Analyze Pattern (Simplified)
                # "Partial Crystal" typically shows anisotropy in ring
                # We check angular variance at dominant frequency
                
                # 1. Radial Profile
                center = tile_size // 2
                y_grid, x_grid = np.ogrid[:tile_size, :tile_size]
                r_grid = np.sqrt((x_grid - center)**2 + (y_grid - center)**2)
                r_int = r_grid.astype(int)
                
                # Find dominant ring (ignore DC component at r=0)
                tbin = np.bincount(r_int.ravel(), magnitude.ravel())
                nr = np.bincount(r_int.ravel())
                radial_profile = tbin / (nr + 1e-9)
                
                if len(radial_profile) > 5:
                    peak_r = np.argmax(radial_profile[2:]) + 2 # Skip center
                    
                    # 2. Angular Variance at peak_r
                    mask_r = (r_grid >= peak_r-1) & (r_grid <= peak_r+1)
                    if np.sum(mask_r) > 0:
                        ring_vals = magnitude[mask_r]
                        variance = np.var(ring_vals)
                        
                        # Partial Crystal Criteria: High variance (broken ring) but not spots
                        if 10 < variance < 100: # Heuristic range
                            # Draw overlay (Yellow/Orange for Partial)
                            stats["partial_crystal_regions"] += 1
                            cv2.rectangle(mask_overlay, (x, y), (x+tile_size, y+tile_size), (0, 165, 255), -1)

        # Blend
        alpha = 0.4
        mask_indices = np.any(mask_overlay > 0, axis=-1)
        overlay[mask_indices] = (overlay[mask_indices] * (1 - alpha) + mask_overlay[mask_indices] * alpha).astype(np.uint8)
        
        log.append(f"Grid Size: {tile_size}px")
        log.append(f"Found {stats['partial_crystal_regions']} partial crystal blocks (Orange)")
        
        def to_b64(im): return base64.b64encode(cv2.imencode('.jpg', im)[1]).decode('utf-8')
        return {"type": "image", "raw_b64": to_b64(img_bgr), "proc_b64": to_b64(overlay), "stats": stats, "log": log}

    # --- [F] Diffraction (Updated Center) ---
    @staticmethod
    def analyze_diffraction_nv5(img_bgr, user_center=None):
        log = []
        gray = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2GRAY)
        
        # 1. Center Logic
        if user_center and len(user_center) == 2 and user_center[0] > 0:
            cx, cy = user_center
            log.append(f"Center: Manual ({cx}, {cy})")
        else:
            _, th = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)
            cy_f, cx_f = center_of_mass(th)
            if np.isnan(cy_f): cy_f, cx_f = gray.shape[0]//2, gray.shape[1]//2
            cx, cy = int(cx_f), int(cy_f)
            log.append(f"Center: Auto ({cx}, {cy})")

        # 2. Background Removal (DoG relative to center?) -> Just DoG
        g1 = cv2.GaussianBlur(gray, (3, 3), 0)
        g2 = cv2.GaussianBlur(gray, (51, 51), 0)
        diff = cv2.subtract(g1, g2)
        
        # 3. Peak Finding (Lattice Points)
        coordinates = peak_local_max(diff, min_distance=10, threshold_abs=20)
        
        # 4. Visualization
        enhanced = np.log1p(diff.astype(np.float32))
        enhanced = cv2.normalize(enhanced, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)
        result_vis = cv2.cvtColor(enhanced, cv2.COLOR_GRAY2BGR)
        
        # Draw Center
        cv2.drawMarker(result_vis, (cx, cy), (0, 0, 255), markerType=cv2.MARKER_CROSS, markerSize=20, thickness=2)
        
        # Draw Peaks
        peak_list = []
        for p in coordinates:
            py, px = p
            if abs(py-cy) < 5 and abs(px-cx) < 5: continue # Skip center itself
            cv2.circle(result_vis, (px, py), 3, (0, 255, 0), 1)
            peak_list.append(f"[{px},{py}]")

        log.append(f"Lattice Points: {len(peak_list)}")
        
        def to_b64(im): return base64.b64encode(cv2.imencode('.jpg', im)[1]).decode('utf-8')
        return {
            "type": "image", 
            "raw_b64": to_b64(img_bgr), 
            "proc_b64": to_b64(result_vis), 
            "stats": {"center": [cx, cy], "spots": len(peak_list)}, 
            "log": log
        }

    # --- [G] SAM Analysis (Updated Thin Film/Particle) ---
    @staticmethod
    def analyze_sam_results_nv5(img_rgb, masks, data_type):
        overlay = img_rgb.copy()
        mask_overlay = np.zeros_like(img_rgb)
        h_img, w_img = img_rgb.shape[:2]
        stats = {"count": 0}
        log = []
        
        valid_masks = []
        for ann in masks:
            area = ann['area']
            bbox = ann['bbox'] # x, y, w, h
            x, y, w, h = [int(v) for v in bbox]
            if area > h_img * w_img * 0.90 or area < 50: continue
            
            # [Update 1-2] Particle Border Kill
            if data_type == "Particle":
                if x <= 1 or y <= 1 or (x + w) >= w_img - 1 or (y + h) >= h_img - 1: continue 
            valid_masks.append(ann)

        # [Update 3] Thin Film Sort (Top to Bottom)
        if data_type == "Thin Film":
            # Sort by Y coordinate
            sorted_masks = sorted(valid_masks, key=lambda x: x['bbox'][1])
        else:
            sorted_masks = sorted(valid_masks, key=lambda x: x['area'], reverse=True)

        thickness_vals = []; roughness_vals = []; diameters = []

        for i, ann in enumerate(sorted_masks):
            m = ann['segmentation']
            bbox = ann['bbox']
            x, y, w, h = [int(v) for v in bbox]
            cx, cy = int(x + w/2), int(y + h/2)
            
            color = (0, 255, 0)
            
            # [Update 3] Thin Film Logic
            if data_type == "Thin Film":
                is_boundary_layer = (i == 0 or i == len(sorted_masks)-1)
                
                if not is_boundary_layer:
                    thk = ann['area'] / w
                    thickness_vals.append(thk)
                    color = (255, 0, 0) # Middle layers are Red
                else:
                    color = (100, 100, 100) # Top/Bottom are Gray (Ignored)

                # Label Layer Number
                cv2.putText(overlay, f"#{i+1}", (cx, cy), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
                
                # Interface Roughness (Bottom of current layer)
                if i < len(sorted_masks) - 1:
                    ys, xs = np.where(m)
                    if len(xs) > 10:
                        # Bottom edge approx: max Y per X
                        # Simplified: Use std dev of all Ys in mask as proxy for interface variance
                        roughness = np.std(ys) 
                        roughness_vals.append(roughness)
                        # Draw Interface Line
                        cv2.line(overlay, (x, y+h), (x+w, y+h), (0, 255, 255), 1)

            elif data_type == "Particle":
                equivalent_diameter = 2 * np.sqrt(ann['area'] / np.pi)
                diameters.append(equivalent_diameter)

            mask_overlay[m] = color
            mask_uint8 = (m * 255).astype(np.uint8)
            cnts, _ = cv2.findContours(mask_uint8, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
            if cnts: cv2.drawContours(overlay, [cnts[0]], -1, color, 2)

        alpha = 0.3
        mask_indices = np.any(mask_overlay > 0, axis=-1)
        overlay[mask_indices] = (overlay[mask_indices] * (1 - alpha) + mask_overlay[mask_indices] * alpha).astype(np.uint8)

        if data_type == "Thin Film":
            avg_thk = round(np.mean(thickness_vals), 2) if thickness_vals else 0
            log.append(f"Layers: {len(sorted_masks)} (Top/Bottom Excl.)")
            log.append(f"Avg Thickness: {avg_thk} px")
            stats["thickness_avg"] = avg_thk
        elif data_type == "Particle":
            avg_dia = round(np.mean(diameters), 2) if diameters else 0
            log.append(f"Particles: {len(sorted_masks)} (Inner Only)")
            log.append(f"Avg Dia: {avg_dia} px")
            stats["diameter_avg"] = avg_dia
        else:
            log.append(f"Detected: {len(sorted_masks)}")

        return overlay, stats, log

    # --- Main Process Router ---
    @staticmethod
    def process_image_nv5(img_bytes, equipment, data_type, mode, goal, user_center=None):
        nparr = np.frombuffer(img_bytes, np.uint8)
        img_raw = cv2.imdecode(nparr, cv2.IMREAD_COLOR)
        if img_raw is None: return {}
        
        # [Update 1] Remove Footer
        body, footer = ScienceProcessorNV5.remove_footer(img_raw)
        
        # Scale Logic (Placeholder for Vision LLM prompt later)
        scale_info = "1 px" 
        
        # [Update 7] Crystal FFT
        if data_type == "Crystal Structure (FFT)":
            return ScienceProcessorNV5.analyze_crystal_structure(body)
            
        # [Update 8] 2D Diffraction
        if data_type == "2D Diffraction":
            return ScienceProcessorNV5.analyze_diffraction_nv5(body, user_center)
        
        # General SAM
        if mode == "None":
             def to_b64(im): return base64.b64encode(cv2.imencode('.jpg', im)[1]).decode('utf-8')
             return {"type":"image", "raw_b64":to_b64(body), "proc_b64":to_b64(body), "stats":{}, "log":["Mode: Raw"]}

        img_proc = body.copy()
        log = []
        if mode == "Auto" or mode == "AI-Adaptive":
            img_proc = cv2.bilateralFilter(img_proc, 9, 75, 75)
            log.append("Preproc: Denoise")

        img_rgb = cv2.cvtColor(img_proc, cv2.COLOR_BGR2RGB)
        
        if mask_generator:
            try:
                masks = mask_generator.generate(img_rgb)
                log.append(f"SAM: {len(masks)} Raw Segments")
                overlay, stats, sam_log = ScienceProcessorNV5.analyze_sam_results_nv5(img_rgb, masks, data_type)
                log.extend(sam_log)
                overlay = cv2.cvtColor(overlay, cv2.COLOR_RGB2BGR)
            except Exception as e:
                log.append(f"SAM Error: {str(e)}")
                overlay = body
                stats = {}
        else:
            log.append("SAM Error: Not Loaded")
            overlay = body
            stats = {}

        def to_b64(im): return base64.b64encode(cv2.imencode('.jpg', im)[1]).decode('utf-8')
        return {"type": "image", "raw_b64": to_b64(body), "proc_b64": to_b64(overlay), "stats": stats, "log": log}

# [4] Helper Async Functions
async def analyze_vision_ollama(image_bytes: bytes, prompt: str) -> str:
    try:
        b64 = base64.b64encode(image_bytes).decode('utf-8')
        res = await asyncio.to_thread(ollama_client.chat, model=VISION_MODEL, messages=[{'role':'user','content':prompt,'images':[b64]}])
        return res['message']['content']
    except: return "Vision Error"

async def interpret_spectrum_data(raw_context: str, equipment: str) -> str:
    try:
        prompt = f"Analyze spectrum ({equipment}). Brief interpretation in Korean. Data: {raw_context}"
        res = await asyncio.to_thread(ollama_client.chat, model=TEXT_MODEL, messages=[{'role':'user','content':prompt}])
        return res['message']['content']
    except: return ""

# [5] App & Job System
app = FastAPI(title="Analyst NV5", lifespan=lifespan)
app.add_middleware(CORSMiddleware, allow_origins=["*"], allow_methods=["*"], allow_headers=["*"])

class TranslateReq(BaseModel): text: str

@app.post("/api/translate")
async def translate_text(req: TranslateReq):
    try:
        res = await asyncio.to_thread(ollama_client.chat, model=TEXT_MODEL, messages=[{'role':'system','content':'Translate to English.'}, {'role':'user','content':req.text}])
        return {"translated": res['message']['content']}
    except Exception as e: return {"translated": str(e)}

async def process_analysis_job(job_id: str, files_map: Dict, configs: Dict):
    try:
        JOBS[job_id]["status"] = "Processing"
        JOBS[job_id]["progress"] = 10
        final_results = []
        total_files = len(configs)
        processed_count = 0

        for filename, config in configs.items():
            if filename not in files_map: continue
            content = files_map[filename]
            eq = config.get("equipment", "General")
            data_type = config.get("data_type", "General")
            goal = config.get("goal", "")
            mode = config.get("mode", "Auto")
            
            # [Update 8] Center Coordinates (String "x,y" -> List [x,y])
            user_center = None
            if config.get("center_coords"):
                try: user_center = [int(v) for v in config["center_coords"].split(",")]
                except: pass

            fname_lower = filename.lower()
            JOBS[job_id]["step"] = f"Analyzing {filename}..."
            
            # [A] Document
            if eq == "Document" or fname_lower.endswith(('.pdf', '.ppt', '.pptx')):
                try:
                    if fname_lower.endswith('.pdf'):
                        images = convert_from_bytes(content, dpi=150, fmt='jpeg')[:10]
                        full_txt = ""; pages = []
                        for idx, img in enumerate(images):
                            buf = io.BytesIO(); img.save(buf, format="JPEG")
                            b64 = base64.b64encode(buf.getvalue()).decode()
                            desc = await analyze_vision_ollama(buf.getvalue(), "한글 요약.")
                            full_txt += f"\nPage {idx+1}: {desc}"
                            pages.append({"page_num": idx+1, "image_b64": b64, "desc": desc})
                        final_results.append({"type": "pdf", "filename": filename, "equipment": "Lit", "raw_context": full_txt, "pages": pages})
                    elif fname_lower.endswith(('.ppt', '.pptx')):
                        prs = Presentation(io.BytesIO(content))
                        slides = []
                        for i, slide in enumerate(prs.slides):
                            txt = ""; imgs = []
                            for s in slide.shapes:
                                if hasattr(s, "text"): txt += s.text + "\n"
                                if s.shape_type == MSO_SHAPE_TYPE.PICTURE:
                                    try:
                                        ib64 = base64.b64encode(s.image.blob).decode('utf-8')
                                        desc = await analyze_vision_ollama(s.image.blob, "한글 설명.")
                                        imgs.append({"b64": ib64, "desc": desc})
                                    except: pass
                            slides.append({"slide_num": i+1, "text": txt, "images": imgs})
                        final_results.append({"type": "ppt", "filename": filename, "equipment": "Lit", "raw_context": "", "slides": slides})
                except Exception as e: final_results.append({"type": "error", "filename": filename, "msg": str(e)})

            # [B] Spectrum
            elif fname_lower.endswith(('.xlsx', '.xls', '.csv', '.txt')):
                try:
                    dfs = ScienceProcessorNV5.read_universal_dataframe(content, filename)
                    for i, df in enumerate(dfs):
                        v_blocks = ScienceProcessorNV5.detect_structured_blocks(df)
                        for v_block in v_blocks:
                            h_blocks = ScienceProcessorNV5.split_horizontal_blocks(v_block)
                            for h_block in h_blocks:
                                series_list = ScienceProcessorNV5.extract_series_from_df(h_block)
                                for s in series_list:
                                    res = ScienceProcessorNV5.process_spectrum(s['x'], s['y'], mode, goal)
                                    step = max(1, len(s['x'])//100)
                                    chart = [{"x": float(res["x"][k]), "y_proc": float(res["y_proc"][k]), "y_raw": float(res["y_raw"][k]), "y_base": float(res["y_base"][k])} for k in range(0, len(s['x']), step)]
                                    ctx = f"Data: {s['name']}\nStats: {res['stats']}"
                                    interp = await interpret_spectrum_data(ctx, eq)
                                    final_results.append({
                                        "type": "spectrum", "filename": f"{filename} ({s['name']})", "equipment": eq,
                                        "raw_context": f"{ctx}\nInterp: {interp}", 
                                        "chart_data": chart, "log": res["log"], "interpretation": interp
                                    })
                except Exception as ex: final_results.append({"type": "error", "filename": filename, "msg": str(ex)})

            # [C] Image (NV5)
            elif fname_lower.endswith(('.png', '.jpg', '.jpeg', '.tif', '.bmp')):
                try:
                    JOBS[job_id]["step"] = f"Vision Analysis: {filename}..."
                    vis_res = await asyncio.to_thread(ScienceProcessorNV5.process_image_nv5, content, eq, data_type, mode, goal, user_center)
                    
                    if vis_res:
                        # [Update 4] Scale Bar Info Request
                        prompt = f"Analyze this {eq} image ({data_type}). Extract Scale Bar text (e.g. 100nm) and calc nm/px. Summary in Korean."
                        desc = await analyze_vision_ollama(base64.b64decode(vis_res["proc_b64"]), prompt)
                        final_results.append({
                            "type": "image", "filename": filename, "equipment": eq, "summary": desc,
                            "raw_context": f"Img {eq}: {desc}",
                            "raw_b64": vis_res["raw_b64"], "proc_b64": vis_res["proc_b64"],
                            "stats": vis_res.get("stats",{}), "log": vis_res.get("log", [])
                        })
                except Exception as ex: final_results.append({"type": "error", "filename": filename, "msg": str(ex)})
            
            processed_count += 1
            JOBS[job_id]["progress"] = 10 + int((processed_count / total_files) * 80)

        JOBS[job_id]["step"] = "Writing Report..."
        final_results = sanitize_json(final_results)
        
        data_ctx = ""
        for r in final_results:
            if r.get("type") != "error":
                raw = r.get("raw_context") or ""
                data_ctx += f"\n=== {r['filename']} ===\n{raw[:1000]}\n"

        final_report = "Fail"
        if data_ctx:
            try:
                system_prompt = "You are a research assistant. Write a report in Korean. Summarize data and physics."
                res = await asyncio.to_thread(ollama_client.chat, model=TEXT_MODEL, messages=[{'role':'system','content':system_prompt}, {'role':'user','content':f"Data:\n{data_ctx}"}])
                final_report = res['message']['content']
            except Exception as e: final_report = str(e)

        JOBS[job_id]["results"] = {"results": final_results, "final_report": final_report}
        JOBS[job_id]["status"] = "Completed"
        JOBS[job_id]["progress"] = 100
    except Exception as e:
        JOBS[job_id]["status"] = "Failed"
        JOBS[job_id]["error"] = str(e)

@app.post("/api/analyze")
async def start_analysis(background_tasks: BackgroundTasks, files: List[UploadFile] = File(...), file_configs: str = Form(...)):
    job_id = str(uuid.uuid4())
    configs = json.loads(file_configs)
    files_map = {f.filename: await f.read() for f in files}
    JOBS[job_id] = {"status": "Pending", "progress": 0, "step": "Init", "results": None}
    background_tasks.add_task(process_analysis_job, job_id, files_map, configs)
    return {"job_id": job_id}

@app.get("/api/status/{job_id}")
async def get_status(job_id: str): return JOBS.get(job_id, {"status": "Not Found"})

@app.get("/", response_class=HTMLResponse)
async def serve_index():
    if os.path.exists("index.html"): return FileResponse("index.html")
    return "<h1>NV5 Backend</h1>"

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)







엔브리 4 픽스 파이널 2 웹 코드

<!DOCTYPE html>
<html lang="ko">
<head>
    <meta charset="UTF-8">
    <title>Analyst NV4 Fixed</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <script src="https://unpkg.com/vue@3/dist/vue.global.js"></script>
    <script src="https://cdn.plot.ly/plotly-2.27.0.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/marked/marked.min.js"></script>
    <script src="https://unpkg.com/lucide@latest"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/html2pdf.js/0.10.1/html2pdf.bundle.min.js"></script>
    <style>
        @import url('https://fonts.googleapis.com/css2?family=Inter:wght@400;600;700&display=swap');
        body { font-family: 'Inter', sans-serif; }
        .prose table { border-collapse: collapse; width: 100%; margin: 1em 0; }
        .prose th, .prose td { border: 1px solid #cbd5e1; padding: 8px; font-size: 0.9rem; }
        .prose th { background-color: #f1f5f9; }
    </style>
</head>
<body class="bg-slate-100 text-slate-800">
    <div id="app" class="min-h-screen p-6">
        <header class="max-w-7xl mx-auto mb-6 flex items-center gap-3">
            <div class="bg-emerald-600 p-2 rounded-lg shadow"><i data-lucide="microscope" class="w-6 h-6 text-white"></i></div>
            <div><h1 class="text-2xl font-bold">Analyst NV4 <span class="text-sm font-normal text-slate-500">Physics-Aware Engine</span></h1></div>
        </header>

        <main class="max-w-7xl mx-auto grid grid-cols-1 lg:grid-cols-12 gap-6">
            <div class="lg:col-span-4 space-y-4">
                <div class="bg-white p-5 rounded-xl shadow-sm border border-slate-200">
                    <h2 class="font-bold mb-3 flex items-center gap-2"><i data-lucide="files" class="w-4 h-4"></i> Data Staging</h2>
                    <div class="border-2 border-dashed border-slate-300 rounded-lg p-6 text-center hover:bg-emerald-50 cursor-pointer" @click="$refs.f.click()">
                        <input type="file" ref="f" multiple class="hidden" @change="addFiles">
                        <p class="text-sm font-semibold text-slate-600">Add Files</p>
                    </div>
                    <div v-if="stagedFiles.length" class="mt-4 space-y-3 max-h-[60vh] overflow-y-auto pr-1">
                        <div v-for="(item, i) in stagedFiles" :key="i" class="p-3 bg-slate-50 border rounded-lg text-sm">
                            <div class="flex justify-between items-center mb-2">
                                <span class="font-bold truncate w-2/3">{{ item.file.name }}</span>
                                <button @click="removeFile(i)" class="text-red-400 hover:text-red-600"><i data-lucide="trash-2" class="w-4 h-4"></i></button>
                            </div>
                            <div class="grid grid-cols-1 gap-2">
                                <select v-model="item.equipment" class="w-full p-1.5 border rounded text-xs bg-white font-semibold text-emerald-700">
                                    <optgroup label="Spectroscopy (1D)">
                                        <option value="XPS">XPS</option>
                                        <option value="XRD">XRD (1D)</option>
                                        <option value="Raman">Raman</option>
                                    </optgroup>
                                    <optgroup label="Microscopy (2D)">
                                        <option value="SEM">SEM</option>
                                        <option value="TEM">TEM</option>
                                        <option value="AFM">AFM</option>
                                        <option value="Optical">Optical</option>
                                    </optgroup>
                                    <optgroup label="Diffraction (2D)">
                                        <option value="Diffraction">2D Diffraction Pattern</option>
                                    </optgroup>
                                    <optgroup label="Literature">
                                        <option value="Document">Document</option>
                                    </optgroup>
                                </select>
                                
                                <select v-if="isImage(item)" v-model="item.dataType" class="w-full p-1.5 border rounded text-xs bg-slate-50 font-bold">
                                    <option v-if="item.equipment==='Diffraction'" value="2D Diffraction">2D Diffraction Analysis</option>
                                    <option v-else value="General">General Object</option>
                                    <option v-if="item.equipment!=='Diffraction'" value="Particle">Particle Analysis (Border Excl.)</option>
                                    <option v-if="item.equipment!=='Diffraction'" value="Thin Film">Thin Film (Thk & Roughness)</option>
                                </select>

                                <select v-if="!isDoc(item)" v-model="item.mode" class="w-full p-1.5 border rounded text-xs bg-white">
                                    <option value="Auto">✨ Auto Process</option>
                                    <option value="None">🚫 Raw Data</option>
                                    <option value="AI-Adaptive">🧠 Agent Mode</option>
                                </select>
                                <input v-if="!isDoc(item)" v-model="item.goal" type="text" placeholder="Goal (e.g. contrast, noise)" class="w-full p-1.5 border rounded text-xs">
                            </div>
                        </div>
                    </div>
                    <button @click="startAnalysis" :disabled="isAnalyzing || stagedFiles.length===0" class="w-full mt-4 py-3 bg-emerald-600 text-white rounded-lg font-bold flex justify-center gap-2 shadow hover:bg-emerald-700 disabled:opacity-50">
                        <span>Generate Report</span>
                    </button>
                </div>
            </div>

            <div class="lg:col-span-8">
                <div v-if="isAnalyzing" class="bg-white p-6 rounded-xl shadow border border-slate-200 text-center py-12">
                    <div class="animate-spin rounded-full h-12 w-12 border-b-2 border-emerald-600 mx-auto mb-4"></div>
                    <h3 class="text-lg font-bold text-slate-700">Analyzing...</h3>
                    <p class="text-sm text-slate-500 mb-4">{{ currentStep }}</p>
                    <div class="w-full bg-slate-200 rounded-full h-2.5 mb-2">
                        <div class="bg-emerald-600 h-2.5 rounded-full transition-all duration-500" :style="{width: progress + '%'}"></div>
                    </div>
                    <p class="text-xs text-right text-slate-400">{{ progress }}%</p>
                </div>

                <div v-if="result && !isAnalyzing" class="space-y-6 animate-fade-in">
                    <div id="report-view" class="bg-white p-8 rounded-xl shadow-sm border border-slate-200">
                        <section class="mb-10">
                            <h2 class="text-lg font-bold text-emerald-700 mb-3 border-b pb-1">1. Synthesis</h2>
                            <div class="prose prose-sm text-slate-700 max-w-none" v-html="md(result.final_report)"></div>
                        </section>
                        <section>
                            <h2 class="text-lg font-bold text-slate-800 mb-5 border-b pb-1">2. Data Evidence</h2>
                            <div v-for="(item, idx) in result.results" :key="idx" class="mb-8 p-4 bg-slate-50 rounded-lg border">
                                <div class="flex items-center justify-between mb-3">
                                    <div class="flex items-center gap-2">
                                        <span class="px-2 py-0.5 bg-white border rounded text-[10px] font-bold uppercase text-slate-500">{{ item.equipment }}</span>
                                        <h3 class="font-bold text-sm">{{ item.filename }}</h3>
                                    </div>
                                    <button @click="item.showDocs = !item.showDocs" class="text-xs text-emerald-600 font-bold hover:underline flex items-center gap-1">
                                        <i :data-lucide="item.showDocs ? 'eye-off' : 'eye'" class="w-3 h-3"></i> Info
                                    </button>
                                </div>

                                <div v-if="item.chart_data" class="bg-white p-2 rounded border mb-3">
                                    <div class="flex gap-2 justify-end mb-1">
                                        <button @click="toggleTrace(idx, 0)" class="text-[10px] px-2 py-0.5 border rounded bg-slate-50">Raw</button>
                                        <button @click="toggleTrace(idx, 1)" class="text-[10px] px-2 py-0.5 border rounded bg-amber-50 text-amber-700">Base</button>
                                        <button @click="toggleTrace(idx, 2)" class="text-[10px] px-2 py-0.5 border rounded font-bold text-emerald-600 bg-emerald-50">Proc</button>
                                    </div>
                                    <div :id="'chart-'+idx" class="w-full h-60"></div>
                                    <div v-if="item.interpretation && item.showDocs" class="mt-3 p-3 bg-emerald-50 border border-emerald-100 rounded text-xs text-emerald-800">
                                        <div class="prose prose-xs" v-html="md(item.interpretation)"></div>
                                    </div>
                                </div>

                                <div v-if="item.raw_b64" class="bg-white p-2 rounded border mb-3">
                                    <div class="flex gap-2 mb-2 text-xs justify-center">
                                        <button @click="item.view='proc'" :class="item.view!=='raw'?'font-bold text-emerald-600':''">Processed</button>
                                        <span class="text-slate-300">|</span>
                                        <button @click="item.view='raw'" :class="item.view==='raw'?'font-bold text-emerald-600':''">Original</button>
                                    </div>
                                    <img :src="'data:image/jpeg;base64,' + (item.view==='raw' ? item.raw_b64 : item.proc_b64)" class="max-h-96 mx-auto object-contain bg-black">
                                    <div v-if="item.stats && Object.keys(item.stats).length > 0" class="mt-2 p-2 bg-emerald-50 border border-emerald-100 rounded text-xs grid grid-cols-2 gap-2">
                                        <div v-for="(v, k) in item.stats" :key="k">
                                            <span class="font-bold text-emerald-700 capitalize">{{ k.replace('_', ' ') }}:</span> {{ v }}
                                        </div>
                                    </div>
                                </div>

                                <div v-if="item.pages" class="grid grid-cols-1 gap-4 mt-2 max-h-96 overflow-y-auto">
                                    <div v-for="p in item.pages" :key="p.page_num" class="flex gap-3 bg-white p-3 rounded border">
                                        <img :src="'data:image/jpeg;base64,'+p.image_b64" class="w-24 object-contain border">
                                        <div class="text-xs prose" v-html="md(p.desc)"></div>
                                    </div>
                                </div>
                                <div v-if="item.slides" class="space-y-4 mt-2 max-h-96 overflow-y-auto">
                                    <div v-for="s in item.slides" :key="s.slide_num" class="bg-white p-3 rounded border">
                                        <div class="text-xs font-bold text-emerald-600 mb-1">Slide {{s.slide_num}}</div>
                                        <div class="text-xs prose mb-2">{{ s.text.substring(0, 200) }}...</div>
                                        <div v-if="s.images.length" class="grid grid-cols-2 gap-2 border-t pt-2">
                                            <div v-for="(img, imx) in s.images" :key="imx" class="bg-slate-50 p-2">
                                                <img :src="'data:image/jpeg;base64,'+img.b64" class="max-h-32 mx-auto object-contain">
                                            </div>
                                        </div>
                                    </div>
                                </div>

                                <div v-if="item.log && item.log.length > 0 && item.showDocs" class="mt-2 p-2 bg-gray-100 rounded border text-[10px] font-mono text-gray-600">
                                    <ul class="list-disc pl-4 space-y-0.5"><li v-for="l in item.log">{{ l }}</li></ul>
                                </div>
                            </div>
                        </section>
                    </div>
                </div>
            </div>
        </main>
    </div>
    <script>
        const { createApp, ref, nextTick } = Vue;
        createApp({
            setup() {
                const stagedFiles = ref([]);
                const isAnalyzing = ref(false);
                const progress = ref(0);
                const currentStep = ref("");
                const result = ref(null);
                const md = (t) => marked.parse(t||'');

                const addFiles = (e) => {
                    Array.from(e.target.files).forEach(f => {
                        let eq = 'SEM';
                        let dt = 'General';
                        if(f.name.match(/\.(pdf|ppt|pptx)$/i)) { eq = 'Document'; }
                        else if(f.name.match(/\.(xlsx|csv|txt)$/i)) { eq = 'XPS'; }
                        stagedFiles.value.push({ file: f, equipment: eq, dataType: dt, goal: '', mode: 'Auto', view: 'proc', showDocs: true });
                    });
                    e.target.value = ''; 
                };
                const removeFile = (i) => stagedFiles.value.splice(i, 1);
                
                const isDoc = (item) => item.equipment === 'Document';
                const isImage = (item) => ['SEM','TEM','AFM','Optical','Diffraction'].includes(item.equipment);

                const startAnalysis = async () => {
                    isAnalyzing.value = true;
                    progress.value = 0;
                    currentStep.value = "Initializing...";
                    const fd = new FormData();
                    const configs = {};
                    stagedFiles.value.forEach(item => {
                        fd.append('files', item.file);
                        configs[item.file.name] = { equipment: item.equipment, data_type: item.equipment === 'Diffraction' ? '2D Diffraction' : item.dataType, goal: item.goal, mode: item.mode };
                    });
                    fd.append('file_configs', JSON.stringify(configs));
                    try {
                        const initRes = await fetch('/api/analyze', { method: 'POST', body: fd });
                        const { job_id } = await initRes.json();
                        const poll = setInterval(async () => {
                            const statusRes = await fetch(`/api/status/${job_id}`);
                            const status = await statusRes.json();
                            progress.value = status.progress || 0;
                            currentStep.value = status.step || "Processing...";
                            if (status.status === "Completed") {
                                clearInterval(poll);
                                result.value = status.results;
                                if(result.value.results) result.value.results.forEach(r => r.showDocs = true);
                                isAnalyzing.value = false;
                                await nextTick();
                                renderCharts(result.value.results