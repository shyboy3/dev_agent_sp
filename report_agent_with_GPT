research_report_server_web.py:

import os
import io
import json
import tempfile
from typing import List, Optional

import pandas as pd
from fastapi import FastAPI, UploadFile, File, Form
from fastapi.responses import FileResponse, HTMLResponse, Response
from fastapi.staticfiles import StaticFiles
from pydantic import BaseModel

from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, Image as RLImage
from reportlab.lib.pagesizes import A4
from reportlab.lib.styles import getSampleStyleSheet
from reportlab.pdfbase import pdfmetrics
from reportlab.pdfbase.ttfonts import TTFont

import pytesseract
from PIL import Image
import httpx
import matplotlib
matplotlib.use("Agg")
import matplotlib.pyplot as plt

# =========================
# ê²½ë¡œ & ì„¤ì •
# =========================

BASE_DIR = os.path.dirname(os.path.abspath(__file__))
STATIC_DIR = os.path.join(BASE_DIR, "static")
FONTS_DIR = os.path.join(BASE_DIR, "fonts")

LLM_API_URL = "http://localhost:11434/api/generate"    # ë¡œì»¬ LLM API (Ollama ì˜ˆì‹œ)
LLM_MODEL = "qwen2.5-7b-instruct"                      # ì‹¤ì œ ì„¤ì¹˜í•œ ëª¨ë¸ ì´ë¦„

FONT_NAME = "KoreanFont"

# í•œê¸€ í°íŠ¸ ë“±ë¡ (NotoSansCJKkr-Regular.otf ë“±)
font_path = os.path.join(FONTS_DIR, "NotoSansCJKkr-Regular.otf")
if os.path.exists(font_path):
    pdfmetrics.registerFont(TTFont(FONT_NAME, font_path))
else:
    # í°íŠ¸ ì—†ìœ¼ë©´ ê¸°ë³¸ í°íŠ¸ ì‚¬ìš© (ì´ ê²½ìš° ë‹¤ì‹œ ë„¤ëª¨ ë¬¸ì œ ìƒê¸¸ ìˆ˜ ìˆìŒ)
    FONT_NAME = "Helvetica"

app = FastAPI(title="Research Report Generator")

if not os.path.exists(STATIC_DIR):
    os.makedirs(STATIC_DIR)

app.mount("/static", StaticFiles(directory=STATIC_DIR), name="static")


# =========================
# LLM í˜¸ì¶œ
# =========================

async def call_llm(prompt: str, system_prompt: str = "") -> str:
    payload = {
        "model": LLM_MODEL,
        "prompt": (system_prompt + "\n\n" + prompt).strip(),
        "stream": False,
    }
    async with httpx.AsyncClient(timeout=600) as client:
        r = await client.post(LLM_API_URL, json=payload)
        r.raise_for_status()
        data = r.json()
        return data.get("response", "")


# =========================
# ë©”íƒ€ë°ì´í„° ëª¨ë¸
# =========================

class ReportMeta(BaseModel):
    project_name: Optional[str] = None
    sample_id: Optional[str] = None
    sample_description: Optional[str] = None
    operator: Optional[str] = None
    date: Optional[str] = None
    techniques: Optional[List[str]] = None
    key_questions: Optional[str] = None


# =========================
# íŒŒì¼ íŒŒì„œ (ì´ë¯¸ì§€ & ìŠ¤í™íŠ¸ëŸ¼ í¬í•¨)
# =========================
import csv, re
from charset_normalizer import from_bytes

def detect_encoding(raw: bytes) -> str:
    # BOM ìš°ì„  ì²˜ë¦¬
    if raw.startswith(b'\xef\xbb\xbf'):
        return 'utf-8-sig'
    # heuristic detection
    best = from_bytes(raw).best()
    if best and best.encoding:
        return best.encoding
    return 'utf-8'  # fallback

def sniff_delimiter(sample_text: str) -> str:
    # csv.Sniffer ë¡œ ì‹œë„ â†’ ì‹¤íŒ¨ì‹œ íœ´ë¦¬ìŠ¤í‹±
    try:
        dialect = csv.Sniffer().sniff(sample_text, delimiters=[',',';','\t','|'])
        return dialect.delimiter
    except Exception:
        # ê°„ë‹¨ íœ´ë¦¬ìŠ¤í‹±: ì„¸ë¯¸ì½œë¡  ìš°ì„¸ â†’ ';' / íƒ­ ìš°ì„¸ â†’ '\t' / ê¸°ë³¸ ','
        counts = {',': sample_text.count(','), ';': sample_text.count(';'), '\t': sample_text.count('\t')}
        return max(counts, key=counts.get) or ','

from charset_normalizer import from_bytes

import csv, re
from charset_normalizer import from_bytes

NUM_TOKEN = re.compile(r"""
    ^[ \t]*                                    # ì• ê³µë°±
    [+-]?                                      # ë¶€í˜¸
    (?:\d+(?:[.,]\d+)?|\.\d+)                  # ìˆ«ì(ì†Œìˆ˜ í—ˆìš©)
    (?:[eE][+-]?\d+)?                          # ì§€ìˆ˜
    [ \t]*$                                    # ë’¤ ê³µë°±
""", re.VERBOSE)

def looks_numeric(token: str) -> bool:
    return bool(NUM_TOKEN.match(token))

def split_tokens(line: str, sep_candidates=(',', ';', '\t', '|')):
    # í›„ë³´ êµ¬ë¶„ì ì¤‘ ì‹¤ì œë¡œ ë§ì´ ë³´ì´ëŠ” ê²ƒìœ¼ë¡œ ë¶„í• 
    best_sep = max(sep_candidates, key=lambda s: line.count(s))
    return line.split(best_sep), best_sep

def detect_encoding(raw: bytes) -> str:
    if raw.startswith(b'\xef\xbb\xbf'):
        return 'utf-8-sig'
    best = from_bytes(raw).best()
    return best.encoding if best and best.encoding else 'utf-8'

def find_table_start(sample_text: str):
    """
    ë¨¸ë¦¿ë§ ì„ì¸ CSVì—ì„œ 'ìˆ«ìí˜• ë ˆì½”ë“œê°€ ì—°ì†ìœ¼ë¡œ 2~3ì¤„ ì´ìƒ' ë‚˜íƒ€ë‚˜ëŠ” ì§€ì ì„
    í…Œì´ë¸” ì‹œì‘ìœ¼ë¡œ ê°„ì£¼. (í—¤ë” 1ì¤„ ë°”ë¡œ ìœ„ì¼ ìˆ˜ë„ ìˆìŒ)
    """
    lines = sample_text.splitlines()
    sep = None
    numeric_run = 0
    start_idx = None

    # 1ì°¨: êµ¬ë¶„ì ì¶”ì • (ê°€ì¥ ìì£¼ ë“±ì¥í•˜ëŠ” êµ¬ë¶„ì)
    counts = {',': sample_text.count(','), ';': sample_text.count(';'),
              '\t': sample_text.count('\t'), '|': sample_text.count('|')}
    sep = max(counts, key=counts.get) if any(counts.values()) else ','

    for i, ln in enumerate(lines):
        # ë¹ˆ ì¤„/ì£¼ì„ ë¼ì¸ì€ ë¨¸ë¦¿ë§ë¡œ ê°„ì£¼
        if not ln.strip() or ln.strip().startswith(('#', '//', ';')):
            numeric_run = 0
            continue

        tokens = ln.split(sep)
        if len(tokens) < 2:
            numeric_run = 0
            continue

        num_like = sum(looks_numeric(t) for t in tokens)
        frac = num_like / max(1, len(tokens))

        # ìˆ«ìí˜• ë¹„ìœ¨ì´ ì¶©ë¶„íˆ ë†’ìœ¼ë©´ ë°ì´í„° ë¼ì¸ì´ë¼ê³  íŒë‹¨
        if num_like >= 2 and frac >= 0.5:
            numeric_run += 1
            if numeric_run >= 2:
                # ì—°ì† 2ì¤„ ì´ìƒ ìˆ«ìí˜•ì´ë©´, ê·¸ ë¸”ë¡ì˜ ì²« ì¤„ì„ í…Œì´ë¸” ì‹œì‘ìœ¼ë¡œ
                start_idx = i - numeric_run + 1
                break
        else:
            numeric_run = 0

    # í—¤ë”ê°€ ë°”ë¡œ ìˆ«ì ì•ì¤„ì— ìˆëŠ” ê²½ìš°(ë¬¸ì í† í° ë‹¤ìˆ˜) ê°ì§€
    header_idx = None
    if start_idx and start_idx > 0:
        prev = lines[start_idx - 1]
        toks = prev.split(sep)
        if len(toks) >= 2 and sum(looks_numeric(t) for t in toks) == 0:
            header_idx = start_idx - 1

    return sep, header_idx, start_idx

def robust_read_csv_bytes(raw: bytes) -> pd.DataFrame:
    """
    ì¸ì½”ë”©/êµ¬ë¶„ì ê°ì§€ + ë¨¸ë¦¿ë§ ìŠ¤í‚µ + í—¤ë”/ë°ì´í„° ìë™ íŒë³„
    """
    raw = raw.replace(b'\x00', b'').replace(b'\r\n', b'\n')
    enc = detect_encoding(raw)

    # ì²« 64KBë§Œìœ¼ë¡œë„ ì‹œì‘ ìœ„ì¹˜ íŒë‹¨ ì¶©ë¶„
    sample_text = raw[:65536].decode(enc, errors='ignore')
    sep, header_idx, start_idx = find_table_start(sample_text)

    # ì‹œë‚˜ë¦¬ì˜¤ë³„ë¡œ ì½ê¸° ì‹œë„
    trial_params = []

    if start_idx is not None:
        if header_idx is not None:
            # í—¤ë” 1ì¤„ + ë°ì´í„° ì‹œì‘
            trial_params.append(dict(encoding=enc, sep=sep, engine='c',
                                     header=header_idx, skiprows=range(0, header_idx)))
        else:
            # í—¤ë” ì—†ì´ ë°”ë¡œ ë°ì´í„°ê°€ ì‹œì‘
            trial_params.append(dict(encoding=enc, sep=sep, engine='c',
                                     header=None, skiprows=range(0, start_idx)))
            # ë˜ëŠ” ì²« ì¤„ì„ í—¤ë”ë¡œ ê°„ì£¼(ì¥ë¹„ê°€ ì²« ì¤„ì— ì»¬ëŸ¼ëª… í¬í•¨í•œ ì¼€ì´ìŠ¤)
            trial_params.append(dict(encoding=enc, sep=sep, engine='c',
                                     header=start_idx, skiprows=range(0, start_idx)))
    else:
        # í…Œì´ë¸” ì‹œì‘ì„ ëª» ì°¾ì•˜ìœ¼ë©´ ì¼ë°˜ ìŠ¤ë‹ˆí•‘ ì „ëµ
        trial_params.append(dict(encoding=enc, sep=sep, engine='c'))

    # python ì—”ì§„, ì¸ì½”ë”© ëŒ€ì²´ ì¬ì‹œë„
    alt_encs = ['utf-8-sig', 'cp949', 'euc-kr', 'latin1']
    for p in list(trial_params):
        trial_params.append({**p, 'engine': 'python', 'on_bad_lines': 'skip'})
    for ae in alt_encs:
        trial_params.append(dict(encoding=ae, sep=sep, engine='python', on_bad_lines='skip'))

    # ì‹œë„
    last_err = None
    for params in trial_params:
        try:
            return pd.read_csv(io.BytesIO(raw), **params)
        except Exception as e:
            last_err = e
            continue
    raise last_err or RuntimeError("CSV parsing failed")

def robust_read_csv_bytes(raw: bytes) -> pd.DataFrame:
    """
    ë‹¤ì–‘í•œ ì¸ì½”ë”©/êµ¬ë¶„ì CSV ìë™ ê°ì§€ í›„ ì•ˆì „í•˜ê²Œ DataFrameìœ¼ë¡œ ë¡œë“œ
    """

    # ë„ë°”ì´íŠ¸/ì˜ëª»ëœ ì¤„ë°”ê¿ˆ ì •ë¦¬
    raw = raw.replace(b'\x00', b'').replace(b'\r\n', b'\n')

    # â‘  ì¸ì½”ë”© ê°ì§€
    enc_guess = 'utf-8'
    try:
        result = from_bytes(raw).best()
        if result and result.encoding:
            enc_guess = result.encoding
    except Exception:
        pass

    # â‘¡ ìƒ˜í”Œ í…ìŠ¤íŠ¸ë¡œ êµ¬ë¶„ì ì¶”ì •
    sample = raw[:4096]
    try:
        sample_text = sample.decode(enc_guess, errors='ignore')
    except Exception:
        # ê°ì§€ ì‹¤íŒ¨ ì‹œ CP949 í•œ ë²ˆ ë” ì‹œë„
        try:
            sample_text = sample.decode('cp949', errors='ignore')
            enc_guess = 'cp949'
        except Exception:
            sample_text = sample.decode('latin1', errors='ignore')
            enc_guess = 'latin1'

    import csv
    try:
        dialect = csv.Sniffer().sniff(sample_text, delimiters=[',',';','\t','|'])
        sep = dialect.delimiter
    except Exception:
        sep = ','

    # â‘¢ ì—¬ëŸ¬ ì—”ì§„/ì¸ì½”ë”© ì‹œë„
    tried = set()
    for enc in [enc_guess, 'utf-8-sig', 'cp949', 'euc-kr', 'latin1']:
        if enc in tried: continue
        tried.add(enc)
        for engine in ['c', 'python']:
            try:
                df = pd.read_csv(
                    io.BytesIO(raw),
                    encoding=enc,
                    sep=sep,
                    engine=engine,
                    on_bad_lines='skip'
                )
                return df
            except UnicodeDecodeError:
                continue
            except pd.errors.ParserError:
                continue
            except Exception:
                continue

    # â‘£ ë§ˆì§€ë§‰ ì‹œë„ ì‹¤íŒ¨ â†’ raise
    raise UnicodeDecodeError("robust_read_csv_bytes", b'', 0, 1, "ëª¨ë“  ì¸ì½”ë”© ì‹œë„ ì‹¤íŒ¨")

def robust_read_csv_bytes(raw: bytes) -> pd.DataFrame:
    # ë„ë°”ì´íŠ¸/ê¹¨ì§„ ì¤„ ì œê±°
  #  raw = raw.replace(b'\x00', b'')
		raw = raw.replace(b'\r\n', b'\n')
    enc = detect_encoding(raw)

    # ìƒ˜í”Œì—ì„œ êµ¬ë¶„ì ì¶”ì •
    sample = raw[:4096].decode(enc, errors='ignore')
    sep = sniff_delimiter(sample)

    # 1ì°¨: C ì—”ì§„, ë¹ ë¥´ê³  ì •ìƒ ì¼€ì´ìŠ¤
    try:
        return pd.read_csv(io.BytesIO(raw), encoding=enc, sep=sep)
    except Exception:
        pass

    # 2ì°¨: python ì—”ì§„ + ìœ ì—° ì˜µì…˜
    try:
        return pd.read_csv(
            io.BytesIO(raw),
            encoding=enc,
            sep=sep,
            engine='python',
            on_bad_lines='skip',   # pandas>=1.3
            quoting=csv.QUOTE_MINIMAL
        )
    except Exception:
        pass

    # 3ì°¨: êµ¬ë¶„ì ì¬ì‹œë„ (ì„¸ë¯¸ì½œë¡ /íƒ­/ì½¤ë§ˆ ìˆœí™˜)
    for alt_sep in [';', '\t', ',']:
        try:
            return pd.read_csv(
                io.BytesIO(raw),
                encoding=enc,
                sep=alt_sep,
                engine='python',
                on_bad_lines='skip'
            )
        except Exception:
            continue

    # 4ì°¨: TSVë¡œ ê°€ì •
    try:
        return pd.read_table(io.BytesIO(raw), encoding=enc, engine='python', on_bad_lines='skip')
    except Exception as e:
        raise e

## ìµœì‹  ë²„ì „
def robust_read_csv_bytes(raw: bytes):
    """
    ì¸ì½”ë”©/êµ¬ë¶„ì ê°ì§€ + ë¨¸ë¦¿ë§ ìŠ¤í‚µ + í—¤ë”/ë°ì´í„° ìë™ íŒë³„
    """
    raw = raw.replace(b'\x00', b'').replace(b'\r\n', b'\n')
    enc = detect_encoding(raw)

    sample_text = raw[:65536].decode(enc, errors='ignore')
    sep, header_idx, start_idx = find_table_start(sample_text)

    # ìŠ¤í‚µí•  ì¤„ ëª©ë¡ ë§Œë“¤ê¸°
    skiprows = None
    header = 'infer'
    if start_idx is not None:
        if header_idx is not None:
            skiprows = list(range(0, header_idx))
            header = header_idx
        else:
            skiprows = list(range(0, start_idx))
            header = None

    # ì‹¤ì œ ë¨¸ë¦¿ë§ í…ìŠ¤íŠ¸ ì¶”ì¶œ (LLM ì»¨í…ìŠ¤íŠ¸ìš©)
    preamble_text = ""
    if start_idx:
        preamble_text = "\n".join(sample_text.splitlines()[:start_idx]).strip()

    # CSV ì½ê¸° ì‹œë„
    df = None
    last_err = None
    for enc_try in [enc, "utf-8-sig", "cp949", "euc-kr", "latin1"]:
        for engine in ["c", "python"]:
            try:
                df = pd.read_csv(io.BytesIO(raw),
                                 encoding=enc_try,
                                 sep=sep or ',',
                                 header=header,
                                 skiprows=skiprows,
                                 engine=engine,
                                 on_bad_lines='skip')
                break
            except Exception as e:
                last_err = e
        if df is not None:
            break

    if df is None:
        raise last_err or RuntimeError("CSV parsing failed")

    return df, preamble_text

def try_read_excel(raw: bytes) -> Optional[pd.DataFrame]:
    try:
        return pd.read_excel(io.BytesIO(raw))  # openpyxl í•„ìš”
    except Exception:
        return None




def save_image_to_tmp(raw: bytes, filename: str, tmp_dir: str) -> str:
    path = os.path.join(tmp_dir, filename)
    with open(path, "wb") as f:
        f.write(raw)
    return path

def try_make_spectrum_plot(df: pd.DataFrame, filename: str, tmp_dir: str) -> Optional[str]:
    """
    ìŠ¤í™íŠ¸ëŸ¼ìœ¼ë¡œ ë³´ì´ëŠ” CSVëŠ” ë¼ì¸ í”Œë¡¯ì„ ìƒì„±í•´ì„œ PNGë¡œ ì €ì¥.
    ë‹¨ìˆœ íœ´ë¦¬ìŠ¤í‹±:
      - ìˆ«ì ì»¬ëŸ¼ì´ 2ê°œ ì´ìƒì´ê³  í–‰ì´ ì¼ì • ê°œìˆ˜ ì´ìƒì´ë©´
      - ì²« ë²ˆì§¸ ìˆ«ì ì»¬ëŸ¼ = x, ë‘ ë²ˆì§¸ ìˆ«ì ì»¬ëŸ¼ = y ë¡œ ì‚¬ìš©
    """
    numeric_cols = df.select_dtypes(include="number").columns.tolist()
    if len(numeric_cols) < 2 or len(df) < 10:
        return None

    x = df[numeric_cols[0]].values
    y = df[numeric_cols[1]].values

    try:
        plt.figure()
        plt.plot(x, y)
        plt.xlabel(str(numeric_cols[0]))
        plt.ylabel(str(numeric_cols[1]))
        plt.title(filename)
        plt.tight_layout()
        plot_path = os.path.join(tmp_dir, f"{os.path.splitext(filename)[0]}_spectrum.png")
        plt.savefig(plot_path, dpi=200)
        plt.close()
        return plot_path
    except Exception:
        plt.close()
        return None

def parse_txt(raw: bytes, filename: str):
    text = raw.decode("utf-8", errors="ignore")
    return {
        "type": "note",
        "filename": filename,
        "text": text
    }

def to_numeric_series(s: pd.Series):
    # '1,234.5' / '1.234,5' / '1 234,5' ë“± ë‹¤ì–‘í•œ ì§€ì—­ í‘œê¸° â†’ float
    txt = s.astype(str).str.replace(r'\s', '', regex=True)
    # ìœ ëŸ½ì‹ ì†Œìˆ˜ì  ëŒ€ì‘: ì‰¼í‘œê°€ ì†Œìˆ˜ì ì´ê³  ì ì´ ì²œë‹¨ìœ„ì¼ ìˆ˜ ìˆìŒ
    # íœ´ë¦¬ìŠ¤í‹±: ì‰¼í‘œì™€ ì ì´ ëª¨ë‘ ìˆê³ , ë§ˆì§€ë§‰ êµ¬ê°„ì´ 3ì ë¯¸ë§Œì´ë©´ ì†Œìˆ˜ì ìœ¼ë¡œ ê°€ì •
    def conv(x):
        if re.search(r'[0-9],[0-9]{1,2}$', x) and x.count('.') > 0:
            x = x.replace('.', '').replace(',', '.')
        else:
            x = x.replace(',', '')
        try:
            return float(x)
        except Exception:
            return float('nan')
    return txt.map(conv)

## Last Version
def parse_csv(raw: bytes, filename: str, tmp_dir: str):
    # 0) ê¸°ë³¸ê°’
    preamble = ""

    # 1) ì—‘ì…€ ë¨¼ì € ì‹œë„
    xlsx_df = try_read_excel(raw)
    if xlsx_df is not None:
        df = xlsx_df
    else:
        # 2) ê²¬ê³ í•œ CSV ë¡œë”: (df, preamble) ë°˜í™˜
        df, preamble = robust_read_csv_bytes(raw)

    # í˜¹ì‹œë¼ë„ dfê°€ íŠœí”Œì¸ ê²½ìš° ë°©ì–´ (ì‹¤ìˆ˜ ëŒ€ë¹„)
    if isinstance(df, tuple):
        df, preamble = df

    # === ì—¬ê¸°ë¶€í„°ëŠ” ê¸°ì¡´ ë¡œì§ ê·¸ëŒ€ë¡œ ===
    # ìˆ«ì ì—´ ê°ì§€
    numeric_cols = df.select_dtypes(include='number').columns.tolist()

    # ìˆ«ì ì—´ì´ ë¶€ì¡±í•˜ë©´ ë¬¸ìâ†’ìˆ«ì ë³€í™˜ ì‹œë„
    if len(numeric_cols) < 2 and len(df.columns) >= 2:
        def to_numeric_series(s: pd.Series):
            txt = s.astype(str).str.replace(r'\s', '', regex=True)
            def conv(x):
                # ìœ ëŸ½ì‹ ì†Œìˆ˜ì  ëŒ€ì‘ (1.234,5 â†’ 1234.5)
                if re.search(r'[0-9],[0-9]{1,2}$', x) and x.count('.') > 0:
                    x = x.replace('.', '').replace(',', '.')
                else:
                    x = x.replace(',', '')
                try:
                    return float(x)
                except Exception:
                    return float('nan')
            return txt.map(conv)

        for c in df.columns:
            if df[c].dtype == object:
                df[c] = to_numeric_series(df[c])
        numeric_cols = df.select_dtypes(include='number').columns.tolist()

    profile = {
        "rows": int(len(df)),
        "cols": int(len(df.columns)),
        "columns": [str(c) for c in df.columns]
    }

    stats = {}
    num_only = df.select_dtypes(include='number')
    if num_only.shape[1] > 0:
        try:
            desc = num_only.describe().to_dict()
            stats = {col: {k: (float(v) if pd.notna(v) else None) for k, v in m.items()}
                     for col, m in desc.items()}
        except Exception:
            stats = {}

    sample_rows = df.head(20).to_dict(orient="records")

    # ìŠ¤í™íŠ¸ëŸ¼ í”Œë¡¯
    spectrum_plot = None
    if len(numeric_cols) >= 2 and len(df) >= 10:
        x = df[numeric_cols[0]].values
        y = df[numeric_cols[1]].values
        try:
            plt.figure()
            plt.plot(x, y)
            plt.xlabel(str(numeric_cols[0])); plt.ylabel(str(numeric_cols[1]))
            plt.title(filename); plt.tight_layout()
            spectrum_plot = os.path.join(tmp_dir, f"{os.path.splitext(filename)[0]}_spectrum.png")
            plt.savefig(spectrum_plot, dpi=200); plt.close()
        except Exception:
            plt.close()

    return {
        "type": "csv_result",
        "filename": filename,
        "note": preamble,              # â¬…ï¸ ë¨¸ë¦¿ë§ì„ LLM ì»¨í…ìŠ¤íŠ¸ë¡œ ì „ë‹¬
        "profile": profile,
        "stats": stats,
        "sample_rows": sample_rows,
        "spectrum_plot": spectrum_plot
    }

def parse_csv(raw: bytes, filename: str, tmp_dir: str):
    # 1) ì—‘ì…€ ë¨¼ì € ì‹œë„
    df = try_read_excel(raw)
    if df is None:
        df = robust_read_csv_bytes(raw)

    # ìˆ«ìì—´ ë¶€ì¡±í•˜ë©´ ë¬¸ìâ†’ìˆ«ì ë³€í™˜ ì‹œë„
    numeric_cols = df.select_dtypes(include='number').columns.tolist()
    if len(numeric_cols) < 2 and len(df.columns) >= 2:
        def to_numeric_series(s: pd.Series):
            txt = s.astype(str).str.replace(r'\s', '', regex=True)
            def conv(x):
                # ìœ ëŸ½ì‹ ì†Œìˆ˜ì  ëŒ€ì‘
                if re.search(r'[0-9],[0-9]{1,2}$', x) and x.count('.') > 0:
                    x = x.replace('.', '').replace(',', '.')
                else:
                    x = x.replace(',', '')
                try:
                    return float(x)
                except Exception:
                    return float('nan')
            return txt.map(conv)
        for c in df.columns:
            if df[c].dtype == object:
                df[c] = to_numeric_series(df[c])
        numeric_cols = df.select_dtypes(include='number').columns.tolist()

    profile = {
        "rows": int(len(df)),
        "cols": int(len(df.columns)),
        "columns": [str(c) for c in df.columns]
    }

    stats = {}
    num_only = df.select_dtypes(include='number')
    if num_only.shape[1] > 0:
        try:
            desc = num_only.describe().to_dict()
            stats = {col: {k: (float(v) if pd.notna(v) else None) for k, v in m.items()} for col, m in desc.items()}
        except Exception:
            stats = {}

    sample_rows = df.head(20).to_dict(orient="records")

    # ìŠ¤í™íŠ¸ëŸ¼ í”Œë¡¯
    spectrum_plot = None
    if len(numeric_cols) >= 2 and len(df) >= 10:
        x = df[numeric_cols[0]].values
        y = df[numeric_cols[1]].values
        try:
            plt.figure()
            plt.plot(x, y)
            plt.xlabel(str(numeric_cols[0])); plt.ylabel(str(numeric_cols[1]))
            plt.title(filename); plt.tight_layout()
            spectrum_plot = os.path.join(tmp_dir, f"{os.path.splitext(filename)[0]}_spectrum.png")
            plt.savefig(spectrum_plot, dpi=200); plt.close()
        except Exception:
            plt.close()

    return {
        "type": "csv_result",
        "filename": filename,
        "profile": profile,
        "stats": stats,
        "sample_rows": sample_rows,
        "spectrum_plot": spectrum_plot
    }


def parse_csv(raw: bytes, filename: str, tmp_dir: str):
    # 1) ì—‘ì…€ ë¨¼ì € ì‹œë„ (xlsxë¥¼ csvë¡œ ì´ë¦„ë§Œ ë°”ê¾¸ëŠ” ê²½ìš°ë„ ìˆì–´ì„œ)
    df = try_read_excel(raw)
    if df is None:
        # 2) ê²¬ê³ í•œ CSV ë¡œë”
        df = robust_read_csv_bytes(raw)

    # í”„ë¡œí•„/ìš”ì•½
    numeric_cols = df.select_dtypes(include='number').columns.tolist()
    # ìˆ«ì ì—´ ì—†ìœ¼ë©´ ë¬¸ì ì—´ì„ ìˆ«ìë¡œ ë³€í™˜ ì‹œë„ (ìŠ¤í™íŠ¸ëŸ¼ í”íˆ ë¬¸ìë¡œ ë“¤ì–´ì˜´)
    if len(numeric_cols) < 2 and len(df.columns) >= 2:
        for c in df.columns:
            if df[c].dtype == object:
                df[c] = to_numeric_series(df[c])
        numeric_cols = df.select_dtypes(include='number').columns.tolist()

    profile = {
        "rows": int(len(df)),
        "cols": int(len(df.columns)),
        "columns": [str(c) for c in df.columns]
    }

    stats = {}
    num_only = df.select_dtypes(include='number')
    if num_only.shape[1] > 0:
        try:
            desc = num_only.describe().to_dict()
            stats = {col: {k: (float(v) if pd.notna(v) else None) for k, v in m.items()} for col, m in desc.items()}
        except Exception:
            stats = {}

    sample_rows = df.head(20).to_dict(orient="records")

    # ìŠ¤í™íŠ¸ëŸ¼ í”Œë¡¯ (ìˆ«ì ì»¬ëŸ¼ 2ê°œ ì´ìƒ)
    spectrum_plot = None
    if len(numeric_cols) >= 2 and len(df) >= 10:
        x = df[numeric_cols[0]].values
        y = df[numeric_cols[1]].values
        try:
            plt.figure()
            plt.plot(x, y)
            plt.xlabel(str(numeric_cols[0]))
            plt.ylabel(str(numeric_cols[1]))
            plt.title(filename)
            plt.tight_layout()
            spectrum_plot = os.path.join(tmp_dir, f"{os.path.splitext(filename)[0]}_spectrum.png")
            plt.savefig(spectrum_plot, dpi=200)
            plt.close()
        except Exception:
            plt.close()

    return {
        "type": "csv_result",
        "filename": filename,
        "profile": profile,
        "stats": stats,
        "sample_rows": sample_rows,
        "spectrum_plot": spectrum_plot
    }

def parse_image(raw: bytes, filename: str, tmp_dir: str):
    path = save_image_to_tmp(raw, filename, tmp_dir)
    try:
        img = Image.open(io.BytesIO(raw))
        ocr = pytesseract.image_to_string(img, lang="eng+kor")
    except Exception:
        ocr = ""
    return {
        "type": "image_result",
        "filename": filename,
        "ocr_hint": ocr.strip(),
        "path": path
    }

def parse_other(raw: bytes, filename: str, tmp_dir: str):
    path = save_image_to_tmp(raw, filename, tmp_dir) if filename.lower().endswith((".png",".jpg",".jpeg",".tif",".tiff")) else None
    return {
        "type": "other_file",
        "filename": filename,
        "path": path,
        "comment": "ì§€ì›í•˜ì§€ ì•ŠëŠ” í˜•ì‹ (ë˜ëŠ” ë³„ë„ ì²˜ë¦¬ í•„ìš”). ì°¸ê³ ìš©ìœ¼ë¡œë§Œ ì–¸ê¸‰."
    }


# =========================
# ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (ì—°êµ¬ ë¦¬í¬íŠ¸)
# =========================

SYSTEM_PROMPT = """
ë‹¹ì‹ ì€ ì¬ë£Œ/ì†Œì/ë‚˜ë…¸êµ¬ì¡° ë¶„ì„ì„ ì „ë¬¸ìœ¼ë¡œ í•˜ëŠ” ì—°êµ¬ ë¦¬í¬íŠ¸ ì‘ì„±ìì…ë‹ˆë‹¤.
ì…ë ¥ ìë£Œë¥¼ ê¸°ë°˜ìœ¼ë¡œ 'ì‹œë£Œ ë¶„ì„ ê²°ê³¼ ë³´ê³ ì„œ' ì´ˆì•ˆì„ ì‘ì„±í•©ë‹ˆë‹¤.

ê·œì¹™:
1. êµ¬ì¡°:
# [Report Title]
## 1. Introduction & Objective
## 2. Sample Information
## 3. Experimental Methods
## 4. Results
## 5. Discussion
## 6. Conclusion
## 7. Suggestions / Future Work
(ì„ íƒ) ## Appendix (í•µì‹¬ íŒŒë¼ë¯¸í„°/ê°„ë‹¨ í‘œë§Œ)

2. ìŠ¤íƒ€ì¼:
- ë…¼ë¬¸/í…Œí¬ë‹ˆì»¬ ë¦¬í¬íŠ¸ í†¤.
- ê³¼ì¥ ì—†ì´: "ê´€ì°°ëœë‹¤", "ì‹œì‚¬í•œë‹¤", "ê°€ëŠ¥ì„±ì´ ìˆë‹¤", "ë°ì´í„° ë²”ìœ„ ë‚´ì—ì„œ" ë“±.
- ì¸ê³¼ê´€ê³„ë¥¼ ì„£ë¶ˆë¦¬ ë‹¨ì •í•˜ì§€ ì•ŠëŠ”ë‹¤.

3. ë°ì´í„° ì‚¬ìš©:
- ì—…ë¡œë“œëœ txt/csv/image ë‚´ìš©ì€ ì°¸ê³ ìš©.
- ìµœì¢… ê²°ê³¼ì—ëŠ” raw ë°ì´í„° ì „ì²´ë¥¼ ë³µë¶™í•˜ì§€ ë§ ê²ƒ.
- ì£¼ìš” í”¼í¬/ìƒ/ì¡°ì„±/ì…ë„/ê³„ë©´ ì •ë³´ ë° ê²½í–¥ë§Œ ìš”ì•½.

4. ì´ë¯¸ì§€:
- ì‹¤ì œ PDFì—ëŠ” ì—…ë¡œë“œëœ ì´ë¯¸ì§€ë¥¼ ëª¨ë‘ í¬í•¨í•œë‹¤ (ì´ëŠ” ì‹œìŠ¤í…œì´ ì²˜ë¦¬).
- í…ìŠ¤íŠ¸ì—ì„œëŠ” "ê·¸ë¦¼ 1", "ê·¸ë¦¼ 2" ë“±ìœ¼ë¡œ ì ì ˆíˆ ì–¸ê¸‰.

5. ë©”íƒ€ë°ì´í„°:
- project_name, sample_id, techniques, key_questionsë¥¼ ì œëª©/ì„œë¡ /ê²°ë¡ ì— ë°˜ì˜.

6. í‘œê¸°:
- í™”í•™ì‹/ì§€ìˆ˜/ì•„ë˜ì²¨ìëŠ” ìœ ë‹ˆì½”ë“œ ë¬¸ì(HfOâ‚‚)ê°€ ì•„ë‹ˆë¼ HTML ìŠ¤íƒ€ì¼ í‘œê¸°(HfO<sub>2</sub>, 10<sup>-3</sup>)ë¡œ ì‘ì„±í•˜ì„¸ìš”.
- ì¤„ë°”ê¿ˆì€ HTMLì˜ <br/>ê°€ ì•„ë‹Œ 'ë¹ˆ ì¤„(ë‘ ì¤„ë°”ê¿ˆ)'ë¡œ í‘œí˜„í•˜ì„¸ìš”. ì‹¤ì œ ì¤„ë°”ê¿ˆ íƒœê·¸ê°€ í•„ìš”í•˜ë©´ <br/>ë§Œ ì‚¬ìš©í•˜ì„¸ìš”.
- í™”í•™ì‹ ì§€ìˆ˜/ì•„ë˜ì²¨ìëŠ” ë°˜ë“œì‹œ HTML í‘œê¸°(HfO<sub>2</sub>, 10<sup>-3</sup>)ë¡œ ì“°ê³ , ìœ ë‹ˆì½”ë“œ ì²¨ì ë¬¸ìëŠ” ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”.
"""


# =========================
# ë¦¬í¬íŠ¸ í…ìŠ¤íŠ¸ ìƒì„±
# =========================

def build_context(meta: ReportMeta, parsed_files):
    # LLMì—ëŠ” path ê°™ì€ ë¡œì»¬ ê²½ë¡œëŠ” í¬ê²Œ ì˜ë¯¸ ì—†ìœ¼ë‹ˆ í•„ìš”í•œ ì •ë³´ë§Œ ì „ë‹¬
    sanitized_files = []
    for f in parsed_files:
        d = dict(f)
        d.pop("path", None)
        d.pop("spectrum_plot", None)
        sanitized_files.append(d)
    return {
        "meta": meta.dict(),
        "files": sanitized_files
    }

## ìµœì‹  ë²„ì „
def build_context(meta: ReportMeta, parsed_files):
    sanitized_files = []
    for f in parsed_files:
        d = dict(f)
        d.pop("path", None)
        d.pop("spectrum_plot", None)
        sanitized_files.append(d)
    return {
        "meta": meta.dict(),
        "files": sanitized_files   # note ë„ ì—¬ê¸° í¬í•¨ë¨
    }

async def generate_report_text(meta: ReportMeta, parsed_files) -> str:
    context = build_context(meta, parsed_files)
    prompt = f"""
ë‹¤ìŒì€ ì‹œë£Œ ë¶„ì„ í”„ë¡œì íŠ¸ì˜ ë©”íƒ€ë°ì´í„°ì™€ íŒŒì¼ ìš”ì•½ì…ë‹ˆë‹¤.
ì´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìœ„ ê·œì¹™ì— ë§ëŠ” 'ì‹œë£Œ ë¶„ì„ ê²°ê³¼ ë³´ê³ ì„œ'ë¥¼ ì‘ì„±í•˜ì„¸ìš”.

[ë©”íƒ€ë°ì´í„° ë° íŒŒì¼ ìš”ì•½]
{json.dumps(context, ensure_ascii=False)}

ìš”êµ¬ì‚¬í•­:
- ì œì‹œëœ êµ¬ì¡°ë¥¼ ë”°ë¥´ë˜, ë‚´ìš© ë¶€ì¡±í•œ ì„¹ì…˜ì€ ê°„ë‹¨íˆ.
- key_questionsë¥¼ Results/Discussion/Conclusionì—ì„œ ì§ì ‘ì ìœ¼ë¡œ ë‹¤ë£° ê²ƒ.
- techniques ì •ë³´ì— ë§ëŠ” í˜„ì‹¤ì ì¸ í•´ì„ë§Œ ì‚¬ìš©í•  ê²ƒ.
"""
    return await call_llm(prompt, SYSTEM_PROMPT)


# =========================
# PDF ë Œë”ë§ (í…ìŠ¤íŠ¸ + ì´ë¯¸ì§€ + ìŠ¤í™íŠ¸ëŸ¼ í”Œë¡¯)
# =========================
import re, html

ALLOWED_SIMPLE_TAGS = {"b","/b","i","/i","u","/u","sub","/sub","sup","/sup"}
BR_PATTERN = re.compile(r"</?\s*br\s*/?\s*>", re.IGNORECASE)

SUB_MAP = {"â‚€":"0","â‚":"1","â‚‚":"2","â‚ƒ":"3","â‚„":"4","â‚…":"5","â‚†":"6","â‚‡":"7","â‚ˆ":"8","â‚‰":"9"}
SUP_MAP = {"â°":"0","Â¹":"1","Â²":"2","Â³":"3","â´":"4","âµ":"5","â¶":"6","â·":"7","â¸":"8","â¹":"9","â»":"-","âº":"+"}

def normalize_sub_sup(text: str) -> str:
    # ìœ ë‹ˆì½”ë“œ ì•„ë˜ì²¨ì â†’ <sub>ìˆ«ì</sub>
    for u, d in SUB_MAP.items():
        text = text.replace(u, f"<sub>{d}</sub>")
    # ìœ ë‹ˆì½”ë“œ ìœ„ì²¨ì ì‹œí€€ìŠ¤ â†’ <sup>...</sup>
    out, i = [], 0
    while i < len(text):
        if text[i] in SUP_MAP:
            buf = []
            while i < len(text) and text[i] in SUP_MAP:
                buf.append(SUP_MAP[text[i]])
                i += 1
            out.append(f"<sup>{''.join(buf)}</sup>")
        else:
            out.append(text[i]); i += 1
    return "".join(out)

def sanitize_markup(raw: str) -> str:
    # 1) ìœ ë‹ˆì½”ë“œ ì²¨ì ì •ê·œí™”
    t = normalize_sub_sup(raw)

    # 2) br êµì •: ì–´ë–¤ í˜•íƒœë“  ì „ë¶€ <br/> ë¡œ
    def _br_repl(m): return "<br/>"
    t = BR_PATTERN.sub(_br_repl, t)

    # 3) ì¼ë‹¨ ì „ì²´ ì´ìŠ¤ì¼€ì´í”„
    t = html.escape(t)

    # 4) í—ˆìš© íƒœê·¸ë§Œ ë˜ëŒë¦¬ê¸° (+ <br/>)
    #   &lt;sub&gt; â†’ <sub>  ë“±ì˜ íŒ¨í„´ì„ ë˜ì‚´ë¦¼
    for tag in ["b","/b","i","/i","u","/u","sub","/sub","sup","/sup"]:
        t = t.replace(f"&lt;{tag}&gt;", f"<{tag}>")
    t = t.replace("&lt;br/&gt;", "<br/>")

    # 5) ì²¨ì íƒœê·¸ ì§ ë§ì¶”ê¸°: ë‹«í˜ ê³¼ì‰ ì œê±°, ë¶€ì¡±í•˜ë©´ ë‹«í˜ ì¶”ê°€
    def fix_tag_balance(txt: str, tag: str) -> str:
        opens = len(re.findall(fr"<{tag}>", txt))
        closes = len(re.findall(fr"</{tag}>", txt))
        if closes > opens:
            # ê³¼ì‰ ë‹«í˜ ì œê±° (ì•ì—ì„œë¶€í„°)
            extra = closes - opens
            txt = re.sub(fr"</{tag}>", "", txt, count=extra)
        elif opens > closes:
            # ë¶€ì¡±í•œ ë‹«í˜ì„ ë§¨ ëì— ì¶”ê°€
            txt += "</{}>".format(tag) * (opens - closes)
        return txt

    t = fix_tag_balance(t, "sub")
    t = fix_tag_balance(t, "sup")

    return t

def render_pdf(report_text: str, parsed_files, tmp_dir: str) -> bytes:
    buffer = io.BytesIO()
    doc = SimpleDocTemplate(buffer, pagesize=A4)
    styles = getSampleStyleSheet()

    # ëª¨ë“  ìŠ¤íƒ€ì¼ì— í•œê¸€ í°íŠ¸ ì ìš©
    for name in ["Normal", "BodyText", "Heading1", "Heading2", "Heading3"]:
        if name in styles:
            styles[name].fontName = FONT_NAME

    story = []

    # 1) LLMì´ ìƒì„±í•œ ë³¸ë¬¸ ë¨¼ì €
  for line in report_text.splitlines():
    t = line.strip()
    if not t:
        story.append(Spacer(1, 6))
        continue

    # ğŸ”¹ ì•„ë˜/ìœ„ì²¨ì ìœ ë‹ˆì½”ë“œ â†’ <sub>/<sup> íƒœê·¸ë¡œ ë³€í™˜
    t = normalize_sub_sup(t)

    if t.startswith("# "):
        story.append(Paragraph(t[2:], styles["Heading1"]))
    elif t.startswith("## "):
        story.append(Paragraph(t[3:], styles["Heading2"]))
    elif t.startswith("### "):
        story.append(Paragraph(t[4:], styles["Heading3"]))
    else:
        story.append(Paragraph(t, styles["BodyText"]))
    story.append(Spacer(1, 4))  
##
		for line in report_text.splitlines():
        t = line.strip()
        if not t:
            story.append(Spacer(1, 6))
            continue
        if t.startswith("# "):
            story.append(Paragraph(t[2:], styles["Heading1"]))
        elif t.startswith("## "):
            story.append(Paragraph(t[3:], styles["Heading2"]))
        elif t.startswith("### "):
            story.append(Paragraph(t[4:], styles["Heading3"]))
        else:
            story.append(Paragraph(t, styles["BodyText"]))
        story.append(Spacer(1, 4))
##
    # 2) Appendix: ì—…ë¡œë“œëœ ì´ë¯¸ì§€ + ìŠ¤í™íŠ¸ëŸ¼ í”Œë¡¯ ì „ë¶€ í¬í•¨
    figs = []

    # ì‹¤ì œ ì´ë¯¸ì§€
    for f in parsed_files:
        if f.get("type") == "image_result" and f.get("path"):
            figs.append(("ì´ë¯¸ì§€", f["filename"], f["path"]))
        # ê¸°íƒ€ íŒŒì¼ ì¤‘ ì´ë¯¸ì§€ì¸ ê²½ìš°
        if f.get("type") == "other_file" and f.get("path") and f["path"].lower().endswith((".png",".jpg",".jpeg",".tif",".tiff")):
            figs.append(("ì´ë¯¸ì§€", f["filename"], f["path"]))

    # ìŠ¤í™íŠ¸ëŸ¼ í”Œë¡¯
    for f in parsed_files:
        if f.get("type") == "csv_result" and f.get("spectrum_plot"):
            figs.append(("ìŠ¤í™íŠ¸ëŸ¼", f["filename"], f["spectrum_plot"]))

    if figs:
        story.append(Spacer(1, 12))
        story.append(Paragraph("Appendix - Figures", styles["Heading2"]))
        story.append(Spacer(1, 6))

        fig_idx = 1
        for kind, fname, path in figs:
            if not os.path.exists(path):
                continue
            caption = f"Figure {fig_idx}. ({kind}) {fname}"
            story.append(Paragraph(caption, styles["BodyText"]))
            story.append(Spacer(1, 4))
            # ì´ë¯¸ì§€ í­ì„ í˜ì´ì§€ì— ë§ê²Œ ì ë‹¹íˆ ì¡°ì •
            img = RLImage(path)
            img._restrictSize(500, 350)
            story.append(img)
            story.append(Spacer(1, 12))
            fig_idx += 1

    doc.build(story)
    pdf_data = buffer.getvalue()
    buffer.close()
    return pdf_data


# =========================
# HTML UI
# =========================

@app.get("/", response_class=HTMLResponse)
async def index():
    index_path = os.path.join(STATIC_DIR, "index.html")
    if os.path.exists(index_path):
        with open(index_path, "r", encoding="utf-8") as f:
            return HTMLResponse(f.read())
    return HTMLResponse("<h1>Research Report Generator</h1><p>static/index.html íŒŒì¼ì„ ì¶”ê°€í•˜ì„¸ìš”.</p>")


# =========================
# API: ì›¹í¼ì—ì„œ í˜¸ì¶œ
# =========================

@app.post("/api/generate-research-report")
async def api_generate_research_report(
    project_name: str = Form(""),
    sample_id: str = Form(""),
    sample_description: str = Form(""),
    operator: str = Form(""),
    date: str = Form(""),
    techniques: str = Form(""),
    key_questions: str = Form(""),
    files: List[UploadFile] = File(...)
):
    tech_list = [t.strip() for t in techniques.split(",") if t.strip()] if techniques else []

    meta = ReportMeta(
        project_name=project_name or None,
        sample_id=sample_id or None,
        sample_description=sample_description or None,
        operator=operator or None,
        date=date or None,
        techniques=tech_list or None,
        key_questions=key_questions or None,
    )

    # temp dir: ì´ë¯¸ì§€/í”Œë¡¯ ì €ì¥ìš©
    tmp_dir = tempfile.mkdtemp()
    parsed_files = []

    for f in files:
        raw = await f.read()
        name = f.filename.lower()

        if name.endswith(".txt"):
            parsed_files.append(parse_txt(raw, f.filename))
        elif name.endswith(".csv"):
            parsed_files.append(parse_csv(raw, f.filename, tmp_dir))
        elif name.endswith((".png", ".jpg", ".jpeg", ".tif", ".tiff")):
            parsed_files.append(parse_image(raw, f.filename, tmp_dir))
        else:
            parsed_files.append(parse_other(raw, f.filename, tmp_dir))

    report_text = await generate_report_text(meta, parsed_files)
    pdf_bytes = render_pdf(report_text, parsed_files, tmp_dir)

    headers = {
        "Content-Disposition": 'inline; filename="research_report.pdf"'
    }
    return Response(content=pdf_bytes, media_type="application/pdf", headers=headers)

@app.post("/api/generate-from-server")
async def api_generate_from_server(job: ServerFileJob, request: Request):
    # 1) ë³´ì•ˆ: ë¡œì»¬ì—ì„œë§Œ í—ˆìš©
    if request.client.host not in ("127.0.0.1", "localhost", "::1"):
        raise HTTPException(status_code=403, detail="ì´ ì—”ë“œí¬ì¸íŠ¸ëŠ” ì„œë²„ ë¡œì»¬ì—ì„œë§Œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

    # 2) ë©”íƒ€ êµ¬ì„± (ê¸°ì¡´ê³¼ ë™ì¼)
    tech_list = [t.strip() for t in job.techniques.split(",") if t.strip()] if job.techniques else []
    meta = ReportMeta(
        project_name=job.project_name or None,
        sample_id=job.sample_id or None,
        sample_description=job.sample_description or None,
        operator=job.operator or None,
        date=job.date or None,
        techniques=tech_list or None,
        key_questions=job.key_questions or None,
    )

    tmp_dir = tempfile.mkdtemp()
    parsed_files = []

    # 3) ê²½ë¡œë³„ë¡œ íŒŒì¼ ì½ê¸°
    for rel in job.paths:
        rel = rel.strip()
        if not rel:
            continue

        # DATA_ROOT ê¸°ì¤€ ìƒëŒ€ ê²½ë¡œë¡œ ì œí•œ
        full_path = os.path.abspath(os.path.join(DATA_ROOT, rel))
        if not full_path.startswith(os.path.abspath(DATA_ROOT)):
            # ìƒìœ„ ë””ë ‰í† ë¦¬ íƒˆì¶œ ë°©ì§€
            continue
        if not os.path.exists(full_path):
            continue

        with open(full_path, "rb") as f:
            raw = f.read()
        name = os.path.basename(full_path).lower()

        # ê¸°ì¡´ ì—…ë¡œë“œ í•¸ë“¤ë§ê³¼ ë™ì¼í•œ ë¶„ê¸°
        if name.endswith(".txt"):
            parsed_files.append(parse_txt(raw, os.path.basename(full_path)))
        elif name.endswith(".csv"):
            parsed_files.append(parse_csv(raw, os.path.basename(full_path), tmp_dir))
        elif name.endswith((".png",".jpg",".jpeg",".tif",".tiff")):
            parsed_files.append(parse_image(raw, os.path.basename(full_path), tmp_dir))
        elif name.endswith(".pdf"):
            parsed_files.append(parse_pdf_local(raw, os.path.basename(full_path), tmp_dir))
        elif name.endswith(".pptx"):
            parsed_files.append(parse_pptx(raw, os.path.basename(full_path), tmp_dir))
        elif name.endswith(".docx"):
            parsed_files.append(parse_docx(raw, os.path.basename(full_path), tmp_dir))
        else:
            path=None
            if name.endswith((".bmp",".gif",".webp")):
                path = os.path.join(tmp_dir, os.path.basename(full_path))
                with open(path,"wb") as out:
                    out.write(raw)
            parsed_files.append({"type":"other_file","filename":os.path.basename(full_path),
                                 "path":path,"comment":"ì°¸ê³  íŒŒì¼"})

    if not parsed_files:
        raise HTTPException(status_code=400, detail="ìœ íš¨í•œ ì„œë²„ ë¡œì»¬ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")

    # ë¬¸ì„œ ì´ë¯¸ì§€ â†’ Qwen-VL ìš”ì•½
    parsed_files = await enrich_items_with_mm(parsed_files)

    # ë³¸ë¬¸ ìƒì„±
    report_text = await generate_report_text(meta, parsed_files)

    # PDF ë Œë”
    pdf_bytes = render_pdf(report_text, parsed_files, tmp_dir)
    headers = {"Content-Disposition": 'inline; filename=\"research_report_server_files.pdf\"'}
    return Response(content=pdf_bytes, media_type="application/pdf", headers=headers)


<div class="section">
  <h3>ì„œë²„ ë¡œì»¬ íŒŒì¼ (ê´€ë¦¬ì)</h3>
  <div style="font-size:12px;color:#666">
    ì„œë²„ì—ì„œ ì ‘ì†í•œ ê²½ìš°, DATA_ROOT ê¸°ì¤€ ìƒëŒ€ ê²½ë¡œë¥¼ í•œ ì¤„ì— í•˜ë‚˜ì”© ì…ë ¥í•˜ì„¸ìš”.<br>
    ì˜ˆ) wafer1/ftir_spectrum.csv<br>
    ì˜ˆ) projectA/report.pdf
  </div>
  <textarea id="server_paths" placeholder="example:&#10;sample1/eels_map.csv&#10;sample1/sem_image.tif"></textarea>
  <button type="button" id="btn-server">ì„œë²„ ë¡œì»¬ íŒŒì¼ë¡œ ë¦¬í¬íŠ¸ ìƒì„±</button>
</div>


const btnServer = document.getElementById('btn-server');

btnServer.addEventListener('click', async () => {
  const serverPathsRaw = document.getElementById('server_paths').value || '';
  const paths = serverPathsRaw
                  .split(/\r?\n/)
                  .map(s => s.trim())
                  .filter(s => s.length > 0);

  if (!paths.length) {
    st.textContent = "âš  ì„œë²„ ë¡œì»¬ íŒŒì¼ ê²½ë¡œë¥¼ í•œ ì¤„ì— í•˜ë‚˜ì”© ì…ë ¥í•˜ì„¸ìš”.";
    return;
  }

  st.textContent = "ì„œë²„ ë¡œì»¬ íŒŒì¼ë¡œ ë¦¬í¬íŠ¸ ìƒì„± ì¤‘...";
  const techs = [...document.querySelectorAll('input[name="techniques"]:checked')].map(x => x.value);

  const payload = {
    project_name: f.project_name.value || "",
    sample_id: f.sample_id.value || "",
    sample_description: f.sample_description.value || "",
    operator: f.operator.value || "",
    date: f.date.value || "",
    techniques: techs.join(','),
    key_questions: f.key_questions.value || "",
    paths: paths
  };

  try {
    const res = await fetch('/api/generate-from-server', {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify(payload)
    });
    if (!res.ok) throw new Error('ì„œë²„ ì˜¤ë¥˜: '+res.status);
    const blob = await res.blob();
    const url = URL.createObjectURL(blob);
    frame.src = url;
    dl.href = url;
    cont.style.display = 'block';
    st.textContent = "âœ… ì™„ë£Œ (ì„œë²„ ë¡œì»¬ íŒŒì¼ ê¸°ë°˜)";
  } catch (err) {
    st.textContent = "âŒ ì˜¤ë¥˜: " + err.message;
  }
});



static/index.html (ìµœì†Œ ë™ì‘ ë²„ì „, ë°”ë¡œ ì‚¬ìš© ê°€ëŠ¥):

<!DOCTYPE html>
<html lang="ko">
<head>
  <meta charset="UTF-8" />
  <title>ì—°êµ¬ ì‹œë£Œ ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„±ê¸°</title>
  <style>
    body { font-family: system-ui, -apple-system, BlinkMacSystemFont, sans-serif; margin: 20px; }
    h1 { margin-bottom: 4px; }
    .subtitle { color: #666; margin-bottom: 20px; }
    .section { margin-bottom: 16px; padding: 12px; border: 1px solid #ddd; border-radius: 8px; }
    .section h2 { margin: 0 0 8px; font-size: 16px; }
    label { display: block; margin-top: 6px; font-size: 13px; }
    input[type="text"], input[type="date"], textarea {
      width: 100%; padding: 6px 8px; margin-top: 2px;
      box-sizing: border-box; font-size: 13px;
    }
    textarea { resize: vertical; min-height: 60px; }
    .chips label { display: inline-flex; align-items: center; margin-right: 8px; margin-top: 4px; }
    .chips input { margin-right: 4px; }
    .file-box { padding: 8px; background:#fafafa; border-radius:6px; border:1px dashed #bbb; }
    button {
      margin-top: 10px; padding: 10px 18px; font-size: 14px;
      border: none; border-radius: 6px; cursor: pointer;
      background: #2563eb; color: #fff;
    }
    button:disabled { opacity: 0.6; cursor: default; }
    #status { margin-top: 10px; font-size: 13px; color: #444; }
    #pdf-container { margin-top: 20px; }
    #pdf-frame { width: 100%; height: 600px; border: 1px solid #ccc; border-radius: 6px; }
    #download-link { display: inline-block; margin-top: 8px; font-size: 13px; }
  </style>
</head>
<body>
  <h1>ì—°êµ¬ ì‹œë£Œ ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„±ê¸°</h1>
  <div class="subtitle">
    ì‹œë£Œ ê´€ë ¨ ë©”íƒ€ë°ì´í„°ì™€ ë¶„ì„ ê²°ê³¼ íŒŒì¼ì„ ì—…ë¡œë“œí•˜ë©´,<br>
    ë¡œì»¬ LLMì´ ìë™ìœ¼ë¡œ ì—°êµ¬ìš© ë¦¬í¬íŠ¸ ì´ˆì•ˆì„ ìƒì„±í•©ë‹ˆë‹¤.
  </div>

  <form id="report-form">
    <div class="section">
      <h2>ê¸°ë³¸ ì •ë³´</h2>
      <label>Project Name
        <input type="text" name="project_name" placeholder="ì˜ˆ: Hf-ZrO2 Gate Stack Reliability Study" />
      </label>
      <label>Sample ID
        <input type="text" name="sample_id" placeholder="ì˜ˆ: HZ-ALD-230901-03" />
      </label>
      <label>Sample Description
        <textarea name="sample_description" placeholder="ì‹œë£Œ êµ¬ì¡°, ê³µì • ì¡°ê±´, ìŠ¤íƒ ì •ë³´ ë“±ì„ ê°„ë‹¨íˆ ê¸°ì…"></textarea>
      </label>
      <label>Operator / Author
        <input type="text" name="operator" placeholder="ì‘ì„±ì ì´ë¦„" />
      </label>
      <label>Date
        <input type="date" name="date" />
      </label>
    </div>

    <div class="section">
      <h2>ë¶„ì„ ê¸°ë²•</h2>
      <div class="chips">
        <label><input type="checkbox" value="TEM"> TEM</label>
        <label><input type="checkbox" value="STEM"> STEM</label>
        <label><input type="checkbox" value="SEM"> SEM</label>
        <label><input type="checkbox" value="EDS"> EDS</label>
        <label><input type="checkbox" value="EELS"> EELS</label>
        <label><input type="checkbox" value="XRD"> XRD</label>
        <label><input type="checkbox" value="XPS"> XPS</label>
        <label><input type="checkbox" value="AFM"> AFM</label>
        <label><input type="checkbox" value="Raman"> Raman</label>
        <label><input type="checkbox" value="PL"> PL</label>
        <label><input type="checkbox" value="IV"> IV/CV</label>
      </div>
    </div>

    <div class="section">
      <h2>í•µì‹¬ ì§ˆë¬¸ / ë¶„ì„ ëª©ì </h2>
      <textarea name="key_questions"
        placeholder="ì˜ˆ: ferroelectric phase fraction í‰ê°€, interface roughness ë° diffusion ì—¬ë¶€, oxygen vacancy ë¶„í¬, leakage ê´€ë ¨ ê²°í•¨ ìœ ë¬´ ë“±"></textarea>
    </div>

    <div class="section">
      <h2>ë°ì´í„° ì—…ë¡œë“œ</h2>
      <div class="file-box">
        <p style="margin:0 0 4px; font-size:12px;">
          ì§€ì›: csv (ìˆ˜ì¹˜ ê²°ê³¼), txt (ë…¸íŠ¸/ë¡œê·¸), png/jpg/tif (ì´ë¯¸ì§€), ê¸°íƒ€ íŒŒì¼ì€ ì°¸ê³ ìš©ìœ¼ë¡œë§Œ í‘œì‹œë©ë‹ˆë‹¤.
        </p>
        <input type="file" id="files" name="files" multiple />
      </div>
    </div>

    <button type="submit" id="submit-btn">ë¦¬í¬íŠ¸ ìƒì„±</button>
    <div id="status"></div>
  </form>

  <div id="pdf-container" style="display:none;">
    <h2>ìƒì„±ëœ ë¦¬í¬íŠ¸</h2>
    <iframe id="pdf-frame"></iframe>
    <a id="download-link" href="#" download="research_report.pdf">ğŸ“¥ PDF ë‹¤ìš´ë¡œë“œ</a>
  </div>

  <script>
    const form = document.getElementById('report-form');
    const statusEl = document.getElementById('status');
    const submitBtn = document.getElementById('submit-btn');
    const pdfContainer = document.getElementById('pdf-container');
    const pdfFrame = document.getElementById('pdf-frame');
    const downloadLink = document.getElementById('download-link');

    form.addEventListener('submit', async (e) => {
      e.preventDefault();
      statusEl.textContent = "ë¦¬í¬íŠ¸ë¥¼ ìƒì„± ì¤‘ì…ë‹ˆë‹¤. (ë¡œì»¬ LLM ì²˜ë¦¬)";
      submitBtn.disabled = true;
      pdfContainer.style.display = 'none';

      const fd = new FormData();

      // ê¸°ë³¸ í•„ë“œ
      fd.append('project_name', form.project_name.value);
      fd.append('sample_id', form.sample_id.value);
      fd.append('sample_description', form.sample_description.value);
      fd.append('operator', form.operator.value);
      fd.append('date', form.date.value);

      // techniques ì²´í¬ë°•ìŠ¤ â†’ "TEM,EELS,XRD" í˜•íƒœ ë¬¸ìì—´
      const techs = [];
      form.querySelectorAll('.chips input[type="checkbox"]:checked').forEach(chk => {
        techs.push(chk.value);
      });
      fd.append('techniques', techs.join(','));

      // key_questions
      fd.append('key_questions', form.key_questions.value);

      // íŒŒì¼ë“¤
      const filesInput = document.getElementById('files');
      if (filesInput.files.length === 0) {
        statusEl.textContent = "âš  ë°ì´í„° íŒŒì¼ì„ í•œ ê°œ ì´ìƒ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.";
        submitBtn.disabled = false;
        return;
      }
      for (const file of filesInput.files) {
        fd.append('files', file);
      }

      try {
        const res = await fetch('/api/generate-research-report', {
          method: 'POST',
          body: fd
        });

        if (!res.ok) {
          throw new Error('ì„œë²„ ì˜¤ë¥˜: ' + res.status);
        }

        const blob = await res.blob();
        const url = URL.createObjectURL(blob);

        // PDF iframeì— í‘œì‹œ
        pdfFrame.src = url;
        // ë‹¤ìš´ë¡œë“œ ë§í¬ ì„¤ì •
        downloadLink.href = url;
        downloadLink.download = "research_report.pdf";

        pdfContainer.style.display = 'block';
        statusEl.textContent = "âœ… ë¦¬í¬íŠ¸ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.";
      } catch (err) {
        console.error(err);
        statusEl.textContent = "âŒ ë¦¬í¬íŠ¸ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: " + err.message;
      } finally {
        submitBtn.disabled = false;
      }
    });
  </script>
</body>
</html>




