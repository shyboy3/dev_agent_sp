Train U-Net1

import os, glob, json, math, random, hashlib
from dataclasses import dataclass, asdict
from typing import List, Dict, Tuple

import numpy as np
import pandas as pd
import cv2
import torch
import torch.nn as nn
import torch.nn.functional as F
import matplotlib.pyplot as plt
from scipy.ndimage import distance_transform_edt


# =========================
# Config (edit here)
# =========================
@dataclass
class CFG:
    # data root
    root: str = "./DATA"
    train_dir: str = "TRAIN_SEM_IMAGE"
    test_dir: str = "TEST_SEM_IMAGE"

    # valid area crop (remove bottom metadata band)
    crop_h: int = 896
    crop_w: int = 1280

    # patching
    patch: int = 384
    stride: int = 48

    # preprocessing
    use_clahe: bool = True
    clahe_clip: float = 2.0
    clahe_tile: int = 8
    bg_kernel: int = 49  # ~48x48 grain scale

    robust_p_lo: float = 1.0
    robust_p_hi: float = 99.0

    # label extraction (red line)
    r_thr: int = 160
    g_thr: int = 140
    b_thr: int = 140

    # U1 target thickening (coarse anchor)
    dilate_radius: int = 2

    # training
    seed: int = 42
    device: str = "cuda" if torch.cuda.is_available() else "cpu"
    batch: int = 4
    epochs: int = 30
    lr: float = 1e-3
    weight_decay: float = 1e-5

    # model
    base: int = 64
    gn_groups: int = 16  # GroupNorm groups (batch=4 안정)

    # eval/visualize
    thr_vis: float = 0.5  # for binarizing predictions
    exp_name: str = "exp_unet1"


# =========================
# Repro
# =========================
def set_seed(seed: int):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)

    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True
    # torch.use_deterministic_algorithms(True)  # 켜면 일부 op에서 에러 날 수 있어 옵션 처리 가능


def worker_init_fn(worker_id: int):
    # make dataloader workers deterministic
    seed = torch.initial_seed() % (2**32)
    np.random.seed(seed + worker_id)
    random.seed(seed + worker_id)


# =========================
# IO & preprocessing
# =========================
def list_jpgs(folder: str) -> List[str]:
    return sorted(glob.glob(os.path.join(folder, "*.jpg")))

def crop_valid(img: np.ndarray, cfg: CFG) -> np.ndarray:
    return img[:cfg.crop_h, :cfg.crop_w]

def read_gray(path: str) -> np.ndarray:
    im = cv2.imread(path, cv2.IMREAD_GRAYSCALE)
    if im is None:
        raise FileNotFoundError(path)
    return im

def read_label_bgr(path: str) -> np.ndarray:
    im = cv2.imread(path, cv2.IMREAD_COLOR)  # BGR
    if im is None:
        raise FileNotFoundError(path)
    return im

def apply_clahe(img01: np.ndarray, cfg: CFG) -> np.ndarray:
    img8 = np.clip(img01 * 255.0, 0, 255).astype(np.uint8)
    clahe = cv2.createCLAHE(clipLimit=cfg.clahe_clip, tileGridSize=(cfg.clahe_tile, cfg.clahe_tile))
    out = clahe.apply(img8).astype(np.float32) / 255.0
    return out

def estimate_background(img01: np.ndarray, cfg: CFG) -> np.ndarray:
    k = cfg.bg_kernel
    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (k, k))
    img8 = np.clip(img01 * 255.0, 0, 255).astype(np.uint8)
    bg8 = cv2.morphologyEx(img8, cv2.MORPH_OPEN, kernel)
    return bg8.astype(np.float32) / 255.0

def robust_rescale(x: np.ndarray, cfg: CFG) -> np.ndarray:
    lo = np.percentile(x, cfg.robust_p_lo)
    hi = np.percentile(x, cfg.robust_p_hi)
    if hi <= lo + 1e-8:
        return np.clip(x, 0, 1)
    y = (x - lo) / (hi - lo)
    return np.clip(y, 0, 1)

def preprocess_sem(img_gray: np.ndarray, cfg: CFG) -> np.ndarray:
    x = crop_valid(img_gray, cfg).astype(np.float32) / 255.0
    if cfg.use_clahe:
        x = apply_clahe(x, cfg)
    bg = estimate_background(x, cfg)
    x = x - bg
    x = robust_rescale(x, cfg)
    return x.astype(np.float32)

def extract_red_boundary_mask(label_bgr: np.ndarray, cfg: CFG) -> np.ndarray:
    lab = crop_valid(label_bgr, cfg)
    B, G, R = lab[..., 0], lab[..., 1], lab[..., 2]
    m = (R > cfg.r_thr) & (G < cfg.g_thr) & (B < cfg.b_thr)
    return m.astype(np.uint8)

def dilate(mask01: np.ndarray, radius: int) -> np.ndarray:
    if radius <= 0:
        return mask01
    k = 2 * radius + 1
    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (k, k))
    m = (mask01 > 0).astype(np.uint8) * 255
    d = cv2.dilate(m, kernel)
    return (d > 0).astype(np.uint8)

def compute_offsets(L: int, patch: int, stride: int) -> List[int]:
    max0 = L - patch
    if max0 < 0:
        return [0]
    offs = list(range(0, max0 + 1, stride))
    if offs[-1] != max0:
        offs.append(max0)  # ensure full coverage
    return offs

def rot_k(a: np.ndarray, k: int) -> np.ndarray:
    return np.rot90(a, k).copy()

def overlay_boundary(gray01: np.ndarray, gt01: np.ndarray, pred01: np.ndarray) -> np.ndarray:
    """
    gray01: [0,1] HxW
    gt01: 0/1 HxW
    pred01: 0/1 HxW
    overlay: GT red, Pred green
    """
    g = np.clip(gray01 * 255.0, 0, 255).astype(np.uint8)
    rgb = cv2.cvtColor(g, cv2.COLOR_GRAY2BGR)
    # GT red
    rgb[gt01 > 0] = (0, 0, 255)
    # Pred green (if overlaps GT -> yellow-ish)
    rgb[pred01 > 0] = (0, 255, 0)
    return rgb


# =========================
# Metrics & Loss
# =========================
def soft_dice_loss(logits: torch.Tensor, target: torch.Tensor, eps: float = 1e-6) -> torch.Tensor:
    prob = torch.sigmoid(logits)
    num = 2.0 * (prob * target).sum(dim=(2,3))
    den = (prob * prob).sum(dim=(2,3)) + (target * target).sum(dim=(2,3)) + eps
    return (1.0 - (num / den)).mean()

def bce_dice_loss(logits: torch.Tensor, target: torch.Tensor) -> Tuple[torch.Tensor, float, float]:
    bce = F.binary_cross_entropy_with_logits(logits, target)
    d = soft_dice_loss(logits, target)
    return bce + d, float(bce.item()), float(d.item())

@torch.no_grad()
def dice_binary(pred: torch.Tensor, gt: torch.Tensor, eps: float = 1e-6) -> float:
    pred = pred.float()
    gt = gt.float()
    inter = (pred * gt).sum(dim=(2,3))
    den = pred.sum(dim=(2,3)) + gt.sum(dim=(2,3)) + eps
    return float(((2.0 * inter) / den).mean().item())


# =========================
# Model: ResUNet + GroupNorm
# =========================
class ResBlockGN(nn.Module):
    def __init__(self, in_ch, out_ch, groups: int):
        super().__init__()
        self.c1 = nn.Conv2d(in_ch, out_ch, 3, padding=1, bias=False)
        self.g1 = nn.GroupNorm(num_groups=min(groups, out_ch), num_channels=out_ch)
        self.c2 = nn.Conv2d(out_ch, out_ch, 3, padding=1, bias=False)
        self.g2 = nn.GroupNorm(num_groups=min(groups, out_ch), num_channels=out_ch)
        self.act = nn.SiLU(inplace=True)
        self.skip = None
        if in_ch != out_ch:
            self.skip = nn.Conv2d(in_ch, out_ch, 1, bias=False)

    def forward(self, x):
        h = self.act(self.g1(self.c1(x)))
        h = self.g2(self.c2(h))
        s = x if self.skip is None else self.skip(x)
        return self.act(h + s)

class Down(nn.Module):
    def __init__(self, in_ch, out_ch, groups: int):
        super().__init__()
        self.pool = nn.MaxPool2d(2)
        self.block = ResBlockGN(in_ch, out_ch, groups)
    def forward(self, x):
        return self.block(self.pool(x))

class Up(nn.Module):
    def __init__(self, in_ch, out_ch, groups: int):
        super().__init__()
        self.up = nn.Upsample(scale_factor=2, mode="bilinear", align_corners=False)
        self.block = ResBlockGN(in_ch, out_ch, groups)
    def forward(self, x1, x2):
        x1 = self.up(x1)
        dy = x2.size(2) - x1.size(2)
        dx = x2.size(3) - x1.size(3)
        x1 = F.pad(x1, [dx//2, dx-dx//2, dy//2, dy-dy//2])
        return self.block(torch.cat([x2, x1], dim=1))

class ResUNetGN(nn.Module):
    def __init__(self, in_channels=1, out_channels=1, base=64, groups=16):
        super().__init__()
        self.inc = ResBlockGN(in_channels, base, groups)
        self.d1 = Down(base, base*2, groups)
        self.d2 = Down(base*2, base*4, groups)
        self.d3 = Down(base*4, base*8, groups)
        self.d4 = Down(base*8, base*16, groups)
        self.u1 = Up(base*16 + base*8, base*8, groups)
        self.u2 = Up(base*8 + base*4, base*4, groups)
        self.u3 = Up(base*4 + base*2, base*2, groups)
        self.u4 = Up(base*2 + base, base, groups)
        self.outc = nn.Conv2d(base, out_channels, 1)

    def forward(self, x):
        x1 = self.inc(x)
        x2 = self.d1(x1)
        x3 = self.d2(x2)
        x4 = self.d3(x3)
        x5 = self.d4(x4)
        x = self.u1(x5, x4)
        x = self.u2(x, x3)
        x = self.u3(x, x2)
        x = self.u4(x, x1)
        return self.outc(x)  # logits


# =========================
# Dataset (patches + rotations)
# =========================
class PatchDS(torch.utils.data.Dataset):
    def __init__(self, jpg_paths: List[str], cfg: CFG):
        self.cfg = cfg
        self.jpg_paths = jpg_paths

        self.cache: Dict[str, Dict[str, np.ndarray]] = {}
        self.index: List[Tuple[int, int, int, int]] = []  # (i, y, x, rot)

        for i, p in enumerate(jpg_paths):
            img = preprocess_sem(read_gray(p), cfg)  # HxW
            ys = compute_offsets(img.shape[0], cfg.patch, cfg.stride)
            xs = compute_offsets(img.shape[1], cfg.patch, cfg.stride)
            for y in ys:
                for x in xs:
                    for rk in (0,1,2,3):
                        self.index.append((i, y, x, rk))

    def __len__(self): return len(self.index)

    def _load(self, i: int):
        p = self.jpg_paths[i]
        if p in self.cache:
            return
        cfg = self.cfg
        img = preprocess_sem(read_gray(p), cfg)
        lp = os.path.splitext(p)[0] + ".png"
        lab = read_label_bgr(lp)
        m = extract_red_boundary_mask(lab, cfg)        # thin GT
        m_u1 = dilate(m, cfg.dilate_radius)           # coarse target
        self.cache[p] = {"img": img, "m": m, "m_u1": m_u1}

    def __getitem__(self, idx: int):
        i, y, x, rk = self.index[idx]
        self._load(i)
        p = self.jpg_paths[i]
        c = self.cache[p]
        img = c["img"][y:y+self.cfg.patch, x:x+self.cfg.patch]
        m_u1 = c["m_u1"][y:y+self.cfg.patch, x:x+self.cfg.patch]

        img = rot_k(img, rk)
        m_u1 = rot_k(m_u1, rk)

        I = torch.from_numpy(img[None].astype(np.float32))
        Y = torch.from_numpy(m_u1[None].astype(np.float32))
        return I, Y


# =========================
# Eval + cache export
# =========================
@torch.no_grad()
def eval_unet1(model: nn.Module, loader, cfg: CFG) -> Dict[str, float]:
    model.eval()
    losses, dices = [], []
    for I, Y in loader:
        I = I.to(cfg.device)
        Y = Y.to(cfg.device)
        logits = model(I)
        loss, _, _ = bce_dice_loss(logits, Y)
        losses.append(float(loss.item()))
        pred = (torch.sigmoid(logits) > cfg.thr_vis).float()
        dices.append(dice_binary(pred, (Y > 0.5).float()))
    return {"loss": float(np.mean(losses)), "dice": float(np.mean(dices))}

@torch.no_grad()
def export_u1_logits(model: nn.Module, jpg_paths: List[str], out_dir: str, cfg: CFG):
    os.makedirs(out_dir, exist_ok=True)
    model.eval()
    for p in jpg_paths:
        img = preprocess_sem(read_gray(p), cfg)
        I = torch.from_numpy(img[None,None].astype(np.float32)).to(cfg.device)
        logits = model(I)[0,0].detach().cpu().numpy().astype(np.float16)

        base = os.path.splitext(os.path.basename(p))[0]
        np.savez_compressed(os.path.join(out_dir, f"{base}.npz"),
                            logits=logits,
                            shape=np.array([cfg.crop_h, cfg.crop_w], dtype=np.int32))

def sha1_of_file(path: str) -> str:
    h = hashlib.sha1()
    with open(path, "rb") as f:
        while True:
            b = f.read(1024*1024)
            if not b:
                break
            h.update(b)
    return h.hexdigest()


@torch.no_grad()
def save_val_overlays(model: nn.Module, val_paths: List[str], out_dir: str, cfg: CFG):
    os.makedirs(out_dir, exist_ok=True)
    model.eval()
    for p in val_paths:
        img01 = preprocess_sem(read_gray(p), cfg)
        lab = read_label_bgr(os.path.splitext(p)[0] + ".png")
        gt = extract_red_boundary_mask(lab, cfg).astype(np.uint8)
        I = torch.from_numpy(img01[None,None].astype(np.float32)).to(cfg.device)
        logits = model(I)[0,0].detach().cpu()
        pred = (torch.sigmoid(logits) > cfg.thr_vis).numpy().astype(np.uint8)
        ov = overlay_boundary(img01, gt, pred)
        base = os.path.splitext(os.path.basename(p))[0]
        cv2.imwrite(os.path.join(out_dir, f"{base}_U1_overlay.png"), ov)


# =========================
# Plotting
# =========================
def plot_curves(df_tr: pd.DataFrame, df_va: pd.DataFrame, out_png: str, title: str):
    plt.figure()
    plt.plot(df_tr["epoch"], df_tr["loss"], label="train loss")
    plt.plot(df_va["epoch"], df_va["loss"], label="val loss")
    plt.plot(df_va["epoch"], df_va["dice"], label="val dice")
    plt.xlabel("epoch"); plt.legend(); plt.title(title); plt.tight_layout()
    plt.savefig(out_png, dpi=200)
    plt.close()

def plot_scatter_loss_vs_metric(df_tr: pd.DataFrame, df_va: pd.DataFrame, out_png: str, xcol="loss", ycol="dice"):
    plt.figure()
    plt.scatter(df_tr[xcol], df_va[ycol])
    plt.xlabel(f"train {xcol}"); plt.ylabel(f"val {ycol}")
    plt.tight_layout()
    plt.savefig(out_png, dpi=200)
    plt.close()


# =========================
# Main
# =========================
def main():
    cfg = CFG()
    set_seed(cfg.seed)

    run_dir = os.path.join("runs", cfg.exp_name)
    ckpt_dir = os.path.join(run_dir, "checkpoints")
    cache_dir = os.path.join(run_dir, "cache_u1_logits")
    os.makedirs(ckpt_dir, exist_ok=True)
    os.makedirs(run_dir, exist_ok=True)

    # --- split: sorted, last 2 as val
    train_folder = os.path.join(cfg.root, cfg.train_dir)
    test_folder = os.path.join(cfg.root, cfg.test_dir)
    train_all = list_jpgs(train_folder)
    test_all = list_jpgs(test_folder)

    assert len(train_all) >= 3, "Need at least 3 training images to hold out 2 for val."
    train_paths = train_all[:-2]
    val_paths = train_all[-2:]

    split = {
        "train_files": [os.path.basename(p) for p in train_paths],
        "val_files": [os.path.basename(p) for p in val_paths],
        "test_files": [os.path.basename(p) for p in test_all],
    }
    with open(os.path.join(run_dir, "split.json"), "w", encoding="utf-8") as f:
        json.dump(split, f, indent=2)

    with open(os.path.join(run_dir, "config.json"), "w", encoding="utf-8") as f:
        json.dump(asdict(cfg), f, indent=2)

    # data
    g = torch.Generator()
    g.manual_seed(cfg.seed)

    ds_tr = PatchDS(train_paths, cfg)
    ds_va = PatchDS(val_paths, cfg)  # val도 patch/rotation 동일하게 평가(이미지 독립성은 이미지 단위로 확보됨)
    dl_tr = torch.utils.data.DataLoader(ds_tr, batch_size=cfg.batch, shuffle=True, num_workers=4,
                                        pin_memory=True, worker_init_fn=worker_init_fn, generator=g)
    dl_va = torch.utils.data.DataLoader(ds_va, batch_size=cfg.batch, shuffle=False, num_workers=2,
                                        pin_memory=True, worker_init_fn=worker_init_fn, generator=g)

    print("Train images:", len(train_paths), "Val images:", len(val_paths), "Test images:", len(test_all))
    print("Train patches:", len(ds_tr), "Val patches:", len(ds_va))

    # model
    model = ResUNetGN(in_channels=1, out_channels=1, base=cfg.base, groups=cfg.gn_groups).to(cfg.device)
    opt = torch.optim.AdamW(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)

    best_dice = -1.0
    rows_tr, rows_va = [], []

    for epoch in range(1, cfg.epochs + 1):
        model.train()
        losses, bces, dices = [], [], []
        for I, Y in dl_tr:
            I = I.to(cfg.device)
            Y = Y.to(cfg.device)
            opt.zero_grad(set_to_none=True)
            logits = model(I)
            loss, bce_v, dice_v = bce_dice_loss(logits, Y)
            loss.backward()
            opt.step()
            losses.append(float(loss.item()))
            bces.append(bce_v)
            dices.append(dice_v)

        tr = {"epoch": epoch, "loss": float(np.mean(losses)), "bce": float(np.mean(bces)), "dice_loss": float(np.mean(dices))}
        va = eval_unet1(model, dl_va, cfg)
        va["epoch"] = epoch
        rows_tr.append(tr)
        rows_va.append(va)

        # save last
        last_path = os.path.join(ckpt_dir, "unet1_last.pth")
        torch.save({
            "model": model.state_dict(),
            "cfg": asdict(cfg),
            "epoch": epoch,
            "val_metric": va,
            "seed": cfg.seed,
        }, last_path)

        # save best by val dice
        if va["dice"] > best_dice:
            best_dice = va["dice"]
            best_path = os.path.join(ckpt_dir, "unet1_best.pth")
            torch.save({
                "model": model.state_dict(),
                "cfg": asdict(cfg),
                "epoch": epoch,
                "val_metric": va,
                "seed": cfg.seed,
            }, best_path)

        print(f"[U1][{epoch:03d}] train_loss={tr['loss']:.4f} val_loss={va['loss']:.4f} val_dice={va['dice']:.4f}")

    # save logs
    df_tr = pd.DataFrame(rows_tr)
    df_va = pd.DataFrame(rows_va)
    df_tr.to_csv(os.path.join(run_dir, "unet1_train.csv"), index=False)
    df_va.to_csv(os.path.join(run_dir, "unet1_val.csv"), index=False)

    plot_curves(df_tr, df_va, os.path.join(run_dir, "unet1_curves.png"), "U-Net1")
    plot_scatter_loss_vs_metric(df_tr, df_va, os.path.join(run_dir, "unet1_scatter_loss_vs_valdice.png"))

    # --- auto export cache using BEST checkpoint (for strict reproducibility)
    best_ckpt_path = os.path.join(ckpt_dir, "unet1_best.pth")
    ckpt = torch.load(best_ckpt_path, map_location=cfg.device)
    model.load_state_dict(ckpt["model"])
    model.eval()

    meta = {
        "created_from_checkpoint": os.path.basename(best_ckpt_path),
        "checkpoint_sha1": sha1_of_file(best_ckpt_path),
        "split": split,
        "config": asdict(cfg),
    }
    with open(os.path.join(cache_dir, "u1_cache_meta.json"), "w", encoding="utf-8") as f:
        json.dump(meta, f, indent=2)

    export_u1_logits(model, train_paths, os.path.join(cache_dir, "train"), cfg)
    export_u1_logits(model, val_paths, os.path.join(cache_dir, "val"), cfg)
    export_u1_logits(model, test_all,  os.path.join(cache_dir, "test"), cfg)

    # overlays on val for qualitative check
    save_val_overlays(model, val_paths, os.path.join(run_dir, "overlays_u1_val"), cfg)

    print("U-Net1 done.")
    print("Artifacts saved to:", run_dir)
    print("U1 logits cache saved to:", cache_dir)


if __name__ == "__main__":
    main()


import os, json
import torch
import numpy as np

def export_cache_only():
    cfg = CFG()  # train_unet1.py에 정의된 CFG 그대로 사용
    run_dir = os.path.join("runs", cfg.exp_name)
    ckpt_dir = os.path.join(run_dir, "checkpoints")
    cache_dir = os.path.join(run_dir, "cache_u1_logits")
    os.makedirs(cache_dir, exist_ok=True)

    # load split/config
    with open(os.path.join(run_dir, "split.json"), "r", encoding="utf-8") as f:
        split = json.load(f)

    train_folder = os.path.join(cfg.root, cfg.train_dir)
    test_folder  = os.path.join(cfg.root, cfg.test_dir)

    train_paths = [os.path.join(train_folder, fn) for fn in split["train_files"]]
    val_paths   = [os.path.join(train_folder, fn) for fn in split["val_files"]]
    test_paths  = [os.path.join(test_folder,  fn) for fn in split["test_files"]]

    best_ckpt_path = os.path.join(ckpt_dir, "unet1_best.pth")
    ckpt = torch.load(best_ckpt_path, map_location=cfg.device)

    model = ResUNetGN(in_channels=1, out_channels=1, base=cfg.base, groups=cfg.gn_groups).to(cfg.device)
    model.load_state_dict(ckpt["model"])
    model.eval()

    meta = {
        "created_from_checkpoint": "unet1_best.pth",
        "split": split,
        "config": asdict(cfg),
    }
    with open(os.path.join(cache_dir, "u1_cache_meta.json"), "w", encoding="utf-8") as f:
        json.dump(meta, f, indent=2)

    export_u1_logits(model, train_paths, os.path.join(cache_dir, "train"), cfg)
    export_u1_logits(model, val_paths,   os.path.join(cache_dir, "val"), cfg)
    export_u1_logits(model, test_paths,  os.path.join(cache_dir, "test"), cfg)

    print("Cache export done:", cache_dir)

if __name__ == "__main__":
    export_cache_only()