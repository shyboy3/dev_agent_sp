뉴 트레인 유넷1

import os, json, math, random, shutil
from dataclasses import dataclass, asdict
from datetime import datetime
from typing import List, Tuple, Dict

import cv2
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.nn.functional as F
import matplotlib.pyplot as plt


# =========================
# Config (edit here)
# =========================
@dataclass
class CFG:
    # dataset root
    root: str = "./DATA"
    train_dir: str = "TRAIN_SEM_IMAGE"
    valid_dir: str = "VALID_SEM_IMAGE"
    test_dir: str = "TEST_SEM_IMAGE"
    train2_dir: str = "TRAIN_2_SEM_IMAGE"

    # SEM valid area crop (bottom meta remove)
    crop_h: int = 896
    crop_w: int = 1280

    # preprocessing
    use_clahe: bool = True
    clahe_clip: float = 2.0
    clahe_tile: int = 8
    bg_kernel: int = 49  # ~ grain size (48) conservative

    robust_p_lo: float = 1.0
    robust_p_hi: float = 99.0

    # patching / augmentation
    patch: int = 384
    stride: int = 48
    rot_aug: bool = True  # 0/90/180/270

    # label extraction (red line)
    r_thr: int = 150
    g_thr: int = 120
    b_thr: int = 120

    # training
    device: str = "cuda" if torch.cuda.is_available() else "cpu"
    seed: int = 42
    batch: int = 4
    epochs: int = 30
    lr: float = 1e-3
    weight_decay: float = 1e-5

    # model
    base: int = 64  # 너가 예전에 쓰던 64 depth 반영
    gn_groups: int = 16

    # dataloader (Windows 안정)
    num_workers: int = 0

    # output
    exp_name: str = "exp_unet1"

    # metrics
    thr_vis: float = 0.5
    sweep_thr_min: float = 0.05
    sweep_thr_max: float = 0.95
    sweep_thr_steps: int = 19

    # exports
    export_all_images: bool = True  # val/test 모두 저장
    export_max_images: int = 9999


# =========================
# Repro
# =========================
def set_seed(seed: int):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


# =========================
# IO / utils
# =========================
def ensure_dir(p: str):
    os.makedirs(p, exist_ok=True)

def list_jpg(folder: str) -> List[str]:
    files = [f for f in os.listdir(folder) if f.lower().endswith(".jpg")]
    files.sort()
    return [os.path.join(folder, f) for f in files]

def read_gray(path: str) -> np.ndarray:
    im = cv2.imread(path, cv2.IMREAD_GRAYSCALE)
    if im is None:
        raise FileNotFoundError(path)
    return im

def read_bgr(path: str) -> np.ndarray:
    im = cv2.imread(path, cv2.IMREAD_COLOR)
    if im is None:
        raise FileNotFoundError(path)
    return im

def crop_valid(img: np.ndarray, H: int, W: int) -> np.ndarray:
    return img[:H, :W]

def robust_rescale(x: np.ndarray, p_lo: float, p_hi: float) -> np.ndarray:
    lo = np.percentile(x, p_lo)
    hi = np.percentile(x, p_hi)
    if hi <= lo + 1e-8:
        return np.clip(x, 0, 1)
    y = (x - lo) / (hi - lo)
    return np.clip(y, 0, 1)

def apply_clahe(img01: np.ndarray, clip: float, tile: int) -> np.ndarray:
    img8 = np.clip(img01 * 255.0, 0, 255).astype(np.uint8)
    clahe = cv2.createCLAHE(clipLimit=clip, tileGridSize=(tile, tile))
    out = clahe.apply(img8).astype(np.float32) / 255.0
    return out

def estimate_background(img01: np.ndarray, k: int) -> np.ndarray:
    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (k, k))
    img8 = np.clip(img01 * 255.0, 0, 255).astype(np.uint8)
    bg8 = cv2.morphologyEx(img8, cv2.MORPH_OPEN, kernel)
    return bg8.astype(np.float32) / 255.0

def preprocess_sem(gray: np.ndarray, cfg: CFG) -> np.ndarray:
    x = crop_valid(gray, cfg.crop_h, cfg.crop_w).astype(np.float32) / 255.0
    if cfg.use_clahe:
        x = apply_clahe(x, cfg.clahe_clip, cfg.clahe_tile)
    bg = estimate_background(x, cfg.bg_kernel)
    x = x - bg
    x = robust_rescale(x, cfg.robust_p_lo, cfg.robust_p_hi)
    return x.astype(np.float32)

def extract_red_mask(label_bgr: np.ndarray, cfg: CFG) -> np.ndarray:
    lab = crop_valid(label_bgr, cfg.crop_h, cfg.crop_w)
    B, G, R = lab[...,0], lab[...,1], lab[...,2]
    m = (R > cfg.r_thr) & (G < cfg.g_thr) & (B < cfg.b_thr)
    return m.astype(np.uint8)

def compute_offsets(L: int, patch: int, stride: int) -> List[int]:
    max0 = L - patch
    if max0 < 0:
        return [0]
    offs = list(range(0, max0 + 1, stride))
    if offs[-1] != max0:
        offs.append(max0)
    return offs

def rot_k(a: np.ndarray, k: int) -> np.ndarray:
    return np.rot90(a, k).copy()

def to_u8(x01: np.ndarray) -> np.ndarray:
    return np.clip(x01 * 255.0, 0, 255).astype(np.uint8)

def overlay(gray01: np.ndarray, gt01: np.ndarray, pred01: np.ndarray) -> np.ndarray:
    g = to_u8(gray01)
    rgb = cv2.cvtColor(g, cv2.COLOR_GRAY2BGR)
    rgb[gt01 > 0] = (0,0,255)
    rgb[pred01 > 0] = (0,255,0)
    return rgb


# =========================
# Metrics
# =========================
def soft_dice_score_np(prob: np.ndarray, gt: np.ndarray, eps: float = 1e-6) -> float:
    prob = prob.astype(np.float64)
    gt = gt.astype(np.float64)
    num = 2.0 * np.sum(prob * gt)
    den = np.sum(prob) + np.sum(gt) + eps
    return float(num / den)

def average_precision_np(y_true: np.ndarray, y_score: np.ndarray, eps: float = 1e-12) -> float:
    y_true = y_true.astype(np.uint8)
    y_score = y_score.astype(np.float64)
    pos = int(y_true.sum())
    if pos == 0:
        return 0.0
    order = np.argsort(-y_score)
    y_true_sorted = y_true[order]
    tp = np.cumsum(y_true_sorted == 1)
    fp = np.cumsum(y_true_sorted == 0)
    precision = tp / np.maximum(tp + fp, 1)
    ap = float(np.sum(precision[y_true_sorted == 1]) / (pos + eps))
    return ap

def sweep_best_f1(y_true: np.ndarray, y_score: np.ndarray, thresholds: np.ndarray, eps: float = 1e-12) -> Tuple[float, float]:
    y_true = y_true.astype(np.uint8)
    y_score = y_score.astype(np.float64)
    pos = y_true.sum()
    if pos == 0:
        return 0.0, float(thresholds[0])

    best_f1, best_thr = 0.0, float(thresholds[0])
    for thr in thresholds:
        y_pred = (y_score >= thr).astype(np.uint8)
        tp = int((y_pred & y_true).sum())
        fp = int((y_pred & (1 - y_true)).sum())
        fn = int(((1 - y_pred) & y_true).sum())
        precision = tp / (tp + fp + eps)
        recall = tp / (tp + fn + eps)
        f1 = 2.0 * precision * recall / (precision + recall + eps)
        if f1 > best_f1:
            best_f1, best_thr = float(f1), float(thr)
    return best_f1, best_thr


# =========================
# Model: ResUNet (GN)
# =========================
class ResBlock(nn.Module):
    def __init__(self, in_ch, out_ch, groups=16):
        super().__init__()
        self.c1 = nn.Conv2d(in_ch, out_ch, 3, padding=1, bias=False)
        self.g1 = nn.GroupNorm(num_groups=min(groups, out_ch), num_channels=out_ch)
        self.c2 = nn.Conv2d(out_ch, out_ch, 3, padding=1, bias=False)
        self.g2 = nn.GroupNorm(num_groups=min(groups, out_ch), num_channels=out_ch)
        self.act = nn.SiLU(inplace=True)
        self.skip = None
        if in_ch != out_ch:
            self.skip = nn.Conv2d(in_ch, out_ch, 1, bias=False)

    def forward(self, x):
        h = self.act(self.g1(self.c1(x)))
        h = self.g2(self.c2(h))
        s = x if self.skip is None else self.skip(x)
        return self.act(h + s)

class Down(nn.Module):
    def __init__(self, in_ch, out_ch, groups=16):
        super().__init__()
        self.pool = nn.MaxPool2d(2)
        self.block = ResBlock(in_ch, out_ch, groups)
    def forward(self, x):
        return self.block(self.pool(x))

class Up(nn.Module):
    def __init__(self, in_ch, out_ch, groups=16):
        super().__init__()
        self.up = nn.Upsample(scale_factor=2, mode="bilinear", align_corners=False)
        self.block = ResBlock(in_ch, out_ch, groups)
    def forward(self, x1, x2):
        x1 = self.up(x1)
        dy = x2.size(2) - x1.size(2)
        dx = x2.size(3) - x1.size(3)
        x1 = F.pad(x1, [dx//2, dx-dx//2, dy//2, dy-dy//2])
        return self.block(torch.cat([x2, x1], dim=1))

class ResUNet(nn.Module):
    def __init__(self, in_ch=1, out_ch=1, base=64, groups=16):
        super().__init__()
        self.inc = ResBlock(in_ch, base, groups)
        self.d1 = Down(base, base*2, groups)
        self.d2 = Down(base*2, base*4, groups)
        self.d3 = Down(base*4, base*8, groups)
        self.d4 = Down(base*8, base*16, groups)
        self.u1 = Up(base*16 + base*8, base*8, groups)
        self.u2 = Up(base*8 + base*4, base*4, groups)
        self.u3 = Up(base*4 + base*2, base*2, groups)
        self.u4 = Up(base*2 + base, base, groups)
        self.outc = nn.Conv2d(base, out_ch, 1)

    def forward(self, x):
        x1 = self.inc(x)
        x2 = self.d1(x1)
        x3 = self.d2(x2)
        x4 = self.d3(x3)
        x5 = self.d4(x4)
        x = self.u1(x5, x4)
        x = self.u2(x, x3)
        x = self.u3(x, x2)
        x = self.u4(x, x1)
        return self.outc(x)  # logits


# =========================
# Dataset (patches)
# =========================
class PatchDS(torch.utils.data.Dataset):
    def __init__(self, jpg_paths: List[str], cfg: CFG):
        self.jpg_paths = jpg_paths
        self.cfg = cfg
        self.cache: Dict[str, Dict[str, np.ndarray]] = {}
        self.index: List[Tuple[int,int,int,int]] = []

        for i, p in enumerate(jpg_paths):
            img = preprocess_sem(read_gray(p), cfg)
            ys = compute_offsets(img.shape[0], cfg.patch, cfg.stride)
            xs = compute_offsets(img.shape[1], cfg.patch, cfg.stride)
            for y in ys:
                for x in xs:
                    rots = (0,1,2,3) if cfg.rot_aug else (0,)
                    for rk in rots:
                        self.index.append((i,y,x,rk))

    def __len__(self): return len(self.index)

    def _load(self, i: int):
        p = self.jpg_paths[i]
        if p in self.cache:
            return
        img01 = preprocess_sem(read_gray(p), self.cfg)
        lab = read_bgr(os.path.splitext(p)[0] + ".png")
        gt = extract_red_mask(lab, self.cfg)
        self.cache[p] = {"img": img01, "gt": gt}

    def __getitem__(self, idx: int):
        i,y,x,rk = self.index[idx]
        self._load(i)
        p = self.jpg_paths[i]
        c = self.cache[p]

        img = c["img"][y:y+self.cfg.patch, x:x+self.cfg.patch]
        gt  = c["gt"][y:y+self.cfg.patch, x:x+self.cfg.patch]

        img = rot_k(img, rk)
        gt  = rot_k(gt, rk)

        I = torch.from_numpy(img[None].astype(np.float32))
        Y = torch.from_numpy(gt[None].astype(np.float32))
        return I, Y


# =========================
# Loss
# =========================
def bce_dice_loss(logits: torch.Tensor, target: torch.Tensor) -> torch.Tensor:
    bce = F.binary_cross_entropy_with_logits(logits, target)
    prob = torch.sigmoid(logits)
    num = 2.0 * (prob * target).sum(dim=(2,3))
    den = (prob * prob).sum(dim=(2,3)) + (target * target).sum(dim=(2,3)) + 1e-6
    dice_loss = (1.0 - (num / den)).mean()
    return bce + dice_loss


# =========================
# Full-image inference + exports
# =========================
@torch.no_grad()
def infer_full_logits(model: nn.Module, img01: np.ndarray, device: str) -> np.ndarray:
    t = torch.from_numpy(img01[None,None].astype(np.float32)).to(device)
    logits = model(t)[0,0].detach().cpu().numpy().astype(np.float32)
    return logits

@torch.no_grad()
def eval_and_collect(model: nn.Module, jpg_paths: List[str], cfg: CFG, device: str) -> Dict[str, float]:
    model.eval()
    all_prob = []
    all_gt = []
    losses = []

    for p in jpg_paths:
        img01 = preprocess_sem(read_gray(p), cfg)
        lab = read_bgr(os.path.splitext(p)[0] + ".png")
        gt = extract_red_mask(lab, cfg).astype(np.uint8)

        logits = infer_full_logits(model, img01, device)
        prob = 1.0 / (1.0 + np.exp(-logits))

        # loss on full image (approx, for monitoring)
        lt = torch.from_numpy(logits[None,None]).to(device)
        yt = torch.from_numpy(gt[None,None].astype(np.float32)).to(device)
        losses.append(float(bce_dice_loss(lt, yt).item()))

        all_prob.append(prob.reshape(-1).astype(np.float32))
        all_gt.append(gt.reshape(-1).astype(np.uint8))

    prob = np.concatenate(all_prob, 0)
    gt = np.concatenate(all_gt, 0)

    thresholds = np.linspace(cfg.sweep_thr_min, cfg.sweep_thr_max, cfg.sweep_thr_steps)
    ap = average_precision_np(gt, prob)
    softdice = soft_dice_score_np(prob, gt)
    best_f1, best_thr = sweep_best_f1(gt, prob, thresholds)

    pred05 = (prob >= cfg.thr_vis).astype(np.uint8)
    inter = 2.0 * float(np.sum(pred05 * gt))
    den = float(np.sum(pred05) + np.sum(gt) + 1e-6)
    dice05 = inter / den

    return {
        "loss": float(np.mean(losses)),
        "ap": float(ap),
        "softdice": float(softdice),
        "dice05": float(dice05),
        "best_f1": float(best_f1),
        "best_thr": float(best_thr),
    }

@torch.no_grad()
def save_paper_outputs(model: nn.Module, jpg_paths: List[str], out_dir: str, cfg: CFG, device: str, tag: str):
    ensure_dir(out_dir)
    model.eval()
    for p in jpg_paths[:cfg.export_max_images]:
        base = os.path.splitext(os.path.basename(p))[0]
        img01 = preprocess_sem(read_gray(p), cfg)
        lab = read_bgr(os.path.splitext(p)[0] + ".png")
        gt = extract_red_mask(lab, cfg).astype(np.uint8)

        logits = infer_full_logits(model, img01, device)
        prob = 1.0 / (1.0 + np.exp(-logits))
        pred = (prob >= cfg.thr_vis).astype(np.uint8)
        ov = overlay(img01, gt, pred)

        cv2.imwrite(os.path.join(out_dir, f"{base}_{tag}_I.png"), to_u8(img01))
        cv2.imwrite(os.path.join(out_dir, f"{base}_{tag}_GT.png"), (gt*255).astype(np.uint8))
        cv2.imwrite(os.path.join(out_dir, f"{base}_{tag}_P.png"), (pred*255).astype(np.uint8))
        cv2.imwrite(os.path.join(out_dir, f"{base}_{tag}_overlay.png"), ov)
        cv2.imwrite(os.path.join(out_dir, f"{base}_{tag}_prob.png"), to_u8(prob))

def plot_curves(df_tr: pd.DataFrame, df_va: pd.DataFrame, out_png: str, title: str):
    plt.figure()
    plt.plot(df_tr["epoch"], df_tr["train_loss"], label="train loss")
    plt.plot(df_va["epoch"], df_va["val_loss"], label="val loss")
    plt.plot(df_va["epoch"], df_va["val_ap"], label="val AP")
    plt.plot(df_va["epoch"], df_va["val_softdice"], label="val soft-dice")
    plt.xlabel("epoch"); plt.legend(); plt.title(title); plt.tight_layout()
    plt.savefig(out_png, dpi=200)
    plt.close()

def plot_scatter(df_tr: pd.DataFrame, df_va: pd.DataFrame, out_png: str):
    plt.figure()
    plt.scatter(df_tr["train_loss"], df_va["val_ap"])
    plt.xlabel("train loss"); plt.ylabel("val AP")
    plt.tight_layout()
    plt.savefig(out_png, dpi=200)
    plt.close()


# =========================
# Create TRAIN_2 (Pt png + npz)
# =========================
@torch.no_grad()
def create_train2_from_valid(model: nn.Module, valid_jpgs: List[str], cfg: CFG, device: str, train2_dir_abs: str):
    ensure_dir(train2_dir_abs)
    model.eval()

    for p in valid_jpgs:
        base = os.path.splitext(os.path.basename(p))[0]
        gt_path = os.path.splitext(p)[0] + ".png"

        # copy sem + gt into TRAIN_2
        shutil.copy2(p, os.path.join(train2_dir_abs, f"{base}.jpg"))
        shutil.copy2(gt_path, os.path.join(train2_dir_abs, f"{base}.png"))

        # infer
        img01 = preprocess_sem(read_gray(p), cfg)
        logits = infer_full_logits(model, img01, device)
        pt = 1.0 / (1.0 + np.exp(-logits))  # Pt=P0

        # save pt.png (8-bit)
        pt_png = (np.clip(pt, 0, 1) * 255.0).astype(np.uint8)
        cv2.imwrite(os.path.join(train2_dir_abs, f"{base}_pt.png"), pt_png)

        # save pt.npz (float16 prob + logits)
        np.savez_compressed(
            os.path.join(train2_dir_abs, f"{base}_pt.npz"),
            pt=pt.astype(np.float16),
            logits=logits.astype(np.float16),
            meta=np.array([cfg.crop_h, cfg.crop_w], dtype=np.int32),
        )


# =========================
# Main
# =========================
def main():
    cfg = CFG()
    set_seed(cfg.seed)

    train_folder = os.path.join(cfg.root, cfg.train_dir)
    valid_folder = os.path.join(cfg.root, cfg.valid_dir)
    test_folder  = os.path.join(cfg.root, cfg.test_dir)
    train2_folder = os.path.join(cfg.root, cfg.train2_dir)

    train_jpgs = list_jpg(train_folder)
    valid_jpgs = list_jpg(valid_folder)
    test_jpgs  = list_jpg(test_folder)

    assert len(train_jpgs) > 0 and len(valid_jpgs) > 0 and len(test_jpgs) > 0, "Empty dataset folder(s)."

    # ---- run dir (timestamp)
    stamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    run_dir = os.path.join("runs", cfg.exp_name, stamp)
    ckpt_dir = os.path.join(run_dir, "checkpoints")
    ensure_dir(run_dir); ensure_dir(ckpt_dir)

    # save config for reproducibility
    with open(os.path.join(run_dir, "config.json"), "w", encoding="utf-8") as f:
        json.dump(asdict(cfg), f, indent=2)

    print("[U1] Train:", len(train_jpgs), "Valid:", len(valid_jpgs), "Test:", len(test_jpgs))
    print("[U1] Run dir:", run_dir)

    ds_tr = PatchDS(train_jpgs, cfg)
    dl_tr = torch.utils.data.DataLoader(
        ds_tr, batch_size=cfg.batch, shuffle=True,
        num_workers=cfg.num_workers, pin_memory=True
    )

    model = ResUNet(in_ch=1, out_ch=1, base=cfg.base, groups=cfg.gn_groups).to(cfg.device)
    opt = torch.optim.AdamW(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)

    best_score = -1.0  # use VALID AP
    rows = []

    for epoch in range(1, cfg.epochs + 1):
        model.train()
        losses = []
        for I, Y in dl_tr:
            I = I.to(cfg.device)
            Y = Y.to(cfg.device)
            opt.zero_grad(set_to_none=True)
            logits = model(I)
            loss = bce_dice_loss(logits, Y)
            loss.backward()
            opt.step()
            losses.append(float(loss.item()))
        train_loss = float(np.mean(losses))

        # VALID evaluation (full images)
        val_m = eval_and_collect(model, valid_jpgs, cfg, cfg.device)
        row = {
            "epoch": epoch,
            "train_loss": train_loss,
            "val_loss": val_m["loss"],
            "val_ap": val_m["ap"],
            "val_softdice": val_m["softdice"],
            "val_dice05": val_m["dice05"],
            "val_bestF1": val_m["best_f1"],
            "val_bestThr": val_m["best_thr"],
        }
        rows.append(row)

        # save last
        torch.save({
            "model": model.state_dict(),
            "cfg": asdict(cfg),
            "epoch": epoch,
            "val_metric": val_m,
        }, os.path.join(ckpt_dir, "unet1_last.pth"))

        # save best by VALID AP
        if val_m["ap"] > best_score:
            best_score = val_m["ap"]
            torch.save({
                "model": model.state_dict(),
                "cfg": asdict(cfg),
                "epoch": epoch,
                "val_metric": val_m,
            }, os.path.join(ckpt_dir, "unet1_best.pth"))

            # best-time paper outputs (so even if crash later, results exist)
            save_paper_outputs(model, valid_jpgs, os.path.join(run_dir, "paper_u1_valid_best"), cfg, cfg.device, tag=f"U1_ep{epoch:03d}")
            save_paper_outputs(model, test_jpgs,  os.path.join(run_dir, "paper_u1_test_best"),  cfg, cfg.device, tag=f"U1_ep{epoch:03d}")

        print(
            f"[U1][{epoch:03d}] train_loss={train_loss:.4f} "
            f"val_loss={val_m['loss']:.4f} val_ap={val_m['ap']:.4f} "
            f"val_softdice={val_m['softdice']:.4f} val_dice05={val_m['dice05']:.4f} "
            f"val_bestF1={val_m['best_f1']:.4f}@{val_m['best_thr']:.2f}"
        )

    # ---- logs/plots
    df = pd.DataFrame(rows)
    df.to_csv(os.path.join(run_dir, "unet1_log.csv"), index=False)

    # separate df for plot convenience
    plot_curves(df[["epoch","train_loss"]], df.rename(columns={
        "val_loss":"val_loss","val_ap":"val_ap","val_softdice":"val_softdice"
    }), os.path.join(run_dir, "unet1_curves.png"), "U-Net1")
    plot_scatter(df[["train_loss"]], df.rename(columns={"val_ap":"val_ap"}), os.path.join(run_dir, "unet1_scatter_loss_vs_valAP.png"))

    # ---- load BEST and export final outputs + TRAIN_2 creation
    ckpt = torch.load(os.path.join(ckpt_dir, "unet1_best.pth"), map_location=cfg.device)
    model.load_state_dict(ckpt["model"])
    model.eval()

    save_paper_outputs(model, valid_jpgs, os.path.join(run_dir, "paper_u1_valid"), cfg, cfg.device, tag="U1")
    save_paper_outputs(model, test_jpgs,  os.path.join(run_dir, "paper_u1_test"),  cfg, cfg.device, tag="U1")

    # create TRAIN_2 from VALID using best U1
    create_train2_from_valid(model, valid_jpgs, cfg, cfg.device, train2_folder)

    print("[U1] Done.")
    print("[U1] Artifacts:", run_dir)
    print("[U1] TRAIN_2 created at:", train2_folder)


if __name__ == "__main__":
    main()



New Train U-net1

import os, glob, json, math, random, hashlib
from dataclasses import dataclass, asdict
from typing import List, Dict, Tuple

import numpy as np
import pandas as pd
import cv2
import torch
import torch.nn as nn
import torch.nn.functional as F
import matplotlib.pyplot as plt
from scipy.ndimage import distance_transform_edt


# =========================
# Config (edit here)
# =========================
@dataclass
class CFG:
    # data root
    root: str = "./DATA"
    train_dir: str = "TRAIN_SEM_IMAGE"
    test_dir: str = "TEST_SEM_IMAGE"

    # valid area crop (remove bottom metadata band)
    crop_h: int = 896
    crop_w: int = 1280

    # patching
    patch: int = 384
    stride: int = 48

    # preprocessing
    use_clahe: bool = True
    clahe_clip: float = 2.0
    clahe_tile: int = 8
    bg_kernel: int = 49  # ~48x48 grain scale

    robust_p_lo: float = 1.0
    robust_p_hi: float = 99.0

    # label extraction (red line)
    r_thr: int = 160
    g_thr: int = 140
    b_thr: int = 140

    # U1 target thickening (coarse anchor)
    dilate_radius: int = 2

    # training
    seed: int = 42
    device: str = "cuda" if torch.cuda.is_available() else "cpu"
    batch: int = 4
    epochs: int = 30
    lr: float = 1e-3
    weight_decay: float = 1e-5

    # model
    base: int = 64
    gn_groups: int = 16  # GroupNorm groups (batch=4 안정)

    # visualization / reference threshold
    thr_vis: float = 0.5

    # experiment
    exp_name: str = "exp_unet1"

    # debug export: preprocessed inputs/labels/patches
    export_debug_samples: bool = True
    export_debug_max_images: int = 12            # per split 저장할 이미지 수 상한
    export_debug_patches_per_image: int = 6      # 이미지당 저장할 패치 개수


# =========================
# Repro
# =========================
def set_seed(seed: int):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)

    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True
    # torch.use_deterministic_algorithms(True)  # 필요하면 옵션으로


def worker_init_fn(worker_id: int):
    seed = torch.initial_seed() % (2**32)
    np.random.seed(seed + worker_id)
    random.seed(seed + worker_id)


# =========================
# IO & preprocessing
# =========================
def ensure_dir(p: str):
    os.makedirs(p, exist_ok=True)

def list_jpgs(folder: str) -> List[str]:
    return sorted(glob.glob(os.path.join(folder, "*.jpg")))

def crop_valid(img: np.ndarray, cfg: CFG) -> np.ndarray:
    return img[:cfg.crop_h, :cfg.crop_w]

def read_gray(path: str) -> np.ndarray:
    im = cv2.imread(path, cv2.IMREAD_GRAYSCALE)
    if im is None:
        raise FileNotFoundError(path)
    return im

def read_label_bgr(path: str) -> np.ndarray:
    im = cv2.imread(path, cv2.IMREAD_COLOR)  # BGR
    if im is None:
        raise FileNotFoundError(path)
    return im

def apply_clahe(img01: np.ndarray, cfg: CFG) -> np.ndarray:
    img8 = np.clip(img01 * 255.0, 0, 255).astype(np.uint8)
    clahe = cv2.createCLAHE(clipLimit=cfg.clahe_clip, tileGridSize=(cfg.clahe_tile, cfg.clahe_tile))
    out = clahe.apply(img8).astype(np.float32) / 255.0
    return out

def estimate_background(img01: np.ndarray, cfg: CFG) -> np.ndarray:
    k = cfg.bg_kernel
    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (k, k))
    img8 = np.clip(img01 * 255.0, 0, 255).astype(np.uint8)
    bg8 = cv2.morphologyEx(img8, cv2.MORPH_OPEN, kernel)
    return bg8.astype(np.float32) / 255.0

def robust_rescale(x: np.ndarray, cfg: CFG) -> np.ndarray:
    lo = np.percentile(x, cfg.robust_p_lo)
    hi = np.percentile(x, cfg.robust_p_hi)
    if hi <= lo + 1e-8:
        return np.clip(x, 0, 1)
    y = (x - lo) / (hi - lo)
    return np.clip(y, 0, 1)

def preprocess_sem(img_gray: np.ndarray, cfg: CFG) -> np.ndarray:
    x = crop_valid(img_gray, cfg).astype(np.float32) / 255.0
    if cfg.use_clahe:
        x = apply_clahe(x, cfg)
    bg = estimate_background(x, cfg)
    x = x - bg
    x = robust_rescale(x, cfg)
    return x.astype(np.float32)

def extract_red_boundary_mask(label_bgr: np.ndarray, cfg: CFG) -> np.ndarray:
    lab = crop_valid(label_bgr, cfg)
    B, G, R = lab[..., 0], lab[..., 1], lab[..., 2]
    m = (R > cfg.r_thr) & (G < cfg.g_thr) & (B < cfg.b_thr)
    return m.astype(np.uint8)

def dilate(mask01: np.ndarray, radius: int) -> np.ndarray:
    if radius <= 0:
        return mask01
    k = 2 * radius + 1
    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (k, k))
    m = (mask01 > 0).astype(np.uint8) * 255
    d = cv2.dilate(m, kernel)
    return (d > 0).astype(np.uint8)

def compute_offsets(L: int, patch: int, stride: int) -> List[int]:
    max0 = L - patch
    if max0 < 0:
        return [0]
    offs = list(range(0, max0 + 1, stride))
    if offs[-1] != max0:
        offs.append(max0)
    return offs

def rot_k(a: np.ndarray, k: int) -> np.ndarray:
    return np.rot90(a, k).copy()

def to_u8(img01: np.ndarray) -> np.ndarray:
    return np.clip(img01 * 255.0, 0, 255).astype(np.uint8)

def overlay_boundary(gray01: np.ndarray, gt01: np.ndarray, pred01: np.ndarray) -> np.ndarray:
    """
    gray01: [0,1] HxW
    gt01: 0/1 HxW
    pred01: 0/1 HxW
    overlay: GT red, Pred green
    """
    g = to_u8(gray01)
    rgb = cv2.cvtColor(g, cv2.COLOR_GRAY2BGR)
    rgb[gt01 > 0] = (0, 0, 255)
    rgb[pred01 > 0] = (0, 255, 0)
    return rgb


def save_preproc_and_patches(
    jpg_paths: List[str],
    out_dir: str,
    cfg: CFG,
    split_name: str,
    patches_per_image: int = 6,
):
    """
    Save:
      - full preprocessed SEM
      - full GT thin + U1 dilated target
      - random sample patches (input + target)
    """
    ensure_dir(out_dir)
    full_dir = os.path.join(out_dir, split_name, "full")
    patch_dir = os.path.join(out_dir, split_name, "patches")
    ensure_dir(full_dir)
    ensure_dir(patch_dir)

    rng = np.random.RandomState(cfg.seed + 123)

    for p in jpg_paths[:cfg.export_debug_max_images]:
        base = os.path.splitext(os.path.basename(p))[0]

        img01 = preprocess_sem(read_gray(p), cfg)
        lab = read_label_bgr(os.path.splitext(p)[0] + ".png")
        gt_thin = extract_red_boundary_mask(lab, cfg).astype(np.uint8)
        gt_u1 = dilate(gt_thin, cfg.dilate_radius).astype(np.uint8)

        cv2.imwrite(os.path.join(full_dir, f"{base}_SEM_preproc.png"), to_u8(img01))
        cv2.imwrite(os.path.join(full_dir, f"{base}_GT_thin.png"), (gt_thin * 255).astype(np.uint8))
        cv2.imwrite(os.path.join(full_dir, f"{base}_GT_u1_dilated.png"), (gt_u1 * 255).astype(np.uint8))

        # quick sanity overlay: GT only
        ov = overlay_boundary(img01, gt_thin, np.zeros_like(gt_thin))
        cv2.imwrite(os.path.join(full_dir, f"{base}_GT_overlay.png"), ov)

        H, W = img01.shape
        ys = compute_offsets(H, cfg.patch, cfg.stride)
        xs = compute_offsets(W, cfg.patch, cfg.stride)
        coords = [(y, x) for y in ys for x in xs]
        if len(coords) == 0:
            continue

        sel = rng.choice(len(coords), size=min(patches_per_image, len(coords)), replace=False)
        for j, si in enumerate(sel):
            y, x = coords[int(si)]
            patch_I = img01[y:y+cfg.patch, x:x+cfg.patch]
            patch_Y = gt_u1[y:y+cfg.patch, x:x+cfg.patch]

            cv2.imwrite(os.path.join(patch_dir, f"{base}_p{j:02d}_I.png"), to_u8(patch_I))
            cv2.imwrite(os.path.join(patch_dir, f"{base}_p{j:02d}_Y.png"), (patch_Y * 255).astype(np.uint8))


# =========================
# Loss
# =========================
def soft_dice_loss(logits: torch.Tensor, target: torch.Tensor, eps: float = 1e-6) -> torch.Tensor:
    prob = torch.sigmoid(logits)
    num = 2.0 * (prob * target).sum(dim=(2,3))
    den = (prob * prob).sum(dim=(2,3)) + (target * target).sum(dim=(2,3)) + eps
    return (1.0 - (num / den)).mean()

def bce_dice_loss(logits: torch.Tensor, target: torch.Tensor) -> Tuple[torch.Tensor, float, float]:
    bce = F.binary_cross_entropy_with_logits(logits, target)
    d = soft_dice_loss(logits, target)
    return bce + d, float(bce.item()), float(d.item())


# =========================
# Better val metrics for sparse boundary
# =========================
def soft_dice_score_np(prob: np.ndarray, gt: np.ndarray, eps: float = 1e-6) -> float:
    prob = prob.astype(np.float64)
    gt = gt.astype(np.float64)
    num = 2.0 * np.sum(prob * gt)
    den = np.sum(prob) + np.sum(gt) + eps
    return float(num / den)

def average_precision_np(y_true: np.ndarray, y_score: np.ndarray, eps: float = 1e-12) -> float:
    """
    Average Precision (AP), PR-AUC 계열. 희소 boundary에서 유리.
    """
    y_true = y_true.astype(np.uint8)
    y_score = y_score.astype(np.float64)

    pos = int(y_true.sum())
    if pos == 0:
        return 0.0

    order = np.argsort(-y_score)
    y_true_sorted = y_true[order]

    tp = np.cumsum(y_true_sorted == 1)
    fp = np.cumsum(y_true_sorted == 0)

    precision = tp / np.maximum(tp + fp, 1)

    ap = float(np.sum(precision[y_true_sorted == 1]) / (pos + eps))
    return ap

def sweep_best_f1(y_true: np.ndarray, y_score: np.ndarray,
                  thresholds: np.ndarray = None, eps: float = 1e-12) -> Tuple[float, float]:
    if thresholds is None:
        thresholds = np.linspace(0.05, 0.95, 19)

    y_true = y_true.astype(np.uint8)
    y_score = y_score.astype(np.float64)

    pos = y_true.sum()
    if pos == 0:
        return 0.0, float(thresholds[0])

    best_f1, best_thr = 0.0, float(thresholds[0])

    for thr in thresholds:
        y_pred = (y_score >= thr).astype(np.uint8)
        tp = int((y_pred & y_true).sum())
        fp = int((y_pred & (1 - y_true)).sum())
        fn = int(((1 - y_pred) & y_true).sum())

        precision = tp / (tp + fp + eps)
        recall = tp / (tp + fn + eps)
        f1 = 2.0 * precision * recall / (precision + recall + eps)

        if f1 > best_f1:
            best_f1, best_thr = float(f1), float(thr)

    return best_f1, best_thr


# =========================
# Model: ResUNet + GroupNorm
# =========================
class ResBlockGN(nn.Module):
    def __init__(self, in_ch, out_ch, groups: int):
        super().__init__()
        self.c1 = nn.Conv2d(in_ch, out_ch, 3, padding=1, bias=False)
        self.g1 = nn.GroupNorm(num_groups=min(groups, out_ch), num_channels=out_ch)
        self.c2 = nn.Conv2d(out_ch, out_ch, 3, padding=1, bias=False)
        self.g2 = nn.GroupNorm(num_groups=min(groups, out_ch), num_channels=out_ch)
        self.act = nn.SiLU(inplace=True)
        self.skip = None
        if in_ch != out_ch:
            self.skip = nn.Conv2d(in_ch, out_ch, 1, bias=False)

    def forward(self, x):
        h = self.act(self.g1(self.c1(x)))
        h = self.g2(self.c2(h))
        s = x if self.skip is None else self.skip(x)
        return self.act(h + s)

class Down(nn.Module):
    def __init__(self, in_ch, out_ch, groups: int):
        super().__init__()
        self.pool = nn.MaxPool2d(2)
        self.block = ResBlockGN(in_ch, out_ch, groups)
    def forward(self, x):
        return self.block(self.pool(x))

class Up(nn.Module):
    def __init__(self, in_ch, out_ch, groups: int):
        super().__init__()
        self.up = nn.Upsample(scale_factor=2, mode="bilinear", align_corners=False)
        self.block = ResBlockGN(in_ch, out_ch, groups)
    def forward(self, x1, x2):
        x1 = self.up(x1)
        dy = x2.size(2) - x1.size(2)
        dx = x2.size(3) - x1.size(3)
        x1 = F.pad(x1, [dx//2, dx-dx//2, dy//2, dy-dy//2])
        return self.block(torch.cat([x2, x1], dim=1))

class ResUNetGN(nn.Module):
    def __init__(self, in_channels=1, out_channels=1, base=64, groups=16):
        super().__init__()
        self.inc = ResBlockGN(in_channels, base, groups)
        self.d1 = Down(base, base*2, groups)
        self.d2 = Down(base*2, base*4, groups)
        self.d3 = Down(base*4, base*8, groups)
        self.d4 = Down(base*8, base*16, groups)
        self.u1 = Up(base*16 + base*8, base*8, groups)
        self.u2 = Up(base*8 + base*4, base*4, groups)
        self.u3 = Up(base*4 + base*2, base*2, groups)
        self.u4 = Up(base*2 + base, base, groups)
        self.outc = nn.Conv2d(base, out_channels, 1)

    def forward(self, x):
        x1 = self.inc(x)
        x2 = self.d1(x1)
        x3 = self.d2(x2)
        x4 = self.d3(x3)
        x5 = self.d4(x4)
        x = self.u1(x5, x4)
        x = self.u2(x, x3)
        x = self.u3(x, x2)
        x = self.u4(x, x1)
        return self.outc(x)  # logits


# =========================
# Dataset (patches + rotations)
# =========================
class PatchDS(torch.utils.data.Dataset):
    def __init__(self, jpg_paths: List[str], cfg: CFG):
        self.cfg = cfg
        self.jpg_paths = jpg_paths

        self.cache: Dict[str, Dict[str, np.ndarray]] = {}
        self.index: List[Tuple[int, int, int, int]] = []  # (i, y, x, rot)

        for i, p in enumerate(jpg_paths):
            img = preprocess_sem(read_gray(p), cfg)
            ys = compute_offsets(img.shape[0], cfg.patch, cfg.stride)
            xs = compute_offsets(img.shape[1], cfg.patch, cfg.stride)
            for y in ys:
                for x in xs:
                    for rk in (0,1,2,3):
                        self.index.append((i, y, x, rk))

    def __len__(self): return len(self.index)

    def _load(self, i: int):
        p = self.jpg_paths[i]
        if p in self.cache:
            return
        cfg = self.cfg
        img = preprocess_sem(read_gray(p), cfg)
        lp = os.path.splitext(p)[0] + ".png"
        lab = read_label_bgr(lp)
        m = extract_red_boundary_mask(lab, cfg)        # thin GT
        m_u1 = dilate(m, cfg.dilate_radius)           # coarse target for U1
        self.cache[p] = {"img": img, "m_u1": m_u1}

    def __getitem__(self, idx: int):
        i, y, x, rk = self.index[idx]
        self._load(i)
        p = self.jpg_paths[i]
        c = self.cache[p]
        img = c["img"][y:y+self.cfg.patch, x:x+self.cfg.patch]
        m_u1 = c["m_u1"][y:y+self.cfg.patch, x:x+self.cfg.patch]

        img = rot_k(img, rk)
        m_u1 = rot_k(m_u1, rk)

        I = torch.from_numpy(img[None].astype(np.float32))
        Y = torch.from_numpy(m_u1[None].astype(np.float32))
        return I, Y


# =========================
# Eval + cache export
# =========================
@torch.no_grad()
def eval_unet1(model: nn.Module, loader, cfg: CFG) -> Dict[str, float]:
    model.eval()

    losses = []
    all_prob = []
    all_gt = []

    for I, Y in loader:
        I = I.to(cfg.device)
        Y = Y.to(cfg.device)

        logits = model(I)
        loss, _, _ = bce_dice_loss(logits, Y)
        losses.append(float(loss.item()))

        prob = torch.sigmoid(logits).detach().cpu().numpy().reshape(-1)
        gt = (Y.detach().cpu().numpy().reshape(-1) > 0.5).astype(np.uint8)

        all_prob.append(prob.astype(np.float32))
        all_gt.append(gt)

    prob = np.concatenate(all_prob, axis=0)
    gt = np.concatenate(all_gt, axis=0).astype(np.uint8)

    soft_dice = soft_dice_score_np(prob, gt)
    ap = average_precision_np(gt, prob)
    best_f1, best_thr = sweep_best_f1(gt, prob, thresholds=np.linspace(0.05, 0.95, 19))

    pred05 = (prob >= cfg.thr_vis).astype(np.uint8)
    inter = 2.0 * float(np.sum(pred05 * gt))
    den = float(np.sum(pred05) + np.sum(gt) + 1e-6)
    dice05 = inter / den

    return {
        "loss": float(np.mean(losses)),
        "dice05": float(dice05),
        "soft_dice": float(soft_dice),
        "ap": float(ap),
        "best_f1": float(best_f1),
        "best_thr": float(best_thr),
    }

@torch.no_grad()
def export_u1_logits(model: nn.Module, jpg_paths: List[str], out_dir: str, cfg: CFG):
    ensure_dir(out_dir)
    model.eval()
    for p in jpg_paths:
        img = preprocess_sem(read_gray(p), cfg)
        I = torch.from_numpy(img[None,None].astype(np.float32)).to(cfg.device)
        logits = model(I)[0,0].detach().cpu().numpy().astype(np.float16)

        base = os.path.splitext(os.path.basename(p))[0]
        np.savez_compressed(os.path.join(out_dir, f"{base}.npz"),
                            logits=logits,
                            shape=np.array([cfg.crop_h, cfg.crop_w], dtype=np.int32))

def sha1_of_file(path: str) -> str:
    h = hashlib.sha1()
    with open(path, "rb") as f:
        while True:
            b = f.read(1024*1024)
            if not b:
                break
            h.update(b)
    return h.hexdigest()

@torch.no_grad()
def save_overlays_for_paths(model: nn.Module, paths: List[str], out_dir: str, cfg: CFG, tag: str):
    ensure_dir(out_dir)
    model.eval()
    for p in paths:
        img01 = preprocess_sem(read_gray(p), cfg)
        lab = read_label_bgr(os.path.splitext(p)[0] + ".png")
        gt = extract_red_boundary_mask(lab, cfg).astype(np.uint8)

        I = torch.from_numpy(img01[None,None].astype(np.float32)).to(cfg.device)
        logits = model(I)[0,0].detach().cpu()
        prob = torch.sigmoid(logits).numpy()
        pred = (prob > cfg.thr_vis).astype(np.uint8)

        ov = overlay_boundary(img01, gt, pred)
        base = os.path.splitext(os.path.basename(p))[0]

        # Save all: input, gt, pred, overlay (paper-friendly)
        cv2.imwrite(os.path.join(out_dir, f"{base}_{tag}_I.png"), to_u8(img01))
        cv2.imwrite(os.path.join(out_dir, f"{base}_{tag}_GT.png"), (gt * 255).astype(np.uint8))
        cv2.imwrite(os.path.join(out_dir, f"{base}_{tag}_P.png"), (pred * 255).astype(np.uint8))
        cv2.imwrite(os.path.join(out_dir, f"{base}_{tag}_overlay.png"), ov)


# =========================
# Plotting
# =========================
def plot_curves(df_tr: pd.DataFrame, df_va: pd.DataFrame, out_png: str, title: str):
    plt.figure()
    plt.plot(df_tr["epoch"], df_tr["loss"], label="train loss")
    plt.plot(df_va["epoch"], df_va["loss"], label="val loss")
    plt.plot(df_va["epoch"], df_va["ap"], label="val AP")
    plt.plot(df_va["epoch"], df_va["soft_dice"], label="val soft-dice")
    plt.plot(df_va["epoch"], df_va["dice05"], label="val dice@0.5")
    plt.xlabel("epoch"); plt.legend(); plt.title(title); plt.tight_layout()
    plt.savefig(out_png, dpi=200)
    plt.close()

def plot_scatter_loss_vs_metric(df_tr: pd.DataFrame, df_va: pd.DataFrame, out_png: str, ycol: str = "ap"):
    plt.figure()
    plt.scatter(df_tr["loss"], df_va[ycol])
    plt.xlabel("train loss"); plt.ylabel(f"val {ycol}")
    plt.tight_layout()
    plt.savefig(out_png, dpi=200)
    plt.close()


# =========================
# Main
# =========================
def main():
    cfg = CFG()
    set_seed(cfg.seed)

    run_dir = os.path.join("runs", cfg.exp_name)
    ckpt_dir = os.path.join(run_dir, "checkpoints")
    cache_dir = os.path.join(run_dir, "cache_u1_logits")

    # ---- IMPORTANT FIX: create cache dir (prevents FileNotFoundError)
    ensure_dir(run_dir)
    ensure_dir(ckpt_dir)
    ensure_dir(cache_dir)

    # --- split: sorted, last 2 as val
    train_folder = os.path.join(cfg.root, cfg.train_dir)
    test_folder = os.path.join(cfg.root, cfg.test_dir)
    train_all = list_jpgs(train_folder)
    test_all = list_jpgs(test_folder)

    assert len(train_all) >= 3, "Need at least 3 training images to hold out 2 for val."
    train_paths = train_all[:-2]
    val_paths = train_all[-2:]

    split = {
        "train_files": [os.path.basename(p) for p in train_paths],
        "val_files": [os.path.basename(p) for p in val_paths],
        "test_files": [os.path.basename(p) for p in test_all],
    }
    with open(os.path.join(run_dir, "split.json"), "w", encoding="utf-8") as f:
        json.dump(split, f, indent=2)
    with open(os.path.join(run_dir, "config.json"), "w", encoding="utf-8") as f:
        json.dump(asdict(cfg), f, indent=2)

    print("Train images:", len(train_paths), "Val images:", len(val_paths), "Test images:", len(test_all))

    # --- export preprocessed full images + sample patches (paper/repro)
    if cfg.export_debug_samples:
        dbg_dir = os.path.join(run_dir, "debug_inputs_u1")
        save_preproc_and_patches(train_paths, dbg_dir, cfg, "train", cfg.export_debug_patches_per_image)
        save_preproc_and_patches(val_paths,   dbg_dir, cfg, "val",   cfg.export_debug_patches_per_image)
        # test도 GT PNG가 있으므로 저장 가능
        save_preproc_and_patches(test_all,    dbg_dir, cfg, "test",  cfg.export_debug_patches_per_image)

    # data
    g = torch.Generator()
    g.manual_seed(cfg.seed)

    ds_tr = PatchDS(train_paths, cfg)
    ds_va = PatchDS(val_paths, cfg)

    dl_tr = torch.utils.data.DataLoader(ds_tr, batch_size=cfg.batch, shuffle=True, num_workers=4,
                                        pin_memory=True, worker_init_fn=worker_init_fn, generator=g)
    dl_va = torch.utils.data.DataLoader(ds_va, batch_size=cfg.batch, shuffle=False, num_workers=2,
                                        pin_memory=True, worker_init_fn=worker_init_fn, generator=g)

    print("Train patches:", len(ds_tr), "Val patches:", len(ds_va))

    # model
    model = ResUNetGN(in_channels=1, out_channels=1, base=cfg.base, groups=cfg.gn_groups).to(cfg.device)
    opt = torch.optim.AdamW(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)

    # ---- best criterion: AP (recommended for sparse boundary)
    best_score = -1.0
    rows_tr, rows_va = [], []

    for epoch in range(1, cfg.epochs + 1):
        model.train()
        losses, bces, dls = [], [], []

        for I, Y in dl_tr:
            I = I.to(cfg.device)
            Y = Y.to(cfg.device)

            opt.zero_grad(set_to_none=True)
            logits = model(I)
            loss, bce_v, dl_v = bce_dice_loss(logits, Y)
            loss.backward()
            opt.step()

            losses.append(float(loss.item()))
            bces.append(float(bce_v))
            dls.append(float(dl_v))

        tr = {"epoch": epoch, "loss": float(np.mean(losses)), "bce": float(np.mean(bces)), "dice_loss": float(np.mean(dls))}
        va = eval_unet1(model, dl_va, cfg)
        va["epoch"] = epoch

        rows_tr.append(tr)
        rows_va.append(va)

        # save last
        last_path = os.path.join(ckpt_dir, "unet1_last.pth")
        torch.save({
            "model": model.state_dict(),
            "cfg": asdict(cfg),
            "epoch": epoch,
            "val_metric": va,
            "seed": cfg.seed,
        }, last_path)

        # save best by AP
        score = va["ap"]
        if score > best_score:
            best_score = score
            best_path = os.path.join(ckpt_dir, "unet1_best.pth")
            torch.save({
                "model": model.state_dict(),
                "cfg": asdict(cfg),
                "epoch": epoch,
                "val_metric": va,
                "seed": cfg.seed,
            }, best_path)

        print(
            f"[U1][{epoch:03d}] train_loss={tr['loss']:.4f} "
            f"val_loss={va['loss']:.4f} val_ap={va['ap']:.4f} "
            f"val_softdice={va['soft_dice']:.4f} val_dice05={va['dice05']:.4f} "
            f"val_bestF1={va['best_f1']:.4f}@{va['best_thr']:.2f}"
        )

    # logs
    df_tr = pd.DataFrame(rows_tr)
    df_va = pd.DataFrame(rows_va)
    df_tr.to_csv(os.path.join(run_dir, "unet1_train.csv"), index=False)
    df_va.to_csv(os.path.join(run_dir, "unet1_val.csv"), index=False)

    plot_curves(df_tr, df_va, os.path.join(run_dir, "unet1_curves.png"), "U-Net1")
    plot_scatter_loss_vs_metric(df_tr, df_va, os.path.join(run_dir, "unet1_scatter_loss_vs_valAP.png"), ycol="ap")

    # --- load BEST checkpoint and auto export cache
    best_ckpt_path = os.path.join(ckpt_dir, "unet1_best.pth")
    ckpt = torch.load(best_ckpt_path, map_location=cfg.device)
    model.load_state_dict(ckpt["model"])
    model.eval()

    meta = {
        "created_from_checkpoint": os.path.basename(best_ckpt_path),
        "checkpoint_sha1": sha1_of_file(best_ckpt_path),
        "split": split,
        "config": asdict(cfg),
        "best_epoch": int(ckpt.get("epoch", -1)),
        "best_val_metric": ckpt.get("val_metric", {}),
    }
    with open(os.path.join(cache_dir, "u1_cache_meta.json"), "w", encoding="utf-8") as f:
        json.dump(meta, f, indent=2)

    export_u1_logits(model, train_paths, os.path.join(cache_dir, "train"), cfg)
    export_u1_logits(model, val_paths,   os.path.join(cache_dir, "val"), cfg)
    export_u1_logits(model, test_all,    os.path.join(cache_dir, "test"), cfg)

    # --- paper-friendly outputs: save overlays for val/test (input/gt/pred/overlay)
    save_overlays_for_paths(model, val_paths,  os.path.join(run_dir, "paper_u1_val"),  cfg, tag="U1")
    save_overlays_for_paths(model, test_all,   os.path.join(run_dir, "paper_u1_test"), cfg, tag="U1")

    print("U-Net1 done.")
    print("Artifacts saved to:", run_dir)
    print("U1 logits cache saved to:", cache_dir)


if __name__ == "__main__":
    main()



Train U-Net1

import os, glob, json, math, random, hashlib
from dataclasses import dataclass, asdict
from typing import List, Dict, Tuple

import numpy as np
import pandas as pd
import cv2
import torch
import torch.nn as nn
import torch.nn.functional as F
import matplotlib.pyplot as plt
from scipy.ndimage import distance_transform_edt


# =========================
# Config (edit here)
# =========================
@dataclass
class CFG:
    # data root
    root: str = "./DATA"
    train_dir: str = "TRAIN_SEM_IMAGE"
    test_dir: str = "TEST_SEM_IMAGE"

    # valid area crop (remove bottom metadata band)
    crop_h: int = 896
    crop_w: int = 1280

    # patching
    patch: int = 384
    stride: int = 48

    # preprocessing
    use_clahe: bool = True
    clahe_clip: float = 2.0
    clahe_tile: int = 8
    bg_kernel: int = 49  # ~48x48 grain scale

    robust_p_lo: float = 1.0
    robust_p_hi: float = 99.0

    # label extraction (red line)
    r_thr: int = 160
    g_thr: int = 140
    b_thr: int = 140

    # U1 target thickening (coarse anchor)
    dilate_radius: int = 2

    # training
    seed: int = 42
    device: str = "cuda" if torch.cuda.is_available() else "cpu"
    batch: int = 4
    epochs: int = 30
    lr: float = 1e-3
    weight_decay: float = 1e-5

    # model
    base: int = 64
    gn_groups: int = 16  # GroupNorm groups (batch=4 안정)

    # eval/visualize
    thr_vis: float = 0.5  # for binarizing predictions
    exp_name: str = "exp_unet1"


# =========================
# Repro
# =========================
def set_seed(seed: int):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)

    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True
    # torch.use_deterministic_algorithms(True)  # 켜면 일부 op에서 에러 날 수 있어 옵션 처리 가능


def worker_init_fn(worker_id: int):
    # make dataloader workers deterministic
    seed = torch.initial_seed() % (2**32)
    np.random.seed(seed + worker_id)
    random.seed(seed + worker_id)


# =========================
# IO & preprocessing
# =========================
def list_jpgs(folder: str) -> List[str]:
    return sorted(glob.glob(os.path.join(folder, "*.jpg")))

def crop_valid(img: np.ndarray, cfg: CFG) -> np.ndarray:
    return img[:cfg.crop_h, :cfg.crop_w]

def read_gray(path: str) -> np.ndarray:
    im = cv2.imread(path, cv2.IMREAD_GRAYSCALE)
    if im is None:
        raise FileNotFoundError(path)
    return im

def read_label_bgr(path: str) -> np.ndarray:
    im = cv2.imread(path, cv2.IMREAD_COLOR)  # BGR
    if im is None:
        raise FileNotFoundError(path)
    return im

def apply_clahe(img01: np.ndarray, cfg: CFG) -> np.ndarray:
    img8 = np.clip(img01 * 255.0, 0, 255).astype(np.uint8)
    clahe = cv2.createCLAHE(clipLimit=cfg.clahe_clip, tileGridSize=(cfg.clahe_tile, cfg.clahe_tile))
    out = clahe.apply(img8).astype(np.float32) / 255.0
    return out

def estimate_background(img01: np.ndarray, cfg: CFG) -> np.ndarray:
    k = cfg.bg_kernel
    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (k, k))
    img8 = np.clip(img01 * 255.0, 0, 255).astype(np.uint8)
    bg8 = cv2.morphologyEx(img8, cv2.MORPH_OPEN, kernel)
    return bg8.astype(np.float32) / 255.0

def robust_rescale(x: np.ndarray, cfg: CFG) -> np.ndarray:
    lo = np.percentile(x, cfg.robust_p_lo)
    hi = np.percentile(x, cfg.robust_p_hi)
    if hi <= lo + 1e-8:
        return np.clip(x, 0, 1)
    y = (x - lo) / (hi - lo)
    return np.clip(y, 0, 1)

def preprocess_sem(img_gray: np.ndarray, cfg: CFG) -> np.ndarray:
    x = crop_valid(img_gray, cfg).astype(np.float32) / 255.0
    if cfg.use_clahe:
        x = apply_clahe(x, cfg)
    bg = estimate_background(x, cfg)
    x = x - bg
    x = robust_rescale(x, cfg)
    return x.astype(np.float32)

def extract_red_boundary_mask(label_bgr: np.ndarray, cfg: CFG) -> np.ndarray:
    lab = crop_valid(label_bgr, cfg)
    B, G, R = lab[..., 0], lab[..., 1], lab[..., 2]
    m = (R > cfg.r_thr) & (G < cfg.g_thr) & (B < cfg.b_thr)
    return m.astype(np.uint8)

def dilate(mask01: np.ndarray, radius: int) -> np.ndarray:
    if radius <= 0:
        return mask01
    k = 2 * radius + 1
    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (k, k))
    m = (mask01 > 0).astype(np.uint8) * 255
    d = cv2.dilate(m, kernel)
    return (d > 0).astype(np.uint8)

def compute_offsets(L: int, patch: int, stride: int) -> List[int]:
    max0 = L - patch
    if max0 < 0:
        return [0]
    offs = list(range(0, max0 + 1, stride))
    if offs[-1] != max0:
        offs.append(max0)  # ensure full coverage
    return offs

def rot_k(a: np.ndarray, k: int) -> np.ndarray:
    return np.rot90(a, k).copy()

def overlay_boundary(gray01: np.ndarray, gt01: np.ndarray, pred01: np.ndarray) -> np.ndarray:
    """
    gray01: [0,1] HxW
    gt01: 0/1 HxW
    pred01: 0/1 HxW
    overlay: GT red, Pred green
    """
    g = np.clip(gray01 * 255.0, 0, 255).astype(np.uint8)
    rgb = cv2.cvtColor(g, cv2.COLOR_GRAY2BGR)
    # GT red
    rgb[gt01 > 0] = (0, 0, 255)
    # Pred green (if overlaps GT -> yellow-ish)
    rgb[pred01 > 0] = (0, 255, 0)
    return rgb


# =========================
# Metrics & Loss
# =========================
def soft_dice_loss(logits: torch.Tensor, target: torch.Tensor, eps: float = 1e-6) -> torch.Tensor:
    prob = torch.sigmoid(logits)
    num = 2.0 * (prob * target).sum(dim=(2,3))
    den = (prob * prob).sum(dim=(2,3)) + (target * target).sum(dim=(2,3)) + eps
    return (1.0 - (num / den)).mean()

def bce_dice_loss(logits: torch.Tensor, target: torch.Tensor) -> Tuple[torch.Tensor, float, float]:
    bce = F.binary_cross_entropy_with_logits(logits, target)
    d = soft_dice_loss(logits, target)
    return bce + d, float(bce.item()), float(d.item())

@torch.no_grad()
def dice_binary(pred: torch.Tensor, gt: torch.Tensor, eps: float = 1e-6) -> float:
    pred = pred.float()
    gt = gt.float()
    inter = (pred * gt).sum(dim=(2,3))
    den = pred.sum(dim=(2,3)) + gt.sum(dim=(2,3)) + eps
    return float(((2.0 * inter) / den).mean().item())


# =========================
# Model: ResUNet + GroupNorm
# =========================
class ResBlockGN(nn.Module):
    def __init__(self, in_ch, out_ch, groups: int):
        super().__init__()
        self.c1 = nn.Conv2d(in_ch, out_ch, 3, padding=1, bias=False)
        self.g1 = nn.GroupNorm(num_groups=min(groups, out_ch), num_channels=out_ch)
        self.c2 = nn.Conv2d(out_ch, out_ch, 3, padding=1, bias=False)
        self.g2 = nn.GroupNorm(num_groups=min(groups, out_ch), num_channels=out_ch)
        self.act = nn.SiLU(inplace=True)
        self.skip = None
        if in_ch != out_ch:
            self.skip = nn.Conv2d(in_ch, out_ch, 1, bias=False)

    def forward(self, x):
        h = self.act(self.g1(self.c1(x)))
        h = self.g2(self.c2(h))
        s = x if self.skip is None else self.skip(x)
        return self.act(h + s)

class Down(nn.Module):
    def __init__(self, in_ch, out_ch, groups: int):
        super().__init__()
        self.pool = nn.MaxPool2d(2)
        self.block = ResBlockGN(in_ch, out_ch, groups)
    def forward(self, x):
        return self.block(self.pool(x))

class Up(nn.Module):
    def __init__(self, in_ch, out_ch, groups: int):
        super().__init__()
        self.up = nn.Upsample(scale_factor=2, mode="bilinear", align_corners=False)
        self.block = ResBlockGN(in_ch, out_ch, groups)
    def forward(self, x1, x2):
        x1 = self.up(x1)
        dy = x2.size(2) - x1.size(2)
        dx = x2.size(3) - x1.size(3)
        x1 = F.pad(x1, [dx//2, dx-dx//2, dy//2, dy-dy//2])
        return self.block(torch.cat([x2, x1], dim=1))

class ResUNetGN(nn.Module):
    def __init__(self, in_channels=1, out_channels=1, base=64, groups=16):
        super().__init__()
        self.inc = ResBlockGN(in_channels, base, groups)
        self.d1 = Down(base, base*2, groups)
        self.d2 = Down(base*2, base*4, groups)
        self.d3 = Down(base*4, base*8, groups)
        self.d4 = Down(base*8, base*16, groups)
        self.u1 = Up(base*16 + base*8, base*8, groups)
        self.u2 = Up(base*8 + base*4, base*4, groups)
        self.u3 = Up(base*4 + base*2, base*2, groups)
        self.u4 = Up(base*2 + base, base, groups)
        self.outc = nn.Conv2d(base, out_channels, 1)

    def forward(self, x):
        x1 = self.inc(x)
        x2 = self.d1(x1)
        x3 = self.d2(x2)
        x4 = self.d3(x3)
        x5 = self.d4(x4)
        x = self.u1(x5, x4)
        x = self.u2(x, x3)
        x = self.u3(x, x2)
        x = self.u4(x, x1)
        return self.outc(x)  # logits


# =========================
# Dataset (patches + rotations)
# =========================
class PatchDS(torch.utils.data.Dataset):
    def __init__(self, jpg_paths: List[str], cfg: CFG):
        self.cfg = cfg
        self.jpg_paths = jpg_paths

        self.cache: Dict[str, Dict[str, np.ndarray]] = {}
        self.index: List[Tuple[int, int, int, int]] = []  # (i, y, x, rot)

        for i, p in enumerate(jpg_paths):
            img = preprocess_sem(read_gray(p), cfg)  # HxW
            ys = compute_offsets(img.shape[0], cfg.patch, cfg.stride)
            xs = compute_offsets(img.shape[1], cfg.patch, cfg.stride)
            for y in ys:
                for x in xs:
                    for rk in (0,1,2,3):
                        self.index.append((i, y, x, rk))

    def __len__(self): return len(self.index)

    def _load(self, i: int):
        p = self.jpg_paths[i]
        if p in self.cache:
            return
        cfg = self.cfg
        img = preprocess_sem(read_gray(p), cfg)
        lp = os.path.splitext(p)[0] + ".png"
        lab = read_label_bgr(lp)
        m = extract_red_boundary_mask(lab, cfg)        # thin GT
        m_u1 = dilate(m, cfg.dilate_radius)           # coarse target
        self.cache[p] = {"img": img, "m": m, "m_u1": m_u1}

    def __getitem__(self, idx: int):
        i, y, x, rk = self.index[idx]
        self._load(i)
        p = self.jpg_paths[i]
        c = self.cache[p]
        img = c["img"][y:y+self.cfg.patch, x:x+self.cfg.patch]
        m_u1 = c["m_u1"][y:y+self.cfg.patch, x:x+self.cfg.patch]

        img = rot_k(img, rk)
        m_u1 = rot_k(m_u1, rk)

        I = torch.from_numpy(img[None].astype(np.float32))
        Y = torch.from_numpy(m_u1[None].astype(np.float32))
        return I, Y


# =========================
# Eval + cache export
# =========================
@torch.no_grad()
def eval_unet1(model: nn.Module, loader, cfg: CFG) -> Dict[str, float]:
    model.eval()
    losses, dices = [], []
    for I, Y in loader:
        I = I.to(cfg.device)
        Y = Y.to(cfg.device)
        logits = model(I)
        loss, _, _ = bce_dice_loss(logits, Y)
        losses.append(float(loss.item()))
        pred = (torch.sigmoid(logits) > cfg.thr_vis).float()
        dices.append(dice_binary(pred, (Y > 0.5).float()))
    return {"loss": float(np.mean(losses)), "dice": float(np.mean(dices))}

@torch.no_grad()
def export_u1_logits(model: nn.Module, jpg_paths: List[str], out_dir: str, cfg: CFG):
    os.makedirs(out_dir, exist_ok=True)
    model.eval()
    for p in jpg_paths:
        img = preprocess_sem(read_gray(p), cfg)
        I = torch.from_numpy(img[None,None].astype(np.float32)).to(cfg.device)
        logits = model(I)[0,0].detach().cpu().numpy().astype(np.float16)

        base = os.path.splitext(os.path.basename(p))[0]
        np.savez_compressed(os.path.join(out_dir, f"{base}.npz"),
                            logits=logits,
                            shape=np.array([cfg.crop_h, cfg.crop_w], dtype=np.int32))

def sha1_of_file(path: str) -> str:
    h = hashlib.sha1()
    with open(path, "rb") as f:
        while True:
            b = f.read(1024*1024)
            if not b:
                break
            h.update(b)
    return h.hexdigest()


@torch.no_grad()
def save_val_overlays(model: nn.Module, val_paths: List[str], out_dir: str, cfg: CFG):
    os.makedirs(out_dir, exist_ok=True)
    model.eval()
    for p in val_paths:
        img01 = preprocess_sem(read_gray(p), cfg)
        lab = read_label_bgr(os.path.splitext(p)[0] + ".png")
        gt = extract_red_boundary_mask(lab, cfg).astype(np.uint8)
        I = torch.from_numpy(img01[None,None].astype(np.float32)).to(cfg.device)
        logits = model(I)[0,0].detach().cpu()
        pred = (torch.sigmoid(logits) > cfg.thr_vis).numpy().astype(np.uint8)
        ov = overlay_boundary(img01, gt, pred)
        base = os.path.splitext(os.path.basename(p))[0]
        cv2.imwrite(os.path.join(out_dir, f"{base}_U1_overlay.png"), ov)


# =========================
# Plotting
# =========================
def plot_curves(df_tr: pd.DataFrame, df_va: pd.DataFrame, out_png: str, title: str):
    plt.figure()
    plt.plot(df_tr["epoch"], df_tr["loss"], label="train loss")
    plt.plot(df_va["epoch"], df_va["loss"], label="val loss")
    plt.plot(df_va["epoch"], df_va["dice"], label="val dice")
    plt.xlabel("epoch"); plt.legend(); plt.title(title); plt.tight_layout()
    plt.savefig(out_png, dpi=200)
    plt.close()

def plot_scatter_loss_vs_metric(df_tr: pd.DataFrame, df_va: pd.DataFrame, out_png: str, xcol="loss", ycol="dice"):
    plt.figure()
    plt.scatter(df_tr[xcol], df_va[ycol])
    plt.xlabel(f"train {xcol}"); plt.ylabel(f"val {ycol}")
    plt.tight_layout()
    plt.savefig(out_png, dpi=200)
    plt.close()


# =========================
# Main
# =========================
def main():
    cfg = CFG()
    set_seed(cfg.seed)

    run_dir = os.path.join("runs", cfg.exp_name)
    ckpt_dir = os.path.join(run_dir, "checkpoints")
    cache_dir = os.path.join(run_dir, "cache_u1_logits")
    os.makedirs(ckpt_dir, exist_ok=True)
    os.makedirs(run_dir, exist_ok=True)

    # --- split: sorted, last 2 as val
    train_folder = os.path.join(cfg.root, cfg.train_dir)
    test_folder = os.path.join(cfg.root, cfg.test_dir)
    train_all = list_jpgs(train_folder)
    test_all = list_jpgs(test_folder)

    assert len(train_all) >= 3, "Need at least 3 training images to hold out 2 for val."
    train_paths = train_all[:-2]
    val_paths = train_all[-2:]

    split = {
        "train_files": [os.path.basename(p) for p in train_paths],
        "val_files": [os.path.basename(p) for p in val_paths],
        "test_files": [os.path.basename(p) for p in test_all],
    }
    with open(os.path.join(run_dir, "split.json"), "w", encoding="utf-8") as f:
        json.dump(split, f, indent=2)

    with open(os.path.join(run_dir, "config.json"), "w", encoding="utf-8") as f:
        json.dump(asdict(cfg), f, indent=2)

    # data
    g = torch.Generator()
    g.manual_seed(cfg.seed)

    ds_tr = PatchDS(train_paths, cfg)
    ds_va = PatchDS(val_paths, cfg)  # val도 patch/rotation 동일하게 평가(이미지 독립성은 이미지 단위로 확보됨)
    dl_tr = torch.utils.data.DataLoader(ds_tr, batch_size=cfg.batch, shuffle=True, num_workers=4,
                                        pin_memory=True, worker_init_fn=worker_init_fn, generator=g)
    dl_va = torch.utils.data.DataLoader(ds_va, batch_size=cfg.batch, shuffle=False, num_workers=2,
                                        pin_memory=True, worker_init_fn=worker_init_fn, generator=g)

    print("Train images:", len(train_paths), "Val images:", len(val_paths), "Test images:", len(test_all))
    print("Train patches:", len(ds_tr), "Val patches:", len(ds_va))

    # model
    model = ResUNetGN(in_channels=1, out_channels=1, base=cfg.base, groups=cfg.gn_groups).to(cfg.device)
    opt = torch.optim.AdamW(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)

    best_dice = -1.0
    rows_tr, rows_va = [], []

    for epoch in range(1, cfg.epochs + 1):
        model.train()
        losses, bces, dices = [], [], []
        for I, Y in dl_tr:
            I = I.to(cfg.device)
            Y = Y.to(cfg.device)
            opt.zero_grad(set_to_none=True)
            logits = model(I)
            loss, bce_v, dice_v = bce_dice_loss(logits, Y)
            loss.backward()
            opt.step()
            losses.append(float(loss.item()))
            bces.append(bce_v)
            dices.append(dice_v)

        tr = {"epoch": epoch, "loss": float(np.mean(losses)), "bce": float(np.mean(bces)), "dice_loss": float(np.mean(dices))}
        va = eval_unet1(model, dl_va, cfg)
        va["epoch"] = epoch
        rows_tr.append(tr)
        rows_va.append(va)

        # save last
        last_path = os.path.join(ckpt_dir, "unet1_last.pth")
        torch.save({
            "model": model.state_dict(),
            "cfg": asdict(cfg),
            "epoch": epoch,
            "val_metric": va,
            "seed": cfg.seed,
        }, last_path)

        # save best by val dice
        if va["dice"] > best_dice:
            best_dice = va["dice"]
            best_path = os.path.join(ckpt_dir, "unet1_best.pth")
            torch.save({
                "model": model.state_dict(),
                "cfg": asdict(cfg),
                "epoch": epoch,
                "val_metric": va,
                "seed": cfg.seed,
            }, best_path)

        print(f"[U1][{epoch:03d}] train_loss={tr['loss']:.4f} val_loss={va['loss']:.4f} val_dice={va['dice']:.4f}")

    # save logs
    df_tr = pd.DataFrame(rows_tr)
    df_va = pd.DataFrame(rows_va)
    df_tr.to_csv(os.path.join(run_dir, "unet1_train.csv"), index=False)
    df_va.to_csv(os.path.join(run_dir, "unet1_val.csv"), index=False)

    plot_curves(df_tr, df_va, os.path.join(run_dir, "unet1_curves.png"), "U-Net1")
    plot_scatter_loss_vs_metric(df_tr, df_va, os.path.join(run_dir, "unet1_scatter_loss_vs_valdice.png"))

    # --- auto export cache using BEST checkpoint (for strict reproducibility)
    best_ckpt_path = os.path.join(ckpt_dir, "unet1_best.pth")
    ckpt = torch.load(best_ckpt_path, map_location=cfg.device)
    model.load_state_dict(ckpt["model"])
    model.eval()

    meta = {
        "created_from_checkpoint": os.path.basename(best_ckpt_path),
        "checkpoint_sha1": sha1_of_file(best_ckpt_path),
        "split": split,
        "config": asdict(cfg),
    }
    with open(os.path.join(cache_dir, "u1_cache_meta.json"), "w", encoding="utf-8") as f:
        json.dump(meta, f, indent=2)

    export_u1_logits(model, train_paths, os.path.join(cache_dir, "train"), cfg)
    export_u1_logits(model, val_paths, os.path.join(cache_dir, "val"), cfg)
    export_u1_logits(model, test_all,  os.path.join(cache_dir, "test"), cfg)

    # overlays on val for qualitative check
    save_val_overlays(model, val_paths, os.path.join(run_dir, "overlays_u1_val"), cfg)

    print("U-Net1 done.")
    print("Artifacts saved to:", run_dir)
    print("U1 logits cache saved to:", cache_dir)


if __name__ == "__main__":
    main()


import os, json
import torch
import numpy as np

def export_cache_only():
    cfg = CFG()  # train_unet1.py에 정의된 CFG 그대로 사용
    run_dir = os.path.join("runs", cfg.exp_name)
    ckpt_dir = os.path.join(run_dir, "checkpoints")
    cache_dir = os.path.join(run_dir, "cache_u1_logits")
    os.makedirs(cache_dir, exist_ok=True)

    # load split/config
    with open(os.path.join(run_dir, "split.json"), "r", encoding="utf-8") as f:
        split = json.load(f)

    train_folder = os.path.join(cfg.root, cfg.train_dir)
    test_folder  = os.path.join(cfg.root, cfg.test_dir)

    train_paths = [os.path.join(train_folder, fn) for fn in split["train_files"]]
    val_paths   = [os.path.join(train_folder, fn) for fn in split["val_files"]]
    test_paths  = [os.path.join(test_folder,  fn) for fn in split["test_files"]]

    best_ckpt_path = os.path.join(ckpt_dir, "unet1_best.pth")
    ckpt = torch.load(best_ckpt_path, map_location=cfg.device)

    model = ResUNetGN(in_channels=1, out_channels=1, base=cfg.base, groups=cfg.gn_groups).to(cfg.device)
    model.load_state_dict(ckpt["model"])
    model.eval()

    meta = {
        "created_from_checkpoint": "unet1_best.pth",
        "split": split,
        "config": asdict(cfg),
    }
    with open(os.path.join(cache_dir, "u1_cache_meta.json"), "w", encoding="utf-8") as f:
        json.dump(meta, f, indent=2)

    export_u1_logits(model, train_paths, os.path.join(cache_dir, "train"), cfg)
    export_u1_logits(model, val_paths,   os.path.join(cache_dir, "val"), cfg)
    export_u1_logits(model, test_paths,  os.path.join(cache_dir, "test"), cfg)

    print("Cache export done:", cache_dir)

if __name__ == "__main__":
    export_cache_only()