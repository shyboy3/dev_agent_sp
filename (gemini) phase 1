Phase 2

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import numpy as np
import os
from datetime import datetime
import pandas as pd
from tqdm import tqdm

# [이전에 정의한 ResUNet 클래스와 ResidualBlock 클래스가 여기에 포함됩니다]

class PatchDataset(Dataset):
    def __init__(self, patch_dir, prefix='patch'):
        self.patch_dir = patch_dir
        # 학습용은 'patch_', 검증용은 'val_' 등의 접두어로 구분 가능하도록 설정
        self.patch_files = sorted([f for f in os.listdir(patch_dir) if f.endswith('_img.npy')])

    def __len__(self):
        return len(self.patch_files)

    def __getitem__(self, idx):
        img_name = self.patch_files[idx]
        mask_name = img_name.replace('_img.npy', '_mask.npy')
        
        img = np.load(os.path.join(self.patch_dir, img_name))
        mask = np.load(os.path.join(self.patch_dir, mask_name))
        
        return torch.from_numpy(img).unsqueeze(0), torch.from_numpy(mask).unsqueeze(0)

def train_unet1():
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    # 경로 설정
    train_dir = './TRAIN_SEM_IMAGE/AUGMENTED_PATCHES'
    val_dir = './VALID_SEM_IMAGE/VALID_PATCHES'
    save_base_dir = './train_results'
    
    run_id = f"train_unet1_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
    run_dir = os.path.join(save_base_dir, run_id)
    os.makedirs(run_dir, exist_ok=True)

    # 데이터 로더
    train_loader = DataLoader(PatchDataset(train_dir), batch_size=16, shuffle=True, num_workers=4)
    val_loader = DataLoader(PatchDataset(val_dir), batch_size=16, shuffle=False, num_workers=2)

    model = ResUNet(filters=64).to(device)
    criterion = nn.BCEWithLogitsLoss()
    optimizer = optim.Adam(model.parameters(), lr=1e-4)

    history = []
    best_val_loss = float('inf')

    for epoch in range(50): # 50 Epochs
        # --- Training ---
        model.train()
        train_loss = 0
        for imgs, masks in tqdm(train_loader, desc=f"Epoch {epoch+1} Train"):
            imgs, masks = imgs.to(device), masks.to(device)
            optimizer.zero_grad()
            loss = criterion(model(imgs), masks)
            loss.backward()
            optimizer.step()
            train_loss += loss.item()
        
        avg_train_loss = train_loss / len(train_loader)

        # --- Validation ---
        model.eval()
        val_loss = 0
        with torch.no_grad():
            for v_imgs, v_masks in val_loader:
                v_imgs, v_masks = v_imgs.to(device), v_masks.to(device)
                v_loss = criterion(model(v_imgs), v_masks)
                val_loss += v_loss.item()
        
        avg_val_loss = val_loss / len(val_loader)
        
        # 로그 기록
        history.append({'epoch': epoch+1, 'train_loss': avg_train_loss, 'val_loss': avg_val_loss})
        print(f"Epoch {epoch+1}: Train Loss {avg_train_loss:.4f}, Val Loss {avg_val_loss:.4f}")

        # Best Model 저장
        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            torch.save(model.state_dict(), os.path.join(run_dir, "best_model.pth"))
            print(f"*** Best Model Updated at Epoch {epoch+1} ***")

    # 결과 저장
    pd.DataFrame(history).to_csv(os.path.join(run_dir, "loss_log.csv"), index=False)
    print(f"학습 완료! 저장 폴더: {run_dir}")

if __name__ == "__main__":
    train_unet1()




# train_unet1 함수 내부 수정 예시
train_loader = DataLoader(PatchDataset(train_patch_dir), batch_size=16, shuffle=True)
val_loader = DataLoader(PatchDataset(val_patch_dir), batch_size=16, shuffle=False)

best_val_loss = float('inf')

for epoch in range(num_epochs):
    # ... Training Step 동일 ...
    
    # Validation Step
    model.eval()
    val_loss = 0
    with torch.no_grad():
        for v_imgs, v_masks in val_loader:
            v_imgs, v_masks = v_imgs.to(device), v_masks.to(device)
            v_outputs = model(v_imgs)
            v_loss = criterion(v_outputs, v_masks)
            val_loss += v_loss.item()
    
    avg_val_loss = val_loss / len(val_loader)
    
    # Best Model 저장
    if avg_val_loss < best_val_loss:
        best_val_loss = avg_val_loss
        torch.save(model.state_dict(), os.path.join(run_dir, "best_model.pth"))
        print(f"--> Best Model Saved at Epoch {epoch+1}")




import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import numpy as np
import os
import cv2
from datetime import datetime
import pandas as pd
from tqdm import tqdm

# 1. ResUNet 모델 정의 (Filter size 64)
class ResidualBlock(nn.Module):
    def __init__(self, in_channels, out_channels, stride=1):
        super(ResidualBlock, self).__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, stride=stride),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True),
            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, stride=1),
            nn.BatchNorm2d(out_channels)
        )
        self.shortcut = nn.Sequential()
        if stride != 1 or in_channels != out_channels:
            self.shortcut = nn.Sequential(
                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride),
                nn.BatchNorm2d(out_channels)
            )

    def forward(self, x):
        return nn.functional.relu(self.conv(x) + self.shortcut(x))

class ResUNet(nn.Module):
    def __init__(self, in_channels=1, out_channels=1, filters=64):
        super(ResUNet, self).__init__()
        # Encoder
        self.enc1 = ResidualBlock(in_channels, filters)
        self.pool1 = nn.MaxPool2d(2)
        self.enc2 = ResidualBlock(filters, filters*2)
        self.pool2 = nn.MaxPool2d(2)
        
        # Bridge
        self.bridge = ResidualBlock(filters*2, filters*4)
        
        # Decoder
        self.up2 = nn.ConvTranspose2d(filters*4, filters*2, kernel_size=2, stride=2)
        self.dec2 = ResidualBlock(filters*4, filters*2)
        self.up1 = nn.ConvTranspose2d(filters*2, filters, kernel_size=2, stride=2)
        self.dec1 = ResidualBlock(filters*2, filters)
        
        self.final = nn.Conv2d(filters, out_channels, kernel_size=1)

    def forward(self, x):
        e1 = self.enc1(x)
        e2 = self.enc2(self.pool1(e1))
        b = self.bridge(self.pool2(e2))
        
        d2 = self.up2(b)
        d2 = self.dec2(torch.cat([d2, e2], dim=1))
        d1 = self.up1(d2)
        d1 = self.dec1(torch.cat([d1, e1], dim=1))
        
        return self.final(d1)

# 2. Dataset 클래스 (npy 패치 로드용)
class PatchDataset(Dataset):
    def __init__(self, patch_dir):
        self.patch_dir = patch_dir
        self.patch_files = sorted([f for f in os.listdir(patch_dir) if f.endswith('_img.npy')])

    def __len__(self):
        return len(self.patch_files)

    def __getitem__(self, idx):
        img_name = self.patch_files[idx]
        mask_name = img_name.replace('_img.npy', '_mask.npy')
        
        img = np.load(os.path.join(self.patch_dir, img_name))
        mask = np.load(os.path.join(self.patch_dir, mask_name))
        
        # (H, W) -> (C, H, W)
        return torch.from_numpy(img).unsqueeze(0), torch.from_numpy(mask).unsqueeze(0)

# 3. 학습 루프
def train_unet1():
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    # 경로 설정
    train_patch_dir = './TRAIN_SEM_IMAGE/AUGMENTED_PATCHES'
    save_base_dir = './train_results'
    os.makedirs(save_base_dir, exist_ok=True)
    
    # 폴더 생성 (시간 기록)
    run_id = f"train_unet1_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
    run_dir = os.path.join(save_base_dir, run_id)
    os.makedirs(run_dir, exist_ok=True)

    # 데이터 로더 (12만개 패치이므로 num_workers 활용 권장)
    train_dataset = PatchDataset(train_patch_dir)
    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=4)

    # 모델, 손실함수, 옵티마이저
    model = ResUNet(filters=64).to(device)
    criterion = nn.BCEWithLogitsLoss()
    optimizer = optim.Adam(model.parameters(), lr=1e-4)

    history = []
    num_epochs = 50

    for epoch in range(num_epochs):
        model.train()
        epoch_loss = 0
        pbar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{num_epochs}")
        
        for imgs, masks in pbar:
            imgs, masks = imgs.to(device), masks.to(device)
            
            optimizer.zero_grad()
            outputs = model(imgs)
            loss = criterion(outputs, masks)
            loss.backward()
            optimizer.step()
            
            epoch_loss += loss.item()
            pbar.set_postfix(loss=loss.item())

        avg_loss = epoch_loss / len(train_loader)
        history.append({'epoch': epoch+1, 'train_loss': avg_loss})
        
        # 모델 저장 (50 에포크 마지막 혹은 중간 체크포인트)
        if (epoch + 1) % 10 == 0 or (epoch + 1) == num_epochs:
            torch.save(model.state_dict(), os.path.join(run_dir, f"model_epoch_{epoch+1}.pth"))

    # 로그 저장
    pd.DataFrame(history).to_csv(os.path.join(run_dir, "loss_log.csv"), index=False)
    print(f"학습 완료! 모델 및 로그 저장 위치: {run_dir}")

if __name__ == "__main__":
    train_unet1()






Phase 1



import cv2
import numpy as np
import os
from tqdm import tqdm

def generate_valid_patches(base_dir):
    """
    VALID SET용 패치 생성: 증강 없이 원본 이미지와 라벨 1:1 매칭
    """
    H_VALID, W_VALID = 896, 1280
    PATCH_SIZE = 384
    STRIDE = 192 # 검증용이므로 조금 더 넓게 잡아도 됩니다 (속도 향상)
    
    save_dir = os.path.join(base_dir, "VALID_PATCHES")
    os.makedirs(save_dir, exist_ok=True)
    
    sem_files = sorted([f for f in os.listdir(base_dir) if f.lower().endswith('.jpg')])
    
    patch_idx = 0
    for s_file in sem_files:
        basename = os.path.splitext(s_file)[0]
        l_file = basename + ".png"
        
        # 이미지 로드 및 전처리 (CLAHE 적용 권장)
        sem_img = cv2.imread(os.path.join(base_dir, s_file), cv2.IMREAD_GRAYSCALE)[:H_VALID, :]
        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))
        sem_norm = clahe.apply(sem_img) / 255.0
        
        label_bgr = cv2.imread(os.path.join(base_dir, l_file))[:H_VALID, :, :]
        _, grain_mask = process_expert_label(label_bgr) # 기존에 만든 함수 사용
        target_mask = (grain_mask > 0).astype(np.float32)
        
        # 패치 생성
        for y in range(0, H_VALID - PATCH_SIZE + 1, STRIDE):
            for x in range(0, W_VALID - PATCH_SIZE + 1, STRIDE):
                img_p = sem_norm[y:y+PATCH_SIZE, x:x+PATCH_SIZE]
                mask_p = target_mask[y:y+PATCH_SIZE, x:x+PATCH_SIZE]
                
                np.save(os.path.join(save_dir, f"val_{patch_idx:04d}_img.npy"), img_p.astype(np.float32))
                np.save(os.path.join(save_dir, f"val_{patch_idx:04d}_mask.npy"), mask_p.astype(np.float32))
                patch_idx += 1
    print(f"Validation patches created: {patch_idx}")

def process_expert_label(label_img):
    """
    흰 바탕(255,255,255)에 붉은 선(255,0,0)인 라벨 처리.
    OpenCV BGR 기준: (0:Blue, 1:Green, 2:Red)
    """
    # 붉은 선(Boundary) 추출: Red는 높고 나머지는 낮은 영역
    red_line = (label_img[:,:,2] > 200) & (label_img[:,:,1] < 100) & (label_img[:,:,0] < 100)
    boundary = red_line.astype(np.uint8) * 255
    
    # Grain Area: 선이 0, 내부가 255인 마스크
    grain_mask = cv2.bitwise_not(boundary)
    return boundary, grain_mask

def generate_augmented_dataset(base_dir):
    # 설정값
    H_VALID, W_VALID = 896, 1280
    PATCH_SIZE = 384
    STRIDE = 48
    
    # SEM(jpg) 파일 목록 추출 후 파일명(basename)으로 정렬
    sem_files = sorted([f for f in os.listdir(base_dir) if f.lower().endswith('.jpg')])
    
    flattened_list = []
    bg_pattern_list = []
    target_mask_list = []

    print(f"Step 1: Analyzing {len(sem_files)} base images from {base_dir}...")
    
    for s_file in sem_files:
        basename = os.path.splitext(s_file)[0]
        l_file = basename + ".png"  # 확장자만 png로 교체
        
        l_path = os.path.join(base_dir, l_file)
        if not os.path.exists(l_path):
            print(f"Warning: Label file {l_file} not found. Skipping...")
            continue
            
        # 이미지 로드 및 메타 영역 제거 (896x1280)
        sem = cv2.imread(os.path.join(base_dir, s_file), cv2.IMREAD_GRAYSCALE)[:H_VALID, :] / 255.0
        label_bgr = cv2.imread(l_path)[:H_VALID, :, :]
        
        # 라벨에서 Grain 추출
        _, grain_mask = process_expert_label(label_bgr)
        
        # 개별 Grain 분리 및 Intensity Flattening
        num_labels, labels = cv2.connectedComponents(grain_mask)
        flat_img = np.zeros_like(sem)
        
        for i in range(1, num_labels):
            m = labels == i
            if np.any(m):
                flat_img[m] = np.mean(sem[m])
        
        flattened_list.append(flat_img)
        bg_pattern_list.append(sem - flat_img)
        target_mask_list.append((labels > 0).astype(np.float32))

    print("Step 2: Generating 14x14 Synthesis and Patches...")
    # 증강 데이터가 저장될 하위 폴더 생성 (관리 용이성 위해 base_dir 내부에 생성)
    save_dir = os.path.join(base_dir, "AUGMENTED_PATCHES")
    os.makedirs(save_dir, exist_ok=True)
    
    patch_idx = 0
    # 14x14 조합 루프
    for i in tqdm(range(len(flattened_list)), desc="Synthesizing"):
        for j in range(len(bg_pattern_list)):
            synth_img = np.clip(flattened_list[i] + bg_pattern_list[j], 0, 1)
            target_mask = target_mask_list[i]
            
            # 패치 슬라이딩
            for y in range(0, H_VALID - PATCH_SIZE + 1, STRIDE):
                for x in range(0, W_VALID - PATCH_SIZE + 1, STRIDE):
                    img_p = synth_img[y:y+PATCH_SIZE, x:x+PATCH_SIZE]
                    mask_p = target_mask[y:y+PATCH_SIZE, x:x+PATCH_SIZE]
                    
                    # 90, 180, 270도 회전
                    for k in range(4):
                        aug_img = np.rot90(img_p, k)
                        aug_mask = np.rot90(mask_p, k)
                        
                        # .npy 파일로 저장
                        np.save(os.path.join(save_dir, f"patch_{patch_idx:06d}_img.npy"), aug_img.astype(np.float32))
                        np.save(os.path.join(save_dir, f"patch_{patch_idx:06d}_mask.npy"), aug_mask.astype(np.float32))
                        patch_idx += 1

    print(f"Success! {patch_idx} patches saved in {save_dir}")

# 실행: 경로만 넣어주면 됩니다.
# generate_augmented_dataset('TRAIN_SEM_IMAGE')
