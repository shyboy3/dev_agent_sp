Phase 2

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import numpy as np
import os
from datetime import datetime
import pandas as pd
from tqdm import tqdm

# 1. 4단계 깊이의 ResUNet_Deep 모델 정의
class ResidualBlock(nn.Module):
    def __init__(self, in_channels, out_channels, stride=1):
        super(ResidualBlock, self).__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, stride=stride),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True),
            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, stride=1),
            nn.BatchNorm2d(out_channels)
        )
        self.shortcut = nn.Sequential()
        if stride != 1 or in_channels != out_channels:
            self.shortcut = nn.Sequential(
                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride),
                nn.BatchNorm2d(out_channels)
            )

    def forward(self, x):
        return nn.functional.relu(self.conv(x) + self.shortcut(x))

class ResUNet_Deep(nn.Module):
    def __init__(self, in_channels=1, out_channels=1, filters=64):
        super(ResUNet_Deep, self).__init__()
        # Encoder (Downsampling)
        self.enc1 = ResidualBlock(in_channels, filters) # 64
        self.enc2 = ResidualBlock(filters, filters*2)   # 128
        self.enc3 = ResidualBlock(filters*2, filters*4) # 256
        self.enc4 = ResidualBlock(filters*4, filters*8) # 512
        self.bridge = ResidualBlock(filters*8, filters*16) # 1024
        
        self.pool = nn.MaxPool2d(2)
        
        # Decoder (Upsampling)
        self.up4 = nn.ConvTranspose2d(filters*16, filters*8, kernel_size=2, stride=2)
        self.dec4 = ResidualBlock(filters*16, filters*8)
        self.up3 = nn.ConvTranspose2d(filters*8, filters*4, kernel_size=2, stride=2)
        self.dec3 = ResidualBlock(filters*8, filters*4)
        self.up2 = nn.ConvTranspose2d(filters*4, filters*2, kernel_size=2, stride=2)
        self.dec2 = ResidualBlock(filters*4, filters*2)
        self.up1 = nn.ConvTranspose2d(filters*2, filters, kernel_size=2, stride=2)
        self.dec1 = ResidualBlock(filters*2, filters)
        
        self.final = nn.Conv2d(filters, out_channels, kernel_size=1)

    def forward(self, x):
        e1 = self.enc1(x)
        e2 = self.enc2(self.pool(e1))
        e3 = self.enc3(self.pool(e2))
        e4 = self.enc4(self.pool(e3))
        b = self.bridge(self.pool(e4))
        
        d4 = self.dec4(torch.cat([self.up4(b), e4], dim=1))
        d3 = self.dec3(torch.cat([self.up3(d4), e3], dim=1))
        d2 = self.dec2(torch.cat([self.up2(d3), e2], dim=1))
        d1 = self.dec1(torch.cat([self.up1(d2), e1], dim=1))
        return self.final(d1)

# 2. Patch Dataset 클래스
class PatchDataset(Dataset):
    def __init__(self, patch_dir):
        self.patch_dir = patch_dir
        self.patch_files = sorted([f for f in os.listdir(patch_dir) if f.endswith('_img.npy')])

    def __len__(self): return len(self.patch_files)

    def __getitem__(self, idx):
        img = np.load(os.path.join(self.patch_dir, self.patch_files[idx]))
        mask = np.load(os.path.join(self.patch_dir, self.patch_files[idx].replace('_img.npy', '_mask.npy')))
        return torch.from_numpy(img).unsqueeze(0), torch.from_numpy(mask).unsqueeze(0)

# 3. 메인 학습 루프
def train_unet1():
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    # 설정 반영: Epoch 50, Batch Size 4
    num_epochs = 50
    batch_size = 4
    
    # 경로 설정
    train_dir = './TRAIN_SEM_IMAGE/AUGMENTED_PATCHES'
    val_dir = './VALID_SEM_IMAGE/VALID_PATCHES'
    run_dir = f"./train_results/train_unet1_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
    os.makedirs(run_dir, exist_ok=True)

    train_loader = DataLoader(PatchDataset(train_dir), batch_size=batch_size, shuffle=True, num_workers=4)
    val_loader = DataLoader(PatchDataset(val_dir), batch_size=batch_size, shuffle=False)

    model = ResUNet_Deep(filters=64).to(device)
    criterion = nn.BCEWithLogitsLoss()
    optimizer = optim.Adam(model.parameters(), lr=1e-4)
    
    history = []
    best_val_loss = float('inf')

    for epoch in range(num_epochs):
        model.train()
        train_loss = 0
        for imgs, masks in tqdm(train_loader, desc=f"Epoch {epoch+1}/{num_epochs}"):
            imgs, masks = imgs.to(device), masks.to(device)
            optimizer.zero_grad()
            loss = criterion(model(imgs), masks)
            loss.backward()
            optimizer.step()
            train_loss += loss.item()

        # Validation
        model.eval()
        val_loss = 0
        with torch.no_grad():
            for v_imgs, v_masks in val_loader:
                v_imgs, v_masks = v_imgs.to(device), v_masks.to(device)
                val_loss += criterion(model(v_imgs), v_masks).item()
        
        avg_train = train_loss / len(train_loader)
        avg_val = val_loss / len(val_loader)
        history.append({'epoch': epoch+1, 'train_loss': avg_train, 'val_loss': avg_val})
        
        print(f"Epoch {epoch+1}: Train {avg_train:.4f}, Val {avg_val:.4f}")

        # Best Model 저장
        if avg_val < best_val_loss:
            best_val_loss = avg_val
            torch.save(model.state_dict(), os.path.join(run_dir, "best_model.pth"))
            print(f"--- Best Model Saved (Val Loss: {best_val_loss:.4f}) ---")

    pd.DataFrame(history).to_csv(os.path.join(run_dir, "loss_log.csv"), index=False)

if __name__ == "__main__":
    train_unet1()






Phase 1



import cv2
import numpy as np
import os
from tqdm import tqdm

def generate_valid_patches(base_dir):
    """
    VALID SET용 패치 생성: 증강 없이 원본 이미지와 라벨 1:1 매칭
    """
    H_VALID, W_VALID = 896, 1280
    PATCH_SIZE = 384
    STRIDE = 192 # 검증용이므로 조금 더 넓게 잡아도 됩니다 (속도 향상)
    
    save_dir = os.path.join(base_dir, "VALID_PATCHES")
    os.makedirs(save_dir, exist_ok=True)
    
    sem_files = sorted([f for f in os.listdir(base_dir) if f.lower().endswith('.jpg')])
    
    patch_idx = 0
    for s_file in sem_files:
        basename = os.path.splitext(s_file)[0]
        l_file = basename + ".png"
        
        # 이미지 로드 및 전처리 (CLAHE 적용 권장)
        sem_img = cv2.imread(os.path.join(base_dir, s_file), cv2.IMREAD_GRAYSCALE)[:H_VALID, :]
        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))
        sem_norm = clahe.apply(sem_img) / 255.0
        
        label_bgr = cv2.imread(os.path.join(base_dir, l_file))[:H_VALID, :, :]
        _, grain_mask = process_expert_label(label_bgr) # 기존에 만든 함수 사용
        target_mask = (grain_mask > 0).astype(np.float32)
        
        # 패치 생성
        for y in range(0, H_VALID - PATCH_SIZE + 1, STRIDE):
            for x in range(0, W_VALID - PATCH_SIZE + 1, STRIDE):
                img_p = sem_norm[y:y+PATCH_SIZE, x:x+PATCH_SIZE]
                mask_p = target_mask[y:y+PATCH_SIZE, x:x+PATCH_SIZE]
                
                np.save(os.path.join(save_dir, f"val_{patch_idx:04d}_img.npy"), img_p.astype(np.float32))
                np.save(os.path.join(save_dir, f"val_{patch_idx:04d}_mask.npy"), mask_p.astype(np.float32))
                patch_idx += 1
    print(f"Validation patches created: {patch_idx}")

def process_expert_label(label_img):
    """
    흰 바탕(255,255,255)에 붉은 선(255,0,0)인 라벨 처리.
    OpenCV BGR 기준: (0:Blue, 1:Green, 2:Red)
    """
    # 붉은 선(Boundary) 추출: Red는 높고 나머지는 낮은 영역
    red_line = (label_img[:,:,2] > 200) & (label_img[:,:,1] < 100) & (label_img[:,:,0] < 100)
    boundary = red_line.astype(np.uint8) * 255
    
    # Grain Area: 선이 0, 내부가 255인 마스크
    grain_mask = cv2.bitwise_not(boundary)
    return boundary, grain_mask

def generate_augmented_dataset(base_dir):
    # 설정값
    H_VALID, W_VALID = 896, 1280
    PATCH_SIZE = 384
    STRIDE = 48
    
    # SEM(jpg) 파일 목록 추출 후 파일명(basename)으로 정렬
    sem_files = sorted([f for f in os.listdir(base_dir) if f.lower().endswith('.jpg')])
    
    flattened_list = []
    bg_pattern_list = []
    target_mask_list = []

    print(f"Step 1: Analyzing {len(sem_files)} base images from {base_dir}...")
    
    for s_file in sem_files:
        basename = os.path.splitext(s_file)[0]
        l_file = basename + ".png"  # 확장자만 png로 교체
        
        l_path = os.path.join(base_dir, l_file)
        if not os.path.exists(l_path):
            print(f"Warning: Label file {l_file} not found. Skipping...")
            continue
            
        # 이미지 로드 및 메타 영역 제거 (896x1280)
        sem = cv2.imread(os.path.join(base_dir, s_file), cv2.IMREAD_GRAYSCALE)[:H_VALID, :] / 255.0
        label_bgr = cv2.imread(l_path)[:H_VALID, :, :]
        
        # 라벨에서 Grain 추출
        _, grain_mask = process_expert_label(label_bgr)
        
        # 개별 Grain 분리 및 Intensity Flattening
        num_labels, labels = cv2.connectedComponents(grain_mask)
        flat_img = np.zeros_like(sem)
        
        for i in range(1, num_labels):
            m = labels == i
            if np.any(m):
                flat_img[m] = np.mean(sem[m])
        
        flattened_list.append(flat_img)
        bg_pattern_list.append(sem - flat_img)
        target_mask_list.append((labels > 0).astype(np.float32))

    print("Step 2: Generating 14x14 Synthesis and Patches...")
    # 증강 데이터가 저장될 하위 폴더 생성 (관리 용이성 위해 base_dir 내부에 생성)
    save_dir = os.path.join(base_dir, "AUGMENTED_PATCHES")
    os.makedirs(save_dir, exist_ok=True)
    
    patch_idx = 0
    # 14x14 조합 루프
    for i in tqdm(range(len(flattened_list)), desc="Synthesizing"):
        for j in range(len(bg_pattern_list)):
            synth_img = np.clip(flattened_list[i] + bg_pattern_list[j], 0, 1)
            target_mask = target_mask_list[i]
            
            # 패치 슬라이딩
            for y in range(0, H_VALID - PATCH_SIZE + 1, STRIDE):
                for x in range(0, W_VALID - PATCH_SIZE + 1, STRIDE):
                    img_p = synth_img[y:y+PATCH_SIZE, x:x+PATCH_SIZE]
                    mask_p = target_mask[y:y+PATCH_SIZE, x:x+PATCH_SIZE]
                    
                    # 90, 180, 270도 회전
                    for k in range(4):
                        aug_img = np.rot90(img_p, k)
                        aug_mask = np.rot90(mask_p, k)
                        
                        # .npy 파일로 저장
                        np.save(os.path.join(save_dir, f"patch_{patch_idx:06d}_img.npy"), aug_img.astype(np.float32))
                        np.save(os.path.join(save_dir, f"patch_{patch_idx:06d}_mask.npy"), aug_mask.astype(np.float32))
                        patch_idx += 1

    print(f"Success! {patch_idx} patches saved in {save_dir}")

# 실행: 경로만 넣어주면 됩니다.
# generate_augmented_dataset('TRAIN_SEM_IMAGE')
