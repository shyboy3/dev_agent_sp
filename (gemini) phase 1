phase 2 Test

import torch
import cv2
import numpy as np

def test_inference(img_path, model_path):
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    # 1. 모델 로드 (ResUNet_Deep 구조 동일해야 함)
    model = ResUNet_Deep(filters=64).to(device)
    model.load_state_dict(torch.load(model_path))
    model.eval()
    
    # 2. 이미지 전처리 (Unet1 검증 때와 동일)
    raw_img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)[:896, :]
    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))
    img_norm = clahe.apply(raw_img) / 255.0
    input_tensor = torch.from_numpy(img_norm).float().unsqueeze(0).unsqueeze(0).to(device)
    
    # 3. 추론
    with torch.no_grad():
        output = torch.sigmoid(model(input_tensor))
        pred_mask = (output.cpu().numpy()[0, 0] * 255).astype(np.uint8)
        
    # 4. 결과 저장 및 확인
    cv2.imwrite('unet1_test_result.png', pred_mask)
    print("Inference complete. Check 'unet1_test_result.png'")

# 실행
# test_inference('./VALID_SEM_IMAGE/test_01.jpg', './train_results/train_unet1_.../best_model.pth')


Phase 4

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import numpy as np
import os
from datetime import datetime
import pandas as pd
from tqdm import tqdm

# 1. Standard UNet 모델 정의 (Residual Block 없음, 입력 2채널)
class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True),
            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True)
        )
    def forward(self, x): return self.conv(x)

class StandardUNet(nn.Module):
    def __init__(self, in_channels=2, out_channels=1, filters=64):
        super().__init__()
        # Encoder
        self.enc1 = ConvBlock(in_channels, filters)
        self.enc2 = ConvBlock(filters, filters*2)
        self.enc3 = ConvBlock(filters*2, filters*4)
        self.enc4 = ConvBlock(filters*4, filters*8)
        self.pool = nn.MaxPool2d(2)
        
        self.bridge = ConvBlock(filters*8, filters*16)
        
        # Decoder
        self.up4 = nn.ConvTranspose2d(filters*16, filters*8, kernel_size=2, stride=2)
        self.dec4 = ConvBlock(filters*16, filters*8)
        self.up3 = nn.ConvTranspose2d(filters*8, filters*4, kernel_size=2, stride=2)
        self.dec3 = ConvBlock(filters*8, filters*4)
        self.up2 = nn.ConvTranspose2d(filters*4, filters*2, kernel_size=2, stride=2)
        self.dec2 = ConvBlock(filters*4, filters*2)
        self.up1 = nn.ConvTranspose2d(filters*2, filters, kernel_size=2, stride=2)
        self.dec1 = ConvBlock(filters*2, filters)
        
        self.final = nn.Conv2d(filters, out_channels, kernel_size=1)

    def forward(self, x):
        s1 = self.enc1(x)
        s2 = self.enc2(self.pool(s1))
        s3 = self.enc3(self.pool(s2))
        s4 = self.enc4(self.pool(s3))
        b = self.bridge(self.pool(s4))
        
        d4 = self.dec4(torch.cat([self.up4(b), s4], dim=1))
        d3 = self.dec3(torch.cat([self.up3(d4), s3], dim=1))
        d2 = self.dec2(torch.cat([self.up2(d3), s2], dim=1))
        d1 = self.dec1(torch.cat([self.up1(d2), s1], dim=1))
        return self.final(d1)

# 2. Unet2 전용 Dataset 클래스 (2채널 npy 로드)
class Unet2Dataset(Dataset):
    def __init__(self, patch_dir):
        self.patch_dir = patch_dir
        # Phase 3에서 생성된 이미지 패치 목록
        self.patch_files = sorted([f for f in os.listdir(patch_dir) if f.endswith('_img.npy')])

    def __len__(self):
        return len(self.patch_files)

    def __getitem__(self, idx):
        img_path = os.path.join(self.patch_dir, self.patch_files[idx])
        mask_path = img_path.replace('_img.npy', '_mask.npy')
        
        # img: (2, 384, 384), mask: (384, 384)
        img = np.load(img_path)
        mask = np.load(mask_path)
        
        # Unet2 타겟은 (1, 384, 384) 형태여야 함
        return torch.from_numpy(img).float(), torch.from_numpy(mask).float().unsqueeze(0)

# 3. Unet2 학습 실행 함수
def train_unet2(patch_dir):
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    # 결과 저장 경로 설정
    run_id = f"train_unet2_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
    run_dir = os.path.join('./train_results', run_id)
    os.makedirs(run_dir, exist_ok=True)

    # 데이터 로더 (데이터가 적으므로 num_workers는 0 또는 2 권장)
    dataset = Unet2Dataset(patch_dir)
    loader = DataLoader(dataset, batch_size=16, shuffle=True)

    # 모델 초기화 (입력 2채널)
    model = StandardUNet(in_channels=2, out_channels=1, filters=64).to(device)
    criterion = nn.BCEWithLogitsLoss()
    optimizer = optim.Adam(model.parameters(), lr=1e-4)

    history = []
    num_epochs = 50

    print(f"Starting Unet2 Training on {len(dataset)} patches...")

    for epoch in range(num_epochs):
        model.train()
        epoch_loss = 0
        pbar = tqdm(loader, desc=f"Unet2 Epoch {epoch+1}/{num_epochs}")
        
        for imgs, masks in pbar:
            imgs, masks = imgs.to(device), masks.to(device)
            
            optimizer.zero_grad()
            outputs = model(imgs)
            loss = criterion(outputs, masks)
            loss.backward()
            optimizer.step()
            
            epoch_loss += loss.item()
            pbar.set_postfix(loss=loss.item())

        avg_loss = epoch_loss / len(loader)
        history.append({'epoch': epoch+1, 'train_loss': avg_loss})
        
        # 일정 주기마다 모델 저장
        if (epoch + 1) % 10 == 0 or (epoch + 1) == num_epochs:
            torch.save(model.state_dict(), os.path.join(run_dir, f"unet2_epoch_{epoch+1}.pth"))

    # 로그 저장
    pd.DataFrame(history).to_csv(os.path.join(run_dir, "unet2_loss_log.csv"), index=False)
    print(f"Unet2 학습 완료! 모델 위치: {run_dir}")

if __name__ == "__main__":
    # Phase 3에서 패치가 저장된 경로 입력
    patch_path = './VALID_SEM_IMAGE/UNET2_PATCHES'
    train_unet2(patch_path)








Phase 3

import torch
import torch.nn as nn
import numpy as np
import os
import cv2
from tqdm import tqdm

# [앞서 정의한 ResUNet_Deep 클래스가 정의되어 있어야 합니다]

def process_expert_label(label_img):
    """ 전문가 라벨(흰바탕, 붉은선)에서 Grain Area(1) 추출 """
    red_line = (label_img[:,:,2] > 200) & (label_img[:,:,1] < 100) & (label_img[:,:,0] < 100)
    boundary = red_line.astype(np.uint8) * 255
    grain_mask = cv2.bitwise_not(boundary)
    return (grain_mask > 0).astype(np.float32)

def prepare_unet2_training_data(unet1_path, data_dir):
    """
    unet1_path: 학습 완료된 unet1의 best_model.pth 경로
    data_dir: VALID_SEM_IMAGE (6세트 데이터가 들어있는 폴더)
    """
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    H, W = 896, 1280
    PATCH_SIZE = 384
    STRIDE = 48 # 데이터가 적으므로 촘촘하게 잘라 패치 수 확보
    
    # 1. Unet1 모델 로드
    model1 = ResUNet_Deep(filters=64).to(device)
    model1.load_state_dict(torch.load(unet1_path))
    model1.eval()
    
    # 2. 결과 저장 폴더 (Unet2 학습용 npy 저장)
    save_dir = os.path.join(data_dir, "UNET2_PATCHES")
    os.makedirs(save_dir, exist_ok=True)
    
    sem_files = sorted([f for f in os.listdir(data_dir) if f.lower().endswith('.jpg')])
    
    patch_idx = 0
    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))

    print(f"Generating Unet2 training data from {len(sem_files)} images...")
    
    with torch.no_grad():
        for s_file in tqdm(sem_files):
            basename = os.path.splitext(s_file)[0]
            l_file = basename + ".png"
            
            # --- 전처리 ---
            # 원본 SEM 이미지 로드 및 CLAHE 적용 (Unet1 검증 때와 동일)
            img_path = os.path.join(data_dir, s_file)
            raw_sem = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)[:H, :]
            sem_norm = clahe.apply(raw_sem) / 255.0
            
            # 전문가 라벨 로드 (Grain Area = 1)
            label_path = os.path.join(data_dir, l_file)
            label_bgr = cv2.imread(label_path)[:H, :, :]
            target_mask = process_expert_label(label_bgr)
            
            # --- Unet1 추론 ---
            sem_tensor = torch.from_numpy(sem_norm).float().unsqueeze(0).unsqueeze(0).to(device)
            # Probability Map 생성 (0~1)
            pred1 = torch.sigmoid(model1(sem_tensor)).cpu().numpy()[0, 0]
            
            # --- 2채널 합성 및 패치화 ---
            # Channel 0: SEM_NORM, Channel 1: UNET1_PRED
            for y in range(0, H - PATCH_SIZE + 1, STRIDE):
                for x in range(0, W - PATCH_SIZE + 1, STRIDE):
                    # 2채널 스택
                    combined = np.stack([
                        sem_norm[y:y+PATCH_SIZE, x:x+PATCH_SIZE],
                        pred1[y:y+PATCH_SIZE, x:x+PATCH_SIZE]
                    ], axis=0)
                    
                    mask_p = target_mask[y:y+PATCH_SIZE, x:x+PATCH_SIZE]
                    
                    # 저장
                    np.save(os.path.join(save_dir, f"u2_patch_{patch_idx:05d}_img.npy"), combined.astype(np.float32))
                    np.save(os.path.join(save_dir, f"u2_patch_{patch_idx:05d}_mask.npy"), mask_p.astype(np.float32))
                    patch_idx += 1

    print(f"Success! Total {patch_idx} patches created for Unet2 training.")

# 사용 예시:
# prepare_unet2_training_data('./train_results/train_unet1_.../best_model.pth', './VALID_SEM_IMAGE')




Phase 2

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import numpy as np
import os
from datetime import datetime
import pandas as pd
from tqdm import tqdm

# 1. 4단계 깊이의 ResUNet_Deep 모델 정의
class ResidualBlock(nn.Module):
    def __init__(self, in_channels, out_channels, stride=1):
        super(ResidualBlock, self).__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, stride=stride),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True),
            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, stride=1),
            nn.BatchNorm2d(out_channels)
        )
        self.shortcut = nn.Sequential()
        if stride != 1 or in_channels != out_channels:
            self.shortcut = nn.Sequential(
                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride),
                nn.BatchNorm2d(out_channels)
            )

    def forward(self, x):
        return nn.functional.relu(self.conv(x) + self.shortcut(x))

class ResUNet_Deep(nn.Module):
    def __init__(self, in_channels=1, out_channels=1, filters=64):
        super(ResUNet_Deep, self).__init__()
        # Encoder (Downsampling)
        self.enc1 = ResidualBlock(in_channels, filters) # 64
        self.enc2 = ResidualBlock(filters, filters*2)   # 128
        self.enc3 = ResidualBlock(filters*2, filters*4) # 256
        self.enc4 = ResidualBlock(filters*4, filters*8) # 512
        self.bridge = ResidualBlock(filters*8, filters*16) # 1024
        
        self.pool = nn.MaxPool2d(2)
        
        # Decoder (Upsampling)
        self.up4 = nn.ConvTranspose2d(filters*16, filters*8, kernel_size=2, stride=2)
        self.dec4 = ResidualBlock(filters*16, filters*8)
        self.up3 = nn.ConvTranspose2d(filters*8, filters*4, kernel_size=2, stride=2)
        self.dec3 = ResidualBlock(filters*8, filters*4)
        self.up2 = nn.ConvTranspose2d(filters*4, filters*2, kernel_size=2, stride=2)
        self.dec2 = ResidualBlock(filters*4, filters*2)
        self.up1 = nn.ConvTranspose2d(filters*2, filters, kernel_size=2, stride=2)
        self.dec1 = ResidualBlock(filters*2, filters)
        
        self.final = nn.Conv2d(filters, out_channels, kernel_size=1)

    def forward(self, x):
        e1 = self.enc1(x)
        e2 = self.enc2(self.pool(e1))
        e3 = self.enc3(self.pool(e2))
        e4 = self.enc4(self.pool(e3))
        b = self.bridge(self.pool(e4))
        
        d4 = self.dec4(torch.cat([self.up4(b), e4], dim=1))
        d3 = self.dec3(torch.cat([self.up3(d4), e3], dim=1))
        d2 = self.dec2(torch.cat([self.up2(d3), e2], dim=1))
        d1 = self.dec1(torch.cat([self.up1(d2), e1], dim=1))
        return self.final(d1)

# 2. Patch Dataset 클래스
class PatchDataset(Dataset):
    def __init__(self, patch_dir):
        self.patch_dir = patch_dir
        self.patch_files = sorted([f for f in os.listdir(patch_dir) if f.endswith('_img.npy')])

    def __len__(self): return len(self.patch_files)

    def __getitem__(self, idx):
        img = np.load(os.path.join(self.patch_dir, self.patch_files[idx]))
        mask = np.load(os.path.join(self.patch_dir, self.patch_files[idx].replace('_img.npy', '_mask.npy')))
        return torch.from_numpy(img).unsqueeze(0), torch.from_numpy(mask).unsqueeze(0)

# 3. 메인 학습 루프
def train_unet1():
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    # 설정 반영: Epoch 50, Batch Size 4
    num_epochs = 50
    batch_size = 4
    
    # 경로 설정
    train_dir = './TRAIN_SEM_IMAGE/AUGMENTED_PATCHES'
    val_dir = './VALID_SEM_IMAGE/VALID_PATCHES'
    run_dir = f"./train_results/train_unet1_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
    os.makedirs(run_dir, exist_ok=True)

    train_loader = DataLoader(PatchDataset(train_dir), batch_size=batch_size, shuffle=True, num_workers=4)
    val_loader = DataLoader(PatchDataset(val_dir), batch_size=batch_size, shuffle=False)

    model = ResUNet_Deep(filters=64).to(device)
    criterion = nn.BCEWithLogitsLoss()
    optimizer = optim.Adam(model.parameters(), lr=1e-4)
    
    history = []
    best_val_loss = float('inf')

    for epoch in range(num_epochs):
        model.train()
        train_loss = 0
        for imgs, masks in tqdm(train_loader, desc=f"Epoch {epoch+1}/{num_epochs}"):
            imgs, masks = imgs.to(device), masks.to(device)
            optimizer.zero_grad()
            loss = criterion(model(imgs), masks)
            loss.backward()
            optimizer.step()
            train_loss += loss.item()

        # Validation
        model.eval()
        val_loss = 0
        with torch.no_grad():
            for v_imgs, v_masks in val_loader:
                v_imgs, v_masks = v_imgs.to(device), v_masks.to(device)
                val_loss += criterion(model(v_imgs), v_masks).item()
        
        avg_train = train_loss / len(train_loader)
        avg_val = val_loss / len(val_loader)
        history.append({'epoch': epoch+1, 'train_loss': avg_train, 'val_loss': avg_val})
        
        print(f"Epoch {epoch+1}: Train {avg_train:.4f}, Val {avg_val:.4f}")

        # Best Model 저장
        if avg_val < best_val_loss:
            best_val_loss = avg_val
            torch.save(model.state_dict(), os.path.join(run_dir, "best_model.pth"))
            print(f"--- Best Model Saved (Val Loss: {best_val_loss:.4f}) ---")

    pd.DataFrame(history).to_csv(os.path.join(run_dir, "loss_log.csv"), index=False)

if __name__ == "__main__":
    train_unet1()






Phase 1



import cv2
import numpy as np
import os
from tqdm import tqdm

def generate_valid_patches(base_dir):
    """
    VALID SET용 패치 생성: 증강 없이 원본 이미지와 라벨 1:1 매칭
    """
    H_VALID, W_VALID = 896, 1280
    PATCH_SIZE = 384
    STRIDE = 192 # 검증용이므로 조금 더 넓게 잡아도 됩니다 (속도 향상)
    
    save_dir = os.path.join(base_dir, "VALID_PATCHES")
    os.makedirs(save_dir, exist_ok=True)
    
    sem_files = sorted([f for f in os.listdir(base_dir) if f.lower().endswith('.jpg')])
    
    patch_idx = 0
    for s_file in sem_files:
        basename = os.path.splitext(s_file)[0]
        l_file = basename + ".png"
        
        # 이미지 로드 및 전처리 (CLAHE 적용 권장)
        sem_img = cv2.imread(os.path.join(base_dir, s_file), cv2.IMREAD_GRAYSCALE)[:H_VALID, :]
        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))
        sem_norm = clahe.apply(sem_img) / 255.0
        
        label_bgr = cv2.imread(os.path.join(base_dir, l_file))[:H_VALID, :, :]
        _, grain_mask = process_expert_label(label_bgr) # 기존에 만든 함수 사용
        target_mask = (grain_mask > 0).astype(np.float32)
        
        # 패치 생성
        for y in range(0, H_VALID - PATCH_SIZE + 1, STRIDE):
            for x in range(0, W_VALID - PATCH_SIZE + 1, STRIDE):
                img_p = sem_norm[y:y+PATCH_SIZE, x:x+PATCH_SIZE]
                mask_p = target_mask[y:y+PATCH_SIZE, x:x+PATCH_SIZE]
                
                np.save(os.path.join(save_dir, f"val_{patch_idx:04d}_img.npy"), img_p.astype(np.float32))
                np.save(os.path.join(save_dir, f"val_{patch_idx:04d}_mask.npy"), mask_p.astype(np.float32))
                patch_idx += 1
    print(f"Validation patches created: {patch_idx}")

def process_expert_label(label_img):
    """
    흰 바탕(255,255,255)에 붉은 선(255,0,0)인 라벨 처리.
    OpenCV BGR 기준: (0:Blue, 1:Green, 2:Red)
    """
    # 붉은 선(Boundary) 추출: Red는 높고 나머지는 낮은 영역
    red_line = (label_img[:,:,2] > 200) & (label_img[:,:,1] < 100) & (label_img[:,:,0] < 100)
    boundary = red_line.astype(np.uint8) * 255
    
    # Grain Area: 선이 0, 내부가 255인 마스크
    grain_mask = cv2.bitwise_not(boundary)
    return boundary, grain_mask

def generate_augmented_dataset(base_dir):
    # 설정값
    H_VALID, W_VALID = 896, 1280
    PATCH_SIZE = 384
    STRIDE = 48
    
    # SEM(jpg) 파일 목록 추출 후 파일명(basename)으로 정렬
    sem_files = sorted([f for f in os.listdir(base_dir) if f.lower().endswith('.jpg')])
    
    flattened_list = []
    bg_pattern_list = []
    target_mask_list = []

    print(f"Step 1: Analyzing {len(sem_files)} base images from {base_dir}...")
    
    for s_file in sem_files:
        basename = os.path.splitext(s_file)[0]
        l_file = basename + ".png"  # 확장자만 png로 교체
        
        l_path = os.path.join(base_dir, l_file)
        if not os.path.exists(l_path):
            print(f"Warning: Label file {l_file} not found. Skipping...")
            continue
            
        # 이미지 로드 및 메타 영역 제거 (896x1280)
        sem = cv2.imread(os.path.join(base_dir, s_file), cv2.IMREAD_GRAYSCALE)[:H_VALID, :] / 255.0
        label_bgr = cv2.imread(l_path)[:H_VALID, :, :]
        
        # 라벨에서 Grain 추출
        _, grain_mask = process_expert_label(label_bgr)
        
        # 개별 Grain 분리 및 Intensity Flattening
        num_labels, labels = cv2.connectedComponents(grain_mask)
        flat_img = np.zeros_like(sem)
        
        for i in range(1, num_labels):
            m = labels == i
            if np.any(m):
                flat_img[m] = np.mean(sem[m])
        
        flattened_list.append(flat_img)
        bg_pattern_list.append(sem - flat_img)
        target_mask_list.append((labels > 0).astype(np.float32))

    print("Step 2: Generating 14x14 Synthesis and Patches...")
    # 증강 데이터가 저장될 하위 폴더 생성 (관리 용이성 위해 base_dir 내부에 생성)
    save_dir = os.path.join(base_dir, "AUGMENTED_PATCHES")
    os.makedirs(save_dir, exist_ok=True)
    
    patch_idx = 0
    # 14x14 조합 루프
    for i in tqdm(range(len(flattened_list)), desc="Synthesizing"):
        for j in range(len(bg_pattern_list)):
            synth_img = np.clip(flattened_list[i] + bg_pattern_list[j], 0, 1)
            target_mask = target_mask_list[i]
            
            # 패치 슬라이딩
            for y in range(0, H_VALID - PATCH_SIZE + 1, STRIDE):
                for x in range(0, W_VALID - PATCH_SIZE + 1, STRIDE):
                    img_p = synth_img[y:y+PATCH_SIZE, x:x+PATCH_SIZE]
                    mask_p = target_mask[y:y+PATCH_SIZE, x:x+PATCH_SIZE]
                    
                    # 90, 180, 270도 회전
                    for k in range(4):
                        aug_img = np.rot90(img_p, k)
                        aug_mask = np.rot90(mask_p, k)
                        
                        # .npy 파일로 저장
                        np.save(os.path.join(save_dir, f"patch_{patch_idx:06d}_img.npy"), aug_img.astype(np.float32))
                        np.save(os.path.join(save_dir, f"patch_{patch_idx:06d}_mask.npy"), aug_mask.astype(np.float32))
                        patch_idx += 1

    print(f"Success! {patch_idx} patches saved in {save_dir}")

# 실행: 경로만 넣어주면 됩니다.
# generate_augmented_dataset('TRAIN_SEM_IMAGE')
